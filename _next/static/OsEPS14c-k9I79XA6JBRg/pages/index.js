(window.webpackJsonp=window.webpackJsonp||[]).push([["d0a3"],{"+a94":function(e){e.exports={bodyContent:"**Organizer:** UIST 2016 (Web and Social Media Chair)\n\n**Review:** CHI 2016--2019, UIST 2017--2019, SCF 2019\n\n**Student Volunteer:** UIST 2016, CHI 2017",bodyHtml:"<p><strong>Organizer:</strong> UIST 2016 (Web and Social Media Chair)</p>\n<p><strong>Review:</strong> CHI 2016--2019, UIST 2017--2019, SCF 2019</p>\n<p><strong>Student Volunteer:</strong> UIST 2016, CHI 2017</p>\n",title:"<strong>Organizer:",dir:"content/output",base:"activities.json",ext:".json",sourceBase:"activities.md",sourceExt:".md"}},"3RXq":function(e){e.exports={id:"lift-tiles",name:"LiftTiles",description:"Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces",title:"LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces",authors:["Ryo Suzuki","Ryosuke Nakayama","Dan Liu","Yasuaki Kakehi","Mark D. Gross","Daniel Leithinger"],year:2020,booktitle:"In Proceedings of the 14th ACM International Conference on Tangible, Embedded and Embodied Interaction (TEI '20)",publisher:"ACM, New York, NY, USA",conference:{name:"TEI 2020",fullname:"The ACM International Conference on Tangible, Embedded and Embodied Interaction (TEI 2020)",url:"https://tei.acm.org/2020/"},video:"https://www.youtube.com/watch?v=SYCmftQCilU",embed:"https://www.youtube.com/embed/SYCmftQCilU",pdf:"tei-2020-lift-tiles.pdf",poster:"uist-2019-lift-tiles-poster.pdf",pageCount:8,slideCount:0,related:{title:"LiftTiles: Modular and Reconfigurable Room-scale Shape Displays through Retractable Inflatable Actuators",authors:["Ryo Suzuki","Ryosuke Nakayama","Dan Liu","Yasuaki Kakehi","Mark D. Gross","Daniel Leithinger"],year:2019,booktitle:"In Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST '19)",publisher:"ACM, New York, NY, USA",pages:"1-3",doi:"https://doi.org/10.1145/3332167.3357105",url:"http://uist.acm.org/uist2019",pdf:"uist-2019-lift-tiles.pdf",suffix:"adjunct",pageCount:3},bodyContent:"# Abstract\n\nLarge-scale shape-changing interfaces have great potential, but creating such systems requires substantial time, cost, space, and efforts, which hinders the research community to explore inter- actions beyond the scale of human hands. We introduce modular inflatable actuators as building blocks for prototyping room-scale shape-changing interfaces. Each actuator can change its height from 15cm to 150cm, actuated and controlled by air pressure. Each unit is low-cost (8 USD), lightweight (10 kg), compact (15 cm), and robust (e.g., can support 10 kg weight) making it well-suited for prototyping room-scale shape transformations. Moreover, our mod- ular and reconfigurable design allows researchers and designers to quickly construct different geometries and to explore various appli- cations. This paper contributes to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block.",bodyHtml:"<h1>Abstract</h1>\n<p>Large-scale shape-changing interfaces have great potential, but creating such systems requires substantial time, cost, space, and efforts, which hinders the research community to explore inter- actions beyond the scale of human hands. We introduce modular inflatable actuators as building blocks for prototyping room-scale shape-changing interfaces. Each actuator can change its height from 15cm to 150cm, actuated and controlled by air pressure. Each unit is low-cost (8 USD), lightweight (10 kg), compact (15 cm), and robust (e.g., can support 10 kg weight) making it well-suited for prototyping room-scale shape transformations. Moreover, our mod- ular and reconfigurable design allows researchers and designers to quickly construct different geometries and to explore various appli- cations. This paper contributes to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block.</p>\n",dir:"content/output/projects",base:"lift-tiles.json",ext:".json",sourceBase:"lift-tiles.md",sourceExt:".md"}},"42TL":function(e,t,a){"use strict";a.r(t);var i=a("0iUn"),o=a("sLSF"),n=a("MI3g"),s=a("a7VT"),r=a("Tit0"),c=a("q1tI"),l=a.n(c),p=(a("IujW"),a("MyTI")),d=function(e){function t(){return Object(i.default)(this,t),Object(n.default)(this,Object(s.default)(t).apply(this,arguments))}return Object(r.default)(t,e),Object(o.default)(t,[{key:"render",value:function(){return l.a.createElement("div",{id:"updates",className:"ui relaxed divided list"},l.a.createElement("h3",null,"Research Experience"),p.map(function(e){return l.a.createElement("div",{className:"item",style:{padding:"20px 0"}},l.a.createElement("div",{className:"ui mini image"},l.a.createElement("img",{src:"/static/images/".concat(e.logo)})),l.a.createElement("div",{className:"middle aligned content"},l.a.createElement("div",{className:"header"},e.institute.name,l.a.createElement("br",null)),l.a.createElement("div",{className:"description"},l.a.createElement("a",{href:e.lab.url},l.a.createElement("b",null,e.lab.name)))),l.a.createElement("div",{className:"content"},l.a.createElement("div",{className:"ui list"},e.advisors.map(function(e){return l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:e.url},e.name))}))),l.a.createElement("div",{className:"extra",style:{marginLeft:"0px",marginTop:"5px"}},l.a.createElement("div",{className:"ui label"},e.period)))}))}}]),t}(l.a.Component);t.default=d},"6T/A":function(e){e.exports={}},"9Gwv":function(e){e.exports=[{date:"2019-12-12",text:"One [**CHI 2020**](https://chi2020.acm.org/) full-paper is accepted (24%)."},{date:"2019-11-25",text:"Traveling to Boston to visit MIT CSAIL and Media Lab"},{date:"2019-10-19",image:"uist-2019.png",text:"Traveling to New Orleans for [**UIST 2019**](http://uist.acm.org/uist2019/)"},{date:"2019-10-10",image:"uist-2019.png",text:"Finished comprehensive exam and became a **PhD candidate** (committee: D Leithinger, M Gross, T Yeh, H Ishii, T Igarashi, E Do)"},{date:"2019-10-08",image:"uist-2019.png",text:"One [**TEI 2020**](https://tei.acm.org/2020/) full-paper is accepted (28%)."},{date:"2019-08-02",image:"uist-2019.png",text:"One [**UIST 2019**](http://uist.acm.org/uist2019/) poster and and doctoral consortium paper are accepted."},{date:"2019-07-01",image:"uist-2019.png",text:"One [**UIST 2019**](http://uist.acm.org/uist2019/) full-paper is accepted (24%)."},{date:"2019-06-27",image:"uist-2019.png",text:"Won [**Best Paper Award**](https://dis2019.com/overview/#track-4-shape-changing-interfaces) for [**DIS 2019**](https://dis2019.com/) (Top 1%)."},{date:"2019-05-20",image:"uist-2019.png",text:"Start intern at [**Adobe Research**](https://research.adobe.com/) in Seattle."},{date:"2019-03-26",image:"dis-2019.png",text:"One [**DIS 2019**](https://dis2019.com/) full-paper is accepted (25%)."},{date:"2018-08-15",text:"Award [**JST ACT-I**](https://www.jst.go.jp/kisoken/act-i/en/project/111C001/111C001_2018.html) funding (mentor [**Takeo Igarashi**](https://www-ui.is.s.u-tokyo.ac.jp/~takeo/))."},{date:"2018-08-06",image:"uist-2018.png",text:"One [**UIST 2018**](https://uist.acm.org/uist2018/) full-paper is accepted (21%)."},{date:"2018-08-03",image:"pg-2018.png",text:"One [**Pacific Graphics 2018**](http://sweb.cityu.edu.hk/pg2018/) short-paper is accepted (26%)."},{date:"2017-12-14",text:"Start intern at [**University of Tokyo**](http://www.jst.go.jp/erato/kawahara/)."},{date:"2017-12-11",image:"chi-2018.png",text:"Two [**CHI 2018**](https://chi2018.acm.org/) full-papers are accepted (25%)."},{date:"2017-06-27",image:"vlhcc-2017.png",text:"One [**VL/HCC 2017**](https://sites.google.com/site/vlhcc2017/) full-paper are accepted (29%)."},{date:"2017-06-21",image:"assets-2017.png",text:"One [**ASSETS 2017**](https://assets17.sigaccess.org/) full-paper are accepted (26%)."},{date:"2017-02-11",image:"chi-2017.png",text:"One [**CHI 2017**](https://chi2017.acm.org/) LBW paper is accepted (38%)."},{date:"2016-12-16",image:"",text:"One [**L@S 2017**](http://learningatscale.acm.org/las2017/) full paper is accepted (22%)."},{date:"2016-12-12",image:"icse-2017.png",text:"One [**ICSE 2017**](http://icse2017.gatech.edu/) full-paper is accepted (19%)."},{date:"2019-05-23",text:"Start intern at [**UC Berkeley**](http://bid.berkeley.edu/)."},{date:"2016-01-15",image:"chi-2016.png",text:"One [**CHI 2016**](https://chi2016.acm.org/wp/) full-paper is accepted (23%)."},{date:"2015-10-01",image:"uist-2016.jpg",text:"I will serve as a web and social media chair for [**UIST 2016**](http://uist.acm.org/uist2016/)"},{date:"2019-05-18",text:"Start intern at [**Stanford**](https://hci.stanford.edu/)."},{date:"2014-10-15",text:"The first day to start HCI research (previously economics/game theory)"}]},"9gtZ":function(e){e.exports={id:"roomshift",name:"RoomShift",description:"Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots",title:"RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots",authors:["Ryo Suzuki","Hooman Hedayati","Clement Zheng","James Bohn","Daniel Szafir","Ellen Yi-Luen Do","Mark D Gross","Daniel Leithinger"],year:2020,booktitle:"In Proceedings of the ACM CHI Conference on Human Factors in Computing Systems (CHI '20)",publisher:"ACM, New York, NY, USA",conference:{name:"CHI 2020",fullname:"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2020)",url:"https://chi2020.acm.org/"},pageCount:0,slideCount:0,bodyContent:"coming soon.\n\n# Abstract\n\nThis paper presents RoomShift, a room-scale dynamic haptic environment for virtual reality, based on a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots based on Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate inside it, the swarm of robots dynamically reconfig- ures the physical environment to match the virtual content. We describe the technical hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.",bodyHtml:"<p>coming soon.</p>\n<h1>Abstract</h1>\n<p>This paper presents RoomShift, a room-scale dynamic haptic environment for virtual reality, based on a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots based on Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate inside it, the swarm of robots dynamically reconfig- ures the physical environment to match the virtual content. We describe the technical hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.</p>\n",dir:"content/output/projects",base:"roomshift.json",ext:".json",sourceBase:"roomshift.md",sourceExt:".md"}},CTYI:function(e){e.exports={id:"flux-marker",name:"FluxMarker",description:"Enhancing Tactile Graphics with Dynamic Tactile Markers for Blind People",title:"FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers",authors:["Ryo Suzuki","Abigale Stangl","Mark D. Gross","Tom Yeh"],year:2017,booktitle:"In Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '17)",publisher:"ACM, New York, NY, USA",pages:"190-199",doi:"https://doi.org/10.1145/3132525.3132548",conference:{name:"ASSETS 2017",fullname:"The International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS 2017)",url:"https://assets17.sigaccess.org/"},pdf:"assets-2017-fluxmarker.pdf",video:"https://www.youtube.com/watch?v=VbwIZ9V6i_g",embed:"https://www.youtube.com/embed/VbwIZ9V6i_g",slide:"assets-2017-fluxmarker-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3132548",arxiv:"https://arxiv.org/abs/1708.03783",pageCount:10,slideCount:53,bodyContent:"# Abstract\n\nFor people with visual impairments, tactile graphics are an impor- tant means to learn and explore information. However, raised line tactile graphics created with traditional materials such as emboss- ing are static. While available refreshable displays can dynamically change the content, they are still too expensive for many users, and are limited in size. These factors limit wide-spread adoption and the representation of large graphics or data sets. In this paper, we present FluxMaker, an inexpensive scalable system that renders dy- namic information on top of static tactile graphics with movable tactile markers. These dynamic tactile markers can be easily re- configured and used to annotate static raised line tactile graphics, including maps, graphs, and diagrams. We developed a hardware prototype that actuates magnetic tactile markers driven by low-cost and scalable electromagnetic coil arrays, which can be fabricated with standard printed circuit board manufacturing. We evaluate our prototype with six participants with visual impairments and found positive results across four application areas: location finding or navigating on tactile maps, data analysis, and physicalization, fea- ture identification for tactile graphics, and drawing support. The user study confirms advantages in application domains such as ed- ucation and data exploration.",bodyHtml:"<h1>Abstract</h1>\n<p>For people with visual impairments, tactile graphics are an impor- tant means to learn and explore information. However, raised line tactile graphics created with traditional materials such as emboss- ing are static. While available refreshable displays can dynamically change the content, they are still too expensive for many users, and are limited in size. These factors limit wide-spread adoption and the representation of large graphics or data sets. In this paper, we present FluxMaker, an inexpensive scalable system that renders dy- namic information on top of static tactile graphics with movable tactile markers. These dynamic tactile markers can be easily re- configured and used to annotate static raised line tactile graphics, including maps, graphs, and diagrams. We developed a hardware prototype that actuates magnetic tactile markers driven by low-cost and scalable electromagnetic coil arrays, which can be fabricated with standard printed circuit board manufacturing. We evaluate our prototype with six participants with visual impairments and found positive results across four application areas: location finding or navigating on tactile maps, data analysis, and physicalization, fea- ture identification for tactile graphics, and drawing support. The user study confirms advantages in application domains such as ed- ucation and data exploration.</p>\n",dir:"content/output/projects",base:"flux-marker.json",ext:".json",sourceBase:"flux-marker.md",sourceExt:".md"}},"Dx8+":function(e,t,a){"use strict";a.r(t);var i=a("0iUn"),o=a("sLSF"),n=a("MI3g"),s=a("a7VT"),r=a("Tit0"),c=a("q1tI"),l=a.n(c),p=a("IujW"),d=a.n(p),h=a("9Gwv"),g=function(e){function t(){return Object(i.default)(this,t),Object(n.default)(this,Object(s.default)(t).apply(this,arguments))}return Object(r.default)(t,e),Object(o.default)(t,[{key:"render",value:function(){return l.a.createElement("div",{id:"updates",className:"ui relaxed divided list"},l.a.createElement("h3",null,"Recent Updates"),h.map(function(e){return l.a.createElement("div",{className:"item"},l.a.createElement("div",{className:"header"},e.date),l.a.createElement("div",{className:"content"},l.a.createElement(d.a,{source:e.text})))}))}}]),t}(l.a.Component);t.default=g},GbvX:function(e){e.exports={id:"dynablock",name:"Dynablock",description:"Dynamic 3D Printing for Instant and Reconstructable Shape Formation",title:"Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation",authors:["Ryo Suzuki","Junichi Yamaoka","Daniel Leithinger","Tom Yeh","Mark D. Gross","Yoshihiro Kawahara","Yasuaki Kakehi"],year:2018,booktitle:"In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology (UIST '18)",publisher:"ACM, New York, NY, USA",pages:"99-111",doi:"https://doi.org/10.1145/3242587.3242659",conference:{name:"UIST 2018",fullname:"The ACM Symposium on User Interface Software and Technology (UIST 2018)",url:"http://uist.acm.org/uist2018"},pdf:"uist-2018-dynablock.pdf",video:"https://www.youtube.com/watch?v=7nPlr3O9xu8",embed:"https://www.youtube.com/embed/7nPlr3O9xu8","short-video":"https://www.youtube.com/watch?v=92eGI-gYYc4",slide:"uist-2018-dynablock-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3242659",talk:"https://www.youtube.com/watch?v=R3FRUtOIiCQ",poster:"uist-2018-dynablock-poster.pdf",pageCount:12,slideCount:53,bodyContent:'\x3c!--\nLinks:\n[**[PDF](http://ryosuzuki.org/publications/uist-2018-dynablock.pdf)**]\n[**[ACM DL](https://dl.acm.org/citation.cfm?id=3242659)**]\n[**[Video](https://www.youtube.com/watch?v=7nPlr3O9xu8)**]\n[**[Slide](http://ryosuzuki.org/publications/uist-2018-dynablock-slide.pdf)**]\n[**[Talk](https://www.youtube.com/watch?v=R3FRUtOIiCQ)**]\n --\x3e\n\n# Abstract\n\nThis paper introduces Dynamic 3D Printing, a fast and re- constructable shape formation system. Dynamic 3D Printing assembles an arbitrary three-dimensional shape from a large number of small physical elements. It can also disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbi- trary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and imple- mentation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/top.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/top.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-1-1.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-1-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-1-2.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-1-2.jpg" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-2.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-3.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-3.jpg" /></a>\n  </div>\n</div>\n\n# Dynamic 3D Printing\n\nWhat if 3D printers could form a physical object in seconds? What if the object, once it is no longer needed, could quickly and easily be disassembled and reconstructed as a new object? Today’s 3D printers take hours to print objects, and output a single static object. However, we envision a future in which 3D printing could instantly create objects from reusable and reconstructable materials.\n\nThis paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.\n\nWe define Dynamic 3D Printing as a class of systems that have the following properties:\n\n- Immediate: The system can form a physical shape in sec- onds.\n\n- Reconstructable: Rendered shapes can be disassembled and reconstructed by hand or with the system, and the blocks are reusable.\n\n- Arbitrary Shapes: It can create arbitrary three dimensional shapes.\n\n- Graspable: The output shapes and structure are graspable and solid.\n\n# Parallel Assembler\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-3-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-3-1.png" /></a>\n  </div>\n</div>\n\nDynamic 3D printing deploys a large number of small dis- crete material elements, which are assembled to form arbitrary shaped macro-scale objects. Individual elements are passive, which requires an external actuator to perform the assembly. As illustrated in the above Figure, the assembler consists of an N x N grid of motorized pins and linear actuators. The elements, which are the same size as the pins, are stacked on top of the pins (Figure 3 A). When stacked, the elements are connected in vertical direction, while discon- nected with nearby elements in horizontal direction. Similar to existing pin-based shape displays, the assembler can incrementally generate 2.5D shapes by individually moving pins to push elements to the surface.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/mechanism.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/mechanism.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-4-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-4-1.png" /></a>\n  </div>\n  <p class="column">\n    This paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.\n  </p>\n</div>\n\n\n# Implementation\n\nThe assembler consists of a 24 x 16 array of motor-driven pins. Each pin moves up and down, driven by a small DC motor (TTMotors TGPP06-D700) and a 3D printed lead screw (2 mm pitch, 4 starts, 120 mm in length). TGPP06-D700 is 6 mm in diameter and 29 mm in length and can rotate 47 rpm with 1:700 gear ratio. The 2 mm 4 starts lead screw can travel 12 mm per second without load, and each motor consumes approximately 60 mA. The pins are 3D printed with a nut at the bottom to travel along the lead screw. Each pin is 120 mm long and has a 7mm square cross section with a 5 mm diameter hole from top to bottom, and an N45 disk magnet (φ 3mm x 2.4 mm thickness) is attached at the top. Guide grids at the top prevent pins from rotating and ensure that pins travel vertically. The 24 x 16 guide grids have 7.5 mm square holes with 10.16 mm pitch and are cut from a 5 mm acrylic plate. We fabricated the pins, the lead screws, and blocks with an inkjet 3D printer (Keyence Agilista 3200) with water soluble support material. In total, we fabricated 384 (= 24 x 16) pins and lead screws, and 3,072 (= 24 x 16 x 8 layers) blocks. To create the magnetic blocks, we embedded spherical magnets in each block by hand and inserted disk magnets using a bench vice.\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-3.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-3.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-2.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-3.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-3.jpg" /></a>\n  </div>\n</div>\n\n\n# Future Vision\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/claytronics.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/claytronics.mp4" type="video/mp4"></source>\n</video>\n[Video Credit: Carnegie Mellon University, Claytronics Vision]\n\n<br/>\n\nWith these capabilities, a 3D printer would become an inter- active medium, rather than merely a fabrication device. For example, such a 3D printer could be used in a Virtual Real- ity or Augmented Reality application to dynamically form a tangible object or controller to provide haptic feedback and engage users physically. For children, it could dynamically form a physical educational manipulative, such as a molec- ular or architectural model, to learn and explore topics, for example in a science museum. Designers could use it to ren- der a physical product to present to clients and interactively change the product’s design through direct manipulation. In this vision, Dynamic 3D printing is an environment in which the user thinks, designs, explores, and communicates through dynamic and interactive physical representation.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-8-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-8-1.png" /></a>\n  </div>\n</div>\n\nDynamic 3D printing would enable a new design workflow for digital fabrication. One notable advantage of dynamic 3D printing is the capability of connecting and disconnecting building blocks through direct manipulation. The user can also define variables or abstract attributes for parametric design through direct and gestural interaction. By leveraging this capability, the user could interactively design and fabri- cate in a physical space, similar to the man-machine dialogue proposed by Frazer et al. and later tangible CAD interfaces.',bodyHtml:'\x3c!--\nLinks:\n[**[PDF](http://ryosuzuki.org/publications/uist-2018-dynablock.pdf)**]\n[**[ACM DL](https://dl.acm.org/citation.cfm?id=3242659)**]\n[**[Video](https://www.youtube.com/watch?v=7nPlr3O9xu8)**]\n[**[Slide](http://ryosuzuki.org/publications/uist-2018-dynablock-slide.pdf)**]\n[**[Talk](https://www.youtube.com/watch?v=R3FRUtOIiCQ)**]\n --\x3e\n<h1>Abstract</h1>\n<p>This paper introduces Dynamic 3D Printing, a fast and re- constructable shape formation system. Dynamic 3D Printing assembles an arbitrary three-dimensional shape from a large number of small physical elements. It can also disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbi- trary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and imple- mentation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/top.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/top.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-1-1.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-1-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-1-2.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-1-2.jpg" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-2.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-3.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-3.jpg" /></a>\n  </div>\n</div>\n<h1>Dynamic 3D Printing</h1>\n<p>What if 3D printers could form a physical object in seconds? What if the object, once it is no longer needed, could quickly and easily be disassembled and reconstructed as a new object? Today’s 3D printers take hours to print objects, and output a single static object. However, we envision a future in which 3D printing could instantly create objects from reusable and reconstructable materials.</p>\n<p>This paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.</p>\n<p>We define Dynamic 3D Printing as a class of systems that have the following properties:</p>\n<ul>\n<li>\n<p>Immediate: The system can form a physical shape in sec- onds.</p>\n</li>\n<li>\n<p>Reconstructable: Rendered shapes can be disassembled and reconstructed by hand or with the system, and the blocks are reusable.</p>\n</li>\n<li>\n<p>Arbitrary Shapes: It can create arbitrary three dimensional shapes.</p>\n</li>\n<li>\n<p>Graspable: The output shapes and structure are graspable and solid.</p>\n</li>\n</ul>\n<h1>Parallel Assembler</h1>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-3-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-3-1.png" /></a>\n  </div>\n</div>\n<p>Dynamic 3D printing deploys a large number of small dis- crete material elements, which are assembled to form arbitrary shaped macro-scale objects. Individual elements are passive, which requires an external actuator to perform the assembly. As illustrated in the above Figure, the assembler consists of an N x N grid of motorized pins and linear actuators. The elements, which are the same size as the pins, are stacked on top of the pins (Figure 3 A). When stacked, the elements are connected in vertical direction, while discon- nected with nearby elements in horizontal direction. Similar to existing pin-based shape displays, the assembler can incrementally generate 2.5D shapes by individually moving pins to push elements to the surface.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/mechanism.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/mechanism.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-4-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-4-1.png" /></a>\n  </div>\n  <p class="column">\n    This paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.\n  </p>\n</div>\n<h1>Implementation</h1>\n<p>The assembler consists of a 24 x 16 array of motor-driven pins. Each pin moves up and down, driven by a small DC motor (TTMotors TGPP06-D700) and a 3D printed lead screw (2 mm pitch, 4 starts, 120 mm in length). TGPP06-D700 is 6 mm in diameter and 29 mm in length and can rotate 47 rpm with 1:700 gear ratio. The 2 mm 4 starts lead screw can travel 12 mm per second without load, and each motor consumes approximately 60 mA. The pins are 3D printed with a nut at the bottom to travel along the lead screw. Each pin is 120 mm long and has a 7mm square cross section with a 5 mm diameter hole from top to bottom, and an N45 disk magnet (φ 3mm x 2.4 mm thickness) is attached at the top. Guide grids at the top prevent pins from rotating and ensure that pins travel vertically. The 24 x 16 guide grids have 7.5 mm square holes with 10.16 mm pitch and are cut from a 5 mm acrylic plate. We fabricated the pins, the lead screws, and blocks with an inkjet 3D printer (Keyence Agilista 3200) with water soluble support material. In total, we fabricated 384 (= 24 x 16) pins and lead screws, and 3,072 (= 24 x 16 x 8 layers) blocks. To create the magnetic blocks, we embedded spherical magnets in each block by hand and inserted disk magnets using a bench vice.</p>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-3.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-3.png" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-2.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-3.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-3.jpg" /></a>\n  </div>\n</div>\n<h1>Future Vision</h1>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/claytronics.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/claytronics.mp4" type="video/mp4"></source>\n</video>\n[Video Credit: Carnegie Mellon University, Claytronics Vision]\n<br/>\n<p>With these capabilities, a 3D printer would become an inter- active medium, rather than merely a fabrication device. For example, such a 3D printer could be used in a Virtual Real- ity or Augmented Reality application to dynamically form a tangible object or controller to provide haptic feedback and engage users physically. For children, it could dynamically form a physical educational manipulative, such as a molec- ular or architectural model, to learn and explore topics, for example in a science museum. Designers could use it to ren- der a physical product to present to clients and interactively change the product’s design through direct manipulation. In this vision, Dynamic 3D printing is an environment in which the user thinks, designs, explores, and communicates through dynamic and interactive physical representation.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-8-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-8-1.png" /></a>\n  </div>\n</div>\n<p>Dynamic 3D printing would enable a new design workflow for digital fabrication. One notable advantage of dynamic 3D printing is the capability of connecting and disconnecting building blocks through direct manipulation. The user can also define variables or abstract attributes for parametric design through direct and gestural interaction. By leveraging this capability, the user could interactively design and fabri- cate in a physical space, similar to the man-machine dialogue proposed by Frazer et al. and later tangible CAD interfaces.</p>\n',dir:"content/output/projects",base:"dynablock.json",ext:".json",sourceBase:"dynablock.md",sourceExt:".md"}},Jg5j:function(e){e.exports={id:"trace-diff",name:"TraceDiff",description:"Debugging Unexpected Code Behavior Using Trace Divergences",title:"TraceDiff: Debugging Unexpected Code Behavior Using Trace Divergences",authors:["Ryo Suzuki","Gustavo Soares","Andrew Head","Elena Glassman","Ruan Reis","Melina Mongiovi","Loris D’Antoni","Bjöern Hartmann"],year:2017,booktitle:"In Proceedings of 2017 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC '17)",publisher:"IEEE Press, Piscataway, NJ, USA",pages:"107-115",doi:"https://doi.org/10.1109/VLHCC.2017.8103457",conference:{name:"VL/HCC 2017",fullname:"IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC 2017)",url:"https://sites.google.com/site/vlhcc2017/"},pdf:"vlhcc-2017-tracediff.pdf",slide:"vlhcc-2017-tracediff-slide.pdf",github:"https://github.com/ryosuzuki/trace-diff",demo:"https://ryosuzuki.github.io/trace-diff/",ieee:"http://ieeexplore.ieee.org/document/8103457/",arxiv:"https://arxiv.org/abs/1708.03786a",related:{title:"Exploring the Design Space of Automatically Synthesized Hints for Introductory Programming Assignments",authors:["Ryo Suzuki","Gustavo Soares","Elena Glassman","Andrew Head","Loris D'Antoni","Björn Hartmann"],year:2017,booktitle:"In Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA '17)",publisher:"ACM, New York, NY, USA",pages:"2951-2958",doi:"https://doi.org/10.1145/3027063.3053187",pdf:"chi-2017-lbw.pdf",suffix:"lbw",pageCount:6},pageCount:9,slideCount:77,bodyContent:'<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/trace-diff/top.mp4" type="video/mp4"></source>\n</video>\n\n# Abstract\n\nRecent advances in program synthesis offer means to automatically debug student submissions and generate personalized feedback in massive programming classrooms. When automatically generating feedback for programming assignments, a key challenge is designing pedagogically useful hints that are as effective as the manual feedback given by teachers. Through an analysis of teachers’ hint-giving practices in 132 online Q&A posts, we establish three design guidelines that an effective feedback design should follow. Based on these guidelines, we develop a feedback system that leverages both program synthesis and visualization techniques. Our system compares the dynamic code execution of both incorrect and fixed code and highlights how the error leads to a difference in behavior and where the incorrect code trace diverges from the expected solution. Results from our study suggest that our system enables students to detect and fix bugs that are not caught by students using another existing visual debugging tool.',bodyHtml:'<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/trace-diff/top.mp4" type="video/mp4"></source>\n</video>\n<h1>Abstract</h1>\n<p>Recent advances in program synthesis offer means to automatically debug student submissions and generate personalized feedback in massive programming classrooms. When automatically generating feedback for programming assignments, a key challenge is designing pedagogically useful hints that are as effective as the manual feedback given by teachers. Through an analysis of teachers’ hint-giving practices in 132 online Q&amp;A posts, we establish three design guidelines that an effective feedback design should follow. Based on these guidelines, we develop a feedback system that leverages both program synthesis and visualization techniques. Our system compares the dynamic code execution of both incorrect and fixed code and highlights how the error leads to a difference in behavior and where the incorrect code trace diverges from the expected solution. Results from our study suggest that our system enables students to detect and fix bugs that are not caught by students using another existing visual debugging tool.</p>\n',dir:"content/output/projects",base:"trace-diff.json",ext:".json",sourceBase:"trace-diff.md",sourceExt:".md"}},LrW7:function(e){e.exports={}},MyTI:function(e){e.exports=[{period:"August, 2015 --- Current",role:"RA",logo:"cu-boulder.png",institute:{name:"CU Boulder",url:"https://colorado.edu"},lab:{name:"THING Lab",url:"https://www.colorado.edu/atlas/thing-lab"},advisors:[{name:"Daniel Lightinger",url:"http://leithinger.com/"},{name:"Mark D. Gross",url:"http://mdgross.net/"},{name:"Tom Yeh",url:"http://tomyeh.info/"}]},{period:"May, 2019 --- August, 2019",role:"Intern",logo:"adobe.png",institute:{name:"Adobe Research",url:"https://colorado.edu"},lab:{name:"Creative Intelligence Lab",url:"http://hcc.colorado.edu/"},advisors:[{name:"Rubaiat Habib",url:"https://rubaiathabib.me/"},{name:"Li-Yi Wei",url:"https://www.liyiwei.org/"},{name:"Stephen Diverdi",url:"http://www.stephendiverdi.com/"},{name:"Danny Kaufman",url:"http://dannykaufman.io/"}]},{period:"December, 2017 --- October, 2018",role:"Intern",logo:"ut.png",institute:{name:"University of Tokyo",url:"https://colorado.edu"},lab:{name:"ERATO UIN",url:"http://www.jst.go.jp/erato/kawahara"},advisors:[{name:"Yasuaki Kakehi",url:"http://www.xlab.sfc.keio.ac.jp/"},{name:"Yoshihiro Kawahara",url:"http://www.akg.t.u-tokyo.ac.jp/"},{name:"Ryuma Niiyama",url:"https://scholar.google.co.jp/citations?user=0NMf5sgAAAAJ&hl=en"}]},{period:"May, 2016 --- August, 2016",role:"Intern",logo:"uc-berkeley.png",institute:{name:"UC Berkeley",url:null},lab:{name:"BiD Lab",url:"http://bid.berkeley.edu/"},advisors:[{name:"Bjoern Hartmann",url:"http://people.eecs.berkeley.edu/~bjoern/"}]},{period:"May, 2015 --- August, 2015",role:"Intern",logo:"stanford.png",institute:{name:"Stanford University",url:"https://stanford.edu"},lab:{name:"HCI Group",url:"http://hci.stanford.edu/"},advisors:[{name:"Michael Bernstein",url:"http://hci.stanford.edu/msb/"}]},{period:"October, 2014 --- May, 2015",role:"RA",logo:"ut.png",institute:{name:"University of Tokyo",url:null},lab:{name:"IIS Lab",url:"http://iis-lab.org/"},advisors:[{name:"Koji Yatani",url:"http://iis-lab.org/member/koji-yatani/"}]},{period:"December, 2014 --- March, 2015",role:"Intern",logo:"aist.png",institute:{name:"AIST",url:null},lab:{name:"Media Interaction",url:"https://staff.aist.go.jp/m.goto/MIG/index-j.html"},advisors:[{name:"Jun Kato",url:"http://junkato.jp/"}]}]},PSd4:function(e){e.exports={id:"mixed-initiative",name:"Mixed-Initiative Code Feedback",description:"Writing Reusable Code Feedback at Scale with Mixed-Initiative Program Synthesis",title:"Writing Reusable Code Feedback at Scale with Mixed-Initiative Program Synthesis",authors:["Andrew Head","Elena Glassman","Gustavo Soares","Ryo Suzuki","Lucas Figueredo","Loris D’Antoni","Björn Hartmann"],note:"(the first three authors equally contributed)",year:2017,booktitle:"In Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale (L@S '17)",publisher:"ACM, New York, NY, USA",pages:"89-98",doi:"https://doi.org/10.1145/3051457.3051467",conference:{name:"L@S 2017",fullname:"The ACM Conference on Learning at Scale (L@S 2017)",url:"http://learningatscale.acm.org/las2017"},pdf:"las-2017-mixed.pdf",slide:"las-2017-mixed-slide.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=3051467",pageCount:10,slideCount:62,image:"mixed-initiative.png",bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"mixed-initiative.json",ext:".json",sourceBase:"mixed-initiative.md",sourceExt:".md"}},RHEb:function(e,t,a){"use strict";a.r(t);for(var i=a("0iUn"),o=a("sLSF"),n=a("MI3g"),s=a("a7VT"),r=a("Tit0"),c=a("q1tI"),l=a.n(c),p=(a("IujW"),a("6T/A"),[]),d=0,h=["roomshift","lift-tiles","shapebots","morphio","dynablock","tabby","reactile","pep","flux-marker","trace-diff","mixed-initiative","refazer","atelier"];d<h.length;d++){var g=h[d],u=a("o0EK")("./".concat(g,".json"));p.push(u)}var m=function(e){function t(){return Object(i.default)(this,t),Object(n.default)(this,Object(s.default)(t).apply(this,arguments))}return Object(r.default)(t,e),Object(o.default)(t,[{key:"componentDidMount",value:function(){}},{key:"render",value:function(){return l.a.createElement("div",{id:"projects"},l.a.createElement("h1",null,"Full Papers"),p.map(function(e){return l.a.createElement("div",{className:"project ui vertical segment stackable grid","data-id":e.id},l.a.createElement("div",{className:"six wide column"},e.image&&l.a.createElement("a",{href:"/".concat(e.id)},l.a.createElement("img",{className:"ui rounded images",src:"/static/images/".concat(e.image)})),!e.image&&l.a.createElement("video",{poster:"/static/posters/".concat(e.id,".png"),autoplay:"",loop:"loop",muted:"true",playsinline:"",width:"100%",onclick:"this.play()",onmouseover:"this.play()"},l.a.createElement("source",{src:"/static/video/".concat(e.id,".mp4"),type:"video/mp4"}))),l.a.createElement("div",{className:"ten wide column"},l.a.createElement("a",{href:"/".concat(e.id)},l.a.createElement("h1",{className:"ui header",style:{marginBottom:"10px"}},l.a.createElement("span",null,e.name),l.a.createElement("span",{className:"ui big label"},e.conference.name),l.a.createElement("span",{className:"ui teal large label",style:{display:["morphio"].includes(e.id)?"inline-block":"none"}},"Best Paper Award")),l.a.createElement("h2",{style:{margin:"5px 0"}},e.description)),l.a.createElement("p",null,e.authors.map(function(e){return e.includes("Ryo Suzuki")?l.a.createElement("strong",null,e):l.a.createElement("span",null,e)}).reduce(function(e,t){return[e,", ",t]}),"   ",l.a.createElement("span",{style:{color:"gray"}},e.note))))}))}}]),t}(l.a.Component);t.default=m},RNiq:function(e,t,a){"use strict";a.r(t);var i=a("0iUn"),o=a("sLSF"),n=a("MI3g"),s=a("a7VT"),r=a("Tit0"),c=a("q1tI"),l=a.n(c),p=a("IujW"),d=a.n(p),h=(a("TIwn"),a("MyTI"),a("yC4j")),g=a("+a94"),u=(a("LrW7"),a("qg4i")),m=a("W+IF"),f=a("RHEb"),b=a("42TL"),v=a("Dx8+"),y=function(e){function t(){return Object(i.default)(this,t),Object(n.default)(this,Object(s.default)(t).apply(this,arguments))}return Object(r.default)(t,e),Object(o.default)(t,[{key:"componentDidMount",value:function(){}},{key:"render",value:function(){return l.a.createElement("div",null,l.a.createElement("title",null,"Ryo Suzuki | University of Colorado Boulder"),l.a.createElement("div",{className:"ui stackable grid"},l.a.createElement("div",{className:"one wide column"}),l.a.createElement("div",{className:"eleven wide column centered"},l.a.createElement(m.default,null),l.a.createElement("section",{id:"container"},l.a.createElement(f.default,null),l.a.createElement("div",{class:"ui divider"}),l.a.createElement("div",{id:"posters"},l.a.createElement("h1",null,"Posters and Demos"),l.a.createElement("div",{className:"ui vertical segment"},l.a.createElement("div",{className:"ui bulleted list"},u.map(function(e){return l.a.createElement("div",{className:"item",target:"_blank",style:{lineHeight:"1.8rem"}},l.a.createElement("b",null,"[",e.series,"]"),l.a.createElement("br",null),l.a.createElement("a",{href:"/publications/"+e.pdf},l.a.createElement("b",null,e.title)),l.a.createElement("br",null),e.author,", ",l.a.createElement("i",null,e.booktitle," (",e.series,")"),". ",e.publisher,", ",e.address,", ",e.pages,".",l.a.createElement("br",null),l.a.createElement("a",{href:"/publications/"+e.pdf,target:"_blank",style:{marginRight:"5px",display:e.pdf?"inline":"none"}},"[PDF]"),l.a.createElement("a",{href:"/publications/"+e.poster,target:"_blank",style:{marginRight:"5px",display:e.poster?"inline":"none"}},"[Poster]"),l.a.createElement("a",{href:"/publications/"+e.slide,target:"_blank",style:{marginRight:"5px",display:e.slide?"inline":"none"}},"[Slide]"),l.a.createElement("a",{href:e.url,target:"_blank",style:{marginRight:"5px",display:e.url?"inline":"none"}},"[DOI]"))})))),l.a.createElement("div",{class:"ui divider"}),l.a.createElement("div",{id:"activities"},l.a.createElement("h1",null,"Professional Activities"),l.a.createElement("div",{className:"ui vertical segment"},l.a.createElement(d.a,{source:g.bodyContent}))),l.a.createElement("div",{class:"ui divider"}),l.a.createElement("div",{id:"fellowship"},l.a.createElement("h1",null,"Scholarship and Fellowship"),l.a.createElement("div",{className:"ui vertical segment"},l.a.createElement(d.a,{source:h.bodyContent}))))),l.a.createElement("div",{id:"side",className:"three wide column centered",style:{marginTop:"50px"}},l.a.createElement(b.default,null),l.a.createElement(v.default,null),l.a.createElement("br",null),l.a.createElement("a",{class:"twitter-timeline",height:"1500px",href:"https://twitter.com/ryosuzk?ref_src=twsrc%5Etfw"},"Tweets by @ryosuzk")," ",l.a.createElement("script",{async:!0,src:"https://platform.twitter.com/widgets.js",charset:"utf-8"})),l.a.createElement("div",{className:"one wide column"})),l.a.createElement("div",{className:"ui stackable grid"},l.a.createElement("div",{className:"sixteen wide column centered"},l.a.createElement("p",{style:{textAlign:"center"}}))))}}]),t}(l.a.Component);t.default=y},"W+IF":function(e,t,a){"use strict";a.r(t);var i=a("0iUn"),o=a("sLSF"),n=a("MI3g"),s=a("a7VT"),r=a("Tit0"),c=a("q1tI"),l=a.n(c),p=function(e){function t(){return Object(i.default)(this,t),Object(n.default)(this,Object(s.default)(t).apply(this,arguments))}return Object(r.default)(t,e),Object(o.default)(t,[{key:"render",value:function(){return l.a.createElement("header",{className:"ui stackable grid"},l.a.createElement("div",{className:"ui sixteen wide column"},l.a.createElement("h1",{className:"ui huge header"},l.a.createElement("img",{style:{maxWidth:"62px",marginRight:"15px"},src:"/static/images/profile.png",className:"ui circular image"}),l.a.createElement("div",{className:"content"},"Ryo Suzuki",l.a.createElement("div",{className:"sub header",style:{fontSize:"1.5rem"}},"University of Colorado Boulder"))),l.a.createElement("video",{id:"top-video",poster:"/static/posters/top.png",preload:"metadata",autoPlay:!0,loop:!0,muted:!0,playsInline:!0,"webkit-playsinline":""},l.a.createElement("source",{src:"/static/video/top.mp4",type:"video/mp4"})),l.a.createElement("div",{id:"profile",style:{fontSize:"1.3rem"}},l.a.createElement("p",null,"I am a Ph.D. student at the ",l.a.createElement("a",{href:"https://www.colorado.edu/cs/",target:"_blank"},l.a.createElement("b",null,"University of Colorado Boulder")),", Department of Computer Science, advised by ",l.a.createElement("a",{href:"http://leithinger.com/",target:"_blank"},l.a.createElement("b",null,"Daniel Leithinger"))," and ",l.a.createElement("a",{href:"http://mdgross.net/",target:"_blank"},l.a.createElement("b",null,"Mark D. Gross"))," in ",l.a.createElement("a",{href:"https://www.colorado.edu/atlas/thing-lab",target:"_blank"},l.a.createElement("b",null,"THING Lab"))," and ",l.a.createElement("a",{href:"http://hcc.colorado.edu/",target:"_blank"},l.a.createElement("b",null,"Human-Computer Interaction Group"))),l.a.createElement("div",{class:"ui segment",style:{borderColor:"#2ECC40"}},l.a.createElement("p",{style:{marginBottom:"-5px"}},l.a.createElement("b",{style:{color:"#2ECC40"}},"I'm on the job market this year, open to both academia and industry. Please feel free to contact me.")),l.a.createElement("p",null,l.a.createElement("a",{className:"ui inverted green button",href:"/static/jobs/Research_Statement.pdf",target:"_blank",style:{marginTop:"20px"}},l.a.createElement("b",null,l.a.createElement("i",{className:"far fa-file-pdf fa-fw"}),"Research Statement")),"   ",l.a.createElement("a",{className:"ui inverted green button",href:"/static/jobs/Teaching_Statement.pdf",target:"_blank",style:{marginTop:"20px"}},l.a.createElement("b",null,l.a.createElement("i",{className:"far fa-file-pdf fa-fw"}),"Teaching Statement")),"   ",l.a.createElement("a",{className:"ui inverted green button",href:"/static/jobs/Diversity_Statement.pdf",target:"_blank",style:{marginTop:"20px"}},l.a.createElement("b",null,l.a.createElement("i",{className:"far fa-file-pdf fa-fw"}),"Diversity Statement")),"   ",l.a.createElement("a",{className:"ui inverted green button",href:"/cv.pdf",target:"_blank",style:{marginTop:"20px"}},l.a.createElement("b",null,l.a.createElement("i",{className:"far fa-file-pdf fa-fw"}),"Resume/CV")))),l.a.createElement("p",null,"My research focus lies in ",l.a.createElement("b",null,"the intersection between Human-Computer Interaction (HCI) and robotics"),". During my PhD, I have developed a novel physical interface made of ",l.a.createElement("b",null,"swarm and soft robots at all scales")," (i.e., from mm- to m-scale). By leveraging techniques from both robotics and HCI, my research explores how we can make the ",l.a.createElement("b",null,"physical environment more adaptive")," with context-aware swarm robotic assistant that can be embedded into our everyday life.")),l.a.createElement("div",{className:"ui horizontal list",style:{marginTop:"20px"}},l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://scholar.google.com/citations?user=klWjaQIAAAAJ",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fas fa-graduation-cap fa-fw"}),"Google Scholar")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"mailto:ryo.suzuki@colorado.edu",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"far fa-envelope fa-fw"}),"Email")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://www.facebook.com/ryosuzk",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fab fa-facebook-square fa-fw"}),"ryosuzk")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://twitter.com/ryosuzk",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fab fa-twitter fa-fw"}),"ryosuzk")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://twitter.com/HCI_Comics",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fab fa-twitter fa-fw"}),"HCI_Comics (ja)")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://github.com/ryosuzuki",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fab fa-github-alt fa-fw"}),"ryosuzuki")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://www.linkedin.com/in/ryosuzuki/",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fab fa-linkedin-in fa-fw"}),"ryosuzuki")))),l.a.createElement("div",{className:"one wide column"}))}}]),t}(l.a.Component);t.default=p},"W/HP":function(e){e.exports={id:"pep",name:"PEP",description:"3D Printed Electronic Papercrafts - An Integrated Approach for 3D Sculpting Paper-based Electronic Devices",title:"PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-based Electronic Devices",authors:["Hyunjoo Oh","Tung D. Ta","Ryo Suzuki","Mark D. Gross","Yoshihiro Kawahara","Lining Yao"],year:2018,booktitle:"In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18)",publisher:"ACM, New York, NY, USA",pages:"Paper 441, 12 pages",doi:"https://doi.org/10.1145/3173574.3174015",conference:{name:"CHI 2018",fullname:"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2018)",url:"https://chi2018.acm.org/"},pdf:"chi-2018-pep.pdf",video:"https://vimeo.com/252080903",embed:"https://www.youtube.com/embed/DTd863suDN0","short-video":"https://www.youtube.com/watch?v=DTd863suDN0","acm-dl":"https://dl.acm.org/citation.cfm?id=3174015",pageCount:12,slideCount:0,bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"pep.json",ext:".json",sourceBase:"pep.md",sourceExt:".md"}},"X0/d":function(e){e.exports={id:"morphio",name:"MorphIO",description:"Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction",title:"MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction",authors:["Ryosuke Nakayama*","Ryo Suzuki*","Satoshi Nakamaru","Ryuma Niiyama","Yoshihiro Kawahara","Yasuaki Kakehi"],note:"(the first two authors equally contributed)",year:2018,booktitle:"In Proceedings of the 2019 on Designing Interactive Systems Conference (DIS '19)",publisher:"ACM, New York, NY, USA",pages:"975-986",doi:"https://doi.org/10.1145/3322276.3322337",conference:{name:"DIS 2019",fullname:"The ACM conference on Designing Interactive Systems (DIS 2019) - Best Paper Award",url:"https://dis2019.com/"},pdf:"dis-2019-morphio.pdf",slide:"dis-2019-morphio-slide.pdf",video:"https://www.youtube.com/watch?v=ZkCcazfFD-M",embed:"https://www.youtube.com/embed/ZkCcazfFD-M","acm-dl":"https://dl.acm.org/citation.cfm?id=3322337",pageCount:12,slideCount:52,bodyContent:'# Abstract\n\nWe introduce **MorphIO, entirely soft sensing and actuation modules** for programming by demonstration of soft robots and shape-changing interfaces. MorphIO’s hardware consists of a **soft pneumatic actuator containing a conductive sponge sensor**. This allows both input and output of three-dimensional deformation of a soft material. Leveraging this capability, MorphIO enables a user to **record and later playback physical motion** of programmable shape-changing materials. In addition, the modular design of MorphIO’s unit allows the user to construct various shapes and topologies through magnetic connection. We demonstrate several application scenarios, including tangible character animation, locomotion experiment of a soft robot, and prototyping tools for animated soft objects. Our user study with six participants confirms the benefits of MorphIO, as compared to the existing programming paradigm.\n\n\n<video poster="/static/projects/morphio/video-poster/top.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/top.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/top.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-3.png" /></a>\n  </div>\n</div>\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-5.png" /></a>\n  </div>\n</div>\n\n# Introduction\n\n**Programmable soft materials** have a great potential for many application domains, such as soft robotics, material interfaces, accessibility, and haptic interfaces.\n**However, programming of such materials is hard.**\nThe dominant programming paradigm of soft robots and material interfaces is largely confined within a digital screen, leaving little room for users to interactively explore physical motion through tangible interaction. In such a workflow—compiling code on a digital screen then trans- ferring it into the physical object—users need to repeatedly switch between the digital and physical worlds. This leaves a large gulf of execution in their programming experiences.\nThus, the traditional programming paradigm significantly limits the user’s ability to experiment with the design of expressive motion. Moreover, due to this barrier, such an opportunity is largely limited to highly skilled programmers and researchers who are proficient in hardware programming.\n\n\n# MorphIO\n\nThis paper introduces **MorphIO, entirely soft sensing and actuation modules** for programming by demonstration of soft robots and shape-changing interfaces.\nMorphIO’s hardware consists of a soft pneumatic actuator containing a conductive sponge sensor. This allows for integrated and entirely soft shape-changing modules that can both sense and actuate a variety of three-dimensional deformations. Leveraging this capability, MorphIO enables the user to program behaviors by **recording and later playing back physical motions** through tangible interaction. In addition, the modular design of MorphIO’s unit allows the user to construct various shapes and topologies through magnetic connection, then **synthesize multiple recorded motions to achieve more complex behaviors**, such as bending, gripping, and walking.\n\n\n<video poster="/static/projects/morphio/video-poster/module.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/module.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/module.mp4" type="video/mp4"></source>\n</video>\n\n<br/>\n\n<video poster="/static/projects/morphio/video-poster/bear.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/bear.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/bear.mp4" type="video/mp4"></source>\n</video>\n\n\n# System Overview\n\nThe programming workflow with MorphIO is the following:\n\n- **Step 1:** A user starts manipulating the MorphIO unit.\n\n- **Step 2:** The demonstrated motion is detected and recorded through internal sensors, and the recorded sensor values are stored in the software.\n\n- **Step 3:** Once the user clicks play in the graphical user interface, the pneumatic pump starts supplying air.\n\n- **Step 4:** By controlling the air flow through switching on and off the solenoid valves, the system can control the behavior of the pneumatic actuator as it plays back the recorded motion.\n\nThe MorphIO system consists of the following components: A sensor and actuation unit, a sensing and actuation control unit, a microcontroller, software to control these units, and a visual interface for users to control behaviors. Figure illustrates the overview architecture of MorphIO.\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-6-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-6-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-6-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-6-2.png" /></a>\n  </div>\n</div>\n\n\n# Entirely Soft Sensing and Actuation Modules\n\nOur main contribution is a design and fabrication method for **a conductive sponge sensor** that can be embedded into an air chamber in the pneumatic actuator. The conductive sponge sensor leverages the porous structure to **sense the three-dimensional deformation by measuring the internal resistance value**; when contracted, the resistance value be- tween the top and bottom surfaces drops, and when extended, it increases. In contrast to existing sensing techniques, an elastic sponge allows for a higher degree of freedom in sensing capability (e.g., stretching, bending, and compression) without sacrificing the softness of the interface.\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-2-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-2-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-2-2.png" /></a>\n  </div>\n</div>\n\n\n<video poster="/static/projects/morphio/video-poster/mechanism.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/mechanism.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/mechanism.mp4" type="video/mp4"></source>\n</video>\n\n\n<br />\n\nMoreover, our **modular design** and **graphical interface** allows for easy experiments involving multiple units. For example, the system can visualize multiple recorded sensor values, so that the user can see, customize, and synthesize recorded motion to construct more complex behaviors. These hardware and software designs were informed by our formative study, wherein we interviewed five experienced researchers from the robotics and HCI communities.\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-3-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-3-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-3-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-3-2.png" /></a>\n  </div>\n</div>\n\n<video poster="/static/projects/morphio/video-poster/unit-x2.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/unit-x2.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/unit-x2.mp4" type="video/mp4"></source>\n</video>\n\n<video poster="/static/projects/morphio/video-poster/unit-x3.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/unit-x3.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/unit-x3.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-3.png" /></a>\n  </div>\n</div>\n\n\n\n\n# Fabrication Process\n\nThe fabrication process follows three steps: 1) Fabricate an elastic sponge, 2) impregnate into conductive ink, and 3) attach electrodes and wires.\nTo fabricate an elastic sponge, we first prepare 6.0 g of elastomer prepolymer solution and 29.1 g of sodium-chloride, then mix them together by using a planetary centrifugal mixer. The mixed solution is injected into a 3D printed cylindrical mold (16mm diameter, 40mm height). Then we dry the material with an oven at 100 C degrees for one hour. Once dried, we immerse the sponge in water, so that the sodium chloride can melt, leaving a porous structure within the elastomer sponge.\n\n<video poster="/static/projects/morphio/video-poster/fabrication.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/fabrication.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/fabrication.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-4.png" /></a>\n  </div>\n</div>\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-5.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-6.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-6.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-7.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-7.png" /></a>\n  </div>\n</div>\n\n\n\n# Applications\n\nWe demonstrate several possible applications scenarios with MorphIO. 1) Tangible character animation, 2) Animating existing soft objects, 3) Remote manipulation of soft grippers, 4) Locomotion experimentation with soft robots.\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-4.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-3.png" /></a>\n  </div>\n</div>\n\n<video poster="/static/projects/morphio/video-poster/locomotion.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/locomotion.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/locomotion.mp4" type="video/mp4"></source>\n</video>\n\n\n# Evaluation\n\nWe conducted a user evaluation study to understand the bene- fits and limitations of MorphIO. In this study, we focused on answering the following research questions:\n\n- **RQ1:** Does MorphIO save time and reduce the number of iterations to program the target behavior, compared to the existing approach?\n\n- **RQ2:** Does MorphIO increase the expressiveness of the physical motion?\n\nTo answer these questions, we conducted a controlled experiment where we compared MorphIO (left) with the current programming approach. We chose Arduino IDE (right) as a base condition for the comparison, as this is the most common programming approach identified through our formative study. We provide three basic tasks to construct a program. For each task, the participants were asked to program three differ- ent emotions—happiness, anger, and sadness—of an animated character. We chose these emotions based on Ekman’s basic emotions for communication.\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-3.png" /></a>\n  </div>\n</div>\n\nThe average time of task completion time of MorphIO was 2m 19s, compared to 5m 21s with the control condition. The average number of iterations of MorphIO was 4.4 times, compared to 6.4 with the control condition, which confirms that MorphIO is significantly efficient in terms of task completion time and the number of iterations. When asked about the achievement of the expressions using a 9-point Likert scale, the average score with MorphIO was 6.5, compared to 6.3 with the control condition. We did not find differences between the two conditions. Thus, we conclude the result of our study as follows: **RQ1: Yes, RQ2: No**.\n\nBased on our post interviews, we discuss the benefits and limitations of our approach: **1) tangible interactions are suitable for sculpting rough motion**, **2) programming allows for fine-tuning more precise adjustments**. Thus, for future research, systems might allow users to quickly make a rough motion, which can automatically be converted into digital parameters so that the user can also precisely control and adjust the motion. The same human- computer cooperation approach can be applied to other design domains: For example, when designing an object, the user can quickly make rough shapes with clay, while letting a machine finish the details. We believe this insight can lead the HCI community to further explore design approaches wherein users and machines cooperate for enhanced interaction design.\n\n\n# Future Vision\nWe believe this approach’s potential for lowering the barrier and opening new opportunities for a larger community to begin designing, prototyping, and exploring soft material motion—not by coding on a screen, but by sculpting behaviors in the physical world.\nWe envision the future where people can interactively explore various behaviors through tangible interactions, **just like sculpting behaviors with clay**.\n\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-14.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-14.png" /></a>\n  </div>\n</div>',bodyHtml:'<h1>Abstract</h1>\n<p>We introduce <strong>MorphIO, entirely soft sensing and actuation modules</strong> for programming by demonstration of soft robots and shape-changing interfaces. MorphIO’s hardware consists of a <strong>soft pneumatic actuator containing a conductive sponge sensor</strong>. This allows both input and output of three-dimensional deformation of a soft material. Leveraging this capability, MorphIO enables a user to <strong>record and later playback physical motion</strong> of programmable shape-changing materials. In addition, the modular design of MorphIO’s unit allows the user to construct various shapes and topologies through magnetic connection. We demonstrate several application scenarios, including tangible character animation, locomotion experiment of a soft robot, and prototyping tools for animated soft objects. Our user study with six participants confirms the benefits of MorphIO, as compared to the existing programming paradigm.</p>\n<video poster="/static/projects/morphio/video-poster/top.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/top.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/top.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-3.png" /></a>\n  </div>\n</div>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-5.png" /></a>\n  </div>\n</div>\n<h1>Introduction</h1>\n<p><strong>Programmable soft materials</strong> have a great potential for many application domains, such as soft robotics, material interfaces, accessibility, and haptic interfaces.\n<strong>However, programming of such materials is hard.</strong>\nThe dominant programming paradigm of soft robots and material interfaces is largely confined within a digital screen, leaving little room for users to interactively explore physical motion through tangible interaction. In such a workflow—compiling code on a digital screen then trans- ferring it into the physical object—users need to repeatedly switch between the digital and physical worlds. This leaves a large gulf of execution in their programming experiences.\nThus, the traditional programming paradigm significantly limits the user’s ability to experiment with the design of expressive motion. Moreover, due to this barrier, such an opportunity is largely limited to highly skilled programmers and researchers who are proficient in hardware programming.</p>\n<h1>MorphIO</h1>\n<p>This paper introduces <strong>MorphIO, entirely soft sensing and actuation modules</strong> for programming by demonstration of soft robots and shape-changing interfaces.\nMorphIO’s hardware consists of a soft pneumatic actuator containing a conductive sponge sensor. This allows for integrated and entirely soft shape-changing modules that can both sense and actuate a variety of three-dimensional deformations. Leveraging this capability, MorphIO enables the user to program behaviors by <strong>recording and later playing back physical motions</strong> through tangible interaction. In addition, the modular design of MorphIO’s unit allows the user to construct various shapes and topologies through magnetic connection, then <strong>synthesize multiple recorded motions to achieve more complex behaviors</strong>, such as bending, gripping, and walking.</p>\n<video poster="/static/projects/morphio/video-poster/module.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/module.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/module.mp4" type="video/mp4"></source>\n</video>\n<br/>\n<video poster="/static/projects/morphio/video-poster/bear.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/bear.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/bear.mp4" type="video/mp4"></source>\n</video>\n<h1>System Overview</h1>\n<p>The programming workflow with MorphIO is the following:</p>\n<ul>\n<li>\n<p><strong>Step 1:</strong> A user starts manipulating the MorphIO unit.</p>\n</li>\n<li>\n<p><strong>Step 2:</strong> The demonstrated motion is detected and recorded through internal sensors, and the recorded sensor values are stored in the software.</p>\n</li>\n<li>\n<p><strong>Step 3:</strong> Once the user clicks play in the graphical user interface, the pneumatic pump starts supplying air.</p>\n</li>\n<li>\n<p><strong>Step 4:</strong> By controlling the air flow through switching on and off the solenoid valves, the system can control the behavior of the pneumatic actuator as it plays back the recorded motion.</p>\n</li>\n</ul>\n<p>The MorphIO system consists of the following components: A sensor and actuation unit, a sensing and actuation control unit, a microcontroller, software to control these units, and a visual interface for users to control behaviors. Figure illustrates the overview architecture of MorphIO.</p>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-6-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-6-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-6-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-6-2.png" /></a>\n  </div>\n</div>\n<h1>Entirely Soft Sensing and Actuation Modules</h1>\n<p>Our main contribution is a design and fabrication method for <strong>a conductive sponge sensor</strong> that can be embedded into an air chamber in the pneumatic actuator. The conductive sponge sensor leverages the porous structure to <strong>sense the three-dimensional deformation by measuring the internal resistance value</strong>; when contracted, the resistance value be- tween the top and bottom surfaces drops, and when extended, it increases. In contrast to existing sensing techniques, an elastic sponge allows for a higher degree of freedom in sensing capability (e.g., stretching, bending, and compression) without sacrificing the softness of the interface.</p>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-2-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-2-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-2-2.png" /></a>\n  </div>\n</div>\n<video poster="/static/projects/morphio/video-poster/mechanism.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/mechanism.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/mechanism.mp4" type="video/mp4"></source>\n</video>\n<br />\n<p>Moreover, our <strong>modular design</strong> and <strong>graphical interface</strong> allows for easy experiments involving multiple units. For example, the system can visualize multiple recorded sensor values, so that the user can see, customize, and synthesize recorded motion to construct more complex behaviors. These hardware and software designs were informed by our formative study, wherein we interviewed five experienced researchers from the robotics and HCI communities.</p>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-3-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-3-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-3-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-3-2.png" /></a>\n  </div>\n</div>\n<video poster="/static/projects/morphio/video-poster/unit-x2.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/unit-x2.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/unit-x2.mp4" type="video/mp4"></source>\n</video>\n<video poster="/static/projects/morphio/video-poster/unit-x3.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/unit-x3.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/unit-x3.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-3.png" /></a>\n  </div>\n</div>\n<h1>Fabrication Process</h1>\n<p>The fabrication process follows three steps: 1) Fabricate an elastic sponge, 2) impregnate into conductive ink, and 3) attach electrodes and wires.\nTo fabricate an elastic sponge, we first prepare 6.0 g of elastomer prepolymer solution and 29.1 g of sodium-chloride, then mix them together by using a planetary centrifugal mixer. The mixed solution is injected into a 3D printed cylindrical mold (16mm diameter, 40mm height). Then we dry the material with an oven at 100 C degrees for one hour. Once dried, we immerse the sponge in water, so that the sodium chloride can melt, leaving a porous structure within the elastomer sponge.</p>\n<video poster="/static/projects/morphio/video-poster/fabrication.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/fabrication.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/fabrication.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-4.png" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-5.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-6.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-6.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-7.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-7.png" /></a>\n  </div>\n</div>\n<h1>Applications</h1>\n<p>We demonstrate several possible applications scenarios with MorphIO. 1) Tangible character animation, 2) Animating existing soft objects, 3) Remote manipulation of soft grippers, 4) Locomotion experimentation with soft robots.</p>\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-4.png" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-3.png" /></a>\n  </div>\n</div>\n<video poster="/static/projects/morphio/video-poster/locomotion.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/locomotion.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/locomotion.mp4" type="video/mp4"></source>\n</video>\n<h1>Evaluation</h1>\n<p>We conducted a user evaluation study to understand the bene- fits and limitations of MorphIO. In this study, we focused on answering the following research questions:</p>\n<ul>\n<li>\n<p><strong>RQ1:</strong> Does MorphIO save time and reduce the number of iterations to program the target behavior, compared to the existing approach?</p>\n</li>\n<li>\n<p><strong>RQ2:</strong> Does MorphIO increase the expressiveness of the physical motion?</p>\n</li>\n</ul>\n<p>To answer these questions, we conducted a controlled experiment where we compared MorphIO (left) with the current programming approach. We chose Arduino IDE (right) as a base condition for the comparison, as this is the most common programming approach identified through our formative study. We provide three basic tasks to construct a program. For each task, the participants were asked to program three differ- ent emotions—happiness, anger, and sadness—of an animated character. We chose these emotions based on Ekman’s basic emotions for communication.</p>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-3.png" /></a>\n  </div>\n</div>\n<p>The average time of task completion time of MorphIO was 2m 19s, compared to 5m 21s with the control condition. The average number of iterations of MorphIO was 4.4 times, compared to 6.4 with the control condition, which confirms that MorphIO is significantly efficient in terms of task completion time and the number of iterations. When asked about the achievement of the expressions using a 9-point Likert scale, the average score with MorphIO was 6.5, compared to 6.3 with the control condition. We did not find differences between the two conditions. Thus, we conclude the result of our study as follows: <strong>RQ1: Yes, RQ2: No</strong>.</p>\n<p>Based on our post interviews, we discuss the benefits and limitations of our approach: <strong>1) tangible interactions are suitable for sculpting rough motion</strong>, <strong>2) programming allows for fine-tuning more precise adjustments</strong>. Thus, for future research, systems might allow users to quickly make a rough motion, which can automatically be converted into digital parameters so that the user can also precisely control and adjust the motion. The same human- computer cooperation approach can be applied to other design domains: For example, when designing an object, the user can quickly make rough shapes with clay, while letting a machine finish the details. We believe this insight can lead the HCI community to further explore design approaches wherein users and machines cooperate for enhanced interaction design.</p>\n<h1>Future Vision</h1>\n<p>We believe this approach’s potential for lowering the barrier and opening new opportunities for a larger community to begin designing, prototyping, and exploring soft material motion—not by coding on a screen, but by sculpting behaviors in the physical world.\nWe envision the future where people can interactively explore various behaviors through tangible interactions, <strong>just like sculpting behaviors with clay</strong>.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-14.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-14.png" /></a>\n  </div>\n</div>',dir:"content/output/projects",base:"morphio.json",ext:".json",sourceBase:"morphio.md",sourceExt:".md"}},ejaO:function(e){e.exports={id:"tabby",name:"Tabby",description:"Explorable Design for 3D Printing Textures",title:"Tabby: Explorable Design for 3D Printing Textures",authors:["Ryo Suzuki","Koji Yatani","Mark D. Gross","Tom Yeh"],year:2018,booktitle:"The Pacific Conference on Computer Graphics and Applications (Pacific Graphics 2018)",publisher:"The Eurographics Association",pages:"1-4",doi:"https://doi.org/10.2312/pg.20181273",conference:{name:"Pacific Graphics 2018",fullname:"The Pacific Conference on Computer Graphics and Applications (Pacific Graphics 2018)",url:"http://sweb.cityu.edu.hk/pg2018/"},pdf:"pg-2018-tabby.pdf",video:"https://www.youtube.com/watch?v=rRgw8lH74CA",embed:"https://www.youtube.com/embed/rRgw8lH74CA","acm-dl":"https://diglib.eg.org/handle/10.2312/pg20181273",slide:"pg-2018-tabby-slide.pdf",pageCount:4,slideCount:40,bodyContent:'# Abstract\nThis paper presents **Tabby, an interactive and explorable design tool for 3D printing textures**. Tabby allows texture design with direct manipulation in the following workflow: 1) select a target surface, 2) sketch and manipulate a texture with 2D drawings, and then 3) generate 3D printing textures onto an arbitrary curved surface. To enable efficient texture creation, Tabby leverages an auto-completion approach which automates the tedious, repetitive process of applying texture, while allowing flexible customization. Our user evaluation study with seven participants confirms that Tabby can effectively support the design exploration of different patterns for both novice and experienced users.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/tabby/top.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-4.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-6.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-6.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-7.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-7.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-8.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-8.png" /></a>\n  </div>\n</div>',bodyHtml:'<h1>Abstract</h1>\n<p>This paper presents <strong>Tabby, an interactive and explorable design tool for 3D printing textures</strong>. Tabby allows texture design with direct manipulation in the following workflow: 1) select a target surface, 2) sketch and manipulate a texture with 2D drawings, and then 3) generate 3D printing textures onto an arbitrary curved surface. To enable efficient texture creation, Tabby leverages an auto-completion approach which automates the tedious, repetitive process of applying texture, while allowing flexible customization. Our user evaluation study with seven participants confirms that Tabby can effectively support the design exploration of different patterns for both novice and experienced users.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/tabby/top.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-4.png" /></a>\n  </div>\n</div>\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-6.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-6.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-7.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-7.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-8.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-8.png" /></a>\n  </div>\n</div>',dir:"content/output/projects",base:"tabby.json",ext:".json",sourceBase:"tabby.md",sourceExt:".md"}},jEBx:function(e){e.exports={id:"reactile",name:"Reactile",description:"Programming Swarm User Interfaces through Direct Physical Manipulation",title:"Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation",authors:["Ryo Suzuki","Jun Kato","Mark D. Gross","Tom Yeh"],year:2018,booktitle:"In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18)",publisher:"ACM, New York, NY, USA",pages:"Paper 199, 13 pages",doi:"https://doi.org/10.1145/3173574.3173773",conference:{name:"CHI 2018",fullname:"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2018)",url:"https://chi2018.acm.org/"},pdf:"chi-2018-reactile.pdf",video:"https://www.youtube.com/watch?v=Gb7brajKCVE",embed:"https://www.youtube.com/embed/Gb7brajKCVE","short-video":"https://www.youtube.com/watch?v=YT7vMJZjohU",slide:"chi-2018-reactile-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3173773",github:"https://github.com/ryosuzuki/reactile",pageCount:12,slideCount:56,bodyContent:'# Abstract\n\nWe explore a new approach to programming swarm user interfaces (Swarm UI) by leveraging direct physical manipulation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high- level interface design. Inspired by current UI programming practices, we introduce a four-step workflow—create elements, abstract attributes, specify behaviors, and propagate changes—for Swarm UI programming. We propose a set of direct physi- cal manipulation techniques to support each step in this work- flow. To demonstrate these concepts, we developed Reac- tile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studies—an in-class survey with 148 students and a lab interview with eight participants—confirm that our approach is intuitive and understandable for programming Swarm UIs.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/top.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-3.png" /></a>\n  </div>\n</div>\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-1.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-2.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-3.png" /></a>\n  </div>\n</div>\n\n\n# Introduction\n\nIn recent years, **Swarm User Interfaces** have emerged as a new paradigm of human-computer interaction. While the idea of coordinated miniature robots was originally proposed in the literature of swarm and micro-robotic systems, HCI researchers have explored the use of these robots as a user interface.\nHowever, this opportunity is currently limited to highly skilled programmers who are proficient in robot programming. For typical programmers inexperienced in robot programming who wish to build a Swarm UI application, it is unclear if the robot programming approach is the most appropriate for UI programming. To design interactive UI applications, pro- grammers often must think in terms of higher-level design for user interaction, whereas robot programming tends to focus on low-level controls of sensors and actuators. Historically, a novel UI platform is adopted only after the advent of an effective programming tool that empowers a larger developer community, and even end-users, to create many applications for the platform; for example, HyperCard for interactive hyper- media, Phidgets for physical interfaces, and Interface Builder for GUI applications. We stipulate that current approaches to programming Swarm UI are too robot-centric to be effec- tive for building rich and interactive applications. Then, what would be a better alternative?\n\n\n# Reactile\n\nThis paper introduces Reactile, a programming environment for Swarm UI applications.\nThe goal of Reactile is to explore a new approach to programming Swarm UI applications. To design an appropriate workflow for Swarm UI programming, we look into existing UI programming paradigm for inspiration. The common workflow of UI programming can be decomposed into four basic steps: create elements, abstract attributes, specify behaviors, and propagate changes. Based on these insights, we propose the following four-step workflow for Swarm UI programming: 1) creates shapes, 2) abstracts shape attributes as variables, 3) specifies data-bindings be- tween dynamic attributes, and 4) the system changes shapes in response to user inputs. With this workflow, a programmer can think in terms of high-level interface and interaction design to build interactive Swarm UI appli- cations, compared to existing, low-level, robot programming approaches.\n\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-3.png" /></a>\n  </div>\n</div>\n\nThe workflow of Swarm UI programming is inspired by the existing UI programming paradigm. We first review the common workflow of UI programming and decompose it into four basic elements that represent high-level steps. Then we discuss how to apply this workflow to Swarm UI programming.\nAs we see in well-known design patterns for interactive UI ap- plications such as reactive programming paradigm, the Model-View-Controller, and the observer pattern, they share a com- mon workflow consisting of four basic elements: 1) create elements, 2) abstract attributes, 3) specify behaviors, and 4) propagate changes.\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-6.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-6.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-4.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-5.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-6.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-6.png" /></a>\n  </div>\n</div>\n\n# Implementation\n\nReactile actuates a swarm of small magnetic markers to move on a 2D canvas with electromagnetic force. We designed and fabricated a board of electromagnetic coil arrays (3,200 coils), which covers an 80 cm x 40 cm area. Reactile tracks the marker positions and detects interactions between a user and swarm markers using a standard RGB camera and computer vision techniques. The system displays spatial information using a DLP projector to allow a programmer to see program states in the same physical context.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/coil.mp4" type="video/mp4"></source>\n</video>\n\n<br />\n\nIn Reactile, a user interface consists of a swarm of passive magnetic markers which move on a 2D workspace driven by electromagnetic forces. Reactile uses a grid of electromagnetic coils to actuate these magnetic markers. Running current through the circuit coils generates a local magnetic field so that each coil can attract a single magnet located within its area. Each coil is aligned with a certain offset in both horizontal and vertical direction with an effective area overlap, which allows the coil to attract the magnet located in the adjacent coil. We design electromagnetic coil arrays to be fabricated with a standard printed circuit board (PCB) manufacturing. This reduces the cost and fabrication complexity, making it easy for the actuation area to scale up.\n\nOur PCB design is a 4-layer board, and each layer contains a set of coils, each of which has an identical circular shape with a 15 mm diameter and a 2.5 mm overlap between nearby coils. Each coil has 15 turns with 0.203 mm (8 mils) spacing between lines, and the distance between centers of two coils is approximately 10 mm, which makes a 10 mm grid for attractive points. The final prototype covers an 80 cm x 40 cm area with 80 x 40 coils by aligning five identical boards horizontally. The fabrication of each board costs approximately $80 USD, including manufacturing of PCB and electronic components.\n\n<br />\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/mechanism.mp4" type="video/mp4"></source>\n</video>',bodyHtml:'<h1>Abstract</h1>\n<p>We explore a new approach to programming swarm user interfaces (Swarm UI) by leveraging direct physical manipulation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high- level interface design. Inspired by current UI programming practices, we introduce a four-step workflow—create elements, abstract attributes, specify behaviors, and propagate changes—for Swarm UI programming. We propose a set of direct physi- cal manipulation techniques to support each step in this work- flow. To demonstrate these concepts, we developed Reac- tile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studies—an in-class survey with 148 students and a lab interview with eight participants—confirm that our approach is intuitive and understandable for programming Swarm UIs.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/top.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-3.png" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-1.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-2.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-3.png" /></a>\n  </div>\n</div>\n<h1>Introduction</h1>\n<p>In recent years, <strong>Swarm User Interfaces</strong> have emerged as a new paradigm of human-computer interaction. While the idea of coordinated miniature robots was originally proposed in the literature of swarm and micro-robotic systems, HCI researchers have explored the use of these robots as a user interface.\nHowever, this opportunity is currently limited to highly skilled programmers who are proficient in robot programming. For typical programmers inexperienced in robot programming who wish to build a Swarm UI application, it is unclear if the robot programming approach is the most appropriate for UI programming. To design interactive UI applications, pro- grammers often must think in terms of higher-level design for user interaction, whereas robot programming tends to focus on low-level controls of sensors and actuators. Historically, a novel UI platform is adopted only after the advent of an effective programming tool that empowers a larger developer community, and even end-users, to create many applications for the platform; for example, HyperCard for interactive hyper- media, Phidgets for physical interfaces, and Interface Builder for GUI applications. We stipulate that current approaches to programming Swarm UI are too robot-centric to be effec- tive for building rich and interactive applications. Then, what would be a better alternative?</p>\n<h1>Reactile</h1>\n<p>This paper introduces Reactile, a programming environment for Swarm UI applications.\nThe goal of Reactile is to explore a new approach to programming Swarm UI applications. To design an appropriate workflow for Swarm UI programming, we look into existing UI programming paradigm for inspiration. The common workflow of UI programming can be decomposed into four basic steps: create elements, abstract attributes, specify behaviors, and propagate changes. Based on these insights, we propose the following four-step workflow for Swarm UI programming: 1) creates shapes, 2) abstracts shape attributes as variables, 3) specifies data-bindings be- tween dynamic attributes, and 4) the system changes shapes in response to user inputs. With this workflow, a programmer can think in terms of high-level interface and interaction design to build interactive Swarm UI appli- cations, compared to existing, low-level, robot programming approaches.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-3.png" /></a>\n  </div>\n</div>\n<p>The workflow of Swarm UI programming is inspired by the existing UI programming paradigm. We first review the common workflow of UI programming and decompose it into four basic elements that represent high-level steps. Then we discuss how to apply this workflow to Swarm UI programming.\nAs we see in well-known design patterns for interactive UI ap- plications such as reactive programming paradigm, the Model-View-Controller, and the observer pattern, they share a com- mon workflow consisting of four basic elements: 1) create elements, 2) abstract attributes, 3) specify behaviors, and 4) propagate changes.</p>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-6.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-6.png" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-4.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-5.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-6.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-6.png" /></a>\n  </div>\n</div>\n<h1>Implementation</h1>\n<p>Reactile actuates a swarm of small magnetic markers to move on a 2D canvas with electromagnetic force. We designed and fabricated a board of electromagnetic coil arrays (3,200 coils), which covers an 80 cm x 40 cm area. Reactile tracks the marker positions and detects interactions between a user and swarm markers using a standard RGB camera and computer vision techniques. The system displays spatial information using a DLP projector to allow a programmer to see program states in the same physical context.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/coil.mp4" type="video/mp4"></source>\n</video>\n<br />\n<p>In Reactile, a user interface consists of a swarm of passive magnetic markers which move on a 2D workspace driven by electromagnetic forces. Reactile uses a grid of electromagnetic coils to actuate these magnetic markers. Running current through the circuit coils generates a local magnetic field so that each coil can attract a single magnet located within its area. Each coil is aligned with a certain offset in both horizontal and vertical direction with an effective area overlap, which allows the coil to attract the magnet located in the adjacent coil. We design electromagnetic coil arrays to be fabricated with a standard printed circuit board (PCB) manufacturing. This reduces the cost and fabrication complexity, making it easy for the actuation area to scale up.</p>\n<p>Our PCB design is a 4-layer board, and each layer contains a set of coils, each of which has an identical circular shape with a 15 mm diameter and a 2.5 mm overlap between nearby coils. Each coil has 15 turns with 0.203 mm (8 mils) spacing between lines, and the distance between centers of two coils is approximately 10 mm, which makes a 10 mm grid for attractive points. The final prototype covers an 80 cm x 40 cm area with 80 x 40 coils by aligning five identical boards horizontally. The fabrication of each board costs approximately $80 USD, including manufacturing of PCB and electronic components.</p>\n<br />\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/mechanism.mp4" type="video/mp4"></source>\n</video>',dir:"content/output/projects",base:"reactile.json",ext:".json",sourceBase:"reactile.md",sourceExt:".md"}},lJku:function(e){e.exports={id:"shapebots",name:"ShapeBots",description:"Shape-changing Swarm Robots",title:"ShapeBots: Shape-changing Swarm Robots",authors:["Ryo Suzuki","Clement Zheng","Yasuaki Kakehi","Tom Yeh","Ellen Yi-Luen Do","Mark D. Gross","Daniel Leithinger"],year:2019,booktitle:"In Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST '19)",publisher:"ACM, New York, NY, USA",pages:"1-13",doi:"https://doi.org/10.1145/3332165.3347911",conference:{name:"UIST 2019",fullname:"The ACM Symposium on User Interface Software and Technology (UIST 2019)",url:"http://uist.acm.org/uist2019"},pdf:"uist-2019-shapebots.pdf",slide:"uist-2019-shapebots-slide.pdf",video:"https://www.youtube.com/watch?v=cwPaof0kKdM",embed:"https://www.youtube.com/embed/cwPaof0kKdM",github:"https://github.com/ryosuzuki/shapebots",poster:"uist-2019-shapebots-poster.pdf",demo:"https://ryosuzuki.github.io/shapebots-simulator/",pageCount:13,slideCount:53,bodyContent:'# Abstract\n\nWe introduce *shape-changing swarm robots*. A swarm of self-transformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions. The modular design of each actuator enables various shapes and geometries of self-transformation. We illustrate potential application scenarios and discuss how this type of interface opens up possibilities for the future of ubiquitous and distributed shape-changing interfaces.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/top.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/top.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-1-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-1-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-1-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-1-1.jpg" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-2-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-2-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-2-3.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-2-3.jpg" /></a>\n  </div>\n</div>\n\n\n# Shape-changing Swarm Robots\n\nThis paper introduces **shape-changing swarm robots** for dis- tributed shape-changing interfaces. Shape-changing swarm robots can both **individually** and **collectively** change their shape, so that they can collectively present information, act as controllers, actuate objects, represent data, and provide dynamic physical affordances.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-3.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-3.png" /></a>\n  </div>\n</div>\n\nThis paper specifically focuses on the user interface aspect of such systems, which we refer to shape-changing swarm user interfaces. We identified three core aspects of shape-changing swarm robots: 1) locomotion, 2) self-transformation, and 3) collective behaviors of many individual elements.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-4.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-4.png" /></a>\n  </div>\n</div>\n\n# ShapeBots\n\n**ShapeBots are self-transformable swarm robots** with modular linear actuators. To enable a large deformation capability of tiny swarm robots, we developed a miniature reel-based linear actuator that is thin (2.5 cm) and fits into the small footprint (3 cm x 3 cm), while able to extend up to 20 cm in both horizontal and vertical directions.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/unit.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/unit.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-5-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-5-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-5-2.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-6-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-6-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-6-2.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-6-2.png" /></a>\n  </div>\n</div>\n\nThe modular design of each linear actuator unit enables the construction of various shapes and geometries of individual shape transformation (e.g., horizontal lines, vertical lines, curved lines, 2D area expan- sion, and 3D volumetric change). Based on these capabilities, we demonstrate application scenarios showing how a swarm of distributed self-transformable robots can support everyday interactions.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/transformation.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/transformation.mp4" type="video/mp4"></source>\n</video>\n\n\n# Tracking and Control\n\nTo track the position and orientation of the swarm robots, we used computer vision and a fiducial marker attached to the bottom of the robot. We used the ArUco fiducial marker printed on a sheet of paper and taped to the bottom of the robot. Our prototype used a 1.5 cm x 1.5 cm size marker with a 4 x 4 grid pattern, which can provide up to 50 unique patterns. For tracking software, we used the OpenCV library and ArUco python module. It can track the position of the markers at 60 frames per second.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/tracking.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/tracking.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-7-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-7-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-7-2.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-7-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-7-3.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-7-3.png" /></a>\n  </div>\n</div>\n\nTo enable the user to easily specify a target shape, we created a web-based interface where users draw a shape or upload an SVG image (Figure 10). The user draws a set of lines, then the main computer calculates target positions, orientations, and actuator lengths to start sending commands.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-8.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-8.png" /></a>\n  </div>\n</div>\n\n\nWe can use the same mechanism to track user input. The system supports four different types of user interaction that our system supports: place, move, orient, and pick-up.\nThe system recognizes as user inputs movement or rotation of a marker that it did not generate.\n\n\n\n# Applications: Robots as Dynamic Physical Media\nWe explore potential application scenarios for the future of human-robot interactions.\nOne interesting application area is to use these **robots as dynamic physical media**, such as showing **data visualization in the physical world**.\nFor example, ShapeBots on the USA map physicalize map data; each robot changes its height to show the population of the state it is on. Users can interact with the dataset by placing a new robot or moving a robot to a different state, and the robots update their physical forms to represent the respective population.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/dataphys.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/dataphys.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-3.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-3.jpg" /></a>\n  </div>\n</div>\n\nSimilarly, ShapeBots can provide a physical preview of a CAD design. ShapeBots physicalizes the actual size of the box. The design and physical rendering are tightly coupled; as the user changes the height of the box in CAD software, the ShapeBots change heights accordingly. The user can change the parameters of the design by moving robots in the physical space, and these changes are reflected in the CAD design.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/cad.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/cad.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-4.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-4.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-5.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-5.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-6.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-6.jpg" /></a>\n  </div>\n</div>\n\n# Applications: Robots as Ambient Assistants\n\nAnother practical aspect of ShapeBots is the ability to actuate objects and act as physical constraints. As an example, the video shows two robots extending their linear actuators to wipe debris off a table, clearing a workspace for the user.\nIn these scenarios, these robots can help as an **ambient assistant for everyday life**.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/cleaning.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/cleaning.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-11-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-11-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-11-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-11-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-11-3.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-11-3.jpg" /></a>\n  </div>\n</div>\n\nBy leveraging the capability of locomotion and height change of each robot, ShapeBots can create a dynamic fence to hide or encompass existing objects for affordances. For example, when the user pours hot coffee into a cup, the robots surround the cup and change their heights to create a vertical fence. The vertical fence visually and physically provides the affordance to indicate that the coffee is too hot and not ready to drink. Once it is ready, the robots start dispersing and allow the user to grab it. These scenarios illustrate how the distributed shape-changing robots can provide a new type of affordance, which we call **distributed dynamic physical affordances**.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/affordance.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/affordance.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-10-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-10-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-10-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-10-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-10-3.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-10-3.jpg" /></a>\n  </div>\n</div>\n\nShapeBots can also act as an interactive physical display. The following figures show how ShapeBots can render different shapes.\nWe highlight the advantage of ShapeBots for rendering contours compared to non self-transformable swarm robots. Using a software simulation, we demonstrate how ShapeBots renders an SVG input at different swarm sizes. You can also play with the [**explorable online simulator**](https://ryosuzuki.github.io/shapebots-simulator/) to see how these robots can render the shape ↓\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/explorable.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/explorable.mp4" type="video/mp4"></source>\n</video>\n\n\n\n# Future Work\n\nShapeBots is just a single example of shape-changing swarm robots.\nThere is a broader design space of shape- changing swarm user interfaces.\nAs future work, we are interested in exploring the different aspct of shape-changing swarm robots.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-12.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-12.png" /></a>\n  </div>\n</div>\n\n\x3c!--\n# Appendix\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/fabrication.mp4" type="video/mp4"></source>\n</video>\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/explorable.mp4" type="video/mp4"></source>\n</video>\n --\x3e',bodyHtml:'<h1>Abstract</h1>\n<p>We introduce <em>shape-changing swarm robots</em>. A swarm of self-transformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions. The modular design of each actuator enables various shapes and geometries of self-transformation. We illustrate potential application scenarios and discuss how this type of interface opens up possibilities for the future of ubiquitous and distributed shape-changing interfaces.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/top.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/top.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-1-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-1-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-1-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-1-1.jpg" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-2-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-2-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-2-3.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-2-3.jpg" /></a>\n  </div>\n</div>\n<h1>Shape-changing Swarm Robots</h1>\n<p>This paper introduces <strong>shape-changing swarm robots</strong> for dis- tributed shape-changing interfaces. Shape-changing swarm robots can both <strong>individually</strong> and <strong>collectively</strong> change their shape, so that they can collectively present information, act as controllers, actuate objects, represent data, and provide dynamic physical affordances.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-3.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-3.png" /></a>\n  </div>\n</div>\n<p>This paper specifically focuses on the user interface aspect of such systems, which we refer to shape-changing swarm user interfaces. We identified three core aspects of shape-changing swarm robots: 1) locomotion, 2) self-transformation, and 3) collective behaviors of many individual elements.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-4.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-4.png" /></a>\n  </div>\n</div>\n<h1>ShapeBots</h1>\n<p><strong>ShapeBots are self-transformable swarm robots</strong> with modular linear actuators. To enable a large deformation capability of tiny swarm robots, we developed a miniature reel-based linear actuator that is thin (2.5 cm) and fits into the small footprint (3 cm x 3 cm), while able to extend up to 20 cm in both horizontal and vertical directions.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/unit.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/unit.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-5-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-5-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-5-2.png" /></a>\n  </div>\n</div>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-6-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-6-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-6-2.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-6-2.png" /></a>\n  </div>\n</div>\n<p>The modular design of each linear actuator unit enables the construction of various shapes and geometries of individual shape transformation (e.g., horizontal lines, vertical lines, curved lines, 2D area expan- sion, and 3D volumetric change). Based on these capabilities, we demonstrate application scenarios showing how a swarm of distributed self-transformable robots can support everyday interactions.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/transformation.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/transformation.mp4" type="video/mp4"></source>\n</video>\n<h1>Tracking and Control</h1>\n<p>To track the position and orientation of the swarm robots, we used computer vision and a fiducial marker attached to the bottom of the robot. We used the ArUco fiducial marker printed on a sheet of paper and taped to the bottom of the robot. Our prototype used a 1.5 cm x 1.5 cm size marker with a 4 x 4 grid pattern, which can provide up to 50 unique patterns. For tracking software, we used the OpenCV library and ArUco python module. It can track the position of the markers at 60 frames per second.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/tracking.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/tracking.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-7-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-7-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-7-2.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-7-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-7-3.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-7-3.png" /></a>\n  </div>\n</div>\n<p>To enable the user to easily specify a target shape, we created a web-based interface where users draw a shape or upload an SVG image (Figure 10). The user draws a set of lines, then the main computer calculates target positions, orientations, and actuator lengths to start sending commands.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-8.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-8.png" /></a>\n  </div>\n</div>\n<p>We can use the same mechanism to track user input. The system supports four different types of user interaction that our system supports: place, move, orient, and pick-up.\nThe system recognizes as user inputs movement or rotation of a marker that it did not generate.</p>\n<h1>Applications: Robots as Dynamic Physical Media</h1>\n<p>We explore potential application scenarios for the future of human-robot interactions.\nOne interesting application area is to use these <strong>robots as dynamic physical media</strong>, such as showing <strong>data visualization in the physical world</strong>.\nFor example, ShapeBots on the USA map physicalize map data; each robot changes its height to show the population of the state it is on. Users can interact with the dataset by placing a new robot or moving a robot to a different state, and the robots update their physical forms to represent the respective population.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/dataphys.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/dataphys.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-3.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-3.jpg" /></a>\n  </div>\n</div>\n<p>Similarly, ShapeBots can provide a physical preview of a CAD design. ShapeBots physicalizes the actual size of the box. The design and physical rendering are tightly coupled; as the user changes the height of the box in CAD software, the ShapeBots change heights accordingly. The user can change the parameters of the design by moving robots in the physical space, and these changes are reflected in the CAD design.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/cad.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/cad.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-4.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-4.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-5.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-5.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-6.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-6.jpg" /></a>\n  </div>\n</div>\n<h1>Applications: Robots as Ambient Assistants</h1>\n<p>Another practical aspect of ShapeBots is the ability to actuate objects and act as physical constraints. As an example, the video shows two robots extending their linear actuators to wipe debris off a table, clearing a workspace for the user.\nIn these scenarios, these robots can help as an <strong>ambient assistant for everyday life</strong>.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/cleaning.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/cleaning.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-11-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-11-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-11-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-11-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-11-3.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-11-3.jpg" /></a>\n  </div>\n</div>\n<p>By leveraging the capability of locomotion and height change of each robot, ShapeBots can create a dynamic fence to hide or encompass existing objects for affordances. For example, when the user pours hot coffee into a cup, the robots surround the cup and change their heights to create a vertical fence. The vertical fence visually and physically provides the affordance to indicate that the coffee is too hot and not ready to drink. Once it is ready, the robots start dispersing and allow the user to grab it. These scenarios illustrate how the distributed shape-changing robots can provide a new type of affordance, which we call <strong>distributed dynamic physical affordances</strong>.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/affordance.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/affordance.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-10-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-10-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-10-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-10-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-10-3.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-10-3.jpg" /></a>\n  </div>\n</div>\n<p>ShapeBots can also act as an interactive physical display. The following figures show how ShapeBots can render different shapes.\nWe highlight the advantage of ShapeBots for rendering contours compared to non self-transformable swarm robots. Using a software simulation, we demonstrate how ShapeBots renders an SVG input at different swarm sizes. You can also play with the <a href="https://ryosuzuki.github.io/shapebots-simulator/"><strong>explorable online simulator</strong></a> to see how these robots can render the shape ↓</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/explorable.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/explorable.mp4" type="video/mp4"></source>\n</video>\n<h1>Future Work</h1>\n<p>ShapeBots is just a single example of shape-changing swarm robots.\nThere is a broader design space of shape- changing swarm user interfaces.\nAs future work, we are interested in exploring the different aspct of shape-changing swarm robots.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-12.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-12.png" /></a>\n  </div>\n</div>\n\x3c!--\n# Appendix\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/fabrication.mp4" type="video/mp4"></source>\n</video>\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/explorable.mp4" type="video/mp4"></source>\n</video>\n --\x3e',dir:"content/output/projects",base:"shapebots.json",ext:".json",sourceBase:"shapebots.md",sourceExt:".md"}},mRot:function(e){e.exports={id:"refazer",name:"Refazer",description:"Learning Syntactic Program Transformations from Examples",title:"Learning Syntactic Program Transformations from Examples",authors:["Reudismam Rolim","Gustavo Soares","Loris D'Antoni","Oleksandr Polozov","Sumit Gulwani","Rohit Gheyi","Ryo Suzuki","Björn Hartmann"],yera:2017,booktitle:"In Proceedings of the 39th International Conference on Software Engineering (ICSE '17)",publisher:"IEEE Press, Piscataway, NJ, USA",pages:"404-415",conference:{name:"ICSE 2017",fullname:"The International Conference on Software Engineering (ICSE 2017)",url:"http://icse2017.gatech.edu/"},pdf:"icse-2017-refazer.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=3097417",arxiv:"https://arxiv.org/abs/1608.09000",pageCount:12,slideCount:0,image:"refazer.png",bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"refazer.json",ext:".json",sourceBase:"refazer.md",sourceExt:".md"}},nWAr:function(e){e.exports={id:"atelier",name:"Atelier",description:"Repurposing Expert Crowdsourcing Tasks as Micro-internships",title:"Atelier: Repurposing Expert Crowdsourcing Tasks as Micro-internships",authors:["Ryo Suzuki","Niloufar Salehi","Michelle S. Lam","Juan C. Marroquin","Michael S. Bernstein"],year:2016,booktitle:"In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16)",publisher:"ACM, New York, NY, USA",pages:"2645-2656",conference:{name:"CHI 2016",fullname:"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2016)",url:"https://chi2016.acm.org/wp/"},pdf:"chi-2016-atelier.pdf",video:"https://www.youtube.com/watch?v=tBojZejtFQo",embed:"https://www.youtube.com/embed/tBojZejtFQo",slide:"chi-2016-atelier-slide.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=2858121",arxiv:"https://arxiv.org/abs/1602.06634",pageCount:12,slideCount:56,image:"atelier.jpg",bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"atelier.json",ext:".json",sourceBase:"atelier.md",sourceExt:".md"}},o0EK:function(e,t,a){var i={"./atelier.json":"nWAr","./dynablock.json":"GbvX","./flux-marker.json":"CTYI","./lift-tiles.json":"3RXq","./mixed-initiative.json":"PSd4","./morphio.json":"X0/d","./pep.json":"W/HP","./reactile.json":"jEBx","./refazer.json":"mRot","./roomshift.json":"9gtZ","./shapebots.json":"lJku","./tabby.json":"ejaO","./trace-diff.json":"Jg5j"};function o(e){var t=n(e);return a(t)}function n(e){var t=i[e];if(!(t+1)){var a=new Error("Cannot find module '"+e+"'");throw a.code="MODULE_NOT_FOUND",a}return t}o.keys=function(){return Object.keys(i)},o.resolve=n,e.exports=o,o.id="o0EK"},qg4i:function(e){e.exports=[{author:"Ryo Suzuki,",title:"Collective Shape-changing Interfaces",pdf:"uist-2019-collective.pdf",booktitle:"Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software & Technology",series:"UIST '19 Doctoral Consortium",year:2019,isbn:"978-1-4503-4656-6",location:"New Orleans, Louisiana, USA",pages:"2951--2958",numpages:2,doi:"xxx/xxx",acmid:3053187,publisher:"ACM",address:"New York, NY, USA",keywords:"shape-changing interfaces"},{author:"Ryo Suzuki, Ryosuke Nakayama, Dan Liu, Yasuaki Kakehi, Mark D. Gross, and Daniel Leithinger,",title:"LiftTiles: Modular and Reconfigurable Room-scale Shape Displays through Retractable Inflatable Actuators",pdf:"uist-2019-lift-tiles.pdf",poster:"uist-2019-lift-tiles-poster.pdf",booktitle:"Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software & Technology",series:"UIST '19 Adjunct",year:2019,isbn:"978-1-4503-4656-6",location:"New Orleans, Louisiana, USA",pages:"2951--2958",numpages:2,doi:"xxx/xxx",acmid:3053187,publisher:"ACM",address:"New York, NY, USA",keywords:"inflatables, shape-changing interfaces,large-scale interactions"},{author:"Ryo Suzuki, Gustavo Soares, Elena Glassman, Andrew Head, Loris D'Antoni, and Bjoern Hartmann,",title:"Exploring the Design Space of Automatically Synthesized Hints for Introductory Programming Assignments",pdf:"chi-2017-lbw.pdf",poster:"chi-2017-lbw-poster.pdf",booktitle:"Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems",series:"CHI EA '17",year:2017,isbn:"978-1-4503-4656-6",location:"Denver, Colorado, USA",pages:"2951--2958",numpages:8,url:"http://doi.acm.org/10.1145/3027063.3053187",doi:"10.1145/3027063.3053187",acmid:3053187,publisher:"ACM",address:"New York, NY, USA",keywords:"automated feedback, program synthesis, programming education"},{author:"Stanford Crowd Research Collective",title:"Daemo: A Self-Governed Crowdsourcing Marketplace",pdf:"uist-2015-daemo.pdf",booktitle:"Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology",series:"UIST '15 Adjunct",year:2015,isbn:"978-1-4503-3780-9",location:"Daegu, Kyungpook, Republic of Korea",pages:"101--102",numpages:2,url:"http://doi.acm.org/10.1145/2815585.2815739",doi:"10.1145/2815585.2815739",acmid:2815739,publisher:"ACM",address:"New York, NY, USA",keywords:"crowd research, crowd work., crowdsourcing"},{author:"Ryo Suzuki,",title:"Toward a Community Enhanced Programming Education",pdf:"chi-2015-workshop.pdf",slide:"chi-2015-workshop-slide.pdf",booktitle:"ACM CHI 2015 Symposium on Emerging Japanese HCI Research Collection",series:"CHI '15 Workshop",year:2015,location:"Seoul, Korea",publisher:"ACM",address:"New York, NY, USA"},{author:"Ryo Suzuki,",title:"Interactive and Collaborative Source Code Annotation",pdf:"icse-2015-cumiki.pdf",poster:"icse-2015-cumiki-poster.pdf",booktitle:"Proceedings of the 37th International Conference on Software Engineering - Volume 2",series:"ICSE '15 Poster",year:2015,location:"Florence, Italy",pages:"799--800",numpages:2,url:"http://dl.acm.org/citation.cfm?id=2819009.2819173",acmid:2819173,publisher:"IEEE Press",address:"Piscataway, NJ, USA"},{author:"Ryo Suzuki,",title:"Network Thresholds and Multiple Equilibria in the Diffusion of Content-Based Platforms",pdf:"wine-2014-network.pdf",poster:"wine-2014-network-poster.pdf",booktitle:"International Conference on Web and Internet Economics",series:"WINE '14 Poster",year:2014,location:"Beijing, China",publisher:"Springer",address:"New York, NY, USA"}]},vlRD:function(e,t,a){(window.__NEXT_P=window.__NEXT_P||[]).push(["/",function(){var e=a("RNiq");return{page:e.default||e}}])},yC4j:function(e){e.exports={bodyContent:"JST ACT-I (Mentor: Takeo Igarashi), 2018\n\nLeave a Nest Fellowship, 2018\n\nNakajima Foundation Scholarship, 2015\n\nJSPS Research Fellow DC1, 2013\n\nJASSO Fellow for Particularly Outstanding Students, 2013\n\nTohso Scholorship Fellowship, 2010\n\n[//]: # (JBMC Microsoft Award, 2013)\n\n[//]: # (Tech Crunch Tokyo 2013 Finalist, 2013)\n\n[//]: # (1st Prize Winner of University of Tokyo Entrepreneur Dojo, 2012)\n\n[//]: # (Honer of MOVIDA School founded by Taizo Son, 2012)",bodyHtml:"<p>JST ACT-I (Mentor: Takeo Igarashi), 2018</p>\n<p>Leave a Nest Fellowship, 2018</p>\n<p>Nakajima Foundation Scholarship, 2015</p>\n<p>JSPS Research Fellow DC1, 2013</p>\n<p>JASSO Fellow for Particularly Outstanding Students, 2013</p>\n<p>Tohso Scholorship Fellowship, 2010</p>\n",title:"JST ACT-I (Mentor: Takeo Igarashi), 2018",dir:"content/output",base:"fellowship.json",ext:".json",sourceBase:"fellowship.md",sourceExt:".md"}}},[["vlRD","5d41","9da1","ad9d"]]]);