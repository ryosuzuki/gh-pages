(window["webpackJsonp"] = window["webpackJsonp"] || []).push([["static/development/pages/index.js"],{

/***/ "./content/output/activities.json":
/*!****************************************!*\
  !*** ./content/output/activities.json ***!
  \****************************************/
/*! exports provided: bodyContent, bodyHtml, title, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"bodyContent":"**Program Committee**: CHI 2023, UIST 2022, ISMAR 2022, VRST 2022, CHI 2022, TEI 2022, UIST 2021, ISMAR 2021, VRST 2021, TEI 2021, GI 2020\n\n**Journal Editorial Board**: ACM Transactions of Human-Robot Interaction, Frontiers in Virtual Reality Haptics\n\n**Organizer:** UIST 2022 (Student Innovation Contest Chair), UIST 2021 (Student Innovation Contest Chair), CHI 2021 (Social Media Chair), CHI 2021 (Student Research Competition Jury), UIST 2016 (Web and Social Media Chair),\n\n**Review:** CHI 2016--2022, UIST 2017--2022, IMSUT 2020--2021, ISS 2021, ISMAR 2020--2022, VRST 2020--2022, CSCW 2021, TOCHI 2020--2021, PACM 2021, DIS 2021, C&C 2021, IEEE VR 2020, VL/HCC 2020, SCF 2019, SIGGRAPH ETech 2019--2021, GI 2020\n\n**Student Volunteer:** UIST 2016, CHI 2017","bodyHtml":"<p><strong>Program Committee</strong>: CHI 2023, UIST 2022, ISMAR 2022, VRST 2022, CHI 2022, TEI 2022, UIST 2021, ISMAR 2021, VRST 2021, TEI 2021, GI 2020</p>\n<p><strong>Journal Editorial Board</strong>: ACM Transactions of Human-Robot Interaction, Frontiers in Virtual Reality Haptics</p>\n<p><strong>Organizer:</strong> UIST 2022 (Student Innovation Contest Chair), UIST 2021 (Student Innovation Contest Chair), CHI 2021 (Social Media Chair), CHI 2021 (Student Research Competition Jury), UIST 2016 (Web and Social Media Chair),</p>\n<p><strong>Review:</strong> CHI 2016--2022, UIST 2017--2022, IMSUT 2020--2021, ISS 2021, ISMAR 2020--2022, VRST 2020--2022, CSCW 2021, TOCHI 2020--2021, PACM 2021, DIS 2021, C&amp;C 2021, IEEE VR 2020, VL/HCC 2020, SCF 2019, SIGGRAPH ETech 2019--2021, GI 2020</p>\n<p><strong>Student Volunteer:</strong> UIST 2016, CHI 2017</p>\n","title":"<strong>Program Committee","dir":"content/output","base":"activities.json","ext":".json","sourceBase":"activities.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/experience.json":
/*!****************************************!*\
  !*** ./content/output/experience.json ***!
  \****************************************/
/*! exports provided: 0, 1, 2, 3, 4, 5, 6, 7, 8, default */
/***/ (function(module) {

module.exports = [{"period":"January, 2021 --- Current","role":"Assistant Professor","logo":"ucalgary.png","institute":{"name":"University of Calgary","url":"https://www.ucalgary.ca/"},"lab":{"name":"HCI Group","url":"https://ilab.cpsc.ucalgary.ca/"},"advisors":[{"name":"Computer Science","url":"https://science.ucalgary.ca/computer-science"}]},{"period":"August, 2015 --- August, 2020","role":"Research Assistant","logo":"cu-boulder.png","institute":{"name":"CU Boulder","url":"https://colorado.edu"},"lab":{"name":"THING Lab","url":"https://www.colorado.edu/atlas/thing-lab"},"advisors":[{"name":"Daniel Lightinger","url":"http://leithinger.com/"},{"name":"Mark D. Gross","url":"http://mdgross.net/"},{"name":"Tom Yeh","url":"http://tomyeh.info/"}]},{"period":"May, 2020 --- August, 2020","role":"Research Intern","logo":"microsoft.png","institute":{"name":"Microsoft Research","url":"https://www.microsoft.com/en-us/research/"},"lab":{"name":"EPIC Group","url":"https://www.microsoft.com/en-us/research/group/epic/"},"advisors":[{"name":"Mar Gonzalez Franco","url":"https://www.microsoft.com/en-us/research/people/margon/"},{"name":"Eyal Ofek","url":"https://www.microsoft.com/en-us/research/people/eyalofek/"},{"name":"Mike Sinclair","url":"https://www.microsoft.com/en-us/research/people/sinclair/"},{"name":"Ken Hinckley","url":"https://www.microsoft.com/en-us/research/people/kenh/"}]},{"period":"May, 2019 --- August, 2019","role":"Research Intern","logo":"adobe.png","institute":{"name":"Adobe Research","url":"https://colorado.edu"},"lab":{"name":"Creative Intelligence Lab","url":"https://research.adobe.com/"},"advisors":[{"name":"Rubaiat Habib","url":"https://rubaiathabib.me/"},{"name":"Li-Yi Wei","url":"https://www.liyiwei.org/"},{"name":"Stephen Diverdi","url":"http://www.stephendiverdi.com/"},{"name":"Danny Kaufman","url":"http://dannykaufman.io/"}]},{"period":"December, 2017 --- October, 2018","role":"Visiting Researcher","logo":"ut.png","institute":{"name":"University of Tokyo","url":"https://colorado.edu"},"lab":{"name":"ERATO UIN","url":"http://www.jst.go.jp/erato/kawahara"},"advisors":[{"name":"Yasuaki Kakehi","url":"http://xlab.iii.u-tokyo.ac.jp/"},{"name":"Yoshihiro Kawahara","url":"http://www.akg.t.u-tokyo.ac.jp/"},{"name":"Ryuma Niiyama","url":"https://scholar.google.co.jp/citations?user=0NMf5sgAAAAJ&hl=en"}]},{"period":"May, 2016 --- August, 2016","role":"Research Intern","logo":"uc-berkeley.png","institute":{"name":"UC Berkeley","url":null},"lab":{"name":"BiD Lab","url":"http://bid.berkeley.edu/"},"advisors":[{"name":"Bjoern Hartmann","url":"http://people.eecs.berkeley.edu/~bjoern/"}]},{"period":"May, 2015 --- August, 2015","role":"Research Intern","logo":"stanford-2.png","institute":{"name":"Stanford University","url":"https://stanford.edu"},"lab":{"name":"HCI Group","url":"http://hci.stanford.edu/"},"advisors":[{"name":"Michael Bernstein","url":"http://hci.stanford.edu/msb/"}]},{"period":"October, 2014 --- May, 2015","role":"Research Assistant","logo":"ut.png","institute":{"name":"University of Tokyo","url":null},"lab":{"name":"IIS Lab","url":"http://iis-lab.org/"},"advisors":[{"name":"Koji Yatani","url":"http://iis-lab.org/member/koji-yatani/"}]},{"period":"December, 2014 --- March, 2015","role":"Research Intern","logo":"aist.png","institute":{"name":"AIST","url":null},"lab":{"name":"Media Interaction","url":"https://staff.aist.go.jp/m.goto/MIG/index-j.html"},"advisors":[{"name":"Jun Kato","url":"http://junkato.jp/"}]}];

/***/ }),

/***/ "./content/output/fellowship.json":
/*!****************************************!*\
  !*** ./content/output/fellowship.json ***!
  \****************************************/
/*! exports provided: bodyContent, bodyHtml, title, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"bodyContent":"## Funding\n\n**Mitacs Globalink Research Award**, $6,000\nhttps://www.mitacs.ca/en/programs/globalink/globalink-research-award\n\n**Mitacs Globalink Research Internship Funding**\nhttps://www.mitacs.ca/en/programs/globalink/globalink-research-internship\n\n**Snap Creative Challenge Funding**, $15,000\nhttps://www.snapcreativechallenge.com/\n\n**NSERC USRA**, $6,000 x 2\n\n**Tohoku University Research Institute of Electrical Communication, Cooperative Research Projects**, $18,000\nhttps://www.riec.tohoku.ac.jp/en/nation-wide/koubo/r3/\n\n**NSERC Discovery Grant Funding**, $145,000\n\n**Innovation Research Funding** - Ministry of Internal Affairs and Communications in Japan, 2020\n(*Title: Adaptive Physical Environments with Distributed Swarm Robots. , $30,000*)\n\n**Adobe Gift Funding**, 2019 (*$5,000*)\n\n**ACT-I Funding for Young Scholars** - JST, 2018 (*Title: Dynamic Physical Interfaces, $30,000 and Mentorship Opportunity - My mentor: Takeo Igarashi*)\n\n**Emerging Research Funding for AI and Interdisciplinary Research** - Leave a Nest Foundation, 2018 (*Title: Programmable Architecture with Soft Inflatable Actuator, $5,000*\n\n**KAKENHI Grants-in-Aid for Scientific Research** - JSPS, 2013-2015 (*Title: Network-based Diffusion Analysis for Online Community, $40,000*)\n\n## Fellowship\n\n**CU Boulder Travel Grant**, 2015-2020 (*$500-$1,200 for each conference travel*)\n\n**Nakajima Foundation Scholarship**, 2015-2020 (*$120,000 stipend for 5 years and 2 years tuition coverage*)\n\n**JSPS Research Fellow DC1**, 2013 (*$72,000 stipend for 2 years*)\n\n**JASSO Fellow**, 2013 (*Total Exemption for Outstanding Students - $20,000 stipend\nfor 2 years*)\n\n**Tohso Fellowship**, 2010 (*$3,600*)\n\n[//]: # (JBMC Microsoft Award, 2013)\n\n[//]: # (Tech Crunch Tokyo 2013 Finalist, 2013)\n\n[//]: # (1st Prize Winner of University of Tokyo Entrepreneur Dojo, 2012)\n\n[//]: # (Honer of MOVIDA School founded by Taizo Son, 2012)","bodyHtml":"<h2>Funding</h2>\n<p><strong>Mitacs Globalink Research Award</strong>, $6,000\nhttps://www.mitacs.ca/en/programs/globalink/globalink-research-award</p>\n<p><strong>Mitacs Globalink Research Internship Funding</strong>\nhttps://www.mitacs.ca/en/programs/globalink/globalink-research-internship</p>\n<p><strong>Snap Creative Challenge Funding</strong>, $15,000\nhttps://www.snapcreativechallenge.com/</p>\n<p><strong>NSERC USRA</strong>, $6,000 x 2</p>\n<p><strong>Tohoku University Research Institute of Electrical Communication, Cooperative Research Projects</strong>, $18,000\nhttps://www.riec.tohoku.ac.jp/en/nation-wide/koubo/r3/</p>\n<p><strong>NSERC Discovery Grant Funding</strong>, $145,000</p>\n<p><strong>Innovation Research Funding</strong> - Ministry of Internal Affairs and Communications in Japan, 2020\n(<em>Title: Adaptive Physical Environments with Distributed Swarm Robots. , $30,000</em>)</p>\n<p><strong>Adobe Gift Funding</strong>, 2019 (<em>$5,000</em>)</p>\n<p><strong>ACT-I Funding for Young Scholars</strong> - JST, 2018 (<em>Title: Dynamic Physical Interfaces, $30,000 and Mentorship Opportunity - My mentor: Takeo Igarashi</em>)</p>\n<p><strong>Emerging Research Funding for AI and Interdisciplinary Research</strong> - Leave a Nest Foundation, 2018 (<em>Title: Programmable Architecture with Soft Inflatable Actuator, $5,000</em></p>\n<p><strong>KAKENHI Grants-in-Aid for Scientific Research</strong> - JSPS, 2013-2015 (<em>Title: Network-based Diffusion Analysis for Online Community, $40,000</em>)</p>\n<h2>Fellowship</h2>\n<p><strong>CU Boulder Travel Grant</strong>, 2015-2020 (<em>$500-$1,200 for each conference travel</em>)</p>\n<p><strong>Nakajima Foundation Scholarship</strong>, 2015-2020 (<em>$120,000 stipend for 5 years and 2 years tuition coverage</em>)</p>\n<p><strong>JSPS Research Fellow DC1</strong>, 2013 (<em>$72,000 stipend for 2 years</em>)</p>\n<p><strong>JASSO Fellow</strong>, 2013 (<em>Total Exemption for Outstanding Students - $20,000 stipend\nfor 2 years</em>)</p>\n<p><strong>Tohso Fellowship</strong>, 2010 (<em>$3,600</em>)</p>\n","title":"Funding","dir":"content/output","base":"fellowship.json","ext":".json","sourceBase":"fellowship.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/ilab-summary.json":
/*!******************************************!*\
  !*** ./content/output/ilab-summary.json ***!
  \******************************************/
/*! exports provided: fileMap, sourceFileArray, default */
/***/ (function(module) {

module.exports = {"fileMap":{"content/output/booktitles.json":{"CHI":{"booktitle":"Proceedings of the CHI Conference on Human Factors in Computing Systems","publisher":"ACM, New York, NY, USA"},"CHI EA":{"booktitle":"Extended Abstracts of the CHI Conference on Human Factors in Computing Systems","publisher":"ACM, New York, NY, USA"},"UIST":{"booktitle":"Proceedings of the Annual ACM Symposium on User Interface Software and Technology","publisher":"ACM, New York, NY, USA"},"IMWUT":{"booktitle":"Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies","publisher":"ACM, New York, NY, USA"},"DIS":{"booktitle":"Proceedings of the ACM on Designing Interactive Systems Conference","publisher":"ACM, New York, NY, USA"},"MobileHCI":{"booktitle":"Proceedings of the International Conference on Human-Computer Interaction with Mobile Devices and Services","publisher":"ACM, New York, NY, USA"},"TEI":{"booktitle":"Proceedings of the International Conference on Tangible, Embedded, and Embodied Interaction","publisher":"ACM, New York, NY, USA"},"HRI":{"booktitle":"Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction","publisher":"ACM, New York, NY, USA"},"VR":{"booktitle":"Proceedings of the IEEE Conference on Virtual Reality and 3D User Interfaces","publisher":"IEEE, New York, NY, USA"},"TVCG":{"booktitle":"IEEE Transactions on Visualization and Computer Graphics","publisher":"IEEE, New York, NY, USA"},"IROS":{"booktitle":"Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems","publisher":"IEEE, New York, NY, USA"},"C&C":{"booktitle":"Proceedings of the ACM on Creativity and Cognition","publisher":"ACM, New York, NY, USA"},"dir":"content/output","base":"booktitles.json","ext":".json","sourceBase":"booktitles.yaml","sourceExt":".yaml"},"content/output/facility.json":{"Prototyping Tools":[{"name":"Form 3","img":"form-3","url":"https://formlabs.com/3d-printers/form-3/"},{"name":"Form Wash","img":"form-wash","url":"https://formlabs.com/wash-cure/"},{"name":"Form Cure","img":"form-cure","url":"https://formlabs.com/wash-cure/"},{"name":"Cetus MK3","img":"cetus","url":"https://shop.tiertime.com/product/cetus-3d-printer-mk3/"},{"name":"Ultimaker 3","img":"ultimaker","url":"https://ultimaker.com/3d-printers/ultimaker-3"},{"name":"X-Carve CNC Machine 1000mm","img":"x-carve","url":"https://www.inventables.com/technologies/x-carve"},{"name":"Mayku Desktop Vacuum Former","img":"mayku","url":"https://www.mayku.me/"},{"name":"Silver Bullet Die Cutter","img":"silver-bullet","url":"https://silverbulletcutters.com/"},{"name":"Epilog Fusion M2 40inch Laser Cutter","img":"epilog","url":"https://www.epiloglaser.com/laser-machines/fusion-laser-series.htm"}],"Electronics":[{"name":"Voltera V-One PCB Printer","img":"voltera","url":"https://www.voltera.io/"},{"name":"Bantam PCB Mill","img":"bantam","url":"https://www.bantamtools.com/"},{"name":"Weller WE1010NA","img":"wellner","url":"https://www.weller-tools.com/we1010na/"},{"name":"Eventek KPS305D DC Power Supply","img":"eventek","url":"https://www.amazon.com/dp/B071RNT1CD"},{"name":"Reflow Oven T962","img":"t962","url":"https://www.amazon.com/dp/B01LZYEF90"},{"name":"Andonstar AD407 Digital Microscope","img":"andonstar","url":"https://www.amazon.com/dp/B07VK52X9C"}],"Kniiting":[{"name":"Brother 930E Knitting Machine","img":"brother"},{"name":"Pfaff Creative 4.5 Embroidery Machine","img":"pfaff","url":"http://www.pfaff.com/en-CA/Machines/creative-4-5"}],"Power Tools":[{"name":"Hitachi C10FCG","img":"hitachi","url":"https://www.amazon.com/dp/B07217ZVP5"},{"name":"WEN 4208 Drill Press","img":"drill-press","url":"https://www.amazon.com/dp/B00HQONFVE"},{"name":"WEN 3959 Band Saw","img":"band-saw","url":"https://www.amazon.com/dp/B077QMBTLP"},{"name":"Black+Decker 20V Drill","img":"black-decker","url":"https://www.amazon.com/dp/B00C625KVE"}],"AR/VR":[{"name":"Vicon Motion Capture","img":"vicon","url":"https://www.vicon.com/"},{"name":"Oculus Quest","img":"oculus-quest","url":"https://www.oculus.com/quest/"},{"name":"Hololens 2","img":"hololens-2","url":"https://www.microsoft.com/en-us/hololens/"},{"name":"Hololens 1","img":"hololens-1","url":"https://docs.microsoft.com/en-us/hololens/hololens1-hardware"},{"name":"Azure Kinect DK","img":"azure-kinect","url":"https://azure.microsoft.com/en-us/services/kinect-dk/"}],"Robotics":[{"name":"Baxter Robot","img":"baxter","url":"https://www.rethinkrobotics.com/"},{"name":"Sony TOIO","img":"toio","url":"https://www.sony.net/SonyInfo/design/stories/toio/"}],"Photography":[{"name":"Sony a7iii","img":"sony-a7","url":"https://www.sony.com/electronics/interchangeable-lens-cameras/ilce-7m3-body-kit"},{"name":"Tamron 28-75mm F/2.8","img":"tamron","url":"https://www.tamron-usa.com/product/lenses/a036.html"},{"name":"DJI Ronin SC 3-Axis Gimbal","img":"dji","url":"https://www.dji.com/ronin-sc"},{"name":"COMAN KX3636 74inch","img":"conman","url":"https://www.amazon.com/dp/B01NA8PIZX"},{"name":"Neewer Camera Slider 39inch","img":"camera-slider","url":"https://www.amazon.com/dp/B07WHXKLFV"},{"name":"Emart Backdrop Stand","img":"emart","url":"https://www.amazon.com/dp/B074R9T4FX"},{"name":"Newer 10x20ft Backdrop","img":"newer","url":"https://www.amazon.com/dp/B00SR28XPM"},{"name":"Hpusn Softbox","img":"hpusn","url":"https://www.amazon.com/dp/B07NBP6D98"},{"name":"LimoStudio Foldable Studio","img":"limostudio","url":"https://www.amazon.com/dp/B00OY9DOCY"}],"dir":"content/output","base":"facility.json","ext":".json","sourceBase":"facility.yaml","sourceExt":".yaml"},"content/output/labs.json":[{"id":"utouch","description":"Physical Interaction and Human-Robot Interaction","prof":"Ehud Sharlin","url":"https://utouch.cpsc.ucalgary.ca/"},{"id":"curiosity","description":"Human-Centered Design for Creativity & Curiosity","prof":"Lora Oehlberg","url":"http://pages.cpsc.ucalgary.ca/~lora.oehlberg/"},{"id":"dataexperience","description":"Visual Data-driven Tools and Experiences","prof":"Wesley Willett","url":"https://dataexperience.cpsc.ucalgary.ca/"},{"id":"suzuki","description":"Programmable Reality Lab - Tangible, AR/VR, and Robotics","prof":"Ryo Suzuki","url":"https://programmable-reality-lab.github.io/"},{"id":"c3-lab","description":"Tech to Bridge Cultural Barriers, Improve Collaboration, & Build Community","prof":"Helen Ai He","url":"https://helenaihe.com/research/"},{"id":"grouplab","description":"Research in HCI, CSCW, and UbiComp","prof":"Saul Greenberg (Emeritus)","url":"http://grouplab.cpsc.ucalgary.ca/"},{"id":"ricelab","description":"Rethinking Interaction, Collaboration, & Engagement","prof":"Anthony Tang (Adjunct - University of Toronto)","url":"https://ricelab.github.io/"},{"id":"innovis","description":"Innovations in Visualization Laboratory","prof":"Sheelagh Carpendale (Adjunct - Simon Fraser University)","url":"http://sheelaghcarpendale.ca/"}],"content/output/news.json":[{"date":"2020-08-07","text":"David Ledo defended his PhD dissertation","icon":"fas fa-graduation-cap"},{"date":"2020-07-10","text":"One paper accepted to UIST 2020","image":"uist-2020.jpg"},{"date":"2020-06-07","text":"One paper accepted to IROS 2020","image":"iros-2020.jpg"},{"date":"2020-03-07","text":"One paper accepted to IMWUT 2020","image":"imwut.jpg"},{"date":"2020-02-05","text":"Four papers accepted to CHI 2020","image":"chi-2020.jpg"}],"content/output/people/abhinav-pillai.json":{"name":"Abhinav Pillai","type":"undergrad","email":"abhinav.arunpillai@ucalgary.ca","linkedin":"https://www.linkedin.com/in/abhinav-pillai/","dir":"content/output/people","base":"abhinav-pillai.json","ext":".json","sourceBase":"abhinav-pillai.yaml","sourceExt":".yaml"},"content/output/people/aditya-gunturu.json":{"name":"Aditya Gunturu","type":"master","url":"https://adigunturu.com/","email":"aditya.gunturu@ucalgary.ca","linkedin":"https://www.linkedin.com/in/adigunturu/","github":"https://github.com/adigunturu","dir":"content/output/people","base":"aditya-gunturu.json","ext":".json","sourceBase":"aditya-gunturu.yaml","sourceExt":".yaml"},"content/output/people/aditya-shekhar-nittala.json":{"name":"Aditya Shekhar Nittala","type":"faculty","title":"Assistant Professor","keywords":["Wearable Computing","Fabrication","Interaction Techniques"],"order":6,"url":"https://sites.google.com/site/adityanittala/","scholar":"https://scholar.google.com/citations?user=pDSbjBsAAAAJ","email":"anittala@ucalgary.ca","linkedin":"https://www.linkedin.com/in/adityashekharn","dir":"content/output/people","base":"aditya-shekhar-nittala.json","ext":".json","sourceBase":"aditya-shekhar-nittala.yaml","sourceExt":".yaml"},"content/output/people/adnan-karim.json":{"name":"Adnan Karim","type":"master","url":"https://sites.google.com/view/adnankarim/","email":"adnan.karim@ucalgary.ca","linkedin":"https://www.linkedin.com/in/adnan-karim-757b98101/","dir":"content/output/people","base":"adnan-karim.json","ext":".json","sourceBase":"adnan-karim.yaml","sourceExt":".yaml"},"content/output/people/anthony-tang.json":{"name":"Anthony Tang","type":"faculty","title":"Adjunct Associate Professor","keywords":["Mixed Reality","CSCW"],"order":7,"url":"https://hcitang.github.io/","scholar":"https://scholar.google.com/citations?user=RG1EQowAAAAJ","twitter":"https://twitter.com/proclubboy","github":"http://github.com/hcitang","dir":"content/output/people","base":"anthony-tang.json","ext":".json","sourceBase":"anthony-tang.yaml","sourceExt":".yaml"},"content/output/people/april-zhang.json":{"name":"April Zhang","type":"master","url":"https://aprilzhang.design/","dir":"content/output/people","base":"april-zhang.json","ext":".json","sourceBase":"april-zhang.yaml","sourceExt":".yaml"},"content/output/people/ashratuz-zavin-asha.json":{"name":"Ashratuz Zavin Asha","type":"phd","url":"https://sites.google.com/view/ashratuzzavinasha","scholar":"https://scholar.google.com/citations?user=E7gtMMoAAAAJ","dir":"content/output/people","base":"ashratuz-zavin-asha.json","ext":".json","sourceBase":"ashratuz-zavin-asha.yaml","sourceExt":".yaml"},"content/output/people/bheesha-kumari.json":{"name":"Bheesha Kumari","type":"undergrad","email":"bheesha.kumari@ucalgary.ca","linkedin":"https://www.linkedin.com/in/bheesha-kumari-/","dir":"content/output/people","base":"bheesha-kumari.json","ext":".json","sourceBase":"bheesha-kumari.yaml","sourceExt":".yaml"},"content/output/people/bon-adriel-aseniero.json":{"name":"Bon Adriel Aseniero","type":"alumni","past":"phd","now":"Autodesk Research","url":"http://bonadriel.com/","scholar":"https://scholar.google.com/citations?user=V4nRMoMAAAAJ","twitter":"https://twitter.com/HexenKoenig","facebook":"https://www.facebook.com/bonadriel","linkedin":"https://www.linkedin.com/in/bon-adriel-aseniero-47140560/","dir":"content/output/people","base":"bon-adriel-aseniero.json","ext":".json","sourceBase":"bon-adriel-aseniero.yaml","sourceExt":".yaml"},"content/output/people/brennan-jones.json":{"name":"Brennan Jones","type":"alumni","past":"phd","now":"Meta Reality Labs","url":"https://brennanjones.com/","scholar":"https://scholar.google.ca/citations?user=yzxiadIAAAAJ","dir":"content/output/people","base":"brennan-jones.json","ext":".json","sourceBase":"brennan-jones.yaml","sourceExt":".yaml"},"content/output/people/carl-gutwin.json":{"name":"Carl Gutwin","type":"alumni","past":"phd","now":"University of Saskatchewan","url":"https://www.cs.usask.ca/~gutwin/","scholar":"https://scholar.google.ca/citations?user=ZWsIEYcAAAAJ","dir":"content/output/people","base":"carl-gutwin.json","ext":".json","sourceBase":"carl-gutwin.yaml","sourceExt":".yaml"},"content/output/people/carman-neustaedter.json":{"name":"Carman Neustaedter","type":"alumni","past":"phd","now":"Simon Fraser University","url":"https://carmster.com/","twitter":"https://mobile.twitter.com/dr_carmster","scholar":"https://scholar.google.ca/citations?user=krQJJwIAAAAJ","linkedin":"https://www.linkedin.com/in/dr-carman-neustaedter-3666591","dir":"content/output/people","base":"carman-neustaedter.json","ext":".json","sourceBase":"carman-neustaedter.yaml","sourceExt":".yaml"},"content/output/people/carmen-hull.json":{"name":"Carmen Hull","type":"alumni","past":"phd","now":"Northeastern University","url":"https://www.carmenhull.com/","dir":"content/output/people","base":"carmen-hull.json","ext":".json","sourceBase":"carmen-hull.yaml","sourceExt":".yaml"},"content/output/people/charlotte-tang.json":{"name":"Charlotte Tang","type":"alumni","past":"phd","now":"University of Michigan","url":"https://charlottetang.ca/","scholar":"https://scholar.google.com/citations?user=RYkK_owAAAAJ","linkedin":"https://www.linkedin.com/in/charlottetang/","dir":"content/output/people","base":"charlotte-tang.json","ext":".json","sourceBase":"charlotte-tang.yaml","sourceExt":".yaml"},"content/output/people/christian-frisson.json":{"name":"Christian Frisson","type":"alumni","past":"postdoc","now":"Society for Arts and Technology","url":"https://frisson.re","scholar":"https://scholar.google.com/citations?user=sZVn1V4AAAAJ","twitter":"https://twitter.com/TuyleriDikenli","facebook":"https://www.facebook.com/christian.frisson","linkedin":"https://www.linkedin.com/in/christianfrisson","dir":"content/output/people","base":"christian-frisson.json","ext":".json","sourceBase":"christian-frisson.yaml","sourceExt":".yaml"},"content/output/people/christopher-rodriguez.json":{"name":"Christopher Rodriguez","type":"alumni","past":"undergrad","email":"christopher.rodrigue@ucalgary.ca","linkedin":"https://www.linkedin.com/in/christopher-rodriguez-74259217a/","dir":"content/output/people","base":"christopher-rodriguez.json","ext":".json","sourceBase":"christopher-rodriguez.yaml","sourceExt":".yaml"},"content/output/people/christopher-smith.json":{"name":"Christopher Smith","type":"master","url":"https://sites.google.com/cse.uiu.ac.bd/ashratuzzavinasha","linkedin":"https://www.linkedin.com/in/christopher-smith-uofc/","dir":"content/output/people","base":"christopher-smith.json","ext":".json","sourceBase":"christopher-smith.yaml","sourceExt":".yaml"},"content/output/people/colin-auyeung.json":{"name":"Colin Au Yeung","type":"master","url":"https://colinauyeung.github.io/","email":"colinauyeung@ucalgary.ca","dir":"content/output/people","base":"colin-auyeung.json","ext":".json","sourceBase":"colin-auyeung.yaml","sourceExt":".yaml"},"content/output/people/dane-bertram.json":{"name":"Dane Bertram","type":"alumni","past":"master","now":"YNAB","url":"https://danebertram.com/","linkedin":"https://www.linkedin.com/in/danebertram/","github":"https://github.com/dbertram","dir":"content/output/people","base":"dane-bertram.json","ext":".json","sourceBase":"dane-bertram.yaml","sourceExt":".yaml"},"content/output/people/darcy-norman.json":{"name":"D'Arcy Norman","type":"alumni","past":"phd","now":"University of Calgary","url":"https://darcynorman.net/","twitter":"https://twitter.com/realdlnorman","dir":"content/output/people","base":"darcy-norman.json","ext":".json","sourceBase":"darcy-norman.yaml","sourceExt":".yaml"},"content/output/people/david-ledo.json":{"name":"David Ledo","type":"alumni","past":"phd","now":"Autodesk Research","url":"https://www.davidledo.com/","scholar":"https://scholar.google.com/citations?user=V_2BZDoAAAAJ","dir":"content/output/people","base":"david-ledo.json","ext":".json","sourceBase":"david-ledo.yaml","sourceExt":".yaml"},"content/output/people/desmond-larsen-rosner.json":{"name":"Desmond Larsen-Rosner","type":"master","email":"drlarsen@ucalgary.ca","github":"https://github.com/desmondlr","linkedin":"https://www.linkedin.com/in/drplr/","dir":"content/output/people","base":"desmond-larsen-rosner.json","ext":".json","sourceBase":"desmond-larsen-rosner.yaml","sourceExt":".yaml"},"content/output/people/dinmukhammed-mukashev.json":{"name":"Dinmukhammed Mukashev","type":"master","email":"dimash.mukashev@ucalgary.ca","scholar":"https://scholar.google.com/citations?user=rUEDTOsAAAAJ","linkedin":"https://www.linkedin.com/in/dinmukhammed-mukashev-120053129","dir":"content/output/people","base":"dinmukhammed-mukashev.json","ext":".json","sourceBase":"dinmukhammed-mukashev.yaml","sourceExt":".yaml"},"content/output/people/donald-cox.json":{"name":"Donald Cox","type":"alumni","past":"master","now":"Ping Identity","linkedin":"https://www.linkedin.com/in/donald-cox-6455435/","dir":"content/output/people","base":"donald-cox.json","ext":".json","sourceBase":"donald-cox.yaml","sourceExt":".yaml"},"content/output/people/doug-schaeffer.json":{"name":"Doug Schaeffer","type":"alumni","past":"master","now":"R2 Solutions","linkedin":"https://www.linkedin.com/in/dougschaffer/","dir":"content/output/people","base":"doug-schaeffer.json","ext":".json","sourceBase":"doug-schaeffer.yaml","sourceExt":".yaml"},"content/output/people/edward-tse.json":{"name":"Edward Tse","type":"alumni","past":"phd","now":"NUITEQ","url":"https://edwardtse.com/","linkedin":"https://ca.linkedin.com/in/edward-tse","twitter":"https://twitter.com/DoctorET","dir":"content/output/people","base":"edward-tse.json","ext":".json","sourceBase":"edward-tse.yaml","sourceExt":".yaml"},"content/output/people/ehud-sharlin.json":{"name":"Ehud Sharlin","type":"faculty","title":"Professor","keywords":["HRI","Robots","Drones"],"order":1,"url":"http://contacts.ucalgary.ca/info/cpsc/profiles/102-3264","scholar":"https://scholar.google.ca/citations?hl=en&user=eAFxlZIAAAAJ","dir":"content/output/people","base":"ehud-sharlin.json","ext":".json","sourceBase":"ehud-sharlin.yaml","sourceExt":".yaml"},"content/output/people/faiz-marsad.json":{"name":"Faiz Marsad","type":"undergrad","email":"faiz.marsad@ucalgary.ca","dir":"content/output/people","base":"faiz-marsad.json","ext":".json","sourceBase":"faiz-marsad.yaml","sourceExt":".yaml"},"content/output/people/georgina-freeman.json":{"name":"Georgina Freeman","type":"phd","dir":"content/output/people","base":"georgina-freeman.json","ext":".json","sourceBase":"georgina-freeman.yaml","sourceExt":".yaml"},"content/output/people/grace-ferguson.json":{"name":"Grace Ferguson","type":"alumni","past":"undergrad","dir":"content/output/people","base":"grace-ferguson.json","ext":".json","sourceBase":"grace-ferguson.yaml","sourceExt":".yaml"},"content/output/people/gregor-mcewan.json":{"name":"Gregor McEwan","type":"alumni","past":"master","now":"Modail Mara","url":"https://www.modailmara.ca/whoweare","scholar":"https://scholar.google.com/citations?user=6xdmrPgAAAAJ","linkedin":"https://www.linkedin.com/in/gregor-mcewan-5076992/","dir":"content/output/people","base":"gregor-mcewan.json","ext":".json","sourceBase":"gregor-mcewan.yaml","sourceExt":".yaml"},"content/output/people/harrison-chen.json":{"name":"Harrison Chen","type":"alumni","past":"undergrad","url":"https://harrisoneverettchen.com","email":"hechen@ucalgary.ca","github":"https://github.com/Hechen93","linkedin":"https://www.linkedin.com/in/harrison-chen-a90b5569/","dir":"content/output/people","base":"harrison-chen.json","ext":".json","sourceBase":"harrison-chen.yaml","sourceExt":".yaml"},"content/output/people/helen-ai-he.json":{"name":"Helen Ai He","type":"faculty","title":"Assistant Professor","keywords":["Inclusive Design","CSCW"],"order":5,"url":"https://helenaihe.com/research","scholar":"https://scholar.google.com/citations?user=FlMKf0gAAAAJ","twitter":"https://twitter.com/helenaihe","email":"helen.he1@ucalgary.ca","dir":"content/output/people","base":"helen-ai-he.json","ext":".json","sourceBase":"helen-ai-he.yaml","sourceExt":".yaml"},"content/output/people/hiroki-kaimoto.json":{"name":"Hiroki Kaimoto","type":"alumni","past":"visiting","now":"University of Tokyo","url":"https://hirokikaimoto.com","scholar":"https://scholar.google.co.jp/citations?user=BPL36T0AAAAJ","twitter":"https://twitter.com/open_origin","email":"hkaimoto@xlab.iii.u-tokyo.ac.jp","dir":"content/output/people","base":"hiroki-kaimoto.json","ext":".json","sourceBase":"hiroki-kaimoto.yaml","sourceExt":".yaml"},"content/output/people/iryna-luchak.json":{"name":"Iryna Luchak","type":"master","email":"iryna.luchak@ucalgary.ca","linkedin":"https://www.linkedin.com/in/irynaluchak/","dir":"content/output/people","base":"iryna-luchak.json","ext":".json","sourceBase":"iryna-luchak.yaml","sourceExt":".yaml"},"content/output/people/james-tam.json":{"name":"James Tam","type":"alumni","past":"master","now":"University of Calgary","url":"http://pages.cpsc.ucalgary.ca/~tamj/index.html","dir":"content/output/people","base":"james-tam.json","ext":".json","sourceBase":"james-tam.yaml","sourceExt":".yaml"},"content/output/people/jarin-thundathil.json":{"name":"Jarin Thundathil","type":"undergrad","email":"jarin.thundathil@ucalgary.ca","linkedin":"https://www.linkedin.com/in/jarin-thundathil-170616150/","dir":"content/output/people","base":"jarin-thundathil.json","ext":".json","sourceBase":"jarin-thundathil.yaml","sourceExt":".yaml"},"content/output/people/jessi-stark.json":{"name":"Jessi Stark","type":"alumni","past":"master","now":"University of Toronto","url":"https://jtstark.com/","scholar":"https://scholar.google.com/citations?user=aRkKN5UAAAAJ","twitter":"https://twitter.com/_jessistark","dir":"content/output/people","base":"jessi-stark.json","ext":".json","sourceBase":"jessi-stark.yaml","sourceExt":".yaml"},"content/output/people/jian-liao.json":{"name":"Jian Liao","type":"undergrad","email":"jian.liao1@ucalgary.ca","twitter":"https://twitter.com/imjliao","linkedin":"https://www.linkedin.com/in/jian-liao/","github":"https://github.com/jlia0","dir":"content/output/people","base":"jian-liao.json","ext":".json","sourceBase":"jian-liao.yaml","sourceExt":".yaml"},"content/output/people/jiannan-li.json":{"name":"Jiannan Li","type":"alumni","past":"master","now":"University of Toronto","url":"https://www.dgp.toronto.edu/~jiannanli/","scholar":"https://scholar.google.com/citations?user=wyW0rJQAAAAJ","dir":"content/output/people","base":"jiannan-li.json","ext":".json","sourceBase":"jiannan-li.yaml","sourceExt":".yaml"},"content/output/people/karly-ross.json":{"name":"Karly Ross","type":"master","linkedin":"https://www.linkedin.com/in/karly-j-ross/","dir":"content/output/people","base":"karly-ross.json","ext":".json","sourceBase":"karly-ross.yaml","sourceExt":".yaml"},"content/output/people/karthik-mahadevan.json":{"name":"Karthik Mahadevan","type":"alumni","past":"master","now":"University of Toronto","url":"https://karthikm0.github.io/","scholar":"https://scholar.google.ca/citations?user=aK4siPkAAAAJ","dir":"content/output/people","base":"karthik-mahadevan.json","ext":".json","sourceBase":"karthik-mahadevan.yaml","sourceExt":".yaml"},"content/output/people/katherine-currier.json":{"name":"Katherine Currier","type":"alumni","past":"master","now":"Insulet Corporation","dir":"content/output/people","base":"katherine-currier.json","ext":".json","sourceBase":"katherine-currier.yaml","sourceExt":".yaml"},"content/output/people/kathryn-blair.json":{"name":"Kathryn Blair","type":"phd","url":"http://kathrynblair.com/","twitter":"https://twitter.com/kathblair","email":"kathryn.blair@ucalgary.ca","linkedin":"https://www.linkedin.com/in/kmblair/","dir":"content/output/people","base":"kathryn-blair.json","ext":".json","sourceBase":"kathryn-blair.yaml","sourceExt":".yaml"},"content/output/people/kathryn-elliot-rounding.json":{"name":"Kathryn Elliot-Rounding","type":"alumni","past":"master","dir":"content/output/people","base":"kathryn-elliot-rounding.json","ext":".json","sourceBase":"kathryn-elliot-rounding.yaml","sourceExt":".yaml"},"content/output/people/kaynen-mitchell.json":{"name":"Kaynen Mitchell","type":"alumni","past":"undergrad","email":"kaynen.mitchell1@ucalgary.ca","linkedin":"https://www.linkedin.com/in/kaynen-mitchell/","dir":"content/output/people","base":"kaynen-mitchell.json","ext":".json","sourceBase":"kaynen-mitchell.yaml","sourceExt":".yaml"},"content/output/people/keiichi-ihara.json":{"name":"Keiichi Ihara","type":"visiting","url":"https://www.iplab.cs.tsukuba.ac.jp/~kihara/","email":"kihara@iplab.cs.tsukuba.ac.jp","dir":"content/output/people","base":"keiichi-ihara.json","ext":".json","sourceBase":"keiichi-ihara.yaml","sourceExt":".yaml"},"content/output/people/kendra-wannamaker.json":{"name":"Kendra Wannamaker","type":"alumni","past":"master","now":"Autodesk Research","dir":"content/output/people","base":"kendra-wannamaker.json","ext":".json","sourceBase":"kendra-wannamaker.yaml","sourceExt":".yaml"},"content/output/people/kevin-van.json":{"name":"Kevin Van","type":"undergrad","email":"kevin.van@ucalgary.ca","url":"https://kevin-van.github.io/Website/","github":"https://github.com/kevin-van","linkedin":"https://www.linkedin.com/in/kevin-van-a66130204/","dir":"content/output/people","base":"kevin-van.json","ext":".json","sourceBase":"kevin-van.yaml","sourceExt":".yaml"},"content/output/people/kimberly-tee.json":{"name":"Kimberly Tee","type":"alumni","past":"master","now":"Shopify","scholar":"https://scholar.google.com/citations?user=srTy2voAAAAJ","linkedin":"https://www.linkedin.com/in/kimberly-tee-7305b159/","dir":"content/output/people","base":"kimberly-tee.json","ext":".json","sourceBase":"kimberly-tee.yaml","sourceExt":".yaml"},"content/output/people/kurtis-danyluk.json":{"name":"Kurtis Danyluk","type":"phd","scholar":"https://scholar.google.com/citations?user=vr-EF5IAAAAJ","dir":"content/output/people","base":"kurtis-danyluk.json","ext":".json","sourceBase":"kurtis-danyluk.yaml","sourceExt":".yaml"},"content/output/people/kyzyl-monteiro.json":{"name":"Kyzyl Monteiro","type":"alumni","past":"visiting","url":"https://kyzyl.me","scholar":"https://scholar.google.ca/citations?user=A9IqYNoAAAAJ","twitter":"https://twitter.com/kyzylmonteiro","email":"kyzylmonteiro@gmail.com","github":"https://github.com/kyzylmonteiro","linkedin":"https://www.linkedin.com/in/kyzylmonteiro/","dir":"content/output/people","base":"kyzyl-monteiro.json","ext":".json","sourceBase":"kyzyl-monteiro.yaml","sourceExt":".yaml"},"content/output/people/linda-tauscher.json":{"name":"Linda Tauscher","type":"alumni","past":"master","dir":"content/output/people","base":"linda-tauscher.json","ext":".json","sourceBase":"linda-tauscher.yaml","sourceExt":".yaml"},"content/output/people/lora-oehlberg.json":{"name":"Lora Oehlberg","type":"faculty","title":"Associate Professor","keywords":["Tangible","Design Tools"],"order":3,"url":"https://pages.cpsc.ucalgary.ca/~lora.oehlberg/","scholar":"https://scholar.google.ca/citations?hl=en&user=8GzaBdwAAAAJ","dir":"content/output/people","base":"lora-oehlberg.json","ext":".json","sourceBase":"lora-oehlberg.yaml","sourceExt":".yaml"},"content/output/people/mackenzie-bowal.json":{"name":"Mackenzie Bowal","type":"alumni","past":"undergrad","email":"mackenzie.bowal@ucalgary.ca","linkedin":"https://www.linkedin.com/in/mackenzie-bowal/","dir":"content/output/people","base":"mackenzie-bowal.json","ext":".json","sourceBase":"mackenzie-bowal.yaml","sourceExt":".yaml"},"content/output/people/mackenzie-hisako-dalton.json":{"name":"Mackenzie Hisako Dalton","type":"undergrad","url":"https://hisak00.weebly.com/","dir":"content/output/people","base":"mackenzie-hisako-dalton.json","ext":".json","sourceBase":"mackenzie-hisako-dalton.yaml","sourceExt":".yaml"},"content/output/people/manjot-khangura.json":{"name":"Manjot Khangura","type":"alumni","past":"undergrad","email":"manjot.khangura@ucalgary.ca","linkedin":"https://www.linkedin.com/in/manjot-khangura/","dir":"content/output/people","base":"manjot-khangura.json","ext":".json","sourceBase":"manjot-khangura.yaml","sourceExt":".yaml"},"content/output/people/manuel-rodriguez.json":{"name":"Manuel Rodriguez","type":"alumni","past":"undergrad","email":"manuel.rodriguez@ucalgary.ca","linkedin":"https://www.linkedin.com/in/manuel-rodriguez/","dir":"content/output/people","base":"manuel-rodriguez.json","ext":".json","sourceBase":"manuel-rodriguez.yaml","sourceExt":".yaml"},"content/output/people/marcus-friedel.json":{"name":"Marcus Friedel","type":"master","facebook":"https://www.facebook.com/marcus.friedel.3","email":"marcus.friedel@ucalgary.ca","linkedin":"https://www.linkedin.com/in/marcusfriedel/","dir":"content/output/people","base":"marcus-friedel.json","ext":".json","sourceBase":"marcus-friedel.yaml","sourceExt":".yaml"},"content/output/people/mark-roseman.json":{"name":"Mark Roseman","type":"alumni","past":"master","url":"https://markroseman.com/","linkedin":"https://www.linkedin.com/in/mroseman/","dir":"content/output/people","base":"mark-roseman.json","ext":".json","sourceBase":"mark-roseman.yaml","sourceExt":".yaml"},"content/output/people/martin-feick.json":{"name":"Martin Feick","type":"alumni","past":"phd","now":"Saarland University","url":"http://martinfeick.com/","scholar":"https://scholar.google.de/citations?user=az0GkfQAAAAJ","github":"https://github.com/MartinFk","twitter":"https://twitter.com/mafeick","dir":"content/output/people","base":"martin-feick.json","ext":".json","sourceBase":"martin-feick.yaml","sourceExt":".yaml"},"content/output/people/matthew-dunlap.json":{"name":"Matthew Dunlap","type":"alumni","past":"master","now":"UNC Chapel Hill","url":"https://odum.unc.edu/people/dunlap/","linkedin":"https://www.linkedin.com/in/matthew-dunlap-47a7b3b3/","github":"https://github.com/matthew-a-dunlap","dir":"content/output/people","base":"matthew-dunlap.json","ext":".json","sourceBase":"matthew-dunlap.yaml","sourceExt":".yaml"},"content/output/people/mehrad-faridan.json":{"name":"Mehrad Faridan","type":"undergrad","url":"https://www.mehradfaridan.com/","email":"mehrad.faridan1@ucalgary.ca","scholar":"https://scholar.google.com/citations?user=amh7v2EAAAAJ","twitter":"https://twitter.com/MehradFaridan","github":"https://github.com/mehradFaridan","linkedin":"https://www.linkedin.com/in/mehrad-f-a34462164/","dir":"content/output/people","base":"mehrad-faridan.json","ext":".json","sourceBase":"mehrad-faridan.yaml","sourceExt":".yaml"},"content/output/people/melissa-hoang.json":{"name":"Melissa Hoang","type":"undergrad","email":"melissa.hoang@ucalgary.ca","linkedin":"https://www.linkedin.com/in/melissa-e-hoang/","dir":"content/output/people","base":"melissa-hoang.json","ext":".json","sourceBase":"melissa-hoang.yaml","sourceExt":".yaml"},"content/output/people/miaosen-wang.json":{"name":"Miaosen Wang","type":"alumni","past":"master","dir":"content/output/people","base":"miaosen-wang.json","ext":".json","sourceBase":"miaosen-wang.yaml","sourceExt":".yaml"},"content/output/people/michael-hung.json":{"name":"Michael Hung","type":"alumni","past":"master","url":"https://michael-hung.ca","email":"myshung@ucalgary.ca","github":"https://github.com/murrrkle","linkedin":"https://www.linkedin.com/in/hungyukshing","dir":"content/output/people","base":"michael-hung.json","ext":".json","sourceBase":"michael-hung.yaml","sourceExt":".yaml"},"content/output/people/micheal-boyle.json":{"name":"Micheal Boyle","type":"alumni","past":"phd","dir":"content/output/people","base":"micheal-boyle.json","ext":".json","sourceBase":"micheal-boyle.yaml","sourceExt":".yaml"},"content/output/people/micheal-nunes.json":{"name":"Micheal Nunes","type":"alumni","past":"master","dir":"content/output/people","base":"micheal-nunes.json","ext":".json","sourceBase":"micheal-nunes.yaml","sourceExt":".yaml"},"content/output/people/micheal-rounding.json":{"name":"Micheal Roudning","type":"alumni","past":"master","dir":"content/output/people","base":"micheal-rounding.json","ext":".json","sourceBase":"micheal-rounding.yaml","sourceExt":".yaml"},"content/output/people/mille-skovhus-lunding.json":{"name":"Mille Skovhus Lunding","type":"visiting","url":"https://pure.au.dk/portal/en/persons/mille-skovhus-lunding(806b2f20-b7ae-4ebc-a32f-98ee39a53380).html","email":"milledsk@cs.au.dk","scholar":"https://scholar.google.com/citations?user=kt6cN_UAAAAJ","linkedin":"https://www.linkedin.com/in/mille-skovhus-lunding-1b419583/","dir":"content/output/people","base":"mille-skovhus-lunding.json","ext":".json","sourceBase":"mille-skovhus-lunding.yaml","sourceExt":".yaml"},"content/output/people/muhammad-mahian.json":{"name":"Muhammad Mahian","type":"alumni","past":"undergrad","email":"muhammad.mahian@ucalgary.ca","dir":"content/output/people","base":"muhammad-mahian.json","ext":".json","sourceBase":"muhammad-mahian.yaml","sourceExt":".yaml"},"content/output/people/nathalie-bressa.json":{"name":"Nathalie Bressa","type":"alumni","past":"visiting","now":"Aarhus University","dir":"content/output/people","base":"nathalie-bressa.json","ext":".json","sourceBase":"nathalie-bressa.yaml","sourceExt":".yaml"},"content/output/people/neil-chulpongsatorn.json":{"name":"Neil Chulpongsatorn","type":"master","url":"https://thobthai.wixsite.com/neilchulpongsatorn","scholar":"https://scholar.google.com/citations?user=D0KbikIAAAAJ&hl=en","linkedin":"https://www.linkedin.com/in/neil-chulpongsatorn-187273204/","email":"thobthai.chulpongsat@ucalgary.ca","dir":"content/output/people","base":"neil-chulpongsatorn.json","ext":".json","sourceBase":"neil-chulpongsatorn.yaml","sourceExt":".yaml"},"content/output/people/nelson-wong.json":{"name":"Nelson Wong","type":"alumni","past":"master","dir":"content/output/people","base":"nelson-wong.json","ext":".json","sourceBase":"nelson-wong.yaml","sourceExt":".yaml"},"content/output/people/nicolai-marquardt.json":{"name":"Nicolai Marquardt","type":"alumni","past":"phd","now":"Microsoft Research","url":"http://www.nicolaimarquardt.com/","scholar":"https://scholar.google.com/citations?user=PXeN0RsAAAAJ","dir":"content/output/people","base":"nicolai-marquardt.json","ext":".json","sourceBase":"nicolai-marquardt.yaml","sourceExt":".yaml"},"content/output/people/nishan-soni.json":{"name":"Nishan Soni","type":"undergrad","url":"https://nishan.web.app","email":"nishan.soni@ucalgary.ca","linkedin":"https://www.linkedin.com/in/nishan-soni/","github":"https://github.com/nishan-soni","dir":"content/output/people","base":"nishan-soni.json","ext":".json","sourceBase":"nishan-soni.yaml","sourceExt":".yaml"},"content/output/people/nour-hammad.json":{"name":"Nour Hammad","type":"alumni","past":"undergrad","dir":"content/output/people","base":"nour-hammad.json","ext":".json","sourceBase":"nour-hammad.yaml","sourceExt":".yaml"},"content/output/people/paul-lapides.json":{"name":"Paul Lapides","type":"alumni","past":"phd","url":"http://www.paullapides.com/","dir":"content/output/people","base":"paul-lapides.json","ext":".json","sourceBase":"paul-lapides.yaml","sourceExt":".yaml"},"content/output/people/paul-saulnier.json":{"name":"Paul Saulnier","type":"alumni","past":"master","now":"Bravura Security","url":"http://paulsaulnier.com/","linkedin":"https://www.linkedin.com/in/paulsaulnier/","dir":"content/output/people","base":"paul-saulnier.json","ext":".json","sourceBase":"paul-saulnier.yaml","sourceExt":".yaml"},"content/output/people/priya-dhawka.json":{"name":"Priya Dhawka","type":"master","email":"priyadarshinee.dhawk@ucalgary.ca","github":"https://github.com/dhawkap","linkedin":"https://www.linkedin.com/in/priya-d-15b743112/","dir":"content/output/people","base":"priya-dhawka.json","ext":".json","sourceBase":"priya-dhawka.yaml","sourceExt":".yaml"},"content/output/people/ran-zhou.json":{"name":"Ran Zhou","type":"alumni","past":"visiting","url":"https://www.ranzhourobot.com/","email":"ran.zhou@colorado.edu","scholar":"https://scholar.google.com/citations?user=QDCBu-cAAAAJ","linkedin":"https://www.linkedin.com/in/ran-zhou-096b8812b/","twitter":"https://twitter.com/ranzhoubot","dir":"content/output/people","base":"ran-zhou.json","ext":".json","sourceBase":"ran-zhou.yaml","sourceExt":".yaml"},"content/output/people/rasmus-lunding.json":{"name":"Rasmus Lunding","type":"visiting","url":"https://pure.au.dk/portal/en/persons/rasmus-skovhus-lunding(a1b1400c-dffa-4227-ac5b-0218dbd72de9).html","email":"rsl@cs.au.dk","linkedin":"https://www.linkedin.com/in/rlunding/","dir":"content/output/people","base":"rasmus-lunding.json","ext":".json","sourceBase":"rasmus-lunding.yaml","sourceExt":".yaml"},"content/output/people/ritik-vatsal.json":{"name":"Ritik Vatsal","type":"alumni","past":"visiting","email":"vatrit@gmail.com","github":"https://github.com/RitikVatsal-2019321","linkedin":"https://www.linkedin.com/in/ritik-vatsal/","dir":"content/output/people","base":"ritik-vatsal.json","ext":".json","sourceBase":"ritik-vatsal.yaml","sourceExt":".yaml"},"content/output/people/roberta-cabral-mota.json":{"name":"Roberta Cabral Mota","type":"phd","url":"https://www.robertacrmota.com/","dir":"content/output/people","base":"roberta-cabral-mota.json","ext":".json","sourceBase":"roberta-cabral-mota.yaml","sourceExt":".yaml"},"content/output/people/roberto-diaz-marino.json":{"name":"Roberto Diaz Marino","type":"alumni","past":"master","dir":"content/output/people","base":"roberto-diaz-marino.json","ext":".json","sourceBase":"roberto-diaz-marino.yaml","sourceExt":".yaml"},"content/output/people/ryo-suzuki.json":{"name":"Ryo Suzuki","type":"faculty","title":"Assistant Professor","keywords":["Tangible","Robots","AR/VR"],"order":4,"url":"https://ryosuzuki.org","scholar":"https://scholar.google.com/citations?user=klWjaQIAAAAJ","twitter":"https://twitter.com/ryosuzk","facebook":"https://www.facebook.com/ryosuzk","email":"ryo.suzuki@ucalgary.ca","github":"https://github.com/ryosuzuki","linkedin":"https://www.linkedin.com/in/ryosuzuki/","dir":"content/output/people","base":"ryo-suzuki.json","ext":".json","sourceBase":"ryo-suzuki.yaml","sourceExt":".yaml"},"content/output/people/ryota-gomi.json":{"name":"Ryota Gomi","type":"visiting","url":"https://www.icd.riec.tohoku.ac.jp/en/about/member/","email":"ryota.gomi.s7@dc.tohoku.ac.jp","linkedin":"https://www.linkedin.com/in/ryota-gomi-6a6029203/","dir":"content/output/people","base":"ryota-gomi.json","ext":".json","sourceBase":"ryota-gomi.yaml","sourceExt":".yaml"},"content/output/people/sabrina-lakhdhir.json":{"name":"Sabrina Lakhdhir","type":"alumni","past":"undergrad","now":"University of Victoria","dir":"content/output/people","base":"sabrina-lakhdhir.json","ext":".json","sourceBase":"sabrina-lakhdhir.yaml","sourceExt":".yaml"},"content/output/people/saja-abufarha.json":{"name":"Saja Abufarha","type":"undergrad","email":"saja.abufarha@ucalgary.ca","linkedin":"https://www.linkedin.com/in/saja-abufarha/","dir":"content/output/people","base":"saja-abufarha.json","ext":".json","sourceBase":"saja-abufarha.yaml","sourceExt":".yaml"},"content/output/people/samin-farajian.json":{"name":"Samin Farajian","type":"master","url":"https://farajiansamin.github.io","linkedin":"https://www.linkedin.com/in/samin-farajian-736018140/","dir":"content/output/people","base":"samin-farajian.json","ext":".json","sourceBase":"samin-farajian.yaml","sourceExt":".yaml"},"content/output/people/sasha-ivanov.json":{"name":"Sasha Ivanov","type":"alumni","past":"master","now":"Meta Reality Labs","dir":"content/output/people","base":"sasha-ivanov.json","ext":".json","sourceBase":"sasha-ivanov.yaml","sourceExt":".yaml"},"content/output/people/saul-greenberg.json":{"name":"Saul Greenberg","type":"faculty","title":"Emeritus Professor","keywords":["UbiComp","CSCW"],"order":8,"url":"http://saul.cpsc.ucalgary.ca/","scholar":"https://scholar.google.com/citations?user=TthhUuoAAAAJ","dir":"content/output/people","base":"saul-greenberg.json","ext":".json","sourceBase":"saul-greenberg.yaml","sourceExt":".yaml"},"content/output/people/setareh-manesh.json":{"name":"Setareh Manesh","type":"alumni","past":"master","dir":"content/output/people","base":"setareh-manesh.json","ext":".json","sourceBase":"setareh-manesh.yaml","sourceExt":".yaml"},"content/output/people/shanna-hollingworth.json":{"name":"Shanna Hollingworth","type":"undergrad","email":"shanna.hollingwor1@ucalgary.ca","linkedin":"https://www.linkedin.com/in/shanna-hollingworth/","dir":"content/output/people","base":"shanna-hollingworth.json","ext":".json","sourceBase":"shanna-hollingworth.yaml","sourceExt":".yaml"},"content/output/people/shaun-kaasten.json":{"name":"Shaun Kaasten","type":"alumni","past":"master","dir":"content/output/people","base":"shaun-kaasten.json","ext":".json","sourceBase":"shaun-kaasten.yaml","sourceExt":".yaml"},"content/output/people/sheelagh-carpendale.json":{"name":"Sheelagh Carpendale","type":"faculty","title":"Adjunct Professor","keywords":["Data Viz","Data Phyz"],"order":9,"url":"https://www.cs.sfu.ca/~sheelagh/","scholar":"https://scholar.google.com/citations?user=43LLX2kAAAAJ","dir":"content/output/people","base":"sheelagh-carpendale.json","ext":".json","sourceBase":"sheelagh-carpendale.yaml","sourceExt":".yaml"},"content/output/people/shivesh-jadon.json":{"name":"Shivesh Jadon","type":"master","url":"https://shivesh.dev/","scholar":"https://scholar.google.com/citations?user=EwqYU2sAAAAJ&hl=en","linkedin":"https://www.linkedin.com/in/shiveshjadon/","twitter":"https://twitter.com/ShiveshJadon","dir":"content/output/people","base":"shivesh-jadon.json","ext":".json","sourceBase":"shivesh-jadon.yaml","sourceExt":".yaml"},"content/output/people/shrivatsa-mishra.json":{"name":"Shrivatsa Mishra","type":"alumni","past":"visiting","email":"shrivatsamishra@gmail.com","github":"https://github.com/ShrivatsaMishra","linkedin":"https://www.linkedin.com/in/shrivatsa-mishra/","dir":"content/output/people","base":"shrivatsa-mishra.json","ext":".json","sourceBase":"shrivatsa-mishra.yaml","sourceExt":".yaml"},"content/output/people/simon-kluber.json":{"name":"Simon Klber","type":"alumni","past":"visiting","dir":"content/output/people","base":"simon-kluber.json","ext":".json","sourceBase":"simon-kluber.yaml","sourceExt":".yaml"},"content/output/people/soren-knudsen.json":{"name":"Sren Knudsen","type":"alumni","past":"postdoc","now":"University of Copenhagen","url":"http://sorenknudsen.com/","dir":"content/output/people","base":"soren-knudsen.json","ext":".json","sourceBase":"soren-knudsen.yaml","sourceExt":".yaml"},"content/output/people/sowmya-somanath.json":{"name":"Sowmya Somanath","type":"alumni","past":"phd","now":"University of Victoria","url":"http://pages.cpsc.ucalgary.ca/~ssomanat/","scholar":"https://scholar.google.ca/citations?user=R9ar1NkAAAAJ","twitter":"https://twitter.com/sowmyasomanath","dir":"content/output/people","base":"sowmya-somanath.json","ext":".json","sourceBase":"sowmya-somanath.yaml","sourceExt":".yaml"},"content/output/people/stephanie-smale.json":{"name":"Stephanie Smale","type":"alumni","past":"master","dir":"content/output/people","base":"stephanie-smale.json","ext":".json","sourceBase":"stephanie-smale.yaml","sourceExt":".yaml"},"content/output/people/sydney-pratte.json":{"name":"Sydney Pratte","type":"phd","url":"https://www.sydneypratte.ca/","dir":"content/output/people","base":"sydney-pratte.json","ext":".json","sourceBase":"sydney-pratte.yaml","sourceExt":".yaml"},"content/output/people/teddy-seyed.json":{"name":"Teddy Seyed","type":"alumni","past":"phd","now":"Microsoft Research","url":"https://www.microsoft.com/en-us/research/people/teddy/","scholar":"https://scholar.google.com/citations?user=A8VSir8AAAAJ","dir":"content/output/people","base":"teddy-seyed.json","ext":".json","sourceBase":"teddy-seyed.yaml","sourceExt":".yaml"},"content/output/people/terrance-mok.json":{"name":"Terrance Mok","type":"phd","url":"http://terrancemok.com/","scholar":"https://scholar.google.ca/citations?user=nHkJDSEAAAAJ","twitter":"https://twitter.com/terrancem","linkedin":"https://www.linkedin.com/in/terrance-mok-22421011","dir":"content/output/people","base":"terrance-mok.json","ext":".json","sourceBase":"terrance-mok.yaml","sourceExt":".yaml"},"content/output/people/theodore-ogrady.json":{"name":"Theodore (Ted) O'Grady","type":"alumni","past":"master","dir":"content/output/people","base":"theodore-ogrady.json","ext":".json","sourceBase":"theodore-ogrady.yaml","sourceExt":".yaml"},"content/output/people/tian-xia.json":{"name":"Tian Xia","type":"alumni","past":"undergrad","linkedin":"https://www.linkedin.com/in/tianxiacs/","email":"tian.xia2@ucalgary.ca","dir":"content/output/people","base":"tian-xia.json","ext":".json","sourceBase":"tian-xia.yaml","sourceExt":".yaml"},"content/output/people/tim-au-yeung.json":{"name":"Tim Au Yeung","type":"phd","linkedin":"https://www.linkedin.com/in/tim-au-yeung-3a29816","dir":"content/output/people","base":"tim-au-yeung.json","ext":".json","sourceBase":"tim-au-yeung.yaml","sourceExt":".yaml"},"content/output/people/wei-wei.json":{"name":"Wei Wei","type":"master","url":"http://www.weiweiff.com/","email":"wei.wei2@ucalgary.ca","linkedin":"https://www.linkedin.com/in/wei-wei-961a211ba/","dir":"content/output/people","base":"wei-wei.json","ext":".json","sourceBase":"wei-wei.yaml","sourceExt":".yaml"},"content/output/people/wesley-willett.json":{"name":"Wesley Willett","type":"faculty","title":"Associate Professor","keywords":["Data Viz","Data Phyz","AR"],"order":3,"url":"http://www.wjwillett.net","scholar":"https://scholar.google.ca/citations?user=Q17-rckAAAAJ","dir":"content/output/people","base":"wesley-willett.json","ext":".json","sourceBase":"wesley-willett.yaml","sourceExt":".yaml"},"content/output/people/william-wright.json":{"name":"William Wright","type":"masters","url":"http://pages.cpsc.ucalgary.ca/~wwright/","scholar":"https://scholar.google.com/citations?user=V4nRMoMAAAAJ","twitter":"https://twitter.com/HexenKoenig","facebook":"https://www.facebook.com/bonadriel","linkedin":"https://www.linkedin.com/in/bon-adriel-aseniero-47140560/","dir":"content/output/people","base":"william-wright.json","ext":".json","sourceBase":"william-wright.yaml","sourceExt":".yaml"},"content/output/people/xiang-anthony-chen.json":{"name":"Xiang 'Anthony' Chen","type":"alumni","past":"master","now":"UCLA","url":"https://xac.is/","scholar":"https://scholar.google.com/citations?user=I2W13z0AAAAJ","twitter":"https://twitter.com/_xiang_chen_","dir":"content/output/people","base":"xiang-anthony-chen.json","ext":".json","sourceBase":"xiang-anthony-chen.yaml","sourceExt":".yaml"},"content/output/people/yibo-sun.json":{"name":"Yibo Sun","type":"alumni","past":"master","dir":"content/output/people","base":"yibo-sun.json","ext":".json","sourceBase":"yibo-sun.yaml","sourceExt":".yaml"},"content/output/people/zachary-mckendrick.json":{"name":"Zachary McKendrick","type":"phd","url":"https://scpa.ucalgary.ca/manageprofile/profiles/zachary-mckendrick","linkedin":"https://www.linkedin.com/in/zach-mckendrick-7a24bb3b","dir":"content/output/people","base":"zachary-mckendrick.json","ext":".json","sourceBase":"zachary-mckendrick.yaml","sourceExt":".yaml"},"content/output/people/zhijie-xia.json":{"name":"Zhijie Xia","type":"undergrad","url":"https://zhijiexia.dev/","linkedin":"https://www.linkedin.com/in/zhijie-xia-678b331b5/","dir":"content/output/people","base":"zhijie-xia.json","ext":".json","sourceBase":"zhijie-xia.yaml","sourceExt":".yaml"},"content/output/publications/assets-2017-suzuki.json":{"date":"2017-10","title":"FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers","authors":["Ryo Suzuki","Abigale Stangl","Mark D. Gross","Tom Yeh"],"series":"ASSETS 2020","doi":"https://doi.org/10.1145/3132525.3132548","keywords":"visual impairment, dynamic tactile markers, tangible interfaces, interactive tactile graphics","video":"https://www.youtube.com/watch?v=VbwIZ9V6i_g","pages":10,"abstract":"For people with visual impairments, tactile graphics are an important means to learn and explore information. However, raised line tactile graphics created with traditional materials such as embossing are static. While available refreshable displays can dynamically change the content, they are still too expensive for many users, and are limited in size. These factors limit wide-spread adoption and the representation of large graphics or data sets. In this paper, we present FluxMaker, an inexpensive scalable system that renders dynamic information on top of static tactile graphics with movable tactile markers. These dynamic tactile markers can be easily reconfigured and used to annotate static raised line tactile graphics, including maps, graphs, and diagrams. We developed a hardware prototype that actuates magnetic tactile markers driven by low-cost and scalable electromagnetic coil arrays, which can be fabricated with standard printed circuit board manufacturing. We evaluate our prototype with six participants with visual impairments and found positive results across four application areas: location finding or navigating on tactile maps, data analysis, and physicalization, feature identification for tactile graphics, and drawing support. The user study confirms advantages in application domains such as education and data exploration.","dir":"content/output/publications","base":"assets-2017-suzuki.json","ext":".json","sourceBase":"assets-2017-suzuki.yaml","sourceExt":".yaml"},"content/output/publications/cga-2019-ivanov.json":{"date":"2019-02","title":"A Walk Among the Data","authors":["Alexander Ivanov","Kurtis Danyluk","Christian Jacob","Wesley Willett"],"series":"IEEE CG&A 2019","doi":"http://dx.doi.org/10.1109/MCG.2019.2898941","keywords":"Data visualization, Visualization, Art, Tools, Virtual environments, Two dimensional displays, Anthropomorphism","pages":9,"video":null,"abstract":"We examine the potential for immersive unit visualizationsinteractive virtual environments populated with objects representing individual items in a dataset. Our virtual reality prototype highlights how immersive unit visualizations can allow viewers to examine data at multiple scales, support immersive exploration, and create affective personal experiences with data.","dir":"content/output/publications","base":"cga-2019-ivanov.json","ext":".json","sourceBase":"cga-2019-ivanov.yaml","sourceExt":".yaml"},"content/output/publications/cgi-2019-danyluk.json":{"date":"2019-06","title":"Evaluating the Performance of Virtual Reality Navigation Techniques for Large Environments","authors":["Kurtis Thorvald Danyluk","Wesley J. Willett"],"series":"CGI 2019","doi":"http://dx.doi.org/10.1007/978-3-030-22514-8_17","keywords":"Human computer interaction (HCI), Virtual reality, Digital maps, Navigation","pages":13,"video":null,"abstract":"We present results from two studies comparing the performance of four different navigation techniques (flight, teleportation, world-in-miniature, and 3D cone-drag) and their combinations in large virtual reality map environments. While prior work has individually examined each of these techniques in other settings, our study presents the first direct comparison between them in large open environments, as well as one of the first comparisons in the context of current-generation virtual reality hardware. Our first study compared common techniques (flight, teleportation, and world-in-miniature) for search and navigation tasks. A follow-up study compared these techniques against 3D cone drag, a direct-manipulation navigation technique used in contemporary tools like Google Earth VR. Our results show the strength of flight as a stand-alone navigation technique, but also highlight five specific ways in which viewers can combine teleportation, world-in-miniature, and 3D cone drag with flight, drawing on the relative strengths of each technique to compensate for the weaknesses of others.","dir":"content/output/publications","base":"cgi-2019-danyluk.json","ext":".json","sourceBase":"cgi-2019-danyluk.yaml","sourceExt":".yaml"},"content/output/publications/chi-2015-aseniero.json":{"date":"2015-04","title":"Stratos: Using Visualization to Support Decisions in Strategic Software Release Planning","authors":["Bon Adriel Aseniero","Tiffany Wun","David Ledo","Guenther Ruhe","Anthony Tang","Sheelagh Carpendale"],"series":"CHI 2015","doi":"https://doi.org/10.1145/2702123.2702426","keywords":"software engineering, information visualization, release planning","video":"https://www.youtube.com/watch?v=qm57aHjTAYc","pages":10,"abstract":"Software is typically developed incrementally and released in stages. Planning these releases involves deciding which features of the system should be implemented for each release. This is a complex planning process involving numerous trade-offs-constraints and factors that often make decisions difficult. Since the success of a product depends on this plan, it is important to understand the trade-offs between different release plans in order to make an informed choice. We present STRATOS, a tool that simultaneously visualizes several software release plans. The visualization shows several attributes about each plan that are important to planners. Multiple plans are shown in a single layout to help planners find and understand the trade-offs between alternative plans. We evaluated our tool via a qualitative study and found that STRATOS enables a range of decision-making processes, helping participants decide on which plan is most optimal.","dir":"content/output/publications","base":"chi-2015-aseniero.json","ext":".json","sourceBase":"chi-2015-aseniero.yaml","sourceExt":".yaml"},"content/output/publications/chi-2015-jones.json":{"date":"2015-04","title":"Mechanics of Camera Work in Mobile Video Collaboration","authors":["Brennan Jones","Anna Witcraft","Scott Bateman","Carman Neustaedter","Anthony Tang"],"series":"CHI 2015","doi":"https://doi.org/10.1145/2702123.2702345","keywords":"video communication, collaboration, mobile computing, handheld devices, cscw","pages":10,"video":"https://www.youtube.com/watch?v=V133YGkLxC8","abstract":"Mobile video conferencing, where one or more participants are moving about in the real world, enables entirely new interaction scenarios (e.g., asking for help to construct or repair an object, or showing a physical location). While we have a good understanding of the challenges of video conferencing in office or home environments, we do not fully understand the mechanics of camera work-how people use mobile devices to communicate with one another-during mobile video calls. To provide an understanding of what people do in mobile video collaboration, we conducted an observational study where pairs of participants completed tasks using a mobile video conferencing system. Our analysis suggests that people use the camera view deliberately to support their interactions-for example, to convey a message or to ask questions-but the limited field of view, and the lack of camera control can make it a frustrating experience.","dir":"content/output/publications","base":"chi-2015-jones.json","ext":".json","sourceBase":"chi-2015-jones.yaml","sourceExt":".yaml"},"content/output/publications/chi-2015-oehlberg.json":{"date":"2015-05","title":"Patterns of Physical Design Remixing in Online Maker Communities","authors":["Lora Oehlberg","Wesley Willett","Wendy E. Mackay"],"series":"CHI 2015","doi":"https://doi.org/10.1145/2702123.2702175","keywords":"customization, maker communities, user innovation, collaboration, hacking, remixing","pages":12,"talk":"https://www.youtube.com/watch?v=vJrVjH04nGc","abstract":"Makers participate in remixing culture by drawing inspiration from, combining, and adapting designs for physical objects. To examine how makers remix each others' designs on a community scale, we analyzed metadata from over 175,000 digital designs from Thingiverse, the largest online design community for digital fabrication. Remixed designs on Thingiverse are predominantly generated designs from Customizer a built-in web app for adjusting parametric designs. However, we find that these designs do not elicit subsequent user activity and the authors who generate them tend not to contribute additional content to Thingiverse. Outside of Customizer, influential sources of remixing include complex assemblies and design primitives, as well as non-physical resources posing as physical designs. Building on our findings, we discuss ways in which online maker communities could become more than just design repositories and better support collaborative remixing.","dir":"content/output/publications","base":"chi-2015-oehlberg.json","ext":".json","sourceBase":"chi-2015-oehlberg.yaml","sourceExt":".yaml"},"content/output/publications/chi-2015-willett.json":{"date":"2015-04","title":"Lightweight Relief Shearing for Enhanced Terrain Perception on Interactive Maps","authors":["Wesley Willett","Bernhard Jenny","Tobias Isenberg","Pierre Dragicevic"],"series":"CHI 2015","doi":"https://doi.org/10.1145/2702123.2702172","keywords":"plan oblique relief, interaction, depth perception, terrain maps, relief shearing","pages":10,"video":"https://www.youtube.com/watch?v=YW31lmzQzpc","abstract":"We explore interactive relief shearing, a set of non-intrusive, direct manipulation interactions that expose depth and shape information in terrain maps using ephemeral animations. Reading and interpreting topography and relief on terrain maps is an important aspect of map use, but extracting depth information from 2D maps is notoriously difficult. Modern mapping software attempts to alleviate this limitation by presenting digital terrain using 3D views. However, 3D views introduce occlusion, complicate distance estimations, and typically require more complex interactions. In contrast, our approach reveals depth information via shearing animations on 2D maps, and can be paired with existing interactions such as pan and zoom. We examine explicit, integrated, and hybrid interactions for triggering relief shearing and present a version that uses device tilt to control depth effects. Our evaluation shows that these interactive techniques improve depth perception when compared to standard 2D and perspective views.","dir":"content/output/publications","base":"chi-2015-willett.json","ext":".json","sourceBase":"chi-2015-willett.yaml","sourceExt":".yaml"},"content/output/publications/chi-2017-aoki.json":{"date":"2017-05","title":"Environmental Protection and Agency: Motivations, Capacity, and Goals in Participatory Sensing","authors":["Paul Aoki","Allison Woodruff","Baladitya Yellapragada","Wesley Willett"],"series":"CHI 2017","doi":"https://doi.org/10.1145/3025453.3025667","keywords":"citizen science, environmental sensing","pages":13,"abstract":"In this paper we consider various genres of citizen science from the perspective of citizen participants. As a mode of scientific inquiry, citizen science has the potential to \"scale up\" scientific data collection efforts and increase lay engagement with science. However, current technological directions risk losing sight of the ways in which citizen science is actually practiced. As citizen science is increasingly used to describe a wide range of activities, we begin by presenting a framework of citizen science genres. We then present findings from four interlocking qualitative studies and technological interventions of community air quality monitoring efforts, examining the motivations and capacities of citizen participants and characterizing their alignment with different types of citizen science. Based on these studies, we suggest that data acquisition involves complex multi-dimensional tradeoffs, and the commonly held view that citizen science systems are a win-win for citizens and science may be overstated.","dir":"content/output/publications","base":"chi-2017-aoki.json","ext":".json","sourceBase":"chi-2017-aoki.yaml","sourceExt":".yaml"},"content/output/publications/chi-2017-hull.json":{"date":"2017-05","title":"Building with Data: Architectural Models as Inspiration for Data Physicalization","authors":["Carmen Hull","Wesley Willett"],"series":"CHI 2017","doi":"https://doi.org/10.1145/3025453.3025850","keywords":"design process, architectural models, data physicalization, embodied interaction, data visualization","pages":12,"video":"https://www.youtube.com/watch?v=bkqLNgYIXek","abstract":"In this paper we analyze the role of physical scale models in the architectural design process and apply insights from architecture for the creation and use of data physicalizations. Based on a survey of the architecture literature on model making and ten interviews with practicing architects, we describe the role of physical models as a tool for exploration and communication. From these observations, we identify trends in the use of physical models in architecture, which have the potential to inform the design of data physicalizations. We identify four functions of architectural modeling that can be directly adapted for use in the process of building rich data models. Finally, we discuss how the visualization community can apply observations from architecture to the design of new data physicalizations.","dir":"content/output/publications","base":"chi-2017-hull.json","ext":".json","sourceBase":"chi-2017-hull.yaml","sourceExt":".yaml"},"content/output/publications/chi-2017-ledo.json":{"date":"2017-05","title":"Pineal: Bringing Passive Objects to Life with Embedded Mobile Devices","authors":["David Ledo","Fraser Anderson","Ryan Schmidt","Lora Oehlberg","Saul Greenberg","Tovi Grossman"],"series":"CHI 2017","doi":"https://doi.org/10.1145/3025453.3025652","pages":10,"keywords":"fabrication, 3d printing, smart objects, rapid prototyping, toolkits, prototyping tool, interaction design","video":"https://www.youtube.com/watch?v=ORN9jljPncc","video-2":"https://www.youtube.com/watch?v=ifEIR7cHWxc","talk":"https://www.youtube.com/watch?v=QB0kbcu_CsY","abstract":"Interactive, smart objects  customized to individuals and uses  are central to many movements, such as tangibles, the internet of things (IoT), and ubiquitous computing. Yet, rapid prototyping both the form and function of these custom objects can be problematic, particularly for those with limited electronics or programming experience. Designers often need to embed custom circuitry; program its workings; and create a form factor that not only reflects the desired user experience but can also house the required circuitry and electronics. To mitigate this, we created Pineal, a design tool that lets end-users: (1) modify 3D models to include a smart watch or phone as its heart; (2) specify high-level interactive behaviours through visual programming; and (3) have the phone or watch act out such behaviours as the objects' \"smarts\". Furthermore, a series of prototypes show how Pineal exploits mobile sensing and output, and automatically generates 3D printed form-factors for rich, interactive, objects.","dir":"content/output/publications","base":"chi-2017-ledo.json","ext":".json","sourceBase":"chi-2017-ledo.yaml","sourceExt":".yaml"},"content/output/publications/chi-2017-somanath.json":{"date":"2017-05","title":"'Maker' within Constraints: Exploratory Study of Young Learners using Arduino at a High School in India","authors":["Sowmya Somanath","Lora Oehlberg","Janette Hughes","Ehud Sharlin","Mario Costa Sousa"],"series":"CHI 2017","doi":"https://doi.org/10.1145/3025453.3025849","pages":13,"keywords":"India, HCI4D, physical computing, DIY, young learners, maker culture","video":"https://www.youtube.com/watch?v=NpIME1h1mH8","abstract":"Do-it-yourself (DIY) inspired activities have gained popularity as a means of creative expression and self-directed learning. However, DIY culture is difficult to implement in places with limited technology infrastructure and traditional learning cultures. Our goal is to understand how learners in such a setting react to DIY activities. We present observations from a physical computing workshop with 12 students (13-15 years old) conducted at a high school in India. We observed unique challenges for these students when tackling DIY activities: a high monetary and psychological cost to exploration, limited independent learning resources, difficulties with finding intellectual courage and assumed technical language proficiency. Our participants, however, overcome some of these challenges by adopting their own local strategies: resilience, nonverbal and verbal learning techniques, and creating documentation and fallback circuit versions. Based on our findings, we discuss a set of lessons learned about makerspaces in a context with socio-technical challenges.","dir":"content/output/publications","base":"chi-2017-somanath.json","ext":".json","sourceBase":"chi-2017-somanath.yaml","sourceExt":".yaml"},"content/output/publications/chi-2018-dillman.json":{"date":"2018-04","title":"A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality","authors":["Kody R. Dillman","Terrance Mok","Anthony Tang","Lora Oehlberg","Alex Mitchell"],"series":"CHI 2018","doi":"https://doi.org/10.1145/3173574.3173714","keywords":"game design, guidance, interaction cues, augmented reality","pages":12,"talk":"https://www.youtube.com/watch?v=3FoZStToALQ","abstract":"Based on an analysis of 49 popular contemporary video games, we develop a descriptive framework of visual interaction cues in video games. These cues are used to inform players what can be interacted with, where to look, and where to go within the game world. These cues vary along three dimensions: the purpose of the cue, the visual design of the cue, and the circumstances under which the cue is shown. We demonstrate that this framework can also be used to describe interaction cues for augmented reality applications. Beyond this, we show how the framework can be used to generatively derive new design ideas for visual interaction cues in augmented reality experiences.","dir":"content/output/publications","base":"chi-2018-dillman.json","ext":".json","sourceBase":"chi-2018-dillman.yaml","sourceExt":".yaml"},"content/output/publications/chi-2018-feick.json":{"date":"2018-04","title":"Perspective on and Re-orientation of Physical Proxies in Object-Focused Remote Collaboration","authors":["Martin Feick","Terrance Tin Hoi Mok","Anthony Tang","Lora Oehlberg","Ehud Sharlin"],"series":"CHI 2018","doi":"https://doi.org/10.1145/3173574.3173855","keywords":"cscw, remote collaboration, object-focused collaboration, physical telepresence, collaborative physical tasks","pages":13,"award":"Honorable Mention","video":"https://www.youtube.com/watch?v=sfxTHsPJWHY","abstract":"Remote collaborators working together on physical objects have difficulty building a shared understanding of what each person is talking about. Conventional video chat systems are insufficient for many situations because they present a single view of the object in a flattened image. To understand how this limited perspective affects collaboration, we designed the Remote Manipulator (ReMa), which can reproduce orientation manipulations on a proxy object at a remote site. We conducted two studies with ReMa, with two main findings. First, a shared perspective is more effective and preferred compared to the opposing perspective offered by conventional video chat systems. Second, the physical proxy and video chat complement one another in a combined system: people used the physical proxy to understand objects, and used video chat to perform gestures and confirm remote actions.","dir":"content/output/publications","base":"chi-2018-feick.json","ext":".json","sourceBase":"chi-2018-feick.yaml","sourceExt":".yaml"},"content/output/publications/chi-2018-heshmat.json":{"date":"2018-04","title":"Geocaching with a Beam: Shared Outdoor Activities through a Telepresence Robot with 360 Degree Viewing","authors":["Yasamin Heshmat","Brennan Jones","Xiaoxuan Xiong","Carman Neustaedter","Anthony Tang","Bernhard E. Riecke","Lillian Yang"],"series":"CHI 2018","doi":"https://doi.org/10.1145/3173574.3173933","keywords":"video communication, telepresence robots, leisure activities, social presence, geocaching","pages":13,"abstract":"People often enjoy sharing outdoor activities together such as walking and hiking. However, when family and friends are separated by distance it can be difficult if not impossible to share such activities. We explore this design space by investigating the benefits and challenges of using a telepresence robot to support outdoor leisure activities. In our study, participants participated in the outdoor activity of geocaching where one person geocached with the help of a remote partner via a telepresence robot. We compared a wide field of view (WFOV) camera to a 360 camera. Results show the benefits of having a physical embodiment and a sense of immersion with the 360 view. Yet challenges related to a lack of environmental awareness, safety issues, and privacy concerns resulting from bystander interactions. These findings illustrate the need to design telepresence robots with the environment and public in mind to provide an enhanced sensory experience while balancing safety and privacy issues resulting from being amongst the general public.","dir":"content/output/publications","base":"chi-2018-heshmat.json","ext":".json","sourceBase":"chi-2018-heshmat.yaml","sourceExt":".yaml"},"content/output/publications/chi-2018-ledo.json":{"date":"2018-04","title":"Evaluation Strategies for HCI Toolkit Research","authors":["David Ledo","Steven Houben","Jo Vermeulen","Nicolai Marquardt","Lora Oehlberg","Saul Greenberg"],"series":"CHI 2018","doi":"https://doi.org/10.1145/3173574.3173610","keywords":"user interfaces, design, evaluation, prototyping, toolkits","pages":17,"video":"https://www.youtube.com/watch?v=3lAwhCk60C4","talk":"https://www.youtube.com/watch?v=NOhsvN_Kv-I","abstract":"Toolkit research plays an important role in the field of HCI, as it can heavily influence both the design and implementation of interactive systems. For publication, the HCI community typically expects toolkit research to include an evaluation component. The problem is that toolkit evaluation is challenging, as it is often unclear what 'evaluating' a toolkit means and what methods are appropriate. To address this problem, we analyzed 68 published toolkit papers. From our analysis, we provide an overview of, reflection on, and discussion of evaluation methods for toolkit contributions. We identify and discuss the value of four toolkit evaluation strategies, including the associated techniques that each employs. We offer a categorization of evaluation strategies for toolkit researchers, along with a discussion of the value, potential limitations, and trade-offs associated with each strategy.","dir":"content/output/publications","base":"chi-2018-ledo.json","ext":".json","sourceBase":"chi-2018-ledo.yaml","sourceExt":".yaml"},"content/output/publications/chi-2018-mahadevan.json":{"date":"2018-04","title":"Communicating Awareness and Intent in Autonomous Vehicle-Pedestrian Interaction","authors":["Karthik Mahadevan","Sowmya Somanath","Ehud Sharlin"],"series":"CHI 2018","doi":"https://doi.org/10.1145/3173574.3174003","keywords":"autonomous vehicle-pedestrian interaction, perceived awareness and intent in autonomous vehicles","pages":12,"video":"https://www.youtube.com/watch?v=D_hhcGVREGA","talk":"https://www.youtube.com/watch?v=08OEKuz93dY","abstract":"Drivers use nonverbal cues such as vehicle speed, eye gaze, and hand gestures to communicate awareness and intent to pedestrians. Conversely, in autonomous vehicles, drivers can be distracted or absent, leaving pedestrians to infer awareness and intent from the vehicle alone. In this paper, we investigate the usefulness of interfaces (beyond vehicle movement) that explicitly communicate awareness and intent of autonomous vehicles to pedestrians, focusing on crosswalk scenarios. We conducted a preliminary study to gain insight on designing interfaces that communicate autonomous vehicle awareness and intent to pedestrians. Based on study outcomes, we developed four prototype interfaces and deployed them in studies involving a Segway and a car. We found interfaces communicating vehicle awareness and intent: (1) can help pedestrians attempting to cross; (2) are not limited to the vehicle and can exist in the environment; and (3) should use a combination of modalities such as visual, auditory, and physical.","dir":"content/output/publications","base":"chi-2018-mahadevan.json","ext":".json","sourceBase":"chi-2018-mahadevan.yaml","sourceExt":".yaml"},"content/output/publications/chi-2018-neustaedter.json":{"date":"2018-04","title":"The Benefits and Challenges of Video Calling for Emergency Situations","authors":["Carman Neustaedter","Brennan Jones","Kenton O'Hara","Abigail Sellen"],"series":"CHI 2018","doi":"https://doi.org/10.1145/3173574.3174231","keywords":"collaboration, situation awareness, emergency calling, call takers, mobile video calling, dispatchers","pages":13,"award":"Honorable Mention","abstract":"In the coming years, emergency calling services in North America will begin to incorporate new modalities for reporting emergencies, including video-based calling. The challenge is that we know little of how video calling systems should be designed and what benefits or challenges video calling might bring. We conducted observations and contextual interviews within three emergency response call centres to investigate these points. We focused on the work practices of call takers and dispatchers. Results show that video calls could provide valuable contextual information about a situation and help to overcome call taker challenges with information ambiguity, location, deceit, and communication issues. Yet video calls have the potential to introduce issues around control, information overload, and privacy if systems are not designed well. These results point to the need to think about emergency video calling along a continuum of visual modalities ranging from audio calls accompanied with images or video clips to one-way video streams to two-way video streams where camera control and camera work need to be carefully designed.","dir":"content/output/publications","base":"chi-2018-neustaedter.json","ext":".json","sourceBase":"chi-2018-neustaedter.yaml","sourceExt":".yaml"},"content/output/publications/chi-2018-oh.json":{"date":"2018-05","title":"PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-Based Electronic Devices","authors":["Hyunjoo Oh","Tung D. Ta","Ryo Suzuki","Mark D. Gross","Yoshihiro Kawahara","Lining Yao"],"series":"CHI 2018","doi":"https://doi.org/10.1145/3173574.3174015","keywords":"paper electronics, 3d sculpting, paper craft, fabrication techniques, prototyping","video":"https://www.youtube.com/watch?v=DTd863suDN0","pages":12,"abstract":"We present PEP (Printed Electronic Papercrafts), a set of design and fabrication techniques to integrate electronic based interactivities into printed papercrafts via 3D sculpting. We explore the design space of PEP, integrating four functions into 3D paper products: actuation, sensing, display, and communication, leveraging the expressive and technical opportunities enabled by paper-like functional layers with a stack of paper. We outline a seven-step workflow, introduce a design tool we developed as an add-on to an existing CAD environment, and demonstrate example applications that combine the electronic enabled functionality, the capability of 3D sculpting, and the unique creative affordances by the materiality of paper.","dir":"content/output/publications","base":"chi-2018-oh.json","ext":".json","sourceBase":"chi-2018-oh.yaml","sourceExt":".yaml"},"content/output/publications/chi-2018-suzuki.json":{"date":"2018-05","title":"Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation","authors":["Ryo Suzuki","Jun Kato","Mark D. Gross","Tom Yeh"],"series":"CHI 2018","doi":"https://doi.org/10.1145/3173574.3173773","keywords":"direct manipulation, tangible programming, swarm user interfaces, programming by demonstration","video":"https://www.youtube.com/watch?v=Gb7brajKCVE","pages":13,"abstract":"We explore a new approach to programming swarm user interfaces (Swarm UI) by leveraging direct physical manipulation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high-level interface design. Inspired by current UI programming practices, we introduce a four-step workflow-create elements, abstract attributes, specify behaviors, and propagate changes-for Swarm UI programming. We propose a set of direct physical manipulation techniques to support each step in this workflow. To demonstrate these concepts, we developed Reactile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studies-an in-class survey with 148 students and a lab interview with eight participants-confirm that our approach is intuitive and understandable for programming Swarm UIs.","dir":"content/output/publications","base":"chi-2018-suzuki.json","ext":".json","sourceBase":"chi-2018-suzuki.yaml","sourceExt":".yaml"},"content/output/publications/chi-2018-wuertz.json":{"date":"2018-04","title":"A Design Framework for Awareness Cues in Distributed Multiplayer Games","authors":["Jason Wuertz","Sultan A. Alharthi","William A. Hamilton","Scott Bateman","Carl Gutwin","Anthony Tang","Zachary O. Toups","Jessica Hammer"],"series":"CHI 2018","doi":"https://doi.org/10.1145/3173574.3173817","keywords":"workspace awareness, situation awareness, game design, distributed multiplayer games, awareness cues","pages":14,"abstract":"In the physical world, teammates develop situation awareness about each other's location, status, and actions through cues such as gaze direction and ambient noise. To support situation awareness, distributed multiplayer games provide awareness cues - information that games automatically make available to players to support cooperative gameplay. The design of awareness cues can be extremely complex, impacting how players experience games and work with teammates. Despite the importance of awareness cues, designers have little beyond experiential knowledge to guide their design. In this work, we describe a design framework for awareness cues, providing insight into what information they provide, how they communicate this information, and how design choices can impact play experience. Our research, based on a grounded theory analysis of current games, is the first to provide a characterization of awareness cues, providing a palette for game designers to improve design practice and a starting point for deeper research into collaborative play.","dir":"content/output/publications","base":"chi-2018-wuertz.json","ext":".json","sourceBase":"chi-2018-wuertz.yaml","sourceExt":".yaml"},"content/output/publications/chi-2019-danyluk.json":{"date":"2019-04","title":"Look-From Camera Control for 3D Terrain Maps","authors":["Kurtis Danyluk","Bernhard Jenny","Wesley Willett"],"series":"CHI 2019","doi":"https://doi.org/10.1145/3290605.3300594","keywords":"terrain, touch, map interaction, look-from camera control","award":"Honorable Mention","pages":12,"abstract":"We introduce three lightweight interactive camera control techniques for 3D terrain maps on touch devices based on a look-from metaphor (Discrete Look-From-At, Continuous Look-From-Forwards, and Continuous Look-From-Towards). These techniques complement traditional touch screen pan, zoom, rotate, and pitch controls allowing viewers to quickly transition between top-down, oblique, and ground-level views. We present the results of a study in which we asked participants to perform elevation comparison and line-of-sight determination tasks using each technique. Our results highlight how look-from techniques can be integrated on top of current direct manipulation navigation approaches by combining several direct manipulation operations into a single look-from operation. Additionally, they show how look-from techniques help viewers complete a variety of common and challenging map-based tasks.","dir":"content/output/publications","base":"chi-2019-danyluk.json","ext":".json","sourceBase":"chi-2019-danyluk.yaml","sourceExt":".yaml"},"content/output/publications/chi-2019-george.json":{"date":"2019-05","title":"Improving Texture Discrimination in Virtual Tasks by using Stochastic Resonance","authors":["Sandeep Zechariah George","Hooman Khosravi","Ryan Peters","Lora Oehlberg","Sonny Chan"],"series":"CHI 2019 LBW","doi":"10.1145/3290607.3312839","keywords":"Stochastic resonance, virtual tasks, haptics, virtual reality, texture rendering","pages":6,"abstract":"We investigate enhancing virtual haptic experiences by applying Stochastic Resonance or SR noise to the user's hands. Specifically, we focus on improving users' ability to discriminate between virtual textures modelled from nine grits of real sandpaper in a virtual texture discrimination task. We applied mechanical SR noise to the participant's skin by attaching five flat actuators to different points on their hand. By fastening a linear voice-coil actuator and a 6-DOF haptic device to participants' index finger, we enabled them to interact and feel virtual sandpapers while inducing different levels of SR noise. We hypothesize that SR will improve their discrimination performance.","dir":"content/output/publications","base":"chi-2019-george.json","ext":".json","sourceBase":"chi-2019-george.yaml","sourceExt":".yaml"},"content/output/publications/chi-2020-anjani.json":{"date":"2020-04","title":"Why do people watch others eat food? An Empirical Study on the Motivations and Practices of Mukbang Viewers","authors":["Laurensia Anjani","Terrance Mok","Anthony Tang","Lora Oehlberg","Wooi Boon Goh"],"series":"CHI 2020","doi":"https://doi.org/10.1145/3313831.3376567","keywords":"video streams, mukbang","pages":13,"talk":"https://www.youtube.com/watch?v=Dkp8A_em90M","abstract":"We present a mixed-methods study of viewers on their practices and motivations around watching mukbang  video streams of people eating large quantities of food. Viewers' experiences provide insight on future technologies for multisensorial video streams and technology-supported commensality (eating with others). We surveyed 104 viewers and interviewed 15 of them about their attitudes and reflections on their mukbang viewing habits, their physiological aspects of watching someone eat, and their perceived social relationship with mukbangers. Based on our findings, we propose design implications for remote commensality, and for synchronized multisensorial video streaming content.","dir":"content/output/publications","base":"chi-2020-anjani.json","ext":".json","sourceBase":"chi-2020-anjani.yaml","sourceExt":".yaml"},"content/output/publications/chi-2020-asha.json":{"date":"2020-04","title":"Views from the Wheelchair: Understanding Interaction between Autonomous Vehicle and Pedestrians with Reduced Mobility","authors":["Ashratuz Zavin Asha","Christopher Smith","Lora Oehlberg","Sowmya Somanath","Ehud Sharlin"],"series":"CHI 2020 LBW","doi":"10.1145/3334480.3383041","keywords":"Autonomous vehicle, pedestrian with reduced mobility","pages":8,"video":"http://www.youtube.com/watch?v=JNc49desa44","abstract":"We are interested in the ways pedestrians will interact with autonomous vehicle (AV) in a future AV transportation ecosystem, when nonverbal cues from the driver such as eye movements, hand gestures, etc. are no longer provided. In this work, we examine a subset of this challenge: interaction between pedestrian with reduced mobility (PRM) and AV. This study explores interface designs between AVs and people in a wheelchair to help them interact with AVs by conducting a preliminary design study. We have assessed the data collected from the study using qualitative analysis and presented different findings on AV-PRM interactions. Our findings reflect on the importance of visual interfaces, changes to the wheelchair and the creative use of the street infrastructure.","dir":"content/output/publications","base":"chi-2020-asha.json","ext":".json","sourceBase":"chi-2020-asha.yaml","sourceExt":".yaml"},"content/output/publications/chi-2020-goffin.json":{"date":"2020-04","title":"Interaction Techniques for Visual Exploration Using Embedded Word-Scale Visualizations","authors":["Pascal Goffin","Tanja Blascheck","Petra Isenberg","Wesley Willett"],"series":"CHI 2020","doi":"https://doi.org/10.1145/3313831.3376842","keywords":"glyphs, word-scale visualization, information visualization, interaction techniques, text visualization","pages":13,"video":"https://www.youtube.com/watch?v=wPaVdSWM8hU","abstract":"We describe a design space of view manipulation interactions for small data-driven contextual visualizations (word-scale visualizations). These interaction techniques support an active reading experience and engage readers through exploration of embedded visualizations whose placement and content connect them to specific terms in a document. A reader could, for example, use our proposed interaction techniques to explore word-scale visualizations of stock market trends for companies listed in a market overview article. When readers wish to engage more deeply with the data, they can collect, arrange, compare, and navigate the document using the embedded word-scale visualizations, permitting more visualization-centric analyses. We support our design space with a concrete implementation, illustrate it with examples from three application domains, and report results from two experiments. The experiments show how view manipulation interactions helped readers examine embedded visualizations more quickly and with less scrolling and yielded qualitative feedback on usability and future opportunities.","dir":"content/output/publications","base":"chi-2020-goffin.json","ext":".json","sourceBase":"chi-2020-goffin.yaml","sourceExt":".yaml"},"content/output/publications/chi-2020-hou.json":{"date":"2020-04","title":"Autonomous Vehicle-Cyclist Interaction: Peril and Promise","authors":["Ming Hou","Karthik Mahadevan","Sowmya Somanath","Ehud Sharlin","Lora Oehlberg"],"series":"CHI 2020","doi":"https://doi.org/10.1145/3313831.3376884","keywords":"autonomous vehicle-cyclist interaction, interfaces for communicating intent and awareness","video":"https://www.youtube.com/watch?v=fsgbUeAaFfI","talk":"https://www.youtube.com/watch?v=DtxkWAW9B1s","pages":12,"abstract":"Autonomous vehicles (AVs) will redefine interactions between road users. Presently, cyclists and drivers communicate through implicit cues (vehicle motion) and explicit but imprecise signals (hand gestures, horns). Future AVs could consistently communicate awareness and intent and other feedback to cyclists based on their sensor data. We present an exploration of AV-cyclist interaction, starting with preliminary design studies which informed the implementation of an immersive VR AV-cyclist simulator, and the design and evaluation of a number of AV-cyclist interfaces. Our findings suggest that AV-cyclist interfaces can improve rider confidence in lane merging scenarios. We contribute an AV-cyclist immersive simulator, insights on trade-offs of various aspects of AV-cyclist interaction design including modalities, location, and complexity, and positive results suggesting improved rider confidence due to AV-cyclist interaction. While we are encouraged by the potential positive impact AV-cyclist interfaces can have on cyclist culture, we also emphasize the risks over-reliance can pose to cyclists.","dir":"content/output/publications","base":"chi-2020-hou.json","ext":".json","sourceBase":"chi-2020-hou.yaml","sourceExt":".yaml"},"content/output/publications/chi-2020-suzuki.json":{"date":"2020-05","title":"RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots","authors":["Ryo Suzuki","Hooman Hedayati","Clement Zheng","James Bohn","Daniel Szafir","Ellen Yi-Luen Do","Mark D Gross","Daniel Leithinger"],"series":"CHI 2020","doi":"https://doi.org/10.1145/3313831.3376523","keywords":"virtual reality, room-scale haptics, haptic interfaces, swarm robots","video":"https://www.youtube.com/watch?v=4OWU60gTOFE","pages":11,"abstract":"RoomShift is a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.","dir":"content/output/publications","base":"chi-2020-suzuki.json","ext":".json","sourceBase":"chi-2020-suzuki.yaml","sourceExt":".yaml"},"content/output/publications/chi-2021-danyluk.json":{"date":"2021-05","title":"A Design Space Exploration of Worlds in Miniature","authors":["Kurtis Danyluk","Barrett Ens","Bernhard Jenny","Wesley Willett"],"series":"CHI 2021","doi":null,"keywords":"Virtual/Augmented Reality, Meta-Analysis/Literature Survey","pages":20,"video":null,"abstract":"Worlds-in-Miniature (WiMs) are interactive worlds within a world and combine the advantages of an input space, a cartographicmap, and an overview+detail interface. They have been used across the extended virtuality spectrum for a variety of applications.Building on an analysis of examples of WiMs from the research literature we contribute a design space for WiMs based on sevendesign dimensions. Further, we expand upon existing definitions of WiMs to provide a definition that applies across the extendedreality spectrum. We identify the design dimensions of size-scope-scale, abstraction, geometry, reference frame, links, multiples, andvirtuality. Using our framework we describe existing Worlds-in-Miniature from the research literature and reveal unexplored researchareas. Finally, we generate new examples of WiMs using our framework to fill some of these gaps. With our findings, we identifyopportunities that can guide future research into WiMs.","dir":"content/output/publications","base":"chi-2021-danyluk.json","ext":".json","sourceBase":"chi-2021-danyluk.yaml","sourceExt":".yaml"},"content/output/publications/chi-2021-ens.json":{"date":"2021-05","title":"Grand Challenges in Immersive Analytics","authors":["Barrett Ens","Benjamin Bach","Maxime Cordeil","Ulrich Engelke","Marcos Serrano","Wesley Willett","Arnaud Prouzeau","Christoph Anthes","Wolfgang Bschel","Cody Dunne","Tim Dwyer","Jens Grubert","Jason H. Haga","Nurit Kishenbaum","Dylan Kobayashi","Tica Lin","Monsurat Olaosebikan","Fabian Pointecker","David Saffo","Nazmus Saquib","Dieter Schmalsteig","Danielle Albers Szafir","Matthew Whitlock","Yalong Yang"],"series":"CHI 2021","doi":null,"keywords":"Immersive analytics, grand research challenges, data visualisation, augmented reality, virtual reality","pages":17,"video":null,"abstract":"Immersive Analytics is a quickly evolving field that unites several areas such as visualisation, immersive environments, and human-computer interaction to support human data analysis with emerging technologies. This research has thrived over the past years with multiple workshops, seminars, and a growing body of publications, spanning several conferences. Given the rapid advancement of interaction technologies and novel application domains, this paper aims toward a broader research agenda to enable widespread adoption. We present 17 key research challenges developed over multiple sessions by a diverse group of 24 international experts, initiated from a virtual scientific workshop at ACM CHI 2020. These challenges aim to coordinate future work by providing a systematic roadmap of current directions and impending hurdles to facilitate productive and effective applications for Immersive Analytics.","dir":"content/output/publications","base":"chi-2021-ens.json","ext":".json","sourceBase":"chi-2021-ens.yaml","sourceExt":".yaml"},"content/output/publications/chi-2021-hammad.json":{"date":"2021-05","title":"Homecoming: Exploring Returns to Long-Term Single Player Games","authors":["Noor Hammad","Owen Brierley","Zachary McKendrick","Sowmya Somanath","Patrick Finn","Jessica Hammer","Ehud Sharlin"],"series":"CHI 2021","doi":"https://doi.org/10.1145/3411764.3445357","keywords":"long-term single player game, autobiographical design, pivot point","pages":13,"video":null,"abstract":"We present an autobiographical design journey exploring the experience of returning to long-term single player games. Continuing progress from a previously saved game, particularly when substantial time has passed, is an understudied area in games research. To begin our exploration in this domain, we investigated what the return experience is like first-hand. By returning to four long-term single player games played extensively in the past, we revealed a phenomenon we call The Pivot Point, a eureka moment in return gameplay. The pivot point anchors our design explorations, where we created prototypes to leverage the pivot point in reconnecting with the experience. These return experiences and subsequent prototyping iterations inform our understanding of how to design better returns to gameplay, which can benefit both producers and consumers of long-term single player games.","dir":"content/output/publications","base":"chi-2021-hammad.json","ext":".json","sourceBase":"chi-2021-hammad.yaml","sourceExt":".yaml"},"content/output/publications/chi-2022-bressa.json":{"date":"2022-04","title":"Data Every Day: Designing and Living with Personal Situated Visualizations","authors":["Nathalie Bressa","Jo Vermeulen","Wesley Willett"],"series":"CHI 2022","doi":"https://doi.org/10.1145/3491102.3517737","keywords":"self-tracking, situated visualization, personal data","pages":18,"video":"https://www.youtube.com/watch?v=B0bKMgDd1xY","abstract":"We explore the design and utility of situated manual self-tracking visualizations on dedicated displays that integrate data tracking into existing practices and physical environments. Situating self-tracking tools in relevant locations is a promising approach to enable reflection on and awareness of data without needing to rely on sensorized tracking or personal devices. In both a long-term autobiographical design process and a co-design study with six participants, we rapidly prototyped and deployed 30 situated self-tracking applications over a ten month period. Grounded in the experience of designing and living with these trackers, we contribute findings on logging and data entry, the use of situated displays, and the visual design and customization of trackers. Our results demonstrate the potential of customizable dedicated self-tracking visualizations that are situated in relevant physical spaces, and suggest future research opportunities and new potential applications for situated visualizations.","dir":"content/output/publications","base":"chi-2022-bressa.json","ext":".json","sourceBase":"chi-2022-bressa.yaml","sourceExt":".yaml"},"content/output/publications/chi-2022-ivanov.json":{"date":"2022-04","title":"One Week in the Future: Previs Design Futuring for HCI Research","authors":["Sasha Ivanov","Tim Au Yeung","Kathryn Blair","Kurtis Danyluk","Georgina Freeman","Marcus Friedel","Carmen Hull","Michael Hung","Sydney Pratte","Wesley Willett"],"series":"CHI 2022","doi":"https://doi.org/10.1145/3491102.3517584","keywords":"design futuring, prototyping, previsualization","pages":15,"video":"https://www.youtube.com/watch?v=qoIwYW83iSU","abstract":"We explore the use of cinematic pre-visualization (previs) techniques as a rapid ideation and design futuring method for human computer interaction (HCI) research. Previs approaches, which are widely used in animation and film production, use digital design tools to create medium-fidelity videos that capture richer interaction, motion, and context than sketches or static illustrations. When used as a design futuring method, previs can facilitate rapid, iterative discussions that reveal tensions, challenges, and opportunities for new research. We performed eight one-week design futuring sprints, in which individual HCI researchers collaborated with a lead designer to produce concept sketches, storyboards, and videos that examined future applications of their research. From these experiences, we identify recurring themes and challenges and present a One Week Futuring Workbook that other researchers can use to guide their own futuring sprints. We also highlight how variations of our approach could support other speculative design practices.","dir":"content/output/publications","base":"chi-2022-ivanov.json","ext":".json","sourceBase":"chi-2022-ivanov.yaml","sourceExt":".yaml"},"content/output/publications/chi-2022-nittala.json":{"date":"2022-04","title":"Next Steps in Epidermal Computing: Opportunities and Challenges for Soft On-Skin Devices","authors":["Aditya Shekhar Nittala","Jrgen Steimle"],"series":"CHI 2022","doi":"https://doi.org/10.1145/3491102.3517668","keywords":"wearable devices, epidermal devices, survey, soft wearables","video":"https://www.youtube.com/watch?v=Lj9Yk5IQsok","pages":22,"abstract":"Skin is a promising interaction medium and has been widely explored for mobile, and expressive interaction. Recent research in HCI has seen the development of Epidermal Computing Devices: ultra-thin and non-invasive devices which reside on the users skin, offering intimate integration with the curved surfaces of the body, while having physical and mechanical properties that are akin to skin, expanding the horizon of on-body interaction. However, with rapid technological advancements in multiple disciplines, we see a need to synthesize the main open research questions and opportunities for the HCI community to advance future research in this area. By systematically analyzing Epidermal Devices contributed in the HCI community, physical sciences research and from our experiences in designing and building Epidermal Devices, we identify opportunities and challenges for advancing research across five themes. This multi-disciplinary synthesis enables multiple research communities to facilitate progression towards more coordinated endeavors for advancing Epidermal Computing.","dir":"content/output/publications","base":"chi-2022-nittala.json","ext":".json","sourceBase":"chi-2022-nittala.yaml","sourceExt":".yaml"},"content/output/publications/chi-2022-suzuki.json":{"date":"2022-04","title":"Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces","authors":["Ryo Suzuki","Adnan Karim","Tian Xia","Hooman Hedayati","Nicolai Marquardt"],"series":"CHI 2022","doi":"https://doi.org/10.1145/3491102.3517719","keywords":"augmented reality, mixed reality, robotics, human-robot interaction, actuated tangible UI, shape-changing UI, AR-HRI, VAM-HRI","video":"https://www.youtube.com/watch?v=MvOWxQC_4uQ","pages":33,"abstract":"This paper contributes to a taxonomy of augmented reality and robotics based on a survey of 460 research papers. Augmented and mixed reality (AR/MR) have emerged as a new way to enhance human-robot interaction (HRI) and robotic interfaces (e.g., actuated and shape-changing interfaces). Recently, an increasing number of studies in HCI, HRI, and robotics have demonstrated how AR enables better interactions between people and robots. However, often research remains focused on individual explorations and key design strategies, and research questions are rarely analyzed systematically. In this paper, we synthesize and categorize this research field in the following dimensions: 1) approaches to augmenting reality; 2) characteristics of robots; 3) purposes and benefits; 4) classification of presented information; 5) design components and strategies for visual augmentation; 6) interaction techniques and modalities; 7) application domains; and 8) evaluation strategies. We formulate key challenges and opportunities to guide and inform future research in AR and robotics.","dir":"content/output/publications","base":"chi-2022-suzuki.json","ext":".json","sourceBase":"chi-2022-suzuki.yaml","sourceExt":".yaml"},"content/output/publications/chi-2023-faridan.json":{"date":"2023-04","title":"ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms","authors":["Mehrad Faridan","Bheesha Kumari","Ryo Suzuki"],"series":"CHI 2023","doi":"https://doi.org/10.1145/3544548.3581381","keywords":"mixed reality, visual cue, remote collaboration, telepresence, remote guidance, human surrogates, hands-on training","video":"https://www.youtube.com/watch?v=VOe3fETd3sk","pages":13,"abstract":"We present ChameleonControl, a real-human teleoperation system for scalable remote instruction in hands-on classrooms. In contrast to the existing video or AR/VR-based remote hands-on education, ChameleonControl uses a real human as a surrogate of a remote instructor. Building on existing human-based telepresence approaches (e.g. ChameleonMask), we contribute a novel method to teleoperate a human surrogate through synchronized mixed reality (MR) hand gestural navigation and verbal communication. By overlaying the remote instructor's virtual hands in the local user's MR view, the remote instructor can guide and control the local user as if they were physically present. This allows the local user/surrogate to synchronize their hand movements and gestures with the remote instructor, effectively ``teleoperating'' a real human. We evaluate our system through the in-the-wild deployment for physiotherapy classrooms, as well as lab-based experiments for other application domains such as mechanical assembly, sign language, and cooking lessons. The study results confirm that our approach can increase engagement and the sense of co-presence, showing potential for the future of remote hands-on classrooms.","dir":"content/output/publications","base":"chi-2023-faridan.json","ext":".json","sourceBase":"chi-2023-faridan.yaml","sourceExt":".yaml"},"content/output/publications/chi-2023-monteiro.json":{"date":"2023-04","title":"Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching","authors":["Kyzyl Monteiro","Ritik Vatsal","Neil Chulpongsatorn","Aman Parnami","Ryo Suzuki"],"series":"CHI 2023","doi":"https://doi.org/10.1145/3544548.3581449","keywords":"augmented reality, mixed reality, prototyping tools, tangible interactions, everyday objects, interactive machine teaching, human-centered machine learning","video":"https://www.youtube.com/watch?v=JssiyfrhIJw","pages":15,"abstract":"This paper introduces Teachable Reality, an augmented reality (AR) prototyping tool for creating interactive tangible AR applications with arbitrary everyday objects. Teachable Reality leverages vision-based interactive machine teaching (e.g., Teachable Machine), which captures real-world interactions for AR prototyping. It identifies the user-defined tangible and gestural interactions using an on-demand computer vision model. Based on this, the user can easily create functional AR prototypes without programming, enabled by a trigger-action authoring interface. Therefore, our approach allows the flexibility, customizability, and generalizability of tangible AR applications that can address the limitation of current marker-based approaches. We explore the design space and demonstrate various AR prototypes, which include tangible and deformable interfaces, context-aware assistants, and body-driven AR applications. The results of our user study and expert interviews confirm that our approach can lower the barrier to creating functional AR prototypes while also allowing flexible and general-purpose prototyping experiences.","dir":"content/output/publications","base":"chi-2023-monteiro.json","ext":".json","sourceBase":"chi-2023-monteiro.yaml","sourceExt":".yaml"},"content/output/publications/chi-ea-2022-blair.json":{"date":"2022-04","title":"Art is Not Research. Research is not Art","authors":["Kathryn Blair","Miriam Sturdee","Lindsay Macdonald Vermeulen","Lora Oehlberg"],"series":"CHI EA 2022","doi":"https://doi.org/10.1145/3491101.3516391","keywords":"Interdisciplinary research, research ethics, arts and computing, research methods, knowledge creation","video":"https://www.youtube.com/watch?v=G0CSYn_uhIE","pages":9,"abstract":"Art is not Research. Research is not Art. is a multimedia, multi-site participatory installation by a collective of artists and researchers from Calgary, Toronto, and Lancaster; it is informed by these contexts. It reflects the tensions between how participants are treated in participatory art and interaction research. It offers a framework through which we can explore how epistemologies might evolve in a blending between Art and Research. Visitors download the paper to read, critically reflect on the relationship between art and research, and experientially engage with the material through a series of creative prompts. A performance variation of the piece will be performed in-person and online through the ACM SIGCHI Conference on Human Factors in Computing Systems alt.chi track.","dir":"content/output/publications","base":"chi-ea-2022-blair.json","ext":".json","sourceBase":"chi-ea-2022-blair.yaml","sourceExt":".yaml"},"content/output/publications/chi-ea-2023-chulpongsatorn.json":{"date":"2023-04","title":"HoloTouch: Interacting with Mixed Reality Visualizations Through Smartphone Proxies","authors":["Neil Chulpongsatorn","Wesley Willett","Ryo Suzuki"],"series":"CHI EA 2023","doi":"https://doi.org/10.1145/3544549.3585738","keywords":"mixed reality, embedded data visualization, tangible interaction, cross-device interaction","pages":8,"abstract":"We contribute interaction techniques for augmenting mixed reality (MR) visualizations with smartphone proxies. By combining head-mounted displays (HMDs) with mobile touchscreens, we can augment low-resolution holographic 3D charts with precise touch input, haptics feedback, high-resolution 2D graphics, and physical manipulation. Our approach aims to complement both MR and physical visualizations. Most current MR visualizations suffer from unreliable tracking, low visual resolution, and imprecise input. Data physicalizations on the other hand, although allowing for natural physical manipulation, are limited in dynamic and interactive modification. We demonstrate how mobile devices such as smartphones or tablets can serve as physical proxies for MR data interactions, creating dynamic visualizations that support precise manipulation and rich input and output. We describe 6 interaction techniques that leverage the combined physicality, sensing, and output capabilities of HMDs and smartphones, and demonstrate those interactions via a prototype system. Based on an evaluation, we outline opportunities for combining the advantages of both MR and physical charts.","dir":"content/output/publications","base":"chi-ea-2023-chulpongsatorn.json","ext":".json","sourceBase":"chi-ea-2023-chulpongsatorn.yaml","sourceExt":".yaml"},"content/output/publications/chi-ea-2023-fang.json":{"date":"2023-04","title":"VR Haptics at Home: Repurposing Everyday Objects and Environment for Casual and On-Demand VR Haptic Experiences","authors":["Cathy Mengying Fang","Ryo Suzuki","Daniel Leithinger"],"series":"CHI EA 2023","doi":"https://doi.org/10.1145/3544549.3585871","keywords":"virtual reality, interaction techniques, passive haptics","pages":7,"abstract":"This paper introduces VR Haptics at Home, a method of repurposing everyday objects in the home to provide casual and on-demand haptic experiences. Current VR haptic devices are often expensive, complex, and unreliable, which limits the opportunities for rich haptic experiences outside research labs. In contrast, we envision that, by repurposing everyday objects as passive haptics props, we can create engaging VR experiences for casual uses with minimal cost and setup. To explore and evaluate this idea, we conducted an in-the-wild study with eight participants, in which they used our proof-of-concept system to turn their surrounding objects such as chairs, tables, and pillows at their own homes into haptic props. The study results show that our method can be adapted to different homes and environments, enabling more engaging VR experiences without the need for complex setup process. Based on our findings, we propose a possible design space to showcase the potential for future investigation.","dir":"content/output/publications","base":"chi-ea-2023-fang.json","ext":".json","sourceBase":"chi-ea-2023-fang.yaml","sourceExt":".yaml"},"content/output/publications/cmj-2020-ko.json":{"date":"2020-01","title":"Construction and Performance Applications of an Augmented Violin: TRAVIS II","authors":["Chantelle Ko","Lora Oehlberg"],"series":"CMJ 2020","doi":"https://doi.org/10.1162/comj_a_00563","pages":14,"abstract":"We present the second iteration of a Touch-Responsive Augmented Violin Interface System, called TRAVIS II, and two compositions that demonstrate its expressivity. TRAVIS II is an augmented acoustic violin with touch sensors integrated into its 3-D printed fingerboard that track left-hand finger gestures in real time. The fingerboard has four strips of conductive PLA filament that produce an electric signal when fingers press down on each string. Although these sensors are physically robust, they are mechanically assembled and thus easy to replace if damaged. The performer can also trigger presets via four sensors attached to the body of the violin. The instrument is completely wireless, giving the performer the freedom to move throughout the performance space. Although the sensing fingerboard is installed in place of the traditional fingerboard, all other electronics can be removed from the augmented instrument, maintaining the aesthetics of a traditional violin. Our design allows violinists to naturally create music for interactive performance and improvisation without requiring new instrumental techniques. The first author composed two compositions to highlight TRAVIS II: Dream State and Kindred Dichotomy. Both of these compositions involve improvisation in their creative process and include interactive visuals. In this article we describe the design of the instrument, experiments leading to the sensing fingerboard, performative applications of the instrument, and compositional considerations for the resultant pieces.","dir":"content/output/publications","base":"cmj-2020-ko.json","ext":".json","sourceBase":"cmj-2020-ko.yaml","sourceExt":".yaml"},"content/output/publications/cnc-2019-hammad.json":{"date":"2019-06","title":"Mutation: Leveraging Performing Arts Practices in Cyborg Transitioning","authors":["Nour Hammad","Elaheh Sanoubari","Patrick Finn","Sowmya Somanath","James E. Young","Ehud Sharlin"],"year":2019,"series":"C&C 2019","keywords":"interaction design, cyborgs, user experience, performing art techniques","video":"https://www.youtube.com/watch?v=HFH59__Fkok","doi":"https://doi.org/10.1145/3325480.3325508","abstract":"We present Mutation: performing arts based approach that can help decrease the cognitive load associated with cyborg transitioning. Cyborgs are human-machine hybrids with organic and mechatronic body parts that can be implanted or worn. The transition into and out of experiencing additional body parts is not fully understood. Our goal is to draw from performing arts techniques in order to help decrease the cognitive load associated with becoming and unbecoming a cyborg. Actors constantly shift between states, whether from one character to another, or from pre- to post- performance. We contribute a straightforward adaptation of classic performing art practices to cyborg transitioning, and a study where actors used these protocols in order to enter a cyborg state, perform as a cyborg, and then exit the cyborg state. Our work on Mutation suggests that classic performing art practices can be useful in cyborg transitioning, as well as in other technology augmented experiences.","dir":"content/output/publications","base":"cnc-2019-hammad.json","ext":".json","sourceBase":"cnc-2019-hammad.yaml","sourceExt":".yaml"},"content/output/publications/cupum-2021-rout.json":{"date":"2021-02","title":"(Big) Data in Urban Design Practice: Supporting High-Level Design Tasks Using a Visualization of Human Movement Data from Smartphones","authors":["Angela Rout","Wesley Willett"],"series":"Urban Informatics and Future Cities","publisher":"Springer","pages":"301-318","doi":"http://hdl.handle.net/1880/113114","video":"https://www.youtube.com/watch?v=Me8cU6RoCiA","keywords":"information visualization, smartphone data, GPS, data visualization, architecture, urban design, task-based framework, high-level tasks","abstract":"We present the SmartCampus visualization tool, representing spatiotemporal data of over 200 student pathways and restpoints on a university campus. Based on our experiences with SmartCampus, we also propose a task-based framework that de-scribes how practicing urban designers (specifically, architects) can use human movement data visualizations in their work. Although extensive amounts of location data are produced daily by smartphones, existing geospatial tools are not customized to specifically support high-level urban design tasks. To help identify opportunities in urban design for visualizing human movement data from devices such as smartphones, we used our SmartCampus prototype to facilitate a series of 3 participatory design sessions (3 participants), a targeted online survey (14 participants), and semi-structured interviews (6 participants) with architectural experts. Our findings showcase the need for location analysis tools tailored to concrete urban design practices, and also highlight opportunities for Smart City researchers interested in developing domain specific, visualization tools.","dir":"content/output/publications","base":"cupum-2021-rout.json","ext":".json","sourceBase":"cupum-2021-rout.yaml","sourceExt":".yaml"},"content/output/publications/dis-2016-jones.json":{"date":"2016-06","title":"Elevating Communication, Collaboration, and Shared Experiences in Mobile Video through Drones","authors":["Brennan Jones","Kody Dillman","Richard Tang","Anthony Tang","Ehud Sharlin","Lora Oehlberg","Carman Neustaedter","Scott Bateman"],"series":"DIS 2016","doi":"https://doi.org/10.1145/2901790.2901847","keywords":"cscw, telepresence, video communication, shared experiences, teleoperation, drones, collaboration, hri","video":"https://www.youtube.com/watch?v=10hbJHIQVX8","pages":13,"abstract":"People are increasingly using mobile video to communicate, collaborate, and share experiences while on the go. Yet this presents challenges in adequately sharing camera views with remote users. In this paper, we study the use of semi-autonomous drones for video conferencing, where an outdoor user (using a smartphone) is connected to a desktop user who can explore the environment from the drone's perspective. We describe findings from a study where pairs collaborated to complete shared navigation and search tasks. We illustrate the benefits of providing the desktop user with a view that is elevated, manipulable, and decoupled from the outdoor user. In addition, we articulate how participants overcame challenges in communicating environmental information and navigational cues, negotiated control of the view, and used the drone as a tool for sharing experiences. This provides a new way of thinking about mobile video conferencing where cameras that are decoupled from both users play an integral role in communication, collaboration, and sharing experiences.","dir":"content/output/publications","base":"dis-2016-jones.json","ext":".json","sourceBase":"dis-2016-jones.yaml","sourceExt":".yaml"},"content/output/publications/dis-2017-mok.json":{"date":"2017-06","title":"Critiquing Physical Prototypes for a Remote Audience","authors":["Terrance Mok","Lora Oehlberg"],"series":"DIS 2017","doi":"https://doi.org/10.1145/3064663.3064722","pages":13,"keywords":"design review, prototype critique, remote collaboration, material experience, open hardware, video conferencing","video":"https://youtu.be/ORN9jljPncc","abstract":"We present an observational study of physical prototype critique that highlights some of the challenges of communicating physical behaviors and materiality at a distance. Geographically distributed open hardware communities often conduct user feedback and peer critique sessions via video conference. However, people have difficulty using current video conferencing tools to demonstrate and critique physical designs. To examine the challenges of remote critique, we conducted an observational lab study in which participants critiqued pairs of physical prototypes (prosthetic hands) for a face-to-face or remote collaborator. In both conditions, participants' material experiences were an important part of their critique, however their attention was divided between interacting with the prototype and finding strategies to communicate `invisible' features. Based on our findings, we propose design implications for remote collaboration tools that support the sharing of material experiences and prototype critique.","dir":"content/output/publications","base":"dis-2017-mok.json","ext":".json","sourceBase":"dis-2017-mok.yaml","sourceExt":".yaml"},"content/output/publications/dis-2018-mikalauskas.json":{"date":"2018-06","title":"Improvising with an Audience-Controlled Robot Performer","authors":["Claire Mikalauskas","Tiffany Wun","Kevin Ta","Joshua Horacsek","Lora Oehlberg"],"series":"DIS 2018","doi":"https://doi.org/10.1145/3196709.3196757","pages":10,"keywords":"human-robot interaction, improvised theatre, creativity-support tools, crowdsourcing","video":"https://youtu.be/ORN9jljPncc","abstract":"In improvisational theatre (improv), actors perform unscripted scenes together, collectively creating a narrative. Audience suggestions introduce randomness and build audience engagement, but can be challenging to mediate at scale. We present Robot Improv Puppet Theatre (RIPT), which includes a performance robot (Pokey) who performs gestures and dialogue in short-form improv scenes based on audience input from a mobile interface. We evaluated RIPT in several initial informal performances, and in a rehearsal with seven professional improvisers. The improvisers noted how audience prompts can have a big impact on the scene - highlighting the delicate balance between ambiguity and constraints in improv. The open structure of RIPT performances allows for multiple interpretations of how to perform with Pokey, including one-on-one conversations or multi-performer scenes. While Pokey lacks key qualities of a good improviser, improvisers found his serendipitous dialogue and gestures particularly rewarding.","dir":"content/output/publications","base":"dis-2018-mikalauskas.json","ext":".json","sourceBase":"dis-2018-mikalauskas.yaml","sourceExt":".yaml"},"content/output/publications/dis-2018-pham.json":{"date":"2018-06","title":"Scale Impacts Elicited Gestures for Manipulating Holograms: Implications for AR Gesture Design","authors":["Tran Pham","Jo Vermeulen","Anthony Tang","Lindsay MacDonald Vermeulen"],"series":"DIS 2018","doi":"https://doi.org/10.1145/3196709.3196719","keywords":"augmented reality, gestures, gesture elicitation, hololens","pages":14,"abstract":"Because gesture design for augmented reality (AR) remains idiosyncratic, people cannot necessarily use gestures learned in one AR application in another. To design discoverable gestures, we need to understand what gestures people expect to use. We explore how the scale of AR affects the gestures people expect to use to interact with 3D holograms. Using an elicitation study, we asked participants to generate gestures in response to holographic task referents, where we varied the scale of holograms from desktop-scale to room-scale objects. We found that the scale of objects and scenes in the AR experience moderates the generated gestures. Most gestures were informed by physical interaction, and when people interacted from a distance, they sought a good perspective on the target object before and during the interaction. These results suggest that gesture designers need to account for scale, and should not simply reuse gestures across different hologram sizes.","dir":"content/output/publications","base":"dis-2018-pham.json","ext":".json","sourceBase":"dis-2018-pham.yaml","sourceExt":".yaml"},"content/output/publications/dis-2018-ta.json":{"date":"2018-06","title":"Bod-IDE: An Augmented Reality Sandbox for eFashion Garments","authors":["Kevin Ta","Ehud Sharlin","Lora Oehlberg"],"series":"DIS 2018","doi":"https://doi.org/10.1145/3197391.3205408","keywords":"augmented reality, electronic fashion, creativity support tool","pages":5,"abstract":"Electronic fashion (eFashion) garments use technology to augment the human body with wearable interaction. In developing ideas, eFashion designers need to prototype the role and behavior of the interactive garment in context; however, current wearable prototyping toolkits require semi-permanent construction with physical materials that cannot easily be altered. We present Bod-IDE, an augmented reality 'mirror' that allows eFashion designers to create virtual interactive garment prototypes. Designers can quickly build, refine, and test on-the-body interactions without the need to connect or program electronics. By envisioning interaction with the body in mind, eFashion designers can focus more on reimagining the relationship between bodies, clothing, and technology.","dir":"content/output/publications","base":"dis-2018-ta.json","ext":".json","sourceBase":"dis-2018-ta.yaml","sourceExt":".yaml"},"content/output/publications/dis-2019-blair.json":{"date":"2019-06","title":"Exploring Public Engagement with the Social Impact of Algorithms","authors":["Kathryn Blair","Jean-Rene Leblanc","Lora Oehlberg"],"series":"DIS 2019","doi":"10.1145/3301019.3323897","keywords":"Fine Arts, Physical Artifact, Education, Social Impact of Technology, Education, Participatory Art Installation","pages":5,"abstract":"We discuss Logical Conclusion, an analog interactive installation which presents issues surrounding the social impacts of algorithms used by corporations and governments via logic puzzles with physical elements that visitors manipulate to solve. We present the combination of physicality and participation as promising tools to engage the public with the ways that complex technologies interact with society. We also pose questions regarding how such strategies might be extended by the addition of responsive tangible computing elements.","dir":"content/output/publications","base":"dis-2019-blair.json","ext":".json","sourceBase":"dis-2019-blair.yaml","sourceExt":".yaml"},"content/output/publications/dis-2019-bressa.json":{"date":"2019-06","title":"Sketching and Ideation Activities for Situated Visualization Design","authors":["Nathalie Bressa","Kendra Wannamaker","Henrik Korsgaard","Wesley Willett","Jo Vermeulen"],"series":"DIS 2019","doi":"https://doi.org/10.1145/3322276.3322326","keywords":"ideation, design workshops, situated visualization, information visualization, small displays, sketching","abstract":"We report on findings from seven design workshops that used ideation and sketching activities to prototype new situated visualizations - representations of data that are displayed in proximity to the physical referents (such as people, objects, and locations) to which the data is related. Designing situated visualizations requires a fine-grained understanding of the context in which the visualizations are placed, as well as an exploration of different options for placement and form factors, which existing methods for visualization design do not account for. Focusing on small displays as a target platform, we reflect on our experiences of using a diverse range of sketching activities, materials, and prompts. Based on these observations, we identify challenges and opportunities for sketching and ideating situated visualizations. We also outline the space of design activities for situated visualization and highlight promising methods for both designers and researchers.","dir":"content/output/publications","base":"dis-2019-bressa.json","ext":".json","sourceBase":"dis-2019-bressa.yaml","sourceExt":".yaml"},"content/output/publications/dis-2019-ledo.json":{"date":"2019-06","title":"Astral: Prototyping Mobile and Smart Object Interactive Behaviours Using Familiar Applications","authors":["David Ledo","Jo Vermeulen","Sheelagh Carpendale","Saul Greenberg","Lora Oehlberg","Sebastian Boring"],"series":"DIS 2019","doi":"https://doi.org/10.1145/3322276.3322329","keywords":"smart objects, mobile interfaces, prototyping, design tool, interactive behaviour","pages":14,"abstract":"Astral is a prototyping tool for authoring mobile and smart object interactive behaviours. It mirrors selected display contents of desktop applications onto mobile devices (smartphones and smartwatches), and streams/remaps mobile sensor data to desktop input events (mouse or keyboard) to manipulate selected desktop contents. This allows designers to use familiar desktop applications (e.g. PowerPoint, AfterEffects) to prototype rich interactive behaviours. Astral combines and integrates display mirroring, sensor streaming and input remapping, where designers can exploit familiar desktop applications to prototype, explore and fine-tune dynamic interactive behaviours. With Astral, designers can visually author rules to test real-time behaviours while interactions take place, as well as after the interaction has occurred. We demonstrate Astral's applicability, workflow and expressiveness within the interaction design process through both new examples and replication of prior approaches that illustrate how various familiar desktop applications are leveraged and repurposed.","dir":"content/output/publications","base":"dis-2019-ledo.json","ext":".json","sourceBase":"dis-2019-ledo.yaml","sourceExt":".yaml"},"content/output/publications/dis-2019-mahadevan.json":{"date":"2019-06","title":"AV-Pedestrian Interaction Design Using a Pedestrian Mixed Traffic Simulator","authors":["Karthik Mahadevan","Elaheh Sanoubari","Sowmya Somanath","James E. Young","Ehud Sharlin"],"series":"DIS 2019","doi":"https://doi.org/10.1145/3322276.3322328","keywords":"mixed traffic, pedestrian simulator, autonomous vehicle-pedestrian interaction","pages":12,"abstract":"AV-pedestrian interaction will impact pedestrian safety, etiquette, and overall acceptance of AV technology. Evaluating AV-pedestrian interaction is challenging given limited availability of AVs and safety concerns. These challenges are compounded by \"mixed traffic\" conditions: studying AV-pedestrian interaction will be difficult in traffic consisting of vehicles varying in autonomy level. We propose immersive pedestrian simulators as design tools to study AV-pedestrian interaction, allowing rapid prototyping and evaluation of future AV-pedestrian interfaces. We present OnFoot: a VR-based simulator that immerses participants in mixed traffic conditions and allows examination of their behavior while controlling vehicles' autonomy-level, traffic and street characteristics, behavior of other virtual pedestrians, and integration of novel AV-pedestrian interfaces. We validated OnFoot against prior simulators and Wizard-of-Oz studies, and conducted a user study, manipulating vehicles' autonomy level, interfaces, and pedestrian group behavior. Our findings highlight the potential to use VR simulators as powerful tools for AV-pedestrian interaction design in mixed traffic.","dir":"content/output/publications","base":"dis-2019-mahadevan.json","ext":".json","sourceBase":"dis-2019-mahadevan.yaml","sourceExt":".yaml"},"content/output/publications/dis-2019-nakayama.json":{"date":"2019-06","title":"MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction","authors":["Ryosuke Nakayama","Ryo Suzuki","Satoshi Nakamaru","Ryuma Niiyama","Yoshihiro Kawahara","Yasuaki Kakehi"],"series":"DIS 2019","doi":"https://doi.org/10.1145/3322276.3322337","keywords":"shape-changing interfaces, programming by demonstration, soft robots, pneumatic actuation, tangible interactions","award":"Best Paper","video":"https://www.youtube.com/watch?v=ZkCcazfFD-M","pages":11,"abstract":"We introduce MorphIO, entirely soft sensing and actuation modules for programming by demonstration of soft robots and shape-changing interfaces.MorphIO's hardware consists of a soft pneumatic actuator containing a conductive sponge sensor.This allows both input and output of three-dimensional deformation of a soft material.Leveraging this capability, MorphIO enables a user to record and later playback physical motion of programmable shape-changing materials.In addition, the modular design of MorphIO's unit allows the user to construct various shapes and topologies through magnetic connection.We demonstrate several application scenarios, including tangible character animation, locomotion experiment of a soft robot, and prototyping tools for animated soft objects.Our user study with six participants confirms the benefits of MorphIO, as compared to the existing programming paradigm.","dir":"content/output/publications","base":"dis-2019-nakayama.json","ext":".json","sourceBase":"dis-2019-nakayama.yaml","sourceExt":".yaml"},"content/output/publications/dis-2019-seyed.json":{"date":"2019-06","title":"Mannequette: Understanding and Enabling Collaboration and Creativity on Avant-Garde Fashion-Tech Runways","authors":["Teddy Seyed","Anthony Tang"],"series":"DIS 2019","doi":"https://doi.org/10.1145/3322276.3322305","keywords":"fashion, haute couture, e-textiles, maker culture, fashion-tech, wearables, avant-garde, haute-tech couture, modular","award":"Honorable Mention","pages":13,"abstract":"Drawing upon multiple disciplines, avant-garde fashion-tech teams push the boundaries between fashion and technology. Many are well trained in envisioning aesthetic qualities of garments, but few have formal training on designing and fabricating technologies themselves. We introduce Mannequette, a prototyping tool for fashion-tech garments that enables teams to experiment with interactive technologies at early stages of their design processes. Mannequette provides an abstraction of light-based outputs and sensor-based inputs for garments through a DJ mixer-like interface that allows for dynamic changes and recording/playback of visual effects. The base of Mannequette can also be incorporated into the final garment, where it is then connected to the final components. We conducted an 8-week deployment study with eight design teams who created new garments for a runway show. Our results revealed Mannequette allowed teams to repeatedly consider new design and technical options early in their creative processes, and to communicate more effectively across disciplinary backgrounds.","dir":"content/output/publications","base":"dis-2019-seyed.json","ext":".json","sourceBase":"dis-2019-seyed.yaml","sourceExt":".yaml"},"content/output/publications/dis-2021-asha.json":{"date":"2021-06","title":"Co-Designing Interactions between Pedestrians in Wheelchairs and Autonomous Vehicles","authors":["Ashratuz Zavin Asha","Christopher Smith","Georgina Freeman","Sean Crump","Sowmya Somanath","Lora Oehlberg","Ehud Sharlin"],"series":"DIS 2021","doi":"10.1145/3461778.3462068","keywords":"pedestrians in wheelchairs, co-design, autonomous vehicles","pages":13,"abstract":"In the near future, mixed traffic consisting of manual and autonomous vehicles (AVs) will be common. Questions surrounding how vulnerable road users such as pedestrians in wheelchairs (PWs) will make crossing decisions in these new situations are underexplored. We conducted a remote co-design study with one of the researchers of this work who has the lived experience as a powered wheelchair user and applied inclusive design practices. This allowed us to identify and reflect on interface design ideas that can help PWs make safe crossing decisions at intersections. Through an iterative five-week study, we implemented interfaces that can be placed on the vehicle, on the wheelchair, and on the street infrastructure and evaluated them during the co-design sessions using a VR simulator testbed. Informed by our findings, we discuss design insights for implementing inclusive interfaces to improve interactions between autonomous vehicles and vulnerable road users.","dir":"content/output/publications","base":"dis-2021-asha.json","ext":".json","sourceBase":"dis-2021-asha.yaml","sourceExt":".yaml"},"content/output/publications/dis-2021-blair.json":{"date":"2021-06","title":"Participatory Art for Public Exploration of Algorithmic Decision-Making","authors":["Kathryn Blair","Pil Hansen","Lora Oehlberg"],"series":"DIS 2021","doi":"10.1145/3461778.3462068","keywords":"Fine Arts, Physical Artifact, Education, Social Impact of Technology, Education, Participatory Art Installation","pages":4,"abstract":"Machine learning and predictive algorithms find patterns in large stores of data and make predictions which corporations and governments use to support decision-making. Yet, the system's representation of reality can be more influential to outcomes than the complexities of daily life. They become problematic when they undermine the inclusivity of public decision making, and when their use perpetuates social or economic inequality. To address these challenges, the public must be able to participate in discourse about the implications of algorithmic systems. I propose a series of participatory installations exploring the impacts of algorithmic systems, providing contexts for active exploration of these concerns. I will conduct phenomenographic interviews to better understand how visitors experience art installations about technical topics, providing insight for subsequent installations. I will consolidate the results into a set of best practices about engaging the public on these topics.","dir":"content/output/publications","base":"dis-2021-blair.json","ext":".json","sourceBase":"dis-2021-blair.yaml","sourceExt":".yaml"},"content/output/publications/dis-2021-wannamaker.json":{"date":"2021-06","title":"I/O Bits: User-Driven, Situated, and Dedicated Self-Tracking","authors":["Kendra Wannamaker","Sandeep Kollannur","Marian Drk","Wesley Willett"],"series":"DIS 2021","doi":"http://hdl.handle.net/1880/113555","keywords":"information visualization, personal informatics, situated visualization","pages":10,"talk":"https://www.youtube.com/watch?v=yhMKURtgFZ0","abstract":"We present I/O Bits, a prototype personal informatics system that explores the potential for user-driven and situated self-tracking. With simple tactile inputs and small e-paper visualizations, I/O Bits are dedicated physical devices that allow individuals to track and visualize different kinds of personal activities in-situ. This is in contrast to most self-tracking systems, which automate data collection, centralize information displays, or integrate into multi-purpose devices like smartwatches or mobile phones. We report findings from an e-paper visualization workshop and a prototype deployment where participants constructed their own I/O Bits and used them to track a range of personal data. Based on these experiences, we contribute insights and opportunities for situated and user-driven personal informatics.","dir":"content/output/publications","base":"dis-2021-wannamaker.json","ext":".json","sourceBase":"dis-2021-wannamaker.yaml","sourceExt":".yaml"},"content/output/publications/frobt-2022-suzuki.json":{"date":"2022-04","title":"Designing Expandable-Structure Robots for Human-Robot Interaction","authors":["Hooman Hedayati","Ryo Suzuki","Wyatt Rees1","Daniel Leithinger","Daniel Szafir"],"series":"Frontiers 2022","doi":"https://doi.org/10.3389/frobt.2022.719639","keywords":"deployable robot, human-robot interaction, modular robot, origami robotics, deployable structures, shape-changing robots","pages":22,"abstract":"In this paper, we survey the emerging design space of expandable structures in robotics, with a focus on how such structures may improve human-robot interactions. We detail various implementation considerations for researchers seeking to integrate such structures in their own work and describe how expandable structures may lead to novel forms of interaction for a variety of different robots and applications, including structures that enable robots to alter their form to augment or gain entirely new capabilities, such as enhancing manipulation or navigation, structures that improve robot safety, structures that enable new forms of communication, and structures for robot swarms that enable the swarm to change shape both individually and collectively. To illustrate how these considerations may be operationalized, we also present three case studies from our own research in expandable structure robots, sharing our design process and our findings regarding how such structures enable robots to produce novel behaviors that may capture human attention, convey information, mimic emotion, and provide new types of dynamic affordances.","dir":"content/output/publications","base":"frobt-2022-suzuki.json","ext":".json","sourceBase":"frobt-2022-suzuki.yaml","sourceExt":".yaml"},"content/output/publications/gecco-2022-ivanov.json":{"date":"2022-07","title":"EvoIsland: Interactive Evolution via an Island-Inspired Spatial User Interface Framework","authors":["Sasha Ivanov","Wesley Willett","Christian Jacob"],"series":"GECCO 2022","doi":"https://doi.org/10.1145/3512290.3528722","keywords":"Interactive Evolutionary Systems, User Interfaces, Visualization","pages":8,"abstract":"We present EvoIsland, a scalable interactive evolutionary user interface framework inspired by the spatially isolated land masses seen on Earth. Our generalizable interaction system encourages creators to spatially explore a wide range of design possibilities through the combination, separation, and rearrangement of hexagonal tiles on a grid. As these tiles are grouped into islandlike clusters, localized populations of designs form through an underlying evolutionary system. The interactions that take place within EvoIsland provide content creators with new ways to shape, display and assess populations in evolutionary systems that produce a wide range of solutions with visual phenotype outputs.","dir":"content/output/publications","base":"gecco-2022-ivanov.json","ext":".json","sourceBase":"gecco-2022-ivanov.yaml","sourceExt":".yaml"},"content/output/publications/gi-2020-rajabiyazdi.json":{"date":"2020-05","title":"Exploring the Design of Patient-Generated Data Visualizations","authors":["Fateme Rajabiyazdi","Charles Perin","Lora Oehlberg","Sheelagh Carpendale"],"series":"GI 2020","doi":"https://hal.archives-ouvertes.fr/hal-02861239","keywords":"visualization, information visualization","pages":12,"abstract":"We were approached by a group of healthcare providers who are involved in the care of chronic patients looking for potential technologies to facilitate the process of reviewing patient-generated data during clinical visits. Aiming at understanding the healthcare providers attitudes towards reviewing patient-generated data, we (1) conducted a focus group with a mixed group of healthcare providers. Next, to gain the patients perspectives, we (2) interviewed eight chronic patients, collected a sample of their data and designed a series of visualizations representing patient data we collected. Last, we (3) sought feedback on the visualization designs from healthcare providers who requested this exploration. We found four factors shaping patient-generated data: data & context, patients motivation, patients time commitment, and patients support circle. Informed by the results of our studies, we discussed the importance of designing patient-generated visualizations for individuals by considering both patient and healthcare provider rather than designing with the purpose of generalization and provided guidelines for designing future patient-generated data visualizations.","dir":"content/output/publications","base":"gi-2020-rajabiyazdi.json","ext":".json","sourceBase":"gi-2020-rajabiyazdi.yaml","sourceExt":".yaml"},"content/output/publications/gi-2021-mactavish.json":{"date":"2021-05","title":"Perspective Charts","authors":["Mia MacTavish","Katayoon Etemad","Faramarz Samavati","Wesley Willett"],"series":"GI 2021","doi":"http://hdl.handle.net/1880/113671","keywords":"information visualization","video":"https://www.youtube.com/watch?v=Sp4Vt8mMhCs&list=PLZQ0ePfDvRH4Ac7xfBdPlMQX0d0NYQ7Y-","pages":10,"abstract":"We introduce three novel data visualizations, called perspective charts, based on the concept of size constancy in linear perspective projection. Bar charts are a popular and commonly used tool for the interpretation of datasets, however, representing datasets with multi-scale variation is challenging in a bar chart due to limitations in viewing space. Each of our designs focuses on the static representation of datasets with large ranges with respect to important variations in the data. Through a user study, we measure the effectiveness of our designs for representing these datasets in comparison to traditional methods, such as a standard bar chart or a broken-axis bar chart, and state-of-the-art methods, such as a scale-stack bar chart. The evaluation reveals that our designs allow pieces of data to be visually compared at a level of accuracy similar to traditional visualizations. Our designs demonstrate advantages when compared to state-of-the-art visualizations designed to represent datasets with large outliers.","dir":"content/output/publications","base":"gi-2021-mactavish.json","ext":".json","sourceBase":"gi-2021-mactavish.yaml","sourceExt":".yaml"},"content/output/publications/gi-2022-hull.json":{"date":"2022-05","title":"Simultaneous Worlds: Supporting Fluid Exploration of Multiple Data Sets via Physical Models","authors":["Carmen Hull","Sren Knudsen","Sheelagh Carpendale","Wesley Willett"],"series":"GI 2022","doi":"http://hdl.handle.net/1880/114742","keywords":"Information visualization, interactive surfaces, data physicalization, architectural models","pages":10,"abstract":"We take the well-established use of physical scale models in architecture and identify new opportunities for using them to interactively visualize and examine multiple streams of geospatial data. Overlaying, comparing, or integrating visualizations of complementary data sets in the same physical space is often challenging given the constraints of various data types and the limited design space of possible visual encodings. Our vision of simultaneous worlds uses physical models as a substrate upon which visualizations of multiple data streams can be dynamically and concurrently integrated. To explore the potential of this concept, we created three design explorations that use an illuminated campus model to integrate visualizations about building energy use, climate, and movement paths on a university campus. We use a research through design approach, documenting how our interdisciplinary collaborations with domain experts, students, and architects informed our designs. Based on our observations, we characterize the benefits of models for 1) situating visualizations, 2) composing visualizations, and 3) manipulating and authoring visualizations. Our work highlights the potential of physical models to support embodied exploration of spatial and non-spatial visualizations through fluid interactions.","dir":"content/output/publications","base":"gi-2022-hull.json","ext":".json","sourceBase":"gi-2022-hull.yaml","sourceExt":".yaml"},"content/output/publications/hri-2018-feick.json":{"date":"2018-04","title":"The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration","authors":["Martin Feick","Lora Oehlberg","Anthony Tang","Andr Miede","Ehud Sharlin"],"series":"HRI 2018","doi":"https://doi.org/10.1145/3173386.3176959","pages":2,"keywords":"movement trajectory & velocity, remote collaboration, robot surrogate","abstract":"In this paper, we discuss the role of the movement trajectory and velocity enabled by our tele-robotic system (ReMa) for remote collaboration on physical tasks. Our system reproduces changes in object orientation and position at a remote location using a humanoid robotic arm. However, even minor kinematics differences between robot and human arm can result in awkward or exaggerated robot movements. As a result, user communication with the robotic system can become less efficient, less fluent and more time intensive.","dir":"content/output/publications","base":"hri-2018-feick.json","ext":".json","sourceBase":"hri-2018-feick.yaml","sourceExt":".yaml"},"content/output/publications/ieee-2021-willett.json":{"date":"2021-09","title":"Perception! Immersion! Empowerment!: Superpowers as Inspiration for Visualization","authors":["Wesley Willett","Bon Adriel Aseniero","Sheelagh Carpendale","Pierre Dragicevic","Yvonne Jansen","Lora Oehlberg","Petra Isenberg"],"series":"IEEE 2021","doi":"10.1109/TVCG.2021.3114844","keywords":"data visualization, visualization, cognition, interactive systems, tools, pragmatics, pattern recognition, superpowers, empowerment, vision, perception, fiction, situated visualization","award":"Best Paper","pages":11,"abstract":"We explore how the lens of fictional superpowers can help characterize how visualizations empower people and provide inspiration for new visualization systems. Researchers and practitioners often tout visualizations' ability to make the invisible visible and to enhance cognitive abilities. Meanwhile superhero comics and other modern fiction often depict characters with similarly fantastic abilities that allow them to see and interpret the world in ways that transcend traditional human perception. We investigate the intersection of these domains, and show how the language of superpowers can be used to characterize existing visualization systems and suggest opportunities for new and empowering ones. We introduce two frameworks: The first characterizes seven underlying mechanisms that form the basis for a variety of visual superpowers portrayed in fiction. The second identifies seven ways in which visualization tools and interfaces can instill a sense of empowerment in the people who use them. Building on these observations, we illustrate a diverse set of visualization superpowers and highlight opportunities for the visualization community to create new systems and interactions that empower new experiences with data. Material and illustrations are available under CC-BY 4.0 at osf.io/8yhfz.","dir":"content/output/publications","base":"ieee-2021-willett.json","ext":".json","sourceBase":"ieee-2021-willett.yaml","sourceExt":".yaml"},"content/output/publications/ijac-2021-hosseini.json":{"date":"2021-05","title":"Optically illusive architecture (OIA): Introduction and evaluation using virtual reality","authors":["Seyed Vahab Hosseini","Usman R Alim","Lora Oehlberg","Joshua M Taron"],"series":"IJAC 2021","doi":"10.1177/14780771211016600","keywords":"architectural representation, optical illusion, design evaluation, virtual reality","pages":24,"abstract":"Architects and designers communicate their ideas within a range of representational methods. No single instance of these methods, either in the form of orthographic projections or perspectival representation, can address all questions regarding the design, but as a whole, they demonstrate a comprehensive range of information about the building or object they intend to represent. This explicates an inevitable degree of deficiency in representation, regardless of its type. In addition, perspective-based optical illusions manipulate our spatial perception by deliberately misrepresenting the reality. In this regard, they are not new concepts to architectural representation. As a consequence, Optically Illusive Architecture (OIA) is proposed, not as a solution to fill the gap between the representing and represented spaces, but as a design paradigm whose concept derives from and accounts for this gap. By OIA we aim to cast light to an undeniable role of viewpoints in designing architectural spaces. The idea is to establish a methodology in a way that the deficiency of current representational techniquesmanifested as specific thread of optical illusionsflourishes into thoughtful results embodied as actual architectural spaces. Within our design paradigm, we define a framework to be able to effectively analyze its precedents, generate new space, and evaluate their efficiencies. Moreover, the framework raises a hierarchical set of questions to differentiate OIA from a visual gimmick. Furthermore, we study two OIA-driven environments, by conducting empirical studies using Virtual Reality (VR). These studies bear essential information, in terms of design performance, and the publics ability to engage and interact with an OIA space, prior to the actual fabrication of the structures.","dir":"content/output/publications","base":"ijac-2021-hosseini.json","ext":".json","sourceBase":"ijac-2021-hosseini.yaml","sourceExt":".yaml"},"content/output/publications/imwut-2020-wang.json":{"date":"2020-06","title":"AssessBlocks: Exploring Toy Block Play Features for Assessing Stress in Young Children after Natural Disasters","authors":["Xiyue Wang","Kazuki Takashima","Tomoaki Adachi","Patrick Finn","Ehud Sharlin","Yoshifumi Kitamura"],"series":"IMWUT 2020","doi":"https://doi.org/10.1145/3381016","keywords":"well being, toy blocks, PTSD, tangibles for health, stress assessment, play, children","pages":29,"video":"https://www.youtube.com/watch?v=fxxvZBY80ug","abstract":"Natural disasters cause long-lasting mental health problems such as PTSD in children. Following the 2011 Earthquake and Tsunami in Japan, we witnessed a shift of toy block play behavior in young children who suffered from stress after the disaster. The behavior reflected their emotional responses to the traumatic event. In this paper, we explore the feasibility of using data captured from block-play to assess children's stress after a major natural disaster. We prototyped sets of sensor-embedded toy blocks, AssessBlocks, that automate quantitative play data acquisition. During a three-year period, the blocks were dispatched to fifty-two post-disaster children. Within a free play session, we captured block features, a child's playing behavior, and stress evaluated by several methods. The result from our analysis reveal correlations between block play features and stress measurements and show initial promise of using the effectiveness of using AssessBlocks to assess children's stress after a disaster. We provide detailed insights into the potential as well as the challenges of our approach and unique conditions. From these insights we summarize guidelines for future research in automated play assessment systems that support children's mental health.","dir":"content/output/publications","base":"imwut-2020-wang.json","ext":".json","sourceBase":"imwut-2020-wang.yaml","sourceExt":".yaml"},"content/output/publications/imx-2020-mok.json":{"date":"2020-06","title":"Talk Like Somebody is Watching: Understanding and Supporting Novice Live Streamers","authors":["Terrance Mok","Colin Au Yeung","Anthony Tang","Lora Oehlberg"],"series":"IMX 2020","doi":"10.1145/3391614.3399392","keywords":"Live streams, Streamers, Chatbot, Audience, Virtual audience","pages":6,"abstract":"We built a chatbot systemAudience Botthat simulates an audience for novice live streamers to engage with while streaming. New live streamers on platforms like Twitch are expected to perform and talk to themselves, even while no one is watching. We ran an observational lab study on how Audience Bot assists novice live streamers as they acclimate to multitaskingsimultaneously playing a video game while performing for a (simulated) audience.","dir":"content/output/publications","base":"imx-2020-mok.json","ext":".json","sourceBase":"imx-2020-mok.yaml","sourceExt":".yaml"},"content/output/publications/iros-2020-hedayati.json":{"date":"2020-09","title":"PufferBot: Actuated Expandable Structures for Aerial Robots","authors":["Hooman Hedayati","Ryo Suzuki","Daniel Leithinger","Daniel Szafir"],"series":"IROS 2020","video":"https://www.youtube.com/watch?v=XtPepCxWcBg","pages":6,"abstract":"We present PufferBot, an aerial robot with an expandable structure that may expand to protect a drones propellers when the robot is close to obstacles or collocated humans. PufferBot is made of a custom 3D printed expandable scissor structure, which utilizes a one degree of freedom actuator with rack and pinion mechanism. We propose four designs for the expandable structure, each with unique charac- terizations which may be useful in different situations. Finally, we present three motivating scenarios in which PufferBot might be useful beyond existing static propeller guard structures.","dir":"content/output/publications","base":"iros-2020-hedayati.json","ext":".json","sourceBase":"iros-2020-hedayati.yaml","sourceExt":".yaml"},"content/output/publications/iros-2022-suzuki.json":{"date":"2022-09","title":"Selective Self-Assembly using Re-Programmable Magnetic Pixels","authors":["Martin Nisser","Yashaswini Makaram","Faraz Faruqi","Ryo Suzuki","Stefanie Mueller"],"series":"IROS 2022","video":"https://www.youtube.com/watch?v=HK9_ynH6A6w","pages":8,"abstract":"This paper introduces a method to generate highly selective encodings that can be magnetically programmed onto physical modules to enable them to self-assemble in chosen configurations. We generate these encodings based on Hadamard matrices, and show how to design the faces of modules to be maximally attractive to their intended mate, while remaining maximally agnostic to other faces. We derive guarantees on these bounds, and verify their attraction and agnosticism experimentally. Using cubic modules whose faces have been covered in soft magnetic material, we show how inexpensive, passive modules with planar faces can be used to selectively self-assemble into target shapes without geometric guides. We show that these modules can be easily re-programmed for new target shapes using a CNC-based magnetic plotter, and demonstrate self-assembly of 8 cubes in a water tank.","dir":"content/output/publications","base":"iros-2022-suzuki.json","ext":".json","sourceBase":"iros-2022-suzuki.yaml","sourceExt":".yaml"},"content/output/publications/mobilehci-2015-ledo.json":{"date":"2015-08","title":"Proxemic-Aware Controls: Designing Remote Controls for Ubiquitous Computing Ecologies","authors":["David Ledo","Saul Greenberg","Nicolai Marquardt","Sebastian Boring"],"series":"MobileHCI 2015","doi":"https://doi.org/10.1145/2785830.2785871","keywords":"ubiquitous computing, proxemic-interaction, mobile interaction, control of appliances","video":"https://www.youtube.com/watch?v=1AlMUmD6E3U","pages":10,"abstract":"Remote controls facilitate interactions at-a-distance with appliances. However, the complexity, diversity, and increasing number of digital appliances in ubiquitous computing ecologies make it increasingly difficult to: (1) discover which appliances are controllable; (2) select a particular appliance from the large number available; (3) view information about its status; and (4) control the appliance in a pertinent manner. To mitigate these problems we contribute proxemic-aware controls, which exploit the spatial relationships between a person's handheld device and all surrounding appliances to create a dynamic appliance control interface. Specifically, a person can discover and select an appliance by the way one orients a mobile device around the room, and then progressively view the appliance's status and control its features in increasing detail by simply moving towards it. We illustrate proxemic-aware controls of assorted appliances through various scenarios. We then provide a generalized conceptual framework that informs future designs of proxemic-aware controls.","dir":"content/output/publications","base":"mobilehci-2015-ledo.json","ext":".json","sourceBase":"mobilehci-2015-ledo.yaml","sourceExt":".yaml"},"content/output/publications/mobilehci-2019-hung.json":{"date":"2019-10","title":"WatchPen: Using Cross-Device Interaction Concepts to Augment Pen-Based Interaction","authors":["Michael Hung","David Ledo","Lora Oehlberg"],"series":"MobileHCI 2019","doi":"https://doi.org/10.1145/3338286.3340122","keywords":"smartwatch, cross-device interaction, pen interaction, interaction techniques","video":"https://www.youtube.com/watch?v=ilyJBzTzQAA","pages":8,"abstract":"Pen-based input is often treated as auxiliary to mobile devices. We posit that cross-device interactions can inspire and extend the design space of pen-based interactions into new, expressive directions. We realize this through WatchPen, a smartwatch mounted on a passive, capacitive stylus that: (1) senses the usage context and leverages it for expression (e.g., changing colour), (2) contains tools and parameters within the display, and (3) acts as an on-demand output. As a result, it provides users with a dynamic relationship between inputs and outputs, awareness of current tool selection and parameters, and increased expressive match (e.g., added ability to mimic physical tools, showing clipboard contents). We discuss and reflect upon a series of interaction techniques that demonstrate WatchPen within a drawing application. We highlight the expressive power of leveraging multiple sensing and output capabilities across both the watch-augmented stylus and the tablet surface.","dir":"content/output/publications","base":"mobilehci-2019-hung.json","ext":".json","sourceBase":"mobilehci-2019-hung.yaml","sourceExt":".yaml"},"content/output/publications/nime-2020-ko.json":{"date":"2020-07","title":"Touch Responsive Augmented Violin Interface System II: Integrating Sensors into a 3D Printed Fingerboard","authors":["Chantelle Ko","Lora Oehlberg"],"series":"NIME 2020","keywords":"violin, touch sensor, FSR, fingerboard, augmented, 3D printing, conductive filament, interactive","talk":"https://www.youtube.com/watch?v=INmDzkcIO14","pages":13,"abstract":"We present TRAVIS II, an augmented acoustic violin with touch sensors integrated into its 3D printed fingerboard that track left-hand finger gestures in real time. The fingerboard has four strips of conductive PLA filament which produce an electric signal when fingers press down on each string. While these sensors are physically robust, they are mechanically assembled and thus easy to replace if damaged. The performer can also trigger presets via four FSRs attached to the body of the violin. The instrument is completely wireless, giving the performer the freedom to move throughout the performance space. While the sensing fingerboard is installed in place of the traditional fingerboard, all other electronics can be removed from the augmented instrument, maintaining the aesthetics of a traditional violin. Our design allows violinists to naturally create music for interactive performance and improvisation without requiring new instrumental techniques. In this paper, we describe the design of the instrument, experiments leading to the sensing fingerboard, and performative applications of the instrument.","dir":"content/output/publications","base":"nime-2020-ko.json","ext":".json","sourceBase":"nime-2020-ko.yaml","sourceExt":".yaml"},"content/output/publications/sui-2017-li.json":{"date":"2017-10","title":"Visibility Perception and Dynamic Viewsheds for Topographic Maps and Models","authors":["Nico Li","Wesley Willett","Ehud Sharlin","Mario Costa Sousa"],"series":"SUI 2017","doi":"https://doi.org/10.1145/3131277.3132178","keywords":"terrain visualization, geospatial visualization, dynamic viewshed, topographic maps, tangible user interfaces","pages":9,"video":"https://vimeo.com/275404995","talk":"https://www.youtube.com/watch?v=aVXUojoQF60","abstract":"We compare the effectiveness of 2D maps and 3D terrain models for visibility tasks and demonstrate how interactive dynamic viewsheds can improve performance for both types of terrain representations. In general, the two-dimensional nature of classic topographic maps limits their legibility and can make complex yet typical cartographic tasks like determining the visibility between locations difficult. Both 3D physical models and interactive techniques like dynamic viewsheds have the potential to improve viewers' understanding of topography, but their impact has not been deeply explored. We evaluate the effectiveness of 2D maps, 3D models, and interactive viewsheds for both simple and complex visibility tasks. Our results demonstrate the benefits of the dynamic viewshed technique and highlight opportunities for additional tactile interactions. Based on these findings we present guidelines for improving the design and usability of future topographic maps and models.","dir":"content/output/publications","base":"sui-2017-li.json","ext":".json","sourceBase":"sui-2017-li.yaml","sourceExt":".yaml"},"content/output/publications/tei-2016-somanath.json":{"date":"2018-06","title":"Engaging 'At-Risk' Students through Maker Culture Activities","authors":["Sowmya Somanath","Laura Morrison","Janette Hughes","Ehud Sharlin","Mario Costa Sousa"],"series":"TEI 2016","doi":"https://doi.org/10.1145/2839462.2839482","keywords":"DIY, 'at-risk' students, maker culture, education, young learners","pages":9,"abstract":"This paper presents a set of lessons learnt from introducing maker culture and DIY paradigms to 'at-risk' students (age 12-14). Our goal is to engage 'at-risk' students through maker culture activities. While improved technology literacy is one of the outcomes we also wanted the learners to use technology to realize concepts and ideas, and to gain freedom of thinking similar to creators, artists and designers. We present our study and a set of high level suggestions to enable thinking about how maker culture activities can facilitate engagement and creative use of technology by 1) thinking about creativity in task, 2) facilitating different entry points, 3) the importance of personal relevance, and 4) relevance to education.","dir":"content/output/publications","base":"tei-2016-somanath.json","ext":".json","sourceBase":"tei-2016-somanath.yaml","sourceExt":".yaml"},"content/output/publications/tei-2019-mikalauskas.json":{"date":"2019-03","title":"Beyond the Bare Stage: Exploring Props as Potential Improviser-Controlled Technology","authors":["Claire Mikalauskas","April Viczko","Lora Oehlberg"],"series":"TEI 2019","doi":"https://doi.org/10.1145/3294109.3295631","pages":9,"keywords":"props, performer-controlled technology, improvisational theatre","abstract":"While improvised theatre (improv) is often performed on a bare stage, improvisers sometimes incorporate physical props to inspire new directions for a scene and to enrich their performance. A tech booth can improvise light and sound technical elements, but coordinating with improvisers' actions on-stage is challenging. Our goal is to inform the design of an augmented prop that lets improvisers tangibly control light and sound technical elements while performing. We interviewed five professional improvisers about their use of physical props in improv, and their expectations of a possible augmented prop that controls technical theatre elements. We propose a set of guidelines for the design of an augmented prop that fits with the existing world of unpredictable improvised performance.","dir":"content/output/publications","base":"tei-2019-mikalauskas.json","ext":".json","sourceBase":"tei-2019-mikalauskas.yaml","sourceExt":".yaml"},"content/output/publications/tei-2019-tolley.json":{"date":"2019-03","title":"WindyWall: Exploring Creative Wind Simulations","authors":["David Tolley","Thi Ngoc Tram Nguyen","Anthony Tang","Nimesha Ranasinghe","Kensaku Kawauchi","Ching-Chiuan Yen"],"series":"TEI 2019","doi":"https://doi.org/10.1145/3294109.3295624","keywords":"tactile/haptic interaction, multimodal interaction, novel actuators/displays","video":"https://www.youtube.com/watch?v=Tn11UmsOsTE","pages":10,"abstract":"Wind simulations are typically one-off implementations for specific applications. We introduce WindyWall, a platform for creative design and exploration of wind simulations. WindyWall is a three-panel 90-fan array that encapsulates users with 270? of wind coverage. We describe the design and implementation of the array panels, discussing how the panels can be re-arranged, where various wind simulations can be realized as simple effects. To understand how people perceive \"wind\" generated from WindyWall, we conducted a pilot study of wind magnitude perception using different wind activation patterns from WindyWall. Our findings suggest that: horizontal wind activations are perceived more readily than vertical ones, and that people's perceptions of wind are highly variable-most individuals will rate airflow differently in subsequent exposures. Based on our findings, we discuss the importance of developing a method for characterizing wind simulations, and provide design directions for others using fan arrays to simulate wind.","dir":"content/output/publications","base":"tei-2019-tolley.json","ext":".json","sourceBase":"tei-2019-tolley.yaml","sourceExt":".yaml"},"content/output/publications/tei-2019-wun.json":{"date":"2019-03","title":"You say Potato, I say Po-Data: Physical Template Tools for Authoring Visualizations","authors":["Tiffany Wun","Lora Oehlberg","Miriam Sturdee","Sheelagh Carpendale"],"series":"TEI 2019","doi":"https://doi.org/10.1145/3294109.3295627","keywords":"potato, tangible tools, authoring visualizations, block-printing, physical template tools, information visualization","pages":10,"abstract":"Providing data visualization authoring tools for the general public remains an ongoing challenge. Inspired by block-printing, we explore how visualization stamps as a physical visualization authoring tool could leverage both visual freedom and ease of repetition. We conducted a workshop with two groups---visualization experts and non-experts---where participants authored visualizations on paper using hand-carved stamps made from potatoes and sponges. The low-fidelity medium freed participants to test new stamp patterns and accept mistakes. From the created visualizations, we observed several unique traits and uses of block-printing tools for visualization authoring, including: modularity of patterns, annotation guides, creation of multiple patterns from one stamp, and various techniques to apply data onto paper. We discuss the issues around expressivity and effectiveness of block-printed stamps in visualization authoring, and identify implications for the design and assembly of primitives in potential visualization stamp kits, as well as applications for future use in non-digital environments.","dir":"content/output/publications","base":"tei-2019-wun.json","ext":".json","sourceBase":"tei-2019-wun.yaml","sourceExt":".yaml"},"content/output/publications/tei-2020-suzuki.json":{"date":"2020-02","title":"LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces","authors":["Ryo Suzuki","Ryosuke Nakayama","Dan Liu","Yasuaki Kakehi","Mark D. Gross","Daniel Leithinger"],"series":"TEI 2020","doi":"https://dl.acm.org/doi/10.1145/3374920.3374941","keywords":"shape-changing interfaces, inflatables, large-scale interactions","pages":9,"video":"https://www.youtube.com/watch?v=0LHeTkOMR84","abstract":"Large-scale shape-changing interfaces have great potential, but creating such systems requires substantial time, cost, space, and efforts, which hinders the research community to explore interactions beyond the scale of human hands. We introduce modular inflatable actuators as building blocks for prototyping room-scale shape-changing interfaces. Each actuator can change its height from 15cm to 150cm, actuated and controlled by air pressure. Each unit is low-cost (8 USD), lightweight (10 kg), compact (15 cm), and robust, making it well-suited for prototyping room-scale shape transformations. Moreover, our modular and reconfigurable design allows researchers and designers to quickly construct different geometries and to explore various applications. This paper contributes to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block.","dir":"content/output/publications","base":"tei-2020-suzuki.json","ext":".json","sourceBase":"tei-2020-suzuki.yaml","sourceExt":".yaml"},"content/output/publications/tei-2021-pratte.json":{"date":"2021-02","title":"Evoking Empathy: A Framework for Describing Empathy Tools","authors":["Sydney Pratte","Anthony Tang","Lora Oehlberg"],"series":"TEI 2021","doi":"https://dl.acm.org/doi/10.1145/3430524.3440644","keywords":"Empathy, Empathy Tools, Design Strategies","pages":13,"video":"https://www.youtube.com/watch?v=JBCzPt5ILxo","abstract":"Empathy tools are experiences designed to evoke empathetic responses by placing the user in anothers lived and felt experience. The problem is that designers do not have a common vocabulary to describe empathy tool experiences; consequently, it is difficult to compare/contrast empathy tool designs or to think about their efficacy. To address this problem, we analyzed 26 publications on empathy tools to develop a descriptive framework for designers of empathy tools. Based on our analysis, we found that empathy tools can be described along three dimensions: (i) the amount of agency the tool allows, (ii) the users perspective while using the tool, and (iii) the type of sensations that are experienced. We show that this framework can be used to describe a wide variety of empathy tools and provide recommendations for empathy tool designers, as well as techniques for measuring the efficacy of an empathy tool experience.","dir":"content/output/publications","base":"tei-2021-pratte.json","ext":".json","sourceBase":"tei-2021-pratte.yaml","sourceExt":".yaml"},"content/output/publications/tochi-2022-nittala.json":{"date":"2022-10","title":"SparseIMU: Computational Design of Sparse IMU Layouts for Sensing Fine-Grained Finger Microgestures","authors":["Adwait Sharma","Christina Salchow-Hmmen","Vimal Suresh Mollyn","Aditya Shekhar Nittala","Michael A. Hedderich","Marion Koelle","Thomas Seel","Jrgen Steimle"],"series":"TOCHI 2022","doi":"https://doi.org/10.1145/3569894","keywords":"Gesture Recognition, Hand Gestures, Sensor Placement, IMU, Objects, Design Tool","pages":40,"abstract":"Gestural interaction with freehands and while grasping an everyday object enables always-available input. To sense such gestures, minimal instrumentation of the users hand is desirable. However, the choice of an effective but minimal IMU layout remains challenging, due to the complexity of the multi-factorial space that comprises diverse finger gestures, objects and grasps. We present SparseIMU, a rapid method for selecting minimal inertial sensor-based layouts for effective gesture recognition. Furthermore, we contribute a computational tool to guide designers with optimal sensor placement. Our approach builds on an extensive microgestures dataset that we collected with a dense network of 17 inertial measurement units (IMUs). We performed a series of analyses, including an evaluation of the entire combinatorial space for freehand and grasping microgestures (393K layouts), and quantified the performance across different layout choices, revealing new gesture detection opportunities with IMUs. Finally, we demonstrate the versatility of our method with four scenarios.","dir":"content/output/publications","base":"tochi-2022-nittala.json","ext":".json","sourceBase":"tochi-2022-nittala.yaml","sourceExt":".yaml"},"content/output/publications/tvcg-2016-lopez.json":{"date":"2016-05","title":"Towards An Understanding of Mobile Touch Navigation in a Stereoscopic Viewing Environment for 3D Data Exploration","authors":["David Lopez","Lora Oehlberg","Candemir Doger","Tobias Isenberg"],"series":"TVCG 2016","doi":"https://doi.org/10.1109/TVCG.2015.2440233","keywords":"visualization of 3D data, human-computer interaction, expert interaction, direct-touch input, mobile displays, stereoscopic environments, VR, AR, conceptual model of interaction, interaction reference frame mapping, observational study","video":"https://www.youtube.com/watch?v=jBtHgTYpJl0","talk":"https://vimeo.com/245846750","pages":13,"abstract":"We discuss touch-based navigation of 3D visualizations in a combined monoscopic and stereoscopic viewing environment. We identify a set of interaction modes, and a workflow that helps users transition between these modes to improve their interaction experience. In our discussion we analyze, in particular, the control-display space mapping between the different reference frames of the stereoscopic and monoscopic displays. We show how this mapping supports interactive data exploration, but may also lead to conflicts between the stereoscopic and monoscopic views due to users' movement in space; we resolve these problems through synchronization. To support our discussion, we present results from an exploratory observational evaluation with domain experts in fluid mechanics and structural biology. These experts explored domain-specific datasets using variations of a system that embodies the interaction modes and workflows; we report on their interactions and qualitative feedback on the system and its workflow.","dir":"content/output/publications","base":"tvcg-2016-lopez.json","ext":".json","sourceBase":"tvcg-2016-lopez.yaml","sourceExt":".yaml"},"content/output/publications/tvcg-2017-goffin.json":{"date":"2017-01","title":"An Exploratory Study of Word-Scale Graphics in Data-Rich Text Documents","authors":["Pascal Goffin","Jeremy Boy","Wesley Willett","Petra Isenberg"],"series":"TVCG 2017","doi":"https://doi.org/10.1109/TVCG.2016.2618797","keywords":"word-scale visualization, word-scale graphic, text visualization, sparklines, authoring tool, information visualization","pages":13,"video":"https://vimeo.com/230834366","abstract":"We contribute an investigation of the design and function of word-scale graphics and visualizations embedded in text documents. Word-scale graphics include both data-driven representations such as word-scale visualizations and sparklines, and non-data-driven visual marks. Their design, function, and use has so far received little research attention. We present the results of an open ended exploratory study with nine graphic designers. The study resulted in a rich collection of different types of graphics, data provenance, and relationships between text, graphics, and data. Based on this corpus, we present a systematic overview of word-scale graphic designs, and examine how designers used them. We also discuss the designers' goals in creating their graphics, and characterize how they used word-scale graphics to visualize data, add emphasis, and create alternative narratives. Building on these examples, we discuss implications for the design of authoring tools for word-scale graphics and visualizations, and explore how new authoring environments could make it easier for designers to integrate them into documents.","dir":"content/output/publications","base":"tvcg-2017-goffin.json","ext":".json","sourceBase":"tvcg-2017-goffin.yaml","sourceExt":".yaml"},"content/output/publications/tvcg-2017-willett.json":{"date":"2017-01","title":"Embedded Data Representations","authors":["Wesley Willett","Yvonne Jansen","Pierre Dragicevic"],"series":"TVCG 2017","doi":"https://doi.org/10.1109/TVCG.2016.2598608","keywords":"information visualization, data physicalization, ambient displays, ubiquitous computing, augmented reality","pages":10,"video":"https://vimeo.com/182971005","talk":"https://www.youtube.com/watch?v=ZS7lU60xChI","abstract":"We introduce embedded data representations, the use of visual and physical representations of data that are deeply integrated with the physical spaces, objects, and entities to which the data refers. Technologies like lightweight wireless displays, mixed reality hardware, and autonomous vehicles are making it increasingly easier to display data in-context. While researchers and artists have already begun to create embedded data representations, the benefits, trade-offs, and even the language necessary to describe and compare these approaches remain unexplored. In this paper, we formalize the notion of physical data referents  the real-world entities and spaces to which data corresponds  and examine the relationship between referents and the visual and physical representations of their data. We differentiate situated representations, which display data in proximity to data referents, and embedded representations, which display data so that it spatially coincides with data referents. Drawing on examples from visualization, ubiquitous computing, and art, we explore the role of spatial indirection, scale, and interaction for embedded representations. We also examine the tradeoffs between non-situated, situated, and embedded data displays, including both visualizations and physicalizations. Based on our observations, we identify a variety of design challenges for embedded data representation, and suggest opportunities for future research and applications.","dir":"content/output/publications","base":"tvcg-2017-willett.json","ext":".json","sourceBase":"tvcg-2017-willett.yaml","sourceExt":".yaml"},"content/output/publications/tvcg-2019-blascheck.json":{"date":"2019-06","title":"Exploration Strategies for Discovery of Interactivity in Visualizations","authors":["Tanja Blascheck","Lindsay MacDonald Vermeulen","Jo Vermeulen","Charles Perin","Wesley Willett","Thomas Ertl","Sheelagh Carpendale"],"series":"TVCG 2019","doi":"https://doi.org/10.1109/TVCG.2018.2802520","keywords":"discovery, visualization, open data, evaluation, eye tracking, interaction logs, think-aloud","video":"https://vimeo.com/289789025","pages":13,"abstract":"We investigate how people discover the functionality of an interactive visualization that was designed for the general public. While interactive visualizations are increasingly available for public use, we still know little about how the general public discovers what they can do with these visualizations and what interactions are available. Developing a better understanding of this discovery process can help inform the design of visualizations for the general public, which in turn can help make data more accessible. To unpack this problem, we conducted a lab study in which participants were free to use their own methods to discover the functionality of a connected set of interactive visualizations of public energy data. We collected eye movement data and interaction logs as well as video and audio recordings. By analyzing this combined data, we extract exploration strategies that the participants employed to discover the functionality in these interactive visualizations. These exploration strategies illuminate possible design directions for improving the discoverability of a visualization's functionality.","dir":"content/output/publications","base":"tvcg-2019-blascheck.json","ext":".json","sourceBase":"tvcg-2019-blascheck.yaml","sourceExt":".yaml"},"content/output/publications/tvcg-2019-walny.json":{"date":"2019-08","title":"Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff","authors":["Jagoda Walny","Christian Frisson","Mieka West","Doris Kosminsky","Sren Knudsen","Sheelagh Carpendale","Wesley Willett"],"series":"TVCG 2019","doi":"https://doi.org/10.1109/TVCG.2019.2934538","keywords":"information visualization, design handoff, data mapping, design process","award":"Best Paper","pages":10,"video":"https://vimeo.com/360483702","talk":"https://vimeo.com/368703151","abstract":"Complex data visualization design projects often entail collaboration between people with different visualization-related skills. For example, many teams include both designers who create new visualization designs and developers who implement the resulting visualization software. We identify gaps between data characterization tools, visualization design tools, and development platforms that pose challenges for designer-developer teams working to create new data visualizations. While it is common for commercial interaction design tools to support collaboration between designers and developers, creating data visualizations poses several unique challenges that are not supported by current tools. In particular, visualization designers must characterize and build an understanding of the underlying data, then specify layouts, data encodings, and other data-driven parameters that will be robust across many different data values. In larger teams, designers must also clearly communicate these mappings and their dependencies to developers, clients, and other collaborators. We report observations and reflections from five large multidisciplinary visualization design projects and highlight six data-specific visualization challenges for design specification and handoff. These challenges include adapting to changing data, anticipating edge cases in data, understanding technical challenges, articulating data-dependent interactions, communicating data mappings, and preserving the integrity of data mappings across iterations. Based on these observations, we identify opportunities for future tools for prototyping, testing, and communicating data-driven designs, which might contribute to more successful and collaborative data visualization design.","dir":"content/output/publications","base":"tvcg-2019-walny.json","ext":".json","sourceBase":"tvcg-2019-walny.yaml","sourceExt":".yaml"},"content/output/publications/tvcg-2020-danyluk.json":{"date":"2020-09","title":"Touch and Beyond: Comparing Physical and Virtual Reality Visualizations","authors":["Kurtis Thorvald Danyluk","Teoman Tomo Ulusoy","Wei Wei","Wesley J. Willett"],"series":"TVCG 2020","doi":"http://dx.doi.org/10.1109/TVCG.2020.3023336","keywords":"Human-Computer Interaction, Visualization, Data Visualization, Virtual Reality, Physicalization.","pages":12,"video":"https://prism.ucalgary.ca/streamview/bitstream/handle/1880/112684/2020-Danyluk-TouchAndBeyond-Video.mp4?sequence=3","abstract":"We compare physical and virtual reality (VR) versions of simple data visualizations and explore how the addition of virtual annotation and filtering tools affects how viewers solve basic data analysis tasks. We report on two studies, inspired by previous examinations of data physicalizations. The first study examines differences in how viewers interact with physical hand-scale, virtual hand-scale, and virtual table-scale visualizations and the impact that the different forms had on viewers problem solving behavior. A second study examines how interactive annotation and filtering tools might support new modes of use that transcend the limitations of physical representations. Our results highlight challenges associated with virtual reality representations and hint at the potential of interactive annotation and filtering tools in VR visualizations.","dir":"content/output/publications","base":"tvcg-2020-danyluk.json","ext":".json","sourceBase":"tvcg-2020-danyluk.yaml","sourceExt":".yaml"},"content/output/publications/uist-2018-suzuki.json":{"date":"2018-10","title":"Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation","authors":["Ryo Suzuki","Junichi Yamaoka","Daniel Leithinger","Tom Yeh","Mark D. Gross","Yoshihiro Kawahara","Yasuaki Kakehi"],"series":"UIST 2018","doi":"https://doi.org/10.1145/3242587.3242659","keywords":"digital materials, dynamic 3D printing, shape displays","video":"https://www.youtube.com/watch?v=92eGI-gYYc4","video-2":"https://www.youtube.com/watch?v=7nPlr3O9xu8","talk":"https://www.youtube.com/watch?v=R3FRUtOIiCQ","pages":16,"abstract":"This paper introduces Dynamic 3D Printing, a fast and reconstructable shape formation system. Dynamic 3D Printing can assemble an arbitrary three-dimensional shape from a large number of small physical elements. Also, it can disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbitrary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and implementation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long-term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper, we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.","dir":"content/output/publications","base":"uist-2018-suzuki.json","ext":".json","sourceBase":"uist-2018-suzuki.yaml","sourceExt":".yaml"},"content/output/publications/uist-2019-suzuki.json":{"date":"2019-10","title":"ShapeBots: Shape-changing Swarm Robots","authors":["Ryo Suzuki","Clement Zheng","Yasuaki Kakehi","Tom Yeh","Ellen Yi-Luen Do","Mark D. Gross","Daniel Leithinger"],"series":"UIST 2019","keywords":"swarm user interfaces, shape-changing user interfaces","doi":"https://doi.org/10.1145/3332165.3347911","video":"https://www.youtube.com/watch?v=cwPaof0kKdM","pages":13,"abstract":"We introduce shape-changing swarm robots. A swarm of self-transformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions. The modular design of each actuator enables various shapes and geometries of self-transformation. We illustrate potential application scenarios and discuss how this type of interface opens up possibilities for the future of ubiquitous and distributed shape-changing interfaces.","dir":"content/output/publications","base":"uist-2019-suzuki.json","ext":".json","sourceBase":"uist-2019-suzuki.yaml","sourceExt":".yaml"},"content/output/publications/uist-2020-suzuki.json":{"date":"2020-10","title":"RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching","authors":["Ryo Suzuki","Rubaiat Habib Kazi","Li-Yi Wei","Stephen DiVerdi","Wilmot Li","Daniel Leithinger"],"series":"UIST 2020","doi":"https://doi.org/10.1145/3379337.3415892","keywords":"augmented reality, embedded data visualization, real-time authoring, sketching interfaces, tangible interaction","award":"Honorable Mention","video":"https://www.youtube.com/watch?v=L0p-BNU9rXU","pages":16,"abstract":"We present RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, floating in mid air without responding to the real world. This paper introduces a new way to embed dynamic and responsive graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-defined programs and configurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.","dir":"content/output/publications","base":"uist-2020-suzuki.json","ext":".json","sourceBase":"uist-2020-suzuki.yaml","sourceExt":".yaml"},"content/output/publications/uist-2020-yixian.json":{"date":"2020-10","title":"ZoomWalls: Dynamic Walls that Simulate Haptic Infrastructure for Room-scale VR World","authors":["Yan Yixian","Kazuki Takashima","Anthony Tang","Takayuki Tanno","Kazuyuki Fujita","Yoshifumi Kitamura"],"series":"UIST 2020","doi":"https://doi.org/10.1145/3379337.3415859","keywords":"encountered-type haptic devices, immersive experience","video":"https://www.youtube.com/watch?v=j2iSNDkBxAY","pages":13,"abstract":"We focus on the problem of simulating the haptic infrastructure of a virtual environment (i.e. walls, doors). Our approach relies on multiple ZoomWalls---autonomous robotic encounter-type haptic wall-shaped props---that coordinate to provide haptic feedback for room-scale virtual reality. Based on a user's movement through the physical space, ZoomWall props are coordinated through a predict-and-dispatch architecture to provide just-in-time haptic feedback for objects the user is about to touch. To refine our system, we conducted simulation studies of different prediction algorithms, which helped us to refine our algorithmic approach to realize the physical ZoomWall prototype. Finally, we evaluated our system through a user experience study, which showed that participants found that ZoomWalls increased their sense of presence in the VR environment. ZoomWalls represents an instance of autonomous mobile reusable props, which we view as an important design direction for haptics in VR.","dir":"content/output/publications","base":"uist-2020-yixian.json","ext":".json","sourceBase":"uist-2020-yixian.yaml","sourceExt":".yaml"},"content/output/publications/uist-2021-suzuki.json":{"date":"2021-10","title":"HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots","authors":["Ryo Suzuki","Eyal Ofek","Mike Sinclair","Daniel Leithinger","Mar Gonzalez-Franco"],"series":"UIST 2021","doi":"https://doi.org/10.1145/3472749.3474821","keywords":"virtual reality, encountered-type haptics, tabletop mobile robots, swarm user interfaces","video":"https://www.youtube.com/watch?v=HTiZgOESJyQ","pages":16,"abstract":"HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic ap- proaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability---these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in- time touch-points on the users hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various ap- plications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.","dir":"content/output/publications","base":"uist-2021-suzuki.json","ext":".json","sourceBase":"uist-2021-suzuki.yaml","sourceExt":".yaml"},"content/output/publications/uist-2022-kaimoto.json":{"date":"2022-10","title":"Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI","authors":["Hiroki Kaimoto","Kyzyl Monteiro","Mehrad Faridan","Jiatong Li","Samin Farajian","Yasuaki Kakehi","Ken Nakagaki","Ryo Suzuki"],"series":"UIST 2022","doi":"https://doi.org/10.1145/3526113.3545626","keywords":"augmented reality, mixed reality, actuated tangible interfaces, swarm user interfaces","video":"https://www.youtube.com/watch?v=xy-IeVgoEpY","pages":12,"abstract":"This paper introduces Sketched Reality, an approach that com- bines AR sketching and actuated tangible user interfaces (TUI) for bi-directional sketching interaction. Bi-directional sketching enables virtual sketches and physical objects to affect each other through physical actuation and digital computation. In the existing AR sketching, the relationship between virtual and physical worlds is only one-directional --- while physical interaction can affect virtual sketches, virtual sketches have no return effect on the physical objects or environment. In contrast, bi-directional sketching interaction allows the seamless coupling between sketches and actuated TUIs. In this paper, we employ tabletop-size small robots (Sony Toio) and an iPad-based AR sketching tool to demonstrate the concept. In our system, virtual sketches drawn and simulated on an iPad (e.g., lines, walls, pendulums, and springs) can move, actuate, collide, and constrain physical Toio robots, as if virtual sketches and the physical objects exist in the same space through seamless coupling between AR and robot motion. This paper contributes a set of novel interactions and a design space of bi-directional AR sketching. We demonstrate a series of potential applications, such as tangible physics education, explorable mechanism, tangible gaming for children, and in-situ robot programming via sketching.","dir":"content/output/publications","base":"uist-2022-kaimoto.json","ext":".json","sourceBase":"uist-2022-kaimoto.yaml","sourceExt":".yaml"},"content/output/publications/uist-2022-liao.json":{"date":"2022-10","title":"RealityTalk: Real-time Speech-driven Augmented Presentation for AR Live Storytelling","authors":["Jian Liao","Adnan Karim","Shivesh Jadon","Rubaiat Habib Kazi","Ryo Suzuki"],"series":"UIST 2022","doi":"https://doi.org/10.1145/3526113.3545702","keywords":"Augmented Reality, Mixed Reality, Augmented Presentation, Natural Language Processing, Gestural and Speech Input, Video","video":"https://www.youtube.com/watch?v=vfIMeICV-7c","pages":12,"abstract":"We present RealityTalk, a system that augments real-time live presentations with speech-driven interactive virtual elements. Augmented presentations leverage embedded visuals and animation for engaging and expressive storytelling. However, existing tools for live presentations often lack interactivity and improvisation, while creating such effects in video editing tools require significant time and expertise. RealityTalk enables users to create live augmented presentations with real-time speech-driven interactions. The user can interactively prompt, move, and manipulate graphical elements through real-time speech and supporting modalities. Based on our analysis of 177 existing video-edited augmented presentations, we propose a novel set of interaction techniques and then incorporated them into RealityTalk. We evaluate our tool from a presenters perspective to demonstrate the effectiveness of our system.","dir":"content/output/publications","base":"uist-2022-liao.json","ext":".json","sourceBase":"uist-2022-liao.yaml","sourceExt":".yaml"},"content/output/publications/uist-2022-nisser.json":{"date":"2022-10","title":"Mixels: Fabricating Interfaces using Programmable Magnetic Pixels","authors":["Martin Nisser","Yashaswini Makaram","Lucian Covarrubias","Amadou Bah","Faraz Faruqi","Ryo Suzuki","Stefanie Mueller"],"series":"UIST 2022","doi":"https://doi.org/10.1145/3526113.3545698","keywords":"programmable materials, magnetic interfaces, fabrication","video":"https://www.youtube.com/watch?v=6SvFCQkVFtw","pages":12,"abstract":"In this paper, we present Mixels, programmable magnetic pixels that can be rapidly fabricated using an electromagnetic printhead mounted on an off-the-shelve 3-axis CNC machine. The ability to program magnetic material pixel-wise with varying magnetic force enables Mixels to create new tangible, tactile, and haptic interfaces. To facilitate the creation of interactive objects with Mixels, we provide a user interface that lets users specify the high-level magnetic behavior and that then computes the underlying magnetic pixel assignments and fabrication instructions to program the magnetic surface. Our custom hardware add-on based on an electromagnetic printhead and hall effect sensor clips onto a standard 3-axis CNC machine and can both write and read magnetic pixel values from magnetic material. Our evaluation shows that our system can reliably program and read magnetic pixels of various strengths, that we can predict the behavior of two interacting magnetic surfaces before programming them, that our electromagnet is strong enough to create pixels that utilize the maximum magnetic strength of the material being programmed, and that this material remains magnetized when removed from the magnetic plotter.","dir":"content/output/publications","base":"uist-2022-nisser.json","ext":".json","sourceBase":"uist-2022-nisser.yaml","sourceExt":".yaml"},"content/output/publications/uist-2022-nittala.json":{"date":"2022-10","title":"Prototyping Soft Devices with Interactive Bioplastics","authors":["Marion Koelle","Madalina Nicolae","Aditya Shekhar Nittala","Marc Teyssier","Jrgen Steimle"],"series":"UIST 2022","doi":"https://doi.org/10.1145/3526113.3545623","keywords":"bioplastics, biomaterials, do-it-yourself, DIY, sustainability","award":"Best Paper","video":"https://www.youtube.com/watch?v=8Paq3P3EsKQ","pages":16,"abstract":"Designers and makers are increasingly interested in leveraging bio-based and bio-degradable do-it-yourself (DIY) materials for sustainable prototyping. Their self-produced bioplastics possess compelling properties such as self-adhesion but have so far not been functionalized to create soft interactive devices, due to a lack of DIY techniques for the fabrication of functional electronic circuits and sensors. In this paper, we contribute a DIY approach for creating Interactive Bioplastics that is accessible to a wide audience, making use of easy-to-obtain bio-based raw materials and familiar tools. We present three types of conductive bioplastic materials and their formulation: sheets, pastes and foams. Our materials enable additive and subtractive fabrication of soft circuits and sensors. Furthermore, we demonstrate how these materials can substitute conventional prototyping materials, be combined with off-the-shelf electronics, and be fed into a sustainable material life-cycle including disassembly, re-use, and re-melting of materials. A formal characterization of our conductors highlights that they are even on-par with commercially available carbon-based conductive pastes.","dir":"content/output/publications","base":"uist-2022-nittala.json","ext":".json","sourceBase":"uist-2022-nittala.yaml","sourceExt":".yaml"},"content/output/publications/uist-sic-2022-faridan.json":{"date":"2022-10","title":"UltraBots: Large-Area Mid-Air Haptics for VR with Robotically Actuated Ultrasound Transducers","authors":["Mehrad Faridan","Marcus Friedel","Ryo Suzuki"],"series":"UIST SIC 2022","doi":"https://doi.org/10.1145/3526114.3561350","keywords":"Virtual Reality, Haptics, Ultrasound Transducers, Robotics","award":"Honorable Mention","pages":3,"abstract":"We introduce UltraBots, a system that combines ultrasound haptic feedback and robotic actuation for large-area mid-air haptics for VR. Ultrasound haptics can provide precise mid-air haptic feedback and versatile shape rendering, but the interaction area is often limited by the small size of the ultrasound devices, restricting the possible interactions for VR. To address this problem, this paper introduces a novel approach that combines robotic actuation with ultrasound haptics. More specifically, we will attach ultrasound transducer arrays to tabletop mobile robots or robotic arms for scalable, extendable, and translatable interaction areas. We plan to use Sony Toio robots for 2D translation and/or commercially available robotic arms for 3D translation. Using robotic actuation and hand tracking measured by a VR HMD (ex: Oculus Quest), our system can keep the ultrasound transducers underneath the users hands to provide on-demand haptics. We demonstrate applications with workspace environments, medical training, education and entertainment.","dir":"content/output/publications","base":"uist-sic-2022-faridan.json","ext":".json","sourceBase":"uist-sic-2022-faridan.yaml","sourceExt":".yaml"},"content/output/publications/vr-2019-satriadi.json":{"date":"2019-03","title":"Augmented Reality Map Navigation with Freehand Gestures","authors":["Kadek Ananta Satriadi","Barrett Ens","Maxime Cordeil","Bernhard Jenny","Tobias Czauderna","Wesley Willett"],"series":"IEEE VR 2019","doi":"https://doi.org/10.1109/VR.2019.8798340","keywords":"augmented reality, gesture recognition, human computer interaction, interactive devices","pages":11,"video":"https://www.youtube.com/watch?v=TE6AJEu8zdY","talk":"https://www.youtube.com/watch?v=jNeEbB3sTn0","abstract":"Freehand gesture interaction has long been proposed as a `natural' input method for Augmented Reality (AR) applications, yet has been little explored for intensive applications like multiscale navigation. In multiscale navigation, such as digital map navigation, pan and zoom are the predominant interactions. A position-based input mapping (e.g. grabbing metaphor) is intuitive for such interactions, but is prone to arm fatigue. This work focuses on improving digital map navigation in AR with mid-air hand gestures, using a horizontal intangible map display. First, we conducted a user study to explore the effects of handedness (unimanual and bimanual) and input mapping (position-based and rate-based). From these findings we designed DiveZoom and TerraceZoom, two novel hybrid techniques that smoothly transition between position- and rate-based mappings. A second user study evaluated these designs. Our results indicate that the introduced input-mapping transitions can reduce perceived arm fatigue with limited impact on performance.","dir":"content/output/publications","base":"vr-2019-satriadi.json","ext":".json","sourceBase":"vr-2019-satriadi.yaml","sourceExt":".yaml"},"content/output/seminar.json":[{"date":"2022-04-08","name":"Antonio Baia Reis","affiliation":"Assistant professor at the University of Passau","url":"https://www.antoniobaiareis.com/","title":"Into the theatreverse we go! Bits and pieces and other metaverse paraphernalia at the crossroads of live performance and XR technologies","abstract":"As the metaverse hype grows, so does the interest in creating and developing artistic activities in it. Even if one does not clearly know what the metaverse is or will become, it is quite clear that in recent years, and with a specific intensity since the Covid-19 pandemic hit, we are witnessing the rise of emergent formats and practices of live performance combined with XR technologies and platforms. From medium to large live acting performances in XR such as the ones produced by the Royal Shakespeare Company, Ferryman Collective, Double-Eye Studios or La Cuarta Pared VR, to seemingly spontaneous performative moments such as poetry slams on VR Chat or Rec Room, live performance is an essential part of a post-pandemic future where digital performance will play a key role in giving live to the metaverse. In this keynote presentation, XR researcher and performer Antnio Baa Reis will provide an overview of the main concepts and ideas related to live performance in XR, as well as the main potentialities and challenges surrounding the future of performing arts in the metaverse experience economy.","bio":"Antonio Baa Reis is a researcher, professor, and digital artist. His work is interdisciplinary, combining areas such as emergent media, communication and performing arts, with a strong focus on immersive media (VR, AR, MR), film, collaborative practices, practice-based research, creativity studies, innovation in education, and social impact."},{"date":"2022-03-18","name":"Luiz Morais","affiliation":"Post-doctoral fellow at INRIA with the Potioc team.","url":"https://luizaugustomm.github.io/","title":"What do we know about humanitarian visualizations?","abstract":"Humanitarian issues such as extreme poverty or refugee crises have been acknowledged worldwide over the past few decades. Organizations often have campaigns that focus on emotional strategies, such as showing the picture of a starving child or dead animals to persuade people to act. Another common tactic used by news media is reporting the issues through data visualizations. The psychology literature has extensively investigated the former approach, but there is still a lack of research about the effect of data visualizations to evoke emotions such as compassion or prosocial behaviors such as donating. This presentation shows recent contributions in that direction and discusses future perspectives of research.","bio":"Luiz Morais holds a Ph.D. in Computer Science from the Universidade Federal de Campina Grande, Brazil, and did an internship at the Sorbonne Universite, France. During his Ph.D. studies, he investigated how visualization design can affect people's compassion towards others. He is currently a postdoctoral fellow at Inria Bordeaux, France, where he investigates the use of situated visualizations to support housework management. Morais also collaborates with other institutions around the globe such as Inria Paris-Saclay, University of Toronto, and Monash University, where he works on projects about the use of visualization for dealing with humanitarian issues or for sustainability."},{"date":"2022-03-04","name":"Ignacio Avellino","affiliation":"CNRS at Sorbonne Universite","url":"http://www.ignacioavellino.com/","title":"From delegation to education: using technology to increase access to surgery","abstract":"Surgery is an indivisible and indispensable part of healthcare, yet 70% of the world population lacks access to safe and affordable surgical care. Having an expert surgeon either travel to the patient or operate remotely through surgical robots is not scalable given the staggering shortage of more than 140 million surgeries annually. I propose to move away from surgical _delegation_ and towards surgical _education_ with the goal of studying and building interactive systems that innovate how surgery is learned, in a vision to reverse the alarming trend of fewer surgeons per population. I will talk about two of my research directions. First, surgical telementoring, when an expert surgeon both remotely assists and teaches a surgeon in real-time while they operate. My aim is to move telementoring forward from the sheer transmission of audio and video, by conceptualizing novel mechanisms that shatter the boundaries of face-to-face settings. Second, interactive videos as learning material both produced and used by surgeons. My aim is to move the creation and consumption of videos away from classic video editing and playback tools, by conceptualizing novel mechanisms that rely on embedded semantic information in various dimensions. Together, these two directions target the alarmingly decreasing rate of surgeons per population and difficult access to surgical care that goes with it, supporting expert-to-expert remote training and expert-to-novice asynchronous training.","bio":"Ignacio Avellino is a permanent researcher (Charg de Recherche) for the CNRS at Sorbonne Universite, France. He earned his PhD from Saclay University, after completing his MSc at RWTH Aachen. Ignacio specializes in HumanComputer Interaction (HCI) and Computer Supported Cooperative Work (CSCW) research in the health domain, focusing on collaborative systems for telementoring as well as interactive systems for creating and consuming video as learning materials. He served as Technical Program Chair Assistant for ACM CHI 2020, and is a regular Associate Chair both at CHI and CSCW."},{"date":"2022-02-19","name":"Rong-Hao Liang","affiliation":"Department of Industrial Design and the Department of Electrical Engineering, Eindhoven University of Technology","url":"https://howieliang.github.io/PersonalPage/","title":"Repurposing Commodity RFID Tags for HCI","abstract":"In this talk, I will share our recent development on repurposing commodity RFID Tags for enabling batteryless and wireless wearable and tangible user interfaces for human-computer interaction. The presentation will cover several research publications including GaussRFID (CHI '16), RFIBricks (CHI '18, CHI '21), RFIMatch (UIST '18), RFTouchPad (UIST '19), and NFCSense (CHI '21). I will also discuss the challenges and opportunities for future work in this direction.","bio":"Rong-Hao Liang is an assistant professor at the Department of Industrial Design and the Department of Electrical Engineering, Eindhoven University of Technology, the Netherlands. He received his Ph.D. in Computer Science (2014) from National Taiwan University and M.S. in Electrical Engineering (2010) from National Taiwan University. He also worked as an assistant research fellow in Intel-NTU Research Center (2014-2016) and co-found a company, GaussToys Inc. (2015). His research focuses on sensing systems and user interface technology for HCI and ubiquitous computing, and he has been awarded the Best Paper Award / Honorable Mentions from ACM CHI and DIS conferences. Until 2021 he has more than 50 technical research publications and held more than 10 granted or published user interface hardware patents. (Personal Website: https://ronghaoliang.page)"},{"date":"2022-02-09","name":"Andrea Bianchi","affiliation":"Korea Advanced Institute of Science and Technology (KAIST), MAKInteract","url":"https://makinteract.kaist.ac.kr/andrea","title":"Supporting makers while creating physical computing prototypes","abstract":"This talk is about tools for supporting makers while creating physical computing prototypes - specifically when engaged in the creation of electronic circuits. I will present several systems based on smart-breadboards and voice-based conversational agents that can be used to support makers in their creative explorations of physical computing prototypes and assist them while assembling circuits. I will also try to show how research can bridge the gap with education in the classroom and try to highlight what are possible future directions in the field.","bio":"Andrea Bianchi is associate professor in the Department of Industrial Design at KAIST, Korea, where he directs the MAKinteract lab. He researches in the field of Human-Computer Interaction (HCI) focusing on building tools for prototyping and devices for body augmentation. Before joining KAIST, he worked at Sungkyunkwan University (Korea) as a faculty member in the Department of Computer Science, and as a video game programmer for a New York startup. Andrea received a Ph.D. from KAIST (Korea) in 2012, an MS in Computer Science from NYU (USA), and a Laurea (BSc+MS) in business administration from Bocconi University (Italy)."},{"date":"2021-09-13","name":"Pedro Lopes","affiliation":"University of Chicago","url":"http://plopes.org/","title":"Integrating Interactive Devices with the Users Body","abstract":"When we look back to the early days of computing, user and device were distant, often located in separate rooms. Then, in the 70s, personal computers 'moved in' with users. In the 90s, mobile devices moved computing into users pockets. More recently, wearable devices brought computing into constant physical contact with the users skin. These transitions proved useful: moving closer to users allowed interactive devices to sense more of their user and act more personal. The main question that drives my research is: what is the next interface paradigm that supersedes wearable devices? The primary way researchers have been investigating this is by asking where future interactive devices will be located with respect to the users body. Many posit that the next generation of interfaces will be implanted inside the users body. However, I argue that their location with respect to the users body is not the primary factor; in fact, implanted devices are already happening in that we have pacemakers, insulin pumps, etc. Instead, I argue that the key factor is how devices will integrate with the users biological senses and actuators. This body-device integration allowed us to engineer interactive devices that intentionally borrow parts of the body for input and output, rather than adding more technology to the body. For example, one such type of body-integrated devices, which I have advanced recently, are interactive systems based on electrical muscle stimulation. These devices move their users muscles using computer-controlled electrical impulses, achieving the functionality of robotic exoskeletons without the bulky motors. The key insight is that engineering devices that intentionally borrow parts of the users biology puts forward a new generation of miniaturized devices; allowing us to circumvent traditional physical constraints. For instance, in the case of our devices based on electrical muscle stimulation, they demonstrate how our body-device integration circumvents the constraints imposed by the ratio of electrical power and size of a motor (i.e., the stronger/larger a motor is, the more current needed to actuate it). Taking this further, we demonstrate how our body-device integration approach allowed us to also miniaturize thermal feedback (hot/cold sensations) without the need for power-hungry devices like Peltiers, air conditioners, or heaters. We believe that these bodily-integrated devices are the natural succession to wearable interfaces and allow us to investigate how interfaces might connect to our bodies in a more direct and personal way.","bio":"Pedro Lopes is an Assistant Professor in Computer Science at the University of Chicago. Pedro focuses onintegrating computer interfaces with the human bodyexploring the interface paradigm that supersedes wearable computing. Some of these new integrated-devices include: muscle stimulation wearable that allows users to manipulate tools they have never seen before or that accelerate their reaction time, or a device that leverages the sense of smell to create an illusion of temperature. Pedros work has received a number ofacademic awards, such as four ACM CHI/UIST Best Papers. It also captured the interest of the media, such asNew York Times or Wired and was exhibited at Ars Electronica & World Economic Forum. (More: https://lab.plopes.org)"},{"date":"2021-08-30","name":"Valkyrie Savage","affiliation":"University of Copenhagen","url":"https://valkyriesavage.com/","title":"Sensing Physical Input Devices","abstract":"Physical input devices are the ultimate bridge between humans and computers, as they translate human actions into digital commands. My work pushes for a world where such devices fit a specific users needs for a particular time, place, and task, culminating in deeply custom interfaces designed, fabricated, or simply picked up to solve a problem. I will discuss my thesis work, which embeds cutting-edge sensing techniques into 3D design tools, allowing fast, cheap, and flexible prototyping for physical devices. I will also describe some in-progress work which measures human anatomy to richly sense user interactions with existing objects, removing the need for dedicated input devices. I'll finish with a brief sketch of what I want to work on next here in Copenhagen: examining how sensing the bodys dynamic capabilities can enhance input device design.","bio":"Valkyrie Savage is a newly-minted Assistant Professor at the University of Copenhagen in the Human-Centred Computing Section of the Department of Computer Science. She received her PhD from UC Berkeley in 2016, where she worked with Bjrn Hartmann; her thesis there was entitled 'Fabbed to Sense: Integrated Design of Geometry and Sensing for Interactive Objects.' Valkyrie has lived many lives before and after that, including founding and working for startups, interning at Google and CERN, presenting work at SXSW, organizing the March for Science Toronto, and becoming an internationally-renowned player in the sport of jugger. She is curious about all kinds of things."},{"date":"2021-08-23","name":"Maxime Cordeil","affiliation":"Monash University","url":"https://sites.google.com/view/cordeil/home","title":"Embodied Data Interaction","abstract":"Visualising data is fundamental to understand complex phenomena and patterns, predict future trends and eventually make informed decisions. The data visualisation process requires human input and therefore significant amounts of interaction are required to support effective exploration of data (e.g. selecting data, filtering, specifying views etc). Interaction with data and their visual representation has largely been designed and developed for 2D screen/mouse/ keyboards devices, which creates a barrier between the data and people who seek to understand them. Modern VR/AR technology allows for more natural, 3D rich interaction and present unique opportunities to redesign the data interaction pipeline to visually explore data in immersive environments to enhance data understanding. In this talk I will present my work on embodied data interaction in mixed and virtual reality. I will present the emerging design space of spatial interaction with complex data and present some exemplar work that aim at reducing the gap between people and their data.","bio":"Maxime Cordeil is a Lecturer of Human Computer Interaction at Monash University and a member of the Data Visualisation and Immersive Analytics Lab. His research focuses on Immersive Analytics, Human Computer Interaction and Data Science. Prior to this he was a Postdoctoral Research Fellow at Monash University, a Software Engineer for the French Civil Aviation R&D department, and he received his PhD from the Higher French Institute of Aeronautics and Space (ISAE). He is actively involved in the IEEE VIS and VR research communities."},{"date":"2021-08-16","name":"Laura Devendorf","affiliation":"University of Colorado Boulder","url":"http://artfordorks.com/","title":"Designing Not Knowing","abstract":"As a design researcher and educator working in human-computer interaction, I often find myself in the business of 'empowering' students by teaching them design. As a professor, I write grant proposals that use the magic of design to bring forth preferable futures. Yet, within the present socio-environ-political context, I find myself increasingly conflicted by these claims and asking myself, what, really, can design do? I will not be able to answer any of these questions during this talk because I don't know, but I will argue that the position of not-knowing, humility, and non-expert is useful for critically reflecting on the relevance of our practices. I will present ways that myself, collaborators, and the students with whom I work have been using weaving (sometimes with circuits, some without) as a practice through which to to try to probe, question, and understand what counts as design and the kinds of narratives we must take on in order to be 'designers'. I aim for this talk to inspire reflection and offer a few tactics for unknowing in order to think otherwise.","bio":"Laura Devendorf is an assistant professor in Information Science and the ATLAS Institute at the University of Colorado Boulder. Her research questions the role of design and making in the wake of increasingly pressing global challenges. She directs the Unstable Design Lab where she works closely with students across engineering, information science, and art to speculate on alternative futures for technology. The lab currently focuses on weaving smart textiles and how themes of slowness, presence, and material negotiation can be used as both a practice and metaphor to formulate these visions. She earned bachelors' degrees in studio art and computer science from the University of California Santa Barbara before earning her Ph.D. at UC Berkeley School of Information. Her research has been featured on National Public Radio and has received multiple best paper awards at top conferences in the field of human-computer interaction."},{"date":"2021-08-09","name":"Haijun Xia","affiliation":"University of California","url":"https://haijunxia.ucsd.edu/","title":"The Power of Representation in Human-Computer Interaction","abstract":"From the dawn of computing, we have been striving to leverage computation to augment our productivity and creativity. While interactive technologies have become increasingly powerful, employing computation for creativity support and problem solving is still a rigid and laborious process. As a Human-Computer Interaction researcher, I seek to enable users to directly and flexibly express their intentions to computers, effectively integrating computation into their thinking processes. My research takes a fundamental approach, where I have been focusing on inventing new representations and primitives of the graphical user interfaces that better match our mental processes. Coupled with the afforded interaction techniques, I will demonstrate how they lead to novel user interface paradigms, which allow us to directly express our intentions to computation and quickly perform interactions that were previously tedious, or even impossible.","bio":"Haijun Xia is an Assistant Professor in the Cognitive Science, Computer Science and Engineering, and the Design Lab at the University of California, San Diego. His research area is Human-Computer Interaction, in which he focuses on augmenting our productivity and creativity. He approaches this through the development of novel representations and interaction techniques. He received his Ph.D. from the DGP Lab at the Department of Computer Science, University of Toronto. For more of his recent work, please visit  https://haijunxia.ucsd.edu/"},{"date":"2021-07-19","name":"Elizabeth Stobert","affiliation":"Carleton University","url":"https://www.stobert.ca/","title":"Password Sharing in Bangladesh","abstract":"Although forbidden by almost all password policies, password-sharing is a common practice worldwide. However, the specific ways in which people manage shared passwords in combination with culturally-based expectations are not well-understood. In this work, we interviewed 30 Bangladeshi users about their password-sharing practices. We found that password sharing is a near ubiquitous practice in Bangladesh, and that a variety of cultural factors affect peoples expectations, practices, and experiences in sharing passwords.","bio":"Elizabeth Stobert is an assistant professor in the School of Computer Science at Carleton University, where she teaches in the computer science and human-computer interaction programs. Her research is in usable security, examining the human factors in security systems. She has worked extensively on the usability of passwords and authentication systems, and also done recent research in security education and healthcare security. Prior to joining Carleton in 2019, she held research roles at the National Research Council of Canada, Concordia University, and ETH Zurich."},{"date":"2021-07-12","name":"Liang He","affiliation":"University of Washington","url":"http://www.lianghe.me/","title":"Beyond Shape: 3D Printing Kinetic Objects for Interactivity","abstract":"Emerging 3D printing technology has enabled the rapid creation of physical shapes. However, 3D-printed objects are typically static with limited or no moving parts. Creating 3D printable objects with kinetic behaviors such as deformation and motion is inherently challenging. To enrich the literature for making movable 3D-printed parts and support a wider spectrum of applications, I introduce the concept of print driver, a class of parametric mechanisms that use uniquely designed mechanical elements and are printed in place to augment 3D-printed objects with the ability of deformation, actuation, and sensing. In this talk, I will present a series of my research works to showcase how the print drivers can be used to lower the barrier for making 3D-printed kinetic objects and to support augmented 3D printable behaviors for interactivity. I will also share my personal thoughts on how to incorporate print drivers into objects, the human body, and space for good.","bio":"Liang He is a Ph.D. candidate in Computer Science & Engineering at the University of Washington, advised by Jon E. Froehlich. He works at the intersection of Human-Computer Interaction (HCI) and digital fabrication. He takes a mechanical perspective to create novel design techniques by exploiting parametric mechanical properties and to develop computational design tools for the design, control, and fabrication of 3D printable augmented behaviors. Prior to joining UW, Liang received his M.S. in Computational Design at Carnegie Mellon University, his M.S. in Computer Science at the University of Chinese Academy of Sciences, and his B.S. in Software Engineering at Beihang University. He also worked in HP Labs, in the VIBE group at Microsoft Research (Redmond), and at Keio-NUS CUTE Center. Liang publishes at top HCI venues such as CHI, UIST, TEI, and ASSETS, and received two best paper awards and one best paper nominee."},{"date":"2021-07-05","name":"Thijs Roumen","affiliation":"Hasso Plattner Institute in Potsdam","url":null,"title":"Portable Laser Cutting","abstract":"A portable format for laser cutting will enable millions of users to benefit from laser-cut models as opposed to the 1000s of tech enthusiasts that engage with laser cutting today. What holds widely adopted use back is the limited ability to modify and fabricate existing models. It may seem like a portable format already exist, as laser cut models are already widely shared in the form of 2D cutting plans. However, such files are susceptible to variations in cutter properties (aka kerf) and do not allow modifying the model in any meaningful way. I consider this format machine specific. In computing, this problem was solved in the 50s by developing compilers. This allowed developers to abstract away from the hardware and as a result, write code that remained relevant to this day. The resulting code is portable, e.g. it can be transferred from one machine to another. This transition has revolutionized not only computing but also all fields that use digital formats like desktop publishing, digital video, digital audio, etc. I believe that by transitioning towards a portable format for laser cutting we can make a similar transition from 1000s of users and one-off models towards millions of users and advanced models developed by multiple creators. My first take on the challenge is to see how far we get by building on the de-facto standard, i.e., 2D cutting plans. I wrote software tools to modify 2D cutting plans, replacing non-portable elements with portable counterparts. This makes the models portable, but it is still hard to modify them. I thus take a more radical approach, which is to move to a 3D exchange format (kyub). This guarantees portability by generating a new machine-specific 2D cutting plan for the local machine when exported. And the models inherently allow for parametric modifications. Instead, it raises the question of compatibility: Files already exist in 2Dhow to get them into 3D? I demonstrate a software tool, assembler3, to reconstruct the 3D geometry of the model encoded in a 2D cutting plan, allows modifying it using a 3D editor, and re-encodes it to a 2D cutting plan. I demonstrate how this approach allows me to make a much wider range of modifications, including scaling, changing material thickness, and even remixing models.","bio":"Thijs Roumen is a PhD candidate in Human Computer Interaction in the lab of Patrick Baudisch, Hasso Plattner Institute in Potsdam, Germany. He received his MSc from the University of Southern Denmark, Snderborg in 2013 and BSc from the Technical University of Eindhoven, Netherlands in 2011. Between the PhD and master he worked at the National University of Singapore as a Research Assistant with Shengdong Zhao. His research interests are in personal fabrication, digital collaboration and enabling increased complexity for laser cutting. His papers are published as full papers in top-tier ACM conferences CHI and UIST. He serves on several ACM program committees including ACM UIST."},{"date":"2021-06-28","name":"Wendy Ju","affiliation":"Cornell Tech","url":"https://www.wendyju.com/","title":"Where did this @#$^@#$%# AV learn to drive?","abstract":"Drivers communicate and negotiate with other drivers, pedestrians and road users implicitly and explicitly through the movement of their cars, as well as through honking, verbal communication, body language and gaze. It is widely recognized that these interaction patterns vary culturally; the advent of autonomy will necessitate a more explicit understanding of the complex manner in which drivers interact. Mismatches in perception, understanding and action between road users can easily cause accidents. We are exploring how drivers implicitly communicate and coordinate with others on the road, and to assess how these driving interactions differ across cultures. By staging situations that demand negotiation, such as ambiguous four-way stops, we can capture how participants communicate with other drivers or pedestrians to coordinate joint action, implicitly through the movements of their virtual car or bodily movement, or explicitly through verbal or gestural exchange. By comparing how people from different cultures coordinate in comparable situations, we can better understand cultural differences in driving interaction.","bio":"Wendy Ju is an Associate Professor at the Jacobs Technion-Cornell Institute at Cornell Tech and the Technion and in the Information Science field at Cornell University. Her work in the areas of human-robot interaction and automated vehicle interfaces highlights the ways that interactive devices can be designed to be safer, more predictable, and more socially appropriate. Professor Ju has innovated numerous methods for early-stage prototyping of automated systems to understand how people will respond to systems before the systems are built. She has a PhD in Mechanical Engineering from Stanford, and a Masters in Media Arts and Sciences from MIT. Her monograph on The Design of Implicit Interactions was published in 2015."},{"date":"2021-06-07","name":"Ge Gao","affiliation":"University of Maryland","url":"https://www.terpconnect.umd.edu/~gegao/","title":"Human-Centered Design for Connected Teams and Communities: A Multilingual Perspective","abstract":"It is increasingly common for people today to interact with others who do not speak the same native language as they do. Despite the wide adoption of English as the lingua franca, extensive research has shown that information exchange in a multilingual setting frequently happens in 'a cocktail of languages' In this talk, I will present a series of studies investigating 1) when people shift between different languages to satisfy their situational needs, and 2) how people manage the costs and benefits of their language choices through daily communication practices. I will conclude by describing several technical solutions (e.g., human-centered translation tools) my group is currently exploring for building connected teams and communities.","bio":"Ge Gao is an Assistant Professor in the College of Information Studies (iSchool) at the University of Maryland, with a joint appointment at the University of Maryland Institute for Advanced Computer Studies (UMIACS). She obtained her PhD in Communication at Cornell University. Ges research interests cover the behavioural aspect of human-computer interaction (HCI). Her recent projects focus on understanding the designing for computer-based work communication, information seeking, knowledge sharing across language boundaries. Findings of this research have been published at top-tier HCI venues such as ACM CHI, CSCW, and Ubicomp."},{"date":"2021-05-03","name":"Stefanie Mueller","affiliation":"MIT","url":"https://hcie.csail.mit.edu/stefanie-mueller.html","title":"Advancing Personal Fabrication by Making Physical Objects as Reprogrammable as Digital Data","abstract":"Computing has revolutionized how we process and interact with data today, unfortunately, these capabilities are constraint to the digital realm and cannot yet be applied to physical matter. For instance, today, we can already quickly update the appearance of a digital photo by applying a filter or adding and removing elements. However, updating physical objects in the same way is not possible today. In this talk, I will show my research groups latest developments that bring us closer to a future in which physical objects are as reprogrammable as data is today. As a first example of this, I will show our research on a new reprogrammable material that can be applied to the surface of physical objects and that allows them to change their appearance within a few minutes. This allows us to update the color of clothing, shoes, and even entire rooms in the same way as we can update a digital photo today. I will then show additional developments that extend this concept to further integrate computing capabilities into physical objects, show our research on how we can print functional objects in one go without the need for assembly, and demonstrate how we can create unified prototyping environments that support engineers and designers in fabricating new types of physical objects.","bio":"Stefanie Mueller is the X-Career Development Assistant Professor in the MIT EECS department joint with MIT Mechanical Engineering and Head of the HCI Engineering Group at MIT CSAIL. For her research, Stefanie has received an NSF CAREER Award, an Alfred P. Sloan Fellowship, a Microsoft Research Faculty Fellowship, and was also named a Forbes 30 under 30 in Science. In addition, Stefanies work has been awarded several Best Paper and Honorable Mention Awards at the ACM CHI and ACM UIST conferences, the premier venues in Human-Computer Interaction. Stefanie has also served as the Program Chair of the ACM UIST 2020 conference and was a Subcommittee Chair for ACM CHI 2019 and 2020. At MIT, Stefanie served as a Program Co-Chair for the MIT EECS Rising Star Workshop in 2018 and is currently serving as the Head of the Human Computer Interaction Communities of Research (HCI CoR) at MIT CSAIL."},{"date":"2021-04-26","name":"Charles Perin","affiliation":"University of Victoria","url":"http://charlesperin.net/","title":"Beyond Visualization Wizardry: The Role of Interaction in Data Visualization","abstract":"Visualization is not just a way of creating pretty pictures and \"intuitive dashboards\". It is not a magic wand that you can apply to your dataset to automatically turn a data mess into \"actionable insights for transformative results\". Far from this wizardry, I argue that understanding data comes at the cost of interacting with it. I will go through several research projects - ranging from manual reordering of matrices to active reading of visualizations to interaction discoverability to composite physicalizations to direct manipulation of graphical encodings - in an attempt to convince you that we can, and should, find better ways for people to interact with data visualizations.","bio":"Charles Perin is an Assistant Professor of Computer Science at the University of Victoria, where he co-leads the Victoria Interactive eXperiences with Information research group specializing in Human-Computer Interaction and Information Visualization. He and his students are particularly interested in designing and studying new interactions for visualizations and in understanding how people may make use of and interact with visualizations in their everyday lives; in designing visualization tools for authoring personal visualizations and for exploring and communicating open data; in sports visualization; and in visualization beyond the desktop. Before joining the University of Victoria in 2018, Charles was a Lecturer at City, University of London, before that a post-doctoral researcher at the University of Calgary, before that a PhD student at University Paris-Sud/INRIA, and long before that a kid in Brittany."},{"date":"2021-04-19","name":"Nicolai Marquardt","affiliation":"University College London","url":"http://www.nicolaimarquardt.com/","title":"Journey through the Design Space of Cross-Device Interactions","abstract":"Designing interfaces or applications that move beyond the bounds of a single device screen enables new ways to engage with digital content. In this talk, I will guide you through the design space of cross-device interactions. In particular, I will give an overview of what the research field of cross-device interactions looks like, and what kind of techniques we can use for designing fluid cross-device interactions. Ill also discuss a few of the open issues in the research field and suggest opportunities of where we can go next.","bio":"Nicolai Marquardt is Associate Professor at the University College London, where he is part of the Department of Computer Science, Faculty of Engineering and the Faculty of Brain Sciences. At the UCL Interaction Centre, he works on projects in the research areas of cross-device interaction, sensor-based systems, prototyping toolkits, and design methods. He received his PhD in Computer Science from the University of Calgary, Canada. Nicolai is co-author of the Sketching User Experiences Workbook (Morgan Kaufmann 2011) and the Proxemic Interactions textbook (Morgan & Claypool 2015)."},{"date":"2021-04-12","name":"Benjamin Bach","affiliation":"University of Edinburgh","url":"https://www.designinformatics.org/person/benjaminbach/","title":"The Immersive Canvas: Data Visualization and Interaction for Immersive Analytics","abstract":"This talk explores the role of interactive data visualization for immersive analytics. Immersive analytics is becoming a complex field that combines many fields of expertise: analytics, big data, infrastructure, virtual and augmented systems, image recognition and many others, as well as human-computer interaction and visualization. In order to make sense of complex data, we need visualization interfaces: in immersive environments, data visualization is freed of the limitedness of the traditional desktop screen; able to expand into an infinite canvas and the third dimension. What potential does this bring for data visualization and immersive visualization? How can we leverage this potential? How is this changing our approach to visualizing and interacting with data?","bio":"Dr Benjamin Bach is a Lecturer in Design Informatics and Visualization at the University of Edinburgh. His research designs and investigates interactive information visualization interfaces to help people explore, communicate, and understand data. Before joining the University of Edinburgh in 2017, Benjamin worked as a postdoc at Harvard University (Visual Computing Group), Monash University, as well as the Microsoft-Research Inria Joint Centre. Benjamin was visiting researcher at the University of Washington and Microsoft Research in 2015. He obtained his PhD in 2014 from the Universit Paris Sud where he worked at the Aviz Group at Inria. The PhD thesis entitled Connections, Changes, and Cubes: Unfolding Dynamic Networks for Visual Exploration got awarded an honorable mention as the Best Thesis by the IEEE Visualization Committee."},{"date":"2021-03-29","name":"James Young","affiliation":"University of Manitoba","url":"http://hci.cs.umanitoba.ca/people/bio/james-e.young","title":"Warning, This robot is not what it seems! A discussion on deception and the future of social robots","abstract":"Social robots are designed to interact with people using human- or animal-like language, gestures, or other techniques. This approach promises intuitive interaction, and can be designed to shape a person's mood and behavior; social robots can even serve as companions. However, I argue that social robots - by design - are fundamentally rooted in deception, which highlights real potential dangers as these robots enter society. On the flip side, considering this deception also provides a positive way forward, a path for developing social robots that can be successful in both our everyday lives.","bio":"Dr. James Young is a professor of computer science at the University of Manitoba."},{"date":"2021-03-15","name":"Jessica Cauchard","affiliation":"Ben Gurion University","url":"https://scholar.google.com/citations?user=Rk1SDB8AAAAJ&hl=en","title":"On Body & Out of Body Interactions","abstract":"Mobile devices have become ubiquitous over the last decade, changing the way we interact with technology and with one another. Mobile devices were at first personal devices carried in our hands or pockets. They are now changing form to fit our lifestyles and an increasingly demanding amount and diversity of information to display. My research focuses on the design, development, and evaluation of novel interaction techniques with mobile devices using a human-centered approach. In this presentation, I will in particular focus on two types of mobile technologies: wearables and drones. I will discuss the use of multiple modalities to interact with technology and in particular how haptics on wearables can support long-term tasks without interrupting the users attention. I will then discuss how autonomous devices such as drones re-invent our understanding of ubiquitous computing and present my current research on collocated natural human-drone interaction.","bio":"Dr. Jessica Cauchard is a lecturer in the department of Industrial Engineering and Management at Ben Gurion University of the Negev in Israel, where she recently founded the Magic Lab. Her research is rooted in the fields of Human-Computer and Human-Robot Interaction with a focus on novel interaction techniques and ubiquitous computing. Previously, she was faculty of Computer Science at the Interdisciplinary Center Herzliya between 2017 and 2019. Before moving to Israel, Dr. Cauchard worked as a postdoctoral scholar at Stanford University. She has a strong interest in autonomous vehicles and intelligent devices and how they change our device ecology. She completed her PhD in Computer Science at the University of Bristol, UK in 2013 and received a Magic Grant for her work on interacting with drones by the Brown Institute for Media Innovation in 2015"},{"date":"2021-03-08","name":"Alberto de Salvatierra","affiliation":"UCalgary School of Architecture, Landscape and Planning","url":"https://sapl.ucalgary.ca/about/people/alberto-de-salvatierra","title":"Soft Infrastructures","abstract":"The COVID-19 pandemic has exposed the fragility of existing models of housing, collective life, and infrastructure. The 99% have been disproportionately marginalized by shelter-in-place orders and quarantines that assume they have the resources to weather this moment of extreme instability. The transition from a quarantine to a post-pandemic city will not only be a fight for collective human health and wellbeing, but will also be the staging ground for our last stand to prevent a forthcoming climate catastrophe. New paradigms of urban design and civic infrastructure must be decoupled from societys carbon-intensive practices and archaic fetishes for \"solidity\" in building. \"Soft Infrastructures\" present an alternative modality for urban design. This lecture will discuss three on-going projects by the Center for Civilization: Civic Commons Catalyst, Eternal Ephemera, and Soft City/Soft Haus.","bio":"Alberto de Salvatierra is an assistant professor of urbanism and data in architecture at the University of Calgary's School of Architecture, Planning and Landscape, director of the Center for Civilizationa design research lab and international think tank, the founding principal of PROXIIMA, and a Global Shaper at the Calgary Hub of the Global Shapers Communityan initiative by the World Economic Forum based in Geneva, Switzerland. An interdisciplinary polymath, architectural designer, and landscape urbanist, Albertos research and work focuses on material flows as infrastructure at the urban and civilizational scales, while his collaborative research agenda centers on fostering, developing and writing on interdisciplinary pedagogy and practices. His work has been published widely and exhibited both domestically and abroad, such as in the United States, the United Kingdom, Mexico, Italy, Japan, Sweden and Serbia, and in such venues as the Priscilla Fowler Fine Art Gallery in Las Vegas, NV, Calatrava-designed Milwaukee Art Museum in Milwaukee, WI, and the National Building Museum in Washington, D.C. In 2019, he was part of the Harvard Kennedy Schools inaugural STS (Science, Technology and Society) program on Expertise, Trust and Democracy, and an invited panelist and delegate to the United Nations."},{"date":"2021-03-01","name":"Alicia Nahmad Vazquez","affiliation":"UCalgary School of Architecture, Landscape and Planning","url":"https://sapl.ucalgary.ca/about/people/alicia-nahmad","title":"Design in the Age of Intelligent Machines","bio":"Alicia Nahmad Vazquez is the founder of Architecture Extrapolated (R-Ex) and an assistant professor in robotics and AI at the University of Calgary School of Architecture Planning and Landscape (SAPL) . She is also co-director of the Laboratory for Integrative Design at UofC. For the past 5 years, Alicia worked as studio master at the Architectural Associational Design Research Laboratory (DRL) masters program. As a research-based practising architect, Alicia explores materials and digital design and fabrication technologies along with the digitization of building trades and the wisdom of traditional building cultures. Her projects include the construction of award-winning Knit-Candela and diverse collaborations with practice and academic institutions. She holds a PhD in human-robot collaborative (HRC) design from Cardiff University and a MArch from the AADRL. Alicia previously worked on developing design tools for practices like Populous and Zaha Hadid Architects. Alicia has also been an Artist-In-Residence at Autodesk Pier 9 and has taught and lectured extensively in Latin America and Europe. Her research has been widely published internationally in journals and conference proceedings."},{"date":"2021-02-08","name":"Rubaiat Habib","affiliation":"Adobe Research","url":"https://rubaiathabib.me/","title":"Dynamic Graphics as a Language","abstract":"How can we make animation as easy as sketching? How can we create dynamic contents in real-time, in the speed of thought? How will dynamic graphics shape our real-time communications and language? In this talk, I'm going to present my research on animation, storytelling, and design, including the design of Sketchbook Motion that was crowned as \"The best iPad app of the year 2016\" by Apple. Most of us experience the power of animated media every day: animation makes it easy to communicate complex ideas beyond verbal language. However, only few of us have the skills to express ourselves through this medium. By making animation as easy, accessible, and fluid as sketching, I intend to make dynamic graphics a powerful medium to think, create, and communicate rapidly.","bio":"Rubaiat Habib is a Sr. Research Scientist at Adobe Research. His research interest lies at the intersection of Computer Graphics and HCI for creative thinking, design, and storytelling. His research in dynamic drawings and animations turned into products that reach a global audience. Rubaiat received several awards for his work including Apple App of the year 2016, three ACM CHI Best Paper Nominations, ACM CHI and ACM UIST Peoples choice best talk awards, and ACM CHI Golden Mouse awards for best research videos. For his PhD at the National University of Singapore, Rubaiat also received a Microsoft Research Asia PhD fellowship. Prior to Adobe, he worked at Autodesk Research and Microsoft Research. rubaiathabib.me"},{"date":"2021-01-25","name":"Xing-Dong Yang","affiliation":"Dartmouth College","url":"https://www.cs.dartmouth.edu/~xingdong/","title":"Creating Smart Everyday Things","abstract":"In my vision, the user interfaces of the future are in a blend of smart physical and virtual environments. My research focuses on the physical side by bringing interactivity to everyday things. I believe this vision is only achievable if people with varying backgrounds and abilities can work together in an accessible and collaborative environment. In this talk, I will describe two major threads of research in interactive everyday things and hardware prototyping tools. The first thread investigates interactive systems to sense the context of use of the things or estimate a users intention when touch input data is noisy. For example, I will demonstrate a tablecloth augmented with a fabric sensor that can sense and recognize non-metallic objects placed on a table, such as food, different types of fruits, liquids, plastic, and paper products. I will also show examples of how this technique can be used for contextual applications. The second thread investigates tools to lower the bar of entry to prototyping electronics, which is an essential skill needed to create smart everyday things. The goal of this line of work is to enable more people with varying backgrounds and abilities to create smart everyday things and eventually a better user experience of smart environments. For example, I will demonstrate an audio-tactile tutorial system for blind or low vision learners to understand circuit diagrams, which is an important task in the circuit prototyping pipeline. Both of these threads share a common goal that is to create a better user experience in smart environments.","bio":"Xing-Dong Yang is an Assistant Professor of Computer Science at Dartmouth College. His research is broadly in Human-Computer Interaction (HCI), where he creates interactive systems using sensing techniques and haptics to enable new applications in smart physical and virtual environments. Xing-Dongs work is recognized through a Best Paper award at UIST 2019, eight Honorable Mention awards with one at UIST 2020, six at CHI (2010, 2016, 2018, 2019  2, 2020), and one at MobileHCI 2009. Aside from academic publications, Xing-Dongs work attracts major public interest via news coverage from a variety of media outlets with different mediums, including TV (e.g., Discovery Daily Planet), print (e.g., The Wall Street Journal, Forbes), and Internet News (e.g., MIT Technology Review, New Scientist)."},{"date":"2021-01-15","name":"Ken Nakagaki","affiliation":"MIT Media Lab","url":"https://www.ken-nakagaki.com/","title":"'Mechanical Shells' for Actuated Tangible UIs - Hybrid Architecture of Active and Passive Machines for Interaction Design","abstract":"Research on actuated and shape-changing Tangible User Interfaces (TUIs) in the field of HCI has been explored widely in the past few decades to enrich interaction with digital information in physical and dynamic ways. In this effort, various types of generic devices of actuated TUIs have been investigated including pin-based shape displays, actuated curve interfaces, and swarm user interfaces. While these approaches are intended to be dynamically reconfigurable to offer generic interactivity, each hardware is inherently limited to the fixed configurations. How can we further expand the versatility of the actuated TUIs for fully expanding their capability for tangible interactions and motion / shape representations?\n\nIn my talk, I propose a mechanical shell, a design concept for actuated TUIs with modular interchangeable components that extends and converts the shape, motion, and interactivity of the hardware. By doing so, compared with the actuated TUI itself, each mechanical shell would bring much more specialized and customized interactivity, while, as the whole architecture, the system can adapt to much more versatile interactions. I present two research instances that demonstrate this concept based-on pin-based shape display and swarm user interface, and introduce proof-of-concept implementation as well as a range of applications. By introducing the novel interaction architecture, my research envisions the future of the physical environment where active and passive machines exist together for enriching tangible and embodied interactions.","bio":"Ken is an interaction designer and HCI researcher from Japan. Currently, he is a Ph.D. Candidate of Tangible Media Group, MIT Media Lab. He is interested in developing interfaces that combine digital information or computational aids into daily physical tools and materials, to develop novel physical and perceptual experiences. His research has been presented in top HCI conferences (ACM CHI, UIST, TEI, etc), and demonstrated in various exhibitions and awards including Ars Electronica, A' Design Award, and Japan Media Arts Festival."}],"content/output/vimeo.json":{"182971005":"https://i.vimeocdn.com/video/592033369_640.webp","230834366":"https://i.vimeocdn.com/video/651490206_640.webp","275404995":"https://i.vimeocdn.com/video/707686230_640.webp","289789025":"https://i.vimeocdn.com/video/725516359_640.webp","360483702":"https://i.vimeocdn.com/video/814665539_640.webp","368703151":"https://i.vimeocdn.com/video/825448765_640.webp","dir":"content/output","base":"vimeo.json","ext":".json","sourceBase":"vimeo.yaml","sourceExt":".yaml"},"content/output/work-in-progress/chi-2020-asha.json":{"date":"2020-05","title":"Views from the Wheelchair: Understanding Interaction between Autonomous Vehicle and Pedestrians with Reduced Mobility","authors":["Ashratuz Zavin Asha","Christopher Smith","Lora Oehlberg","Sowmya Somanath","Ehud Sharlin"],"series":"CHI EA 2020","doi":"https://doi.org/10.1145/3334480.3383041","keywords":"pedestrian with reduced mobility, autonomous vehicle","pages":8,"video":"https://www.youtube.com/watch?v=JNc49desa44","abstract":"We are interested in the ways pedestrians will interact with autonomous vehicle (AV) in a future AV transportation ecosystem, when nonverbal cues from the driver such as eye movements, hand gestures, etc. are no longer provided. In this work, we examine a subset of this challenge: interaction between pedestrian with reduced mobility (PRM) and AV. This study explores interface designs between AVs and people in a wheelchair to help them interact with AVs by conducting a preliminary design study. We have assessed the data collected from the study using qualitative analysis and presented different findings on AV-PRM interactions. Our findings reflect on the importance of visual interfaces, changes to the wheelchair and the creative use of the street infrastructure.","dir":"content/output/work-in-progress","base":"chi-2020-asha.json","ext":".json","sourceBase":"chi-2020-asha.yaml","sourceExt":".yaml"},"content/output/work-in-progress/dis-2018-ta.json":{"date":"2018-06","title":"Bod-IDE: An Augmented Reality Sandbox for eFashion Garments","authors":["Kevin Ta","Ehud Sharlin","Lora Oehlberg"],"series":"DIS 2018","doi":"https://doi.org/10.1145/3197391.3205408","keywords":"augmented reality, electronic fashion, creativity support tool","pages":5,"abstract":"Electronic fashion (eFashion) garments use technology to augment the human body with wearable interaction. In developing ideas, eFashion designers need to prototype the role and behavior of the interactive garment in context; however, current wearable prototyping toolkits require semi-permanent construction with physical materials that cannot easily be altered. We present Bod-IDE, an augmented reality 'mirror' that allows eFashion designers to create virtual interactive garment prototypes. Designers can quickly build, refine, and test on-the-body interactions without the need to connect or program electronics. By envisioning interaction with the body in mind, eFashion designers can focus more on reimagining the relationship between bodies, clothing, and technology.","dir":"content/output/work-in-progress","base":"dis-2018-ta.json","ext":".json","sourceBase":"dis-2018-ta.yaml","sourceExt":".yaml"}},"sourceFileArray":["content/booktitles.yaml","content/facility.yaml","content/labs.yaml","content/news.yaml","content/people/abhinav-pillai.yaml","content/people/aditya-gunturu.yaml","content/people/aditya-shekhar-nittala.yaml","content/people/adnan-karim.yaml","content/people/anthony-tang.yaml","content/people/april-zhang.yaml","content/people/ashratuz-zavin-asha.yaml","content/people/bheesha-kumari.yaml","content/people/bon-adriel-aseniero.yaml","content/people/brennan-jones.yaml","content/people/carl-gutwin.yaml","content/people/carman-neustaedter.yaml","content/people/carmen-hull.yaml","content/people/charlotte-tang.yaml","content/people/christian-frisson.yaml","content/people/christopher-rodriguez.yaml","content/people/christopher-smith.yaml","content/people/colin-auyeung.yaml","content/people/dane-bertram.yaml","content/people/darcy-norman.yaml","content/people/david-ledo.yaml","content/people/desmond-larsen-rosner.yaml","content/people/dinmukhammed-mukashev.yaml","content/people/donald-cox.yaml","content/people/doug-schaeffer.yaml","content/people/edward-tse.yaml","content/people/ehud-sharlin.yaml","content/people/faiz-marsad.yaml","content/people/georgina-freeman.yaml","content/people/grace-ferguson.yaml","content/people/gregor-mcewan.yaml","content/people/harrison-chen.yaml","content/people/helen-ai-he.yaml","content/people/hiroki-kaimoto.yaml","content/people/iryna-luchak.yaml","content/people/james-tam.yaml","content/people/jarin-thundathil.yaml","content/people/jessi-stark.yaml","content/people/jian-liao.yaml","content/people/jiannan-li.yaml","content/people/karly-ross.yaml","content/people/karthik-mahadevan.yaml","content/people/katherine-currier.yaml","content/people/kathryn-blair.yaml","content/people/kathryn-elliot-rounding.yaml","content/people/kaynen-mitchell.yaml","content/people/keiichi-ihara.yaml","content/people/kendra-wannamaker.yaml","content/people/kevin-van.yaml","content/people/kimberly-tee.yaml","content/people/kurtis-danyluk.yaml","content/people/kyzyl-monteiro.yaml","content/people/linda-tauscher.yaml","content/people/lora-oehlberg.yaml","content/people/mackenzie-bowal.yaml","content/people/mackenzie-hisako-dalton.yaml","content/people/manjot-khangura.yaml","content/people/manuel-rodriguez.yaml","content/people/marcus-friedel.yaml","content/people/mark-roseman.yaml","content/people/martin-feick.yaml","content/people/matthew-dunlap.yaml","content/people/mehrad-faridan.yaml","content/people/melissa-hoang.yaml","content/people/miaosen-wang.yaml","content/people/michael-hung.yaml","content/people/micheal-boyle.yaml","content/people/micheal-nunes.yaml","content/people/micheal-rounding.yaml","content/people/mille-skovhus-lunding.yaml","content/people/muhammad-mahian.yaml","content/people/nathalie-bressa.yaml","content/people/neil-chulpongsatorn.yaml","content/people/nelson-wong.yaml","content/people/nicolai-marquardt.yaml","content/people/nishan-soni.yaml","content/people/nour-hammad.yaml","content/people/paul-lapides.yaml","content/people/paul-saulnier.yaml","content/people/priya-dhawka.yaml","content/people/ran-zhou.yaml","content/people/rasmus-lunding.yaml","content/people/ritik-vatsal.yaml","content/people/roberta-cabral-mota.yaml","content/people/roberto-diaz-marino.yaml","content/people/ryo-suzuki.yaml","content/people/ryota-gomi.yaml","content/people/sabrina-lakhdhir.yaml","content/people/saja-abufarha.yaml","content/people/samin-farajian.yaml","content/people/sasha-ivanov.yaml","content/people/saul-greenberg.yaml","content/people/setareh-manesh.yaml","content/people/shanna-hollingworth.yaml","content/people/shaun-kaasten.yaml","content/people/sheelagh-carpendale.yaml","content/people/shivesh-jadon.yaml","content/people/shrivatsa-mishra.yaml","content/people/simon-kluber.yaml","content/people/soren-knudsen.yaml","content/people/sowmya-somanath.yaml","content/people/stephanie-smale.yaml","content/people/sydney-pratte.yaml","content/people/teddy-seyed.yaml","content/people/terrance-mok.yaml","content/people/theodore-ogrady.yaml","content/people/tian-xia.yaml","content/people/tim-au-yeung.yaml","content/people/wei-wei.yaml","content/people/wesley-willett.yaml","content/people/william-wright.yaml","content/people/xiang-anthony-chen.yaml","content/people/yibo-sun.yaml","content/people/zachary-mckendrick.yaml","content/people/zhijie-xia.yaml","content/publications/assets-2017-suzuki.yaml","content/publications/cga-2019-ivanov.yaml","content/publications/cgi-2019-danyluk.yaml","content/publications/chi-2015-aseniero.yaml","content/publications/chi-2015-jones.yaml","content/publications/chi-2015-oehlberg.yaml","content/publications/chi-2015-willett.yaml","content/publications/chi-2017-aoki.yaml","content/publications/chi-2017-hull.yaml","content/publications/chi-2017-ledo.yaml","content/publications/chi-2017-somanath.yaml","content/publications/chi-2018-dillman.yaml","content/publications/chi-2018-feick.yaml","content/publications/chi-2018-heshmat.yaml","content/publications/chi-2018-ledo.yaml","content/publications/chi-2018-mahadevan.yaml","content/publications/chi-2018-neustaedter.yaml","content/publications/chi-2018-oh.yaml","content/publications/chi-2018-suzuki.yaml","content/publications/chi-2018-wuertz.yaml","content/publications/chi-2019-danyluk.yaml","content/publications/chi-2019-george.yaml","content/publications/chi-2020-anjani.yaml","content/publications/chi-2020-asha.yaml","content/publications/chi-2020-goffin.yaml","content/publications/chi-2020-hou.yaml","content/publications/chi-2020-suzuki.yaml","content/publications/chi-2021-danyluk.yaml","content/publications/chi-2021-ens.yaml","content/publications/chi-2021-hammad.yaml","content/publications/chi-2022-bressa.yaml","content/publications/chi-2022-ivanov.yaml","content/publications/chi-2022-nittala.yaml","content/publications/chi-2022-suzuki.yaml","content/publications/chi-2023-faridan.yaml","content/publications/chi-2023-monteiro.yaml","content/publications/chi-ea-2022-blair.yaml","content/publications/chi-ea-2023-chulpongsatorn.yaml","content/publications/chi-ea-2023-fang.yaml","content/publications/cmj-2020-ko.yaml","content/publications/cnc-2019-hammad.yaml","content/publications/cupum-2021-rout.yaml","content/publications/dis-2016-jones.yaml","content/publications/dis-2017-mok.yaml","content/publications/dis-2018-mikalauskas.yaml","content/publications/dis-2018-pham.yaml","content/publications/dis-2018-ta.yaml","content/publications/dis-2019-blair.yaml","content/publications/dis-2019-bressa.yaml","content/publications/dis-2019-ledo.yaml","content/publications/dis-2019-mahadevan.yaml","content/publications/dis-2019-nakayama.yaml","content/publications/dis-2019-seyed.yaml","content/publications/dis-2021-asha.yaml","content/publications/dis-2021-blair.yaml","content/publications/dis-2021-wannamaker.yaml","content/publications/frobt-2022-suzuki.yaml","content/publications/gecco-2022-ivanov.yaml","content/publications/gi-2020-rajabiyazdi.yaml","content/publications/gi-2021-mactavish.yaml","content/publications/gi-2022-hull.yaml","content/publications/hri-2018-feick.yaml","content/publications/ieee-2021-willett.yaml","content/publications/ijac-2021-hosseini.yaml","content/publications/imwut-2020-wang.yaml","content/publications/imx-2020-mok.yaml","content/publications/iros-2020-hedayati.yaml","content/publications/iros-2022-suzuki.yaml","content/publications/mobilehci-2015-ledo.yaml","content/publications/mobilehci-2019-hung.yaml","content/publications/nime-2020-ko.yaml","content/publications/sui-2017-li.yaml","content/publications/tei-2016-somanath.yaml","content/publications/tei-2019-mikalauskas.yaml","content/publications/tei-2019-tolley.yaml","content/publications/tei-2019-wun.yaml","content/publications/tei-2020-suzuki.yaml","content/publications/tei-2021-pratte.yaml","content/publications/tochi-2022-nittala.yaml","content/publications/tvcg-2016-lopez.yaml","content/publications/tvcg-2017-goffin.yaml","content/publications/tvcg-2017-willett.yaml","content/publications/tvcg-2019-blascheck.yaml","content/publications/tvcg-2019-walny.yaml","content/publications/tvcg-2020-danyluk.yaml","content/publications/uist-2018-suzuki.yaml","content/publications/uist-2019-suzuki.yaml","content/publications/uist-2020-suzuki.yaml","content/publications/uist-2020-yixian.yaml","content/publications/uist-2021-suzuki.yaml","content/publications/uist-2022-kaimoto.yaml","content/publications/uist-2022-liao.yaml","content/publications/uist-2022-nisser.yaml","content/publications/uist-2022-nittala.yaml","content/publications/uist-sic-2022-faridan.yaml","content/publications/vr-2019-satriadi.yaml","content/seminar.yaml","content/vimeo.yaml","content/work-in-progress/chi-2020-asha.yaml","content/work-in-progress/dis-2018-ta.yaml"]};

/***/ }),

/***/ "./content/output/news.json":
/*!**********************************!*\
  !*** ./content/output/news.json ***!
  \**********************************/
/*! exports provided: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, default */
/***/ (function(module) {

module.exports = [{"date":"2023-04-23","text":"Attend [**CHI 2023 LBW**](https://chi2023.acm.org/) in Hamburg to present two full-papers and two late-breaking work"},{"date":"2023-02-26","text":"Two [**CHI 2023 LBW**](https://chi2023.acm.org/) papers were accepted (34%)"},{"date":"2023-01-13","text":"Two [**CHI 2023**](https://chi2023.acm.org/) full-papers are accepted (28%)"},{"date":"2023-01-11","text":"[**CHI 2023 PC Meeting**](https://chi2023.acm.org/subcommittees/selecting-a-subcommittee/#blending_interaction) (Blending Interaction Subcommittee)."},{"date":"2022-12-13","text":"Three papers were submitted to R&R for [**CHI 2023**](https://chi2023.acm.org/)"},{"date":"2022-12-01","text":"Visit [**ATLAS Institute**](https://www.colorado.edu/atlas/) at [**CU Boulder**](https://www.colorado.edu/)"},{"date":"2022-11-02","text":"Mehrad Faridan and Marcus Friedel won Honorable Mention Award for [**UIST 2022**](https://uist.acm.org/uist2022/) Student Innovation Contest"},{"date":"2022-10-30","text":"Attend [**UIST 2022**](https://uist.acm.org/uist2022/) in Bend"},{"date":"2022-10-25","text":"Attend [**IROS 2022**](https://iros2022.org/) in Kyoto"},{"date":"2022-10-23","text":"Visit [**Kyoto University**](https://www.kyoto-u.ac.jp/en)"},{"date":"2022-10-15","text":"Visit [**Tohoku University**](https://www.riec.tohoku.ac.jp/~kitamura/) (Yoshifumi Kitamura Lab)."},{"date":"2022-10-15","text":"Traveling to Japan."},{"date":"2022-09-30","text":"[**TEI 2022 PC Meeting**](https://tei.acm.org/2023/participate/paper/)."},{"date":"2022-09-01","text":"[**VRST 2022 PC Meeting**](https://vrst.acm.org/vrst2022/organizers/)."},{"date":"2022-08-05","text":"[**ISMAR 2022 PC Meeting**](https://ismar2022.org/ismar-st-program-committee/)."},{"date":"2022-07-31","text":"Final Day for the Mitacs Globalink Summer Intern Students."},{"date":"2022-07-29","text":"Co-Charing [**UIST 2022 SIC**](https://www.youtube.com/watch?v=c1i7YrR3ZKo) with Thijs Rouman (Ultrasound Levitation Kit)."},{"date":"2022-07-10","text":"Visited [**Seoul National University**](https://en.snu.ac.kr/) (to see Inrak Choi)."},{"date":"2022-07-08","text":"Attended [**SIGCHI Korea**](https://2022summer.sigchi.kr/) Summer Event at the University of Seoul."},{"date":"2022-07-05","text":"Invited Talk at [**KAIST**](https://hci.kaist.ac.kr/) (hosted by Andrea Bianchi and Juho Kim)."},{"date":"2022-07-04","text":"Visit [**KAIST HCI Group**](https://hci.kaist.ac.kr/) in Korea."},{"date":"2022-06-30","text":"One [**IROS 2022**](https://iros2022.org/) full-paper is accepted (48%)."},{"date":"2022-06-25","text":"Three [**UIST 2022**](https://uist.acm.org/uist2022/) full-paper are accepted (26%)."},{"date":"2022-06-23","text":"[**UIST 2022 PC Meeting**](https://uist.acm.org/uist2022/organizers.html)."},{"date":"2022-06-16","text":"Visit [**SFU HC Group**](https://ixlab.cs.sfu.ca/) in Vancouver."},{"date":"2022-05-16","text":"Invited Talk at [**Calgary Public Library**](https://calgarylibrary.ca/)."},{"date":"2022-05-01","text":"Presented [**AR and Robotics Taxonomy**](https://ilab.ucalgary.ca/ar-and-robotics/) paper at [**CHI 2022**](https://chi2022.acm.org/) in New Orleans."},{"date":"2022-04-28","text":"Invited Talk at [**Microsoft Research EPIC Group**](https://www.microsoft.com/en-us/research/group/epic/) in Redmond (hosted by Ken Hinckley)."},{"date":"2022-03-11","text":"Invited Talk at [**CU Boulder ATLAS**](https://www.colorado.edu/atlas/) (hosted by Ellen Yi-Luen Do and Mark Gross)."},{"date":"2022-02-11","text":"One [**CHI 2022**](https://ilab.ucalgary.ca/ar-and-robotics/) full-paper is accepted (26%)."},{"date":"2022-01-21","text":"One [**ICRA 2022**](https://hcie.csail.mit.edu/research/Electrovoxel/electrovoxel.html) full-paper is accepted."},{"date":"2021-12-17","text":"Presented **RealitySketch** at [**SIGGRAPH Asia 2021 Real-time Live!**](https://sa2021.siggraph.org/en/attend/real-time-live/6/sessions) in front of thousands of audience."},{"date":"2021-11-11","text":"[**CHI 2022 PC Meeting**](https://chi2022.acm.org/for-authors/presenting/papers/selecting-a-subcommittee/#blending_interaction) (Blending Interaction)."},{"date":"2021-10-14","text":"Samin Farajian won [**UIST 2021**](https://uist.acm.org/uist2021/) [**Best Student Innovation Contest Award**](https://www.youtube.com/watch?v=R98vR3tukMY)"},{"date":"2021-09-30","text":"[**TEI 2022 PC Meeting**](https://tei.acm.org/2022/participate/papers/)."},{"date":"2021-09-01","text":"Started [**Programmable Reality Lab**](https://programmable-reality-lab.github.io/)."},{"date":"2021-08-28","text":"[**VRST 2021 PC Meeting**](https://vrst.acm.org/vrst2021/organizers/)."},{"date":"2021-08-28","text":"Our [**UIST 2021**](https://ryosuzuki.org/hapticbots/) full-paper is accepted (25%)."},{"date":"2021-07-19","text":"[**ISMAR 2021 PC Meeting**](https://ismar21.org/science-and-technology-program-committee/)."},{"date":"2021-06-23","text":"[**UIST 2021 PC Meeting**](https://uist.acm.org/uist2021/organizers.html)."},{"date":"2021-06-16","text":"We won [**Snap Creative Challenge Award**](https://www.snapcreativechallenge.com/)"},{"date":"2021-04-16","text":"Received [**NSERC Discovery Grant**](https://www.nserc-crsng.gc.ca/professors-professeurs/grants-subs/dgigp-psigp_eng.asp) Funding"},{"date":"2020-08-28","text":"Our [**UIST paper**](https://ryosuzuki.org/realitysketch/) won  **Honorable Mention Awarad**."},{"date":"2020-08-07","text":"Joining the [**University of Calgary**](https://www.ucalgary.ca/) as an Assisatnat Professor in CS and [**HCI Group**](https://ilab.cpsc.ucalgary.ca/) starting in Winter 2021."},{"date":"2020-07-28","text":"[**PhD Dissertation**](https://ryosuzuki.org/publications/phd-dissertation.pdf) is submitted and officially graduated! :)"},{"date":"2020-07-01","text":"One [**IROS 2020**](https://www.iros2020.org/) full-paper is accepted (47%)."},{"date":"2020-06-24","text":"One [**UIST 2020**](http://uist.acm.org/uist2020/) full-paper is accepted (21%)."},{"date":"2020-05-18","text":"Start intern at [**Microsoft Research**](https://www.microsoft.com/en-us/research/) (mentor: [**Mar Gonzalez Franco**](https://www.microsoft.com/en-us/research/people/margon/))."},{"date":"2020-05-13","text":"**[PhD Defense](https://ryosuzuki.org/phd-thesis) ... and Passed! :)** (committee: D Leithinger, M Gross, T Yeh, H Ishii, T Igarashi)"},{"date":"2020-02-14---05-13","text":"Traveling for on-site job interviews (UW, UCSB, Boston, Virginia Tech, Calgary)"},{"date":"2019-12-12","text":"One [**CHI 2020**](https://chi2020.acm.org/) full-paper is accepted (24%)."},{"date":"2019-11-25","text":"Traveling to Boston to visit **MIT CSAIL** and **Media Lab**"},{"date":"2019-10-19","image":"uist-2019.png","text":"Traveling to New Orleans for [**UIST 2019**](http://uist.acm.org/uist2019/)"},{"date":"2019-10-10","image":"uist-2019.png","text":"Became a **PhD candidate** (committee: D Leithinger, M Gross, T Yeh, H Ishii, T Igarashi, E Do)"},{"date":"2019-10-08","image":"uist-2019.png","text":"One [**TEI 2020**](https://tei.acm.org/2020/) full-paper is accepted (28%)."},{"date":"2019-08-02","image":"uist-2019.png","text":"One [**UIST 2019**](http://uist.acm.org/uist2019/) poster and and doctoral consortium paper are accepted."},{"date":"2019-07-01","image":"uist-2019.png","text":"One [**UIST 2019**](http://uist.acm.org/uist2019/) full-paper is accepted (24%)."},{"date":"2019-06-27","image":"uist-2019.png","text":"Won [**Best Paper Award**](https://dis2019.com/overview/#track-4-shape-changing-interfaces) for [**DIS 2019**](https://dis2019.com/) (Top 1%)."},{"date":"2019-05-20","image":"uist-2019.png","text":"Start intern at [**Adobe Research**](https://research.adobe.com/) (mentor: [**Rubaiat Habib**](https://rubaiathabib.me/))."},{"date":"2019-03-26","image":"dis-2019.png","text":"One [**DIS 2019**](https://dis2019.com/) full-paper is accepted (25%)."},{"date":"2018-08-15","text":"Award [**JST ACT-I**](https://www.jst.go.jp/kisoken/act-i/en/project/111C001/111C001_2018.html) funding (mentor [**Takeo Igarashi**](https://www-ui.is.s.u-tokyo.ac.jp/~takeo/))."},{"date":"2018-08-06","image":"uist-2018.png","text":"One [**UIST 2018**](https://uist.acm.org/uist2018/) full-paper is accepted (21%)."},{"date":"2018-08-03","image":"pg-2018.png","text":"One [**Pacific Graphics 2018**](http://sweb.cityu.edu.hk/pg2018/) short-paper is accepted (26%)."},{"date":"2017-12-14","text":"Start intern at [**The University of Tokyo**](http://www.jst.go.jp/erato/kawahara/) (mentor: [**Yasuaki Kakehi**](http://xlab.iii.u-tokyo.ac.jp/))."},{"date":"2017-12-11","image":"chi-2018.png","text":"Two [**CHI 2018**](https://chi2018.acm.org/) full-papers are accepted (25%)."},{"date":"2017-06-27","image":"vlhcc-2017.png","text":"One [**VL/HCC 2017**](https://sites.google.com/site/vlhcc2017/) full-paper are accepted (29%)."},{"date":"2017-06-21","image":"assets-2017.png","text":"One [**ASSETS 2017**](https://assets17.sigaccess.org/) full-paper are accepted (26%)."},{"date":"2017-02-11","image":"chi-2017.png","text":"One [**CHI 2017**](https://chi2017.acm.org/) LBW paper is accepted (38%)."},{"date":"2016-12-16","image":"","text":"One [**L@S 2017**](http://learningatscale.acm.org/las2017/) full paper is accepted (22%)."},{"date":"2016-12-12","image":"icse-2017.png","text":"One [**ICSE 2017**](http://icse2017.gatech.edu/) full-paper is accepted (19%)."},{"date":"2016-05-23","text":"Start intern at [**UC Berkeley**](http://bid.berkeley.edu/) (mentor: [**Bjoern Hartmann**](http://people.eecs.berkeley.edu/~bjoern/))."},{"date":"2016-01-15","image":"chi-2016.png","text":"One [**CHI 2016**](https://chi2016.acm.org/wp/) full-paper is accepted (23%)."},{"date":"2015-10-01","image":"uist-2016.jpg","text":"I will serve as a web and social media chair for [**UIST 2016**](http://uist.acm.org/uist2016/)"},{"date":"2015-05-18","text":"Start intern at [**Stanford**](https://hci.stanford.edu/) (mentor: [**Michael Bernstein**](https://hci.stanford.edu/msb/))."},{"date":"2014-10-15","text":"The very first day of starting my HCI research (mentor: [**Koji Yatani**](http://iis-lab.org/member/koji-yatani/))"}];

/***/ }),

/***/ "./content/output/posters sync recursive ^\\.\\/.*\\.json$":
/*!****************************************************!*\
  !*** ./content/output/posters sync ^\.\/.*\.json$ ***!
  \****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var map = {
	"./ultrabots.json": "./content/output/posters/ultrabots.json"
};


function webpackContext(req) {
	var id = webpackContextResolve(req);
	return __webpack_require__(id);
}
function webpackContextResolve(req) {
	var id = map[req];
	if(!(id + 1)) { // check for number or string
		var e = new Error("Cannot find module '" + req + "'");
		e.code = 'MODULE_NOT_FOUND';
		throw e;
	}
	return id;
}
webpackContext.keys = function webpackContextKeys() {
	return Object.keys(map);
};
webpackContext.resolve = webpackContextResolve;
module.exports = webpackContext;
webpackContext.id = "./content/output/posters sync recursive ^\\.\\/.*\\.json$";

/***/ }),

/***/ "./content/output/posters.json":
/*!*************************************!*\
  !*** ./content/output/posters.json ***!
  \*************************************/
/*! exports provided: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, default */
/***/ (function(module) {

module.exports = [{"author":"Neil Chulpongsatorn, Wesley Willett, Ryo Suzuki","id":"holotouch","name":"HoloTouch","title":"HoloTouch: Interacting with Mixed Reality Visualizations Through Smartphone Proxies","pdf":"chi-2023-holotouch.pdf","series":"CHI '23 LBW","doi":"10.1145/3544549.3585738","year":2023},{"author":"Cathy Mengying Fang, Ryo Suzuki, Daniel Leithinger","id":"haptics-at-home","name":"VR Haptics at Home","title":"VR Haptics at Home: Repurposing Everyday Objects and Environment for Casual and On-Demand VR Haptic Experiences","pdf":"chi-2023-haptics-at-home.pdf","series":"CHI '23 LBW","doi":"10.1145/3544549.3585871","year":2023},{"author":"Mehrad Faridan, Marcus Friedel, Ryo Suzuki","id":"ultrabots","name":"UltraBots","title":"UltraBots: Large-Area Mid-Air Haptics for VR with Robotically Actuated Ultrasound Transducers","pdf":"uist-2022-ultrabots.pdf","booktitle":"Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software & Technology","series":"UIST '22 SIC","year":2022,"video":"https://www.youtube.com/watch?v=jMYAQzzQ_PI","location":"Bend, Oregon","numpages":3,"doi":"10.1145/3526114.3561350","publisher":"ACM","address":"New York, NY, USA","pages":"1--3"},{"author":"Marcus Friedel, Ehud Sharlin, Ryo Suzuki","id":"haptic-lever","name":"HapticLever","title":"HapticLever: Kinematic Force Feedback using a 3D Pantograph","pdf":"uist-2022-hapticlever.pdf","booktitle":"Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software & Technology","series":"UIST '22 Poster","year":2022,"location":"Bend, Oregon","numpages":3,"doi":"10.1145/3526114.3558736","publisher":"ACM","address":"New York, NY, USA","pages":"1--3"},{"author":"Ryo Suzuki, Rubaiat Habib, Li-Yi Wei, Stephen Diverdi, Wilmot Li, Daniel Leithinger","id":"reality-sketch","name":"RealitySketch","title":"RealitySketch: Augmented Reality Sketching for Real-time Embedded and Responsive Visualiza- tions","pdf":"siggraph-asia-2021-realitysketch.pdf","booktitle":"Adjunct Proceedings of the 34th Annual ACM Symposium on User Interface Software & Technology","series":"SIGGRAPH Asia '21 RTL","year":2021,"location":"Tokyo, Japan","numpages":1,"doi":"xxx/xxx","publisher":"ACM","address":"New York, NY, USA","pages":"1--1"},{"author":"Samin Farajian, Hiroki Kaimoto, Ryo Suzuki","id":"swarm-fabrication","name":"Swarm Fabrication","title":"Swarm Fabrication: Reconfigurable 3D Printers and Drawing Plotters Made of Swarm Robots","pdf":"uist-2021-swarm-fabrication.pdf","booktitle":"Adjunct Proceedings of the 34th Annual ACM Symposium on User Interface Software & Technology","series":"UIST '21 SIC","year":2021,"video":"https://www.youtube.com/watch?v=R98vR3tukMY","location":"New Orleans, Louisiana, USA","numpages":2,"publisher":"ACM","address":"New York, NY, USA","pages":"1--2"},{"author":"Martin Nisser, Leon Cheng, Yashaswini Makaram, Ryo Suzuki, Stefanie Mueller","id":"programmable-polarities","name":"Programmable Polarities","title":"Programmable Polarities: Actuating Interactive Prototypes withProgrammable Electromagnets","pdf":"uist-2021-programmable.pdf","booktitle":"Adjunct Proceedings of the 34th Annual ACM Symposium on User Interface Software & Technology","series":"UIST '21 Demo","year":2021,"video":"https://www.youtube.com/watch?v=8F-MEuwspIM","isbn":"978-1-4503-4656-6","location":"New Orleans, Louisiana, USA","pages":"2951--2958","numpages":2,"doi":"10.1145/3474349.3480198","acmid":3053187,"publisher":"ACM","address":"New York, NY, USA","keywords":"modular self-reconfigurable robots"},{"author":"Ryo Suzuki,","id":"collective","name":"Collective Shape-Changing UI","title":"Collective Shape-changing Interfaces","pdf":"uist-2019-collective.pdf","booktitle":"Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software & Technology","series":"UIST '19 DC","year":2019,"isbn":"978-1-4503-4656-6","location":"New Orleans, Louisiana, USA","pages":"2951--2958","numpages":2,"doi":"10.1145/3332167.3356877","acmid":3053187,"publisher":"ACM","address":"New York, NY, USA","keywords":"shape-changing interfaces"},{"author":"Ryo Suzuki, Ryosuke Nakayama, Dan Liu, Yasuaki Kakehi, Mark D. Gross, and Daniel Leithinger,","id":"lift-tiles","name":"LiftTiles","title":"LiftTiles: Modular and Reconfigurable Room-scale Shape Displays through Retractable Inflatable Actuators","pdf":"uist-2019-lift-tiles.pdf","poster":"uist-2019-lift-tiles-poster.pdf","booktitle":"Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software & Technology","series":"UIST '19 Poster","year":2019,"video":"https://www.youtube.com/watch?v=SYCmftQCilU","isbn":"978-1-4503-4656-6","location":"New Orleans, Louisiana, USA","pages":"2951--2958","numpages":2,"doi":"10.1145/3332167.3357105","acmid":3053187,"publisher":"ACM","address":"New York, NY, USA","keywords":"inflatables, shape-changing interfaces,large-scale interactions"},{"author":"Ryo Suzuki, Gustavo Soares, Elena Glassman, Andrew Head, Loris D'Antoni, and Bjoern Hartmann,","id":"synthesized-hints","name":"Synthesized Hints","title":"Exploring the Design Space of Automatically Synthesized Hints for Introductory Programming Assignments","pdf":"chi-2017-lbw.pdf","poster":"chi-2017-lbw-poster.pdf","booktitle":"Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems","series":"CHI '17 LBW","year":2017,"isbn":"978-1-4503-4656-6","location":"Denver, Colorado, USA","pages":"2951--2958","numpages":8,"doi":"10.1145/3027063.3053187","acmid":3053187,"publisher":"ACM","address":"New York, NY, USA","keywords":"automated feedback, program synthesis, programming education"},{"author":"Stanford Crowd Research Collective","id":"daemo","name":"Daemo","title":"Daemo: A Self-Governed Crowdsourcing Marketplace","pdf":"uist-2015-daemo.pdf","booktitle":"Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology","series":"UIST '15 Poster","year":2015,"isbn":"978-1-4503-3780-9","location":"Daegu, Kyungpook, Republic of Korea","pages":"101--102","numpages":2,"doi":"10.1145/2815585.2815739","acmid":2815739,"publisher":"ACM","address":"New York, NY, USA","keywords":"crowd research, crowd work., crowdsourcing"},{"author":"Ryo Suzuki","id":"cumiki","name":"Cumiki","title":"Interactive and Collaborative Source Code Annotation","pdf":"icse-2015-cumiki.pdf","poster":"icse-2015-cumiki-poster.pdf","booktitle":"Proceedings of the 37th International Conference on Software Engineering - Volume 2","series":"ICSE '15 Poster","year":2015,"video":"https://www.youtube.com/watch?v=0-CzQhxMRkc","location":"Florence, Italy","pages":"799--800","numpages":2,"url":"http://dl.acm.org/citation.cfm?id=2819009.2819173","doi":"10.5555/2819009.2819173","acmid":2819173,"publisher":"IEEE Press","address":"Piscataway, NJ, USA"}];

/***/ }),

/***/ "./content/output/posters/ultrabots.json":
/*!***********************************************!*\
  !*** ./content/output/posters/ultrabots.json ***!
  \***********************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, pages, conference, pdf, video, embed, slide, acm-dl, arxiv, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"ultrabots","name":"Atelier","description":"Repurposing Expert Crowdsourcing Tasks as Micro-internships","title":"Atelier: Repurposing Expert Crowdsourcing Tasks as Micro-internships","authors":["Ryo Suzuki","Niloufar Salehi","Michelle S. Lam","Juan C. Marroquin","Michael S. Bernstein"],"year":2016,"booktitle":"In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16)","publisher":"ACM, New York, NY, USA","pages":"2645-2656","conference":{"name":"CHI 2016","fullname":"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2016)","url":"https://chi2016.acm.org/wp/"},"pdf":"chi-2016-atelier.pdf","video":"https://www.youtube.com/watch?v=tBojZejtFQo","embed":"https://www.youtube.com/embed/tBojZejtFQo","slide":"chi-2016-atelier-slide.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=2858121","arxiv":"https://arxiv.org/abs/1602.06634","pageCount":12,"slideCount":56,"bodyContent":"","bodyHtml":"","dir":"content/output/posters","base":"ultrabots.json","ext":".json","sourceBase":"ultrabots.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/press.json":
/*!***********************************!*\
  !*** ./content/output/press.json ***!
  \***********************************/
/*! exports provided: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, default */
/***/ (function(module) {

module.exports = [{"date":"2022-02","tag":"electro-voxel","media":"Engadget","title":"Scientists create cube robots that can shapeshift in space","url":"https://www.engadget.com/shapeshifting-robots-space-exploration-mit-csail-electrovoxels-140026431.html"},{"date":"2022-02","tag":"electro-voxel","media":"TechXplore","title":"Robotic cubes: Self-reconfiguring ElectroVoxels use embedded electromagnets to test applications for space exploration","url":"https://techxplore.com/news/2022-02-robotic-cubes-self-reconfiguring-electrovoxels-embedded.html"},{"date":"2022-02","tag":"electro-voxel","media":"TechEBlog","title":"MIT Researchers Develop Shape-Shifting ElectroVoxel Robots for Space Exploration","url":"https://www.techeblog.com/mit-electrovoxel-shape-shifting-robot-space/"},{"date":"2022-02","tag":"electro-voxel","media":"IEEE Spectrum","title":"Video Friday: Your weekly selection of awesome robot videos","url":"https://spectrum.ieee.org/video-friday-lunar-rover"},{"date":"2022-02","tag":"electro-voxel","media":"Arduino Blog","title":"ElectroVoxel robots reconfigure themselves using magnets","url":"https://blog.arduino.cc/2022/02/08/electrovoxel-robots-reconfigure-themselves-using-magnets/"},{"date":"2022-02","tag":"electro-voxel","media":"Hackster.io","title":"These Magnetic Robots Assemble Like Voltron","url":"https://www.hackster.io/news/these-magnetic-robots-assemble-like-voltron-37e4b1f4ec5d"},{"date":"2021-11","tag":"snap","media":"UCalgary News","title":"Touchable spoken words bring the fantastic to life","url":"https://science.ucalgary.ca/news/touchable-spoken-words-bring-fantastic-life"},{"date":"2021-07","tag":"roomshift","media":"IEEE Computer Graphics and Applications","title":"Cover Story of Real Virtual Reality (vol. 41)","url":"https://www.computer.org/csdl/magazine/cg/2021/04/09487526/1vg3mB6LNJe"},{"date":"2021-03","tag":"realitysketch","media":"IT Media News","title":"Evolution of AR Drawing? RealitySketch, a sketching technology that works with objects in reality","url":"https://www.itmedia.co.jp/news/articles/2103/18/news027.html"},{"date":"2020-12","tag":"realitysketch","media":"TechXplore","title":"RealitySketch: An AR interface to create responsive sketches","url":"https://techxplore.com/news/2020-12-realitysketch-ar-interface-responsive.html"},{"date":"2020-10","tag":"pufferbot","media":"ACM TechNews","title":"Pufferfish-inspired robot could improve drone safety","url":"https://cacm.acm.org/news/248245-pufferfish-inspired-robot-could-improve-drone-safety/fulltext"},{"date":"2020-10","tag":"pufferbot","media":"Interesting Engineering","title":"Pufferfish Mimicking Drones to Improve Aerial Safety","url":"https://interestingengineering.com/pufferfish-mimicking-drones-to-improve-aerial-safety"},{"date":"2020-10","tag":"pufferbot","media":"New Atlas","title":"Drone draws on the pufferfish to protect itself and others","url":"https://newatlas.com/drones/pufferbot-drone"},{"date":"2020-10","tag":"roomshift","media":"Techable","title":"University of Colorado researchers unveil RoomShift to move props in VR space in real life","url":"https://techable.jp/archives/139024"},{"date":"2020-10","tag":"roomshift","media":"Hackster.io","title":"Putting the Reality in Virtual Reality","url":"https://www.hackster.io/news/putting-the-reality-in-virtual-reality-8c21db67e605"},{"date":"2020-09","tag":"pufferbot","media":"Hackster.io","title":"PufferBot Is an Aerial Robot That Can Change Shape In-Flight","url":"https://www.hackster.io/news/pufferbot-is-an-aerial-robot-that-can-change-shape-in-flight-0be9c211ce07"},{"date":"2020-09","tag":"roomshift","media":"TechXplore","title":"RoomShift: A room-scale haptic and dynamic environment for VR applications","url":"https://techxplore.com/news/2020-09-roomshift-room-scale-haptic-dynamic-environment.html"},{"date":"2020-09","tag":"pufferbot","media":"Engineering 360","title":"Team builds drone inspired by the pufferfish","url":"https://insights.globalspec.com/article/15041/team-builds-drone-inspired-by-the-pufferfish"},{"date":"2020-09","tag":"roomshift","media":"TechXplore","title":"PufferBot: A flying robot with an expandable body","url":"https://techxplore.com/news/2020-09-pufferbot-robot-body.html"},{"date":"2020-09","tag":"roomshift","media":"Yahoo News Japan","title":"The University of Colorado Announced RoomShift where Robot Rearranges Furniture to Create Virtual Spaces in a Realistic Way","url":"https://www.itmedia.co.jp/news/articles/2009/07/news097.html"},{"date":"2020-09","tag":"roomshift","media":"IT Media News","title":"RoomShift: Reconfigurable Environments for Virtual Reality","url":"https://www.itmedia.co.jp/news/articles/2009/07/news097.html"},{"date":"2020-02","tag":"pufferbot","media":"IT Media News","title":"Giant whistle module expands the room with the University of Colorado and other LiftTiles developments","url":"https://www.itmedia.co.jp/news/articles/2002/26/news029.html"},{"date":"2020-01","tag":"lift-tiles","media":"Arduino Blog","title":"Prototype room-scale, shape-changing interfaces with LiftTiles","url":"https://blog.arduino.cc/2020/01/27/prototype-room-scale-shape-changing-interfaces-with-lifttiles/"},{"date":"2020-01","tag":"lift-tiles","media":"TechXplore","title":"LiftTiles: Actuator-based Building Blocks for Shape-changing Interfaces","url":"https://techxplore.com/news/2020-01-lifttiles-actuator-based-blocks-shape-changing-interfaces.html"},{"date":"2020-01","tag":"shapebots","media":"ITMedia News","title":"A Swarm of Self-transforming Robots to Assist People","url":"https://www.itmedia.co.jp/news/articles/2001/15/news032.html"},{"date":"2019-10","tag":"lift-tiles","media":"Hackster.io","title":"LiftTiles Turn Walls and Floors Into Reconfigurable Structures on Demand","url":"https://www.hackster.io/news/lifttiles-turn-walls-and-floors-into-reconfigurable-structures-on-demand-4a226d58bc74"},{"date":"2019-10","tag":"lift-tiles","media":"Element 14","title":"Engineers Develop LiftTiles, a Scale Shape-changing Interface","url":"https://www.element14.com/community/community/applications/industrial-automation-space/blog/2019/10/25/engineers-develop-lifttiles-a-scale-shape-changing-interface?CMP=SOM-PRG-TWITTER-BLOG-CATWELL-LIFT-TILES-COMM"},{"date":"2019-11","tag":"shapebots","media":"Bouncy","title":"Swarm Robots that can Change Shape to Visualize Data","url":"https://bouncy.news/53532?fbclid=IwAR0jyfBKo8LJ3aiUidDfZUsQqJ5-oSMxRuiZyJju0g_F6A_hi1tOeboPM4E"},{"date":"2019-10","tag":"shapebots","media":"Hackster.io","title":"Swarming Robots Can Change Their Configuration to Handle Different Tasks","url":"https://www.hackster.io/news/shapebots-swarming-robots-can-change-their-configuration-to-handle-different-tasks-59a5ae926e1d"},{"date":"2019-09","tag":"shapebots","media":"TechXplore","title":"ShapeBots: A Swarm of Shape-shifting Robots that Visually Display Data","url":"https://techxplore.com/news/2019-09-shapebots-swarm-shape-shifting-robots-visually.html"},{"date":"2019-09","tag":"shapebots","media":"Hackaday","title":"Tiny Robots that Grow Taller and Wider","url":"https://hackaday.com/2019/10/04/tiny-robots-that-grow-taller-and-wider/"},{"date":"2019-09","media":"Robotic Gizmo","title":"ShapeBots: Shape-changing Swarm Robots","url":"https://www.roboticgizmos.com/shapebots/"},{"date":"2019-09","tag":"shapebots","media":"Gadgetify","title":"ShapeBots: Shape Changing Swarm Robots","url":"http://www.gadgetify.com/shapebots/"},{"date":"2018-10","tag":"dynablock","media":"3DPrint.com","title":"Dynablock: 3D Prints That Assemble and Disassemble in Seconds","url":"https://3dprint.com/227781/3d-prints-that-assemble-in-seconds/"},{"date":"2018-10","tag":"dynablock","media":"Hackster.io","title":"The Dynamic 3D Printing That Assembles and Disassembles Objects in Seconds","url":"https://www.hackster.io/news/dynablock-the-dynamic-3d-printing-that-assembles-and-disassembles-objects-in-seconds-a7f7d4bf6cad"},{"date":"2018-10","tag":"dynablock","media":"Arduino Blog","title":"Create Shapes Over and Over with the Dynablock 3D Printer","url":"https://blog.arduino.cc/2018/10/22/create-shapes-over-and-over-with-the-dynablock-3d-printer/"},{"date":"2018-10","tag":"dynablock","media":"3DRuck.com","title":"Dynablock: Dynamic 3D Printer Creates Objects in Seconds","url":"https://3druck.com/forschung/dynablock-dynamischer-3d-drucker-erstellt-objekte-in-sekunden-2776738/"},{"date":"2018-10","tag":"dynablock","media":"World Business Satellite","title":"Repeatable 3D Printer","url":"https://txbiz.tv-tokyo.co.jp/wbs/trend_tamago/post_168589/"},{"date":"2018-10","tag":"dynablock","media":"Nikkei Newspaper","title":"Modeling 3D Objects with Magnet-Embedded Blocks","url":"https://active.nikkeibp.co.jp/atclact/active/17/071100318/101600559/"},{"date":"2016-06","tag":"atelier","media":"Wired","title":"Its Not Just Robots: Skilled Jobs Are Going to Meatware","url":"https://www.wired.com/2016/06/its-not-just-robots-skilled-jobs-are-going-to-meatware/"}];

/***/ }),

/***/ "./content/output/projects sync recursive ^\\.\\/.*\\.json$":
/*!*****************************************************!*\
  !*** ./content/output/projects sync ^\.\/.*\.json$ ***!
  \*****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var map = {
	"./ar-and-robotics.json": "./content/output/projects/ar-and-robotics.json",
	"./atelier.json": "./content/output/projects/atelier.json",
	"./chameleon-control.json": "./content/output/projects/chameleon-control.json",
	"./dynablock.json": "./content/output/projects/dynablock.json",
	"./electro-voxel.json": "./content/output/projects/electro-voxel.json",
	"./expandable-robots.json": "./content/output/projects/expandable-robots.json",
	"./flux-marker.json": "./content/output/projects/flux-marker.json",
	"./hapticbots.json": "./content/output/projects/hapticbots.json",
	"./lift-tiles.json": "./content/output/projects/lift-tiles.json",
	"./mixed-initiative.json": "./content/output/projects/mixed-initiative.json",
	"./mixels.json": "./content/output/projects/mixels.json",
	"./morphio.json": "./content/output/projects/morphio.json",
	"./pep.json": "./content/output/projects/pep.json",
	"./phd-thesis.json": "./content/output/projects/phd-thesis.json",
	"./pufferbot.json": "./content/output/projects/pufferbot.json",
	"./reactile.json": "./content/output/projects/reactile.json",
	"./realitysketch.json": "./content/output/projects/realitysketch.json",
	"./realitytalk.json": "./content/output/projects/realitytalk.json",
	"./refazer.json": "./content/output/projects/refazer.json",
	"./roomshift.json": "./content/output/projects/roomshift.json",
	"./selective-self-assembly.json": "./content/output/projects/selective-self-assembly.json",
	"./shapebots.json": "./content/output/projects/shapebots.json",
	"./sketched-reality.json": "./content/output/projects/sketched-reality.json",
	"./tabby.json": "./content/output/projects/tabby.json",
	"./teachable-reality.json": "./content/output/projects/teachable-reality.json",
	"./trace-diff.json": "./content/output/projects/trace-diff.json"
};


function webpackContext(req) {
	var id = webpackContextResolve(req);
	return __webpack_require__(id);
}
function webpackContextResolve(req) {
	var id = map[req];
	if(!(id + 1)) { // check for number or string
		var e = new Error("Cannot find module '" + req + "'");
		e.code = 'MODULE_NOT_FOUND';
		throw e;
	}
	return id;
}
webpackContext.keys = function webpackContextKeys() {
	return Object.keys(map);
};
webpackContext.resolve = webpackContextResolve;
module.exports = webpackContext;
webpackContext.id = "./content/output/projects sync recursive ^\\.\\/.*\\.json$";

/***/ }),

/***/ "./content/output/projects/ar-and-robotics.json":
/*!******************************************************!*\
  !*** ./content/output/projects/ar-and-robotics.json ***!
  \******************************************************/
/*! exports provided: id, name, description, title, authors, year, conference, external, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"ar-and-robotics","name":"Augmented Reality and Robotics","description":"A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces","title":"Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces","authors":["Ryo Suzuki","Adnan Karim","Tian Xia","Hooman Hedayati","Nicolai Marquardt"],"year":2022,"conference":{"name":"CHI 2022","fullname":"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2022)","url":"https://chi2022.acm.org/"},"external":"https://ilab.ucalgary.ca/ar-and-robotics/","bodyContent":"","bodyHtml":"","dir":"content/output/projects","base":"ar-and-robotics.json","ext":".json","sourceBase":"ar-and-robotics.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/atelier.json":
/*!**********************************************!*\
  !*** ./content/output/projects/atelier.json ***!
  \**********************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, pages, conference, pdf, video, embed, slide, acm-dl, arxiv, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"atelier","name":"Atelier","description":"Repurposing Expert Crowdsourcing Tasks as Micro-internships","title":"Atelier: Repurposing Expert Crowdsourcing Tasks as Micro-internships","authors":["Ryo Suzuki","Niloufar Salehi","Michelle S. Lam","Juan C. Marroquin","Michael S. Bernstein"],"year":2016,"booktitle":"In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16)","publisher":"ACM, New York, NY, USA","pages":"2645-2656","conference":{"name":"CHI 2016","fullname":"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2016)","url":"https://chi2016.acm.org/wp/"},"pdf":"chi-2016-atelier.pdf","video":"https://www.youtube.com/watch?v=tBojZejtFQo","embed":"https://www.youtube.com/embed/tBojZejtFQo","slide":"chi-2016-atelier-slide.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=2858121","arxiv":"https://arxiv.org/abs/1602.06634","pageCount":12,"slideCount":56,"bodyContent":"","bodyHtml":"","dir":"content/output/projects","base":"atelier.json","ext":".json","sourceBase":"atelier.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/chameleon-control.json":
/*!********************************************************!*\
  !*** ./content/output/projects/chameleon-control.json ***!
  \********************************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, doi, conference, pdf, video, embed, arxiv, acm-dl, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"chameleon-control","name":"ChameleonControl","description":"Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms","title":"ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms","authors":["Mehrad Faridan","Bheesha Kumari","Ryo Suzuki"],"year":2023,"booktitle":"In Proceedings of the ACM CHI Conference on Human Factors in Computing Systems (CHI '23)","publisher":"ACM, New York, NY, USA","doi":"https://doi.org/10.1145/3544548.3581449","conference":{"name":"CHI 2023","fullname":"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2023)","url":"https://chi2023.acm.org/"},"pdf":"chi-2023-chameleon-control.pdf","video":"https://www.youtube.com/watch?v=VOe3fETd3sk","embed":"https://www.youtube.com/embed/VOe3fETd3sk","arxiv":"https://arxiv.org/abs/2302.11053","acm-dl":"https://dl.acm.org/doi/10.1145/3544548.3581449","pageCount":13,"slideCount":0,"bodyContent":"# Abstract\n\nWe present ChameleonControl, a real-human teleoperation system for scalable remote instruction in hands-on classrooms. In contrast to existing video or AR/VR-based remote hands-on education, ChameleonControl uses a real human as a surrogate of a remote instructor. Building on existing human-based telepresence approaches, we contribute a novel method to teleoperate a human surrogate through synchronized mixed reality hand gestural navigation and verbal communication. By overlaying the remote instructor's virtual hands in the local user's MR view, the remote instructor can guide and control the local user as if they were physically present. This allows the local user/surrogate to synchronize their hand movements and gestures with the remote instructor, effectively teleoperating a real human. We deploy and evaluate our system in classrooms of physiotherapy training, as well as other application domains such as mechanical assembly, sign language and cooking lessons. The study results confirm that our approach can increase engagement and the sense of co-presence, showing potential for the future of remote hands-on classrooms.","bodyHtml":"<h1>Abstract</h1>\n<p>We present ChameleonControl, a real-human teleoperation system for scalable remote instruction in hands-on classrooms. In contrast to existing video or AR/VR-based remote hands-on education, ChameleonControl uses a real human as a surrogate of a remote instructor. Building on existing human-based telepresence approaches, we contribute a novel method to teleoperate a human surrogate through synchronized mixed reality hand gestural navigation and verbal communication. By overlaying the remote instructor's virtual hands in the local user's MR view, the remote instructor can guide and control the local user as if they were physically present. This allows the local user/surrogate to synchronize their hand movements and gestures with the remote instructor, effectively teleoperating a real human. We deploy and evaluate our system in classrooms of physiotherapy training, as well as other application domains such as mechanical assembly, sign language and cooking lessons. The study results confirm that our approach can increase engagement and the sense of co-presence, showing potential for the future of remote hands-on classrooms.</p>\n","dir":"content/output/projects","base":"chameleon-control.json","ext":".json","sourceBase":"chameleon-control.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/dynablock.json":
/*!************************************************!*\
  !*** ./content/output/projects/dynablock.json ***!
  \************************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, pages, doi, conference, pdf, video, embed, short-video, slide, acm-dl, talk, poster, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"dynablock","name":"Dynablock","description":"Dynamic 3D Printing for Instant and Reconstructable Shape Formation","title":"Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation","authors":["Ryo Suzuki","Junichi Yamaoka","Daniel Leithinger","Tom Yeh","Mark D. Gross","Yoshihiro Kawahara","Yasuaki Kakehi"],"year":2018,"booktitle":"In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology (UIST '18)","publisher":"ACM, New York, NY, USA","pages":"99-111","doi":"https://doi.org/10.1145/3242587.3242659","conference":{"name":"UIST 2018","fullname":"The ACM Symposium on User Interface Software and Technology (UIST 2018)","url":"http://uist.acm.org/uist2018"},"pdf":"uist-2018-dynablock.pdf","video":"https://www.youtube.com/watch?v=7nPlr3O9xu8","embed":"https://www.youtube.com/embed/7nPlr3O9xu8","short-video":"https://www.youtube.com/watch?v=92eGI-gYYc4","slide":"uist-2018-dynablock-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3242659","talk":"https://www.youtube.com/watch?v=R3FRUtOIiCQ","poster":"uist-2018-dynablock-poster.pdf","pageCount":12,"slideCount":53,"bodyContent":"<!--\nLinks:\n[**[PDF](http://ryosuzuki.org/publications/uist-2018-dynablock.pdf)**]\n[**[ACM DL](https://dl.acm.org/citation.cfm?id=3242659)**]\n[**[Video](https://www.youtube.com/watch?v=7nPlr3O9xu8)**]\n[**[Slide](http://ryosuzuki.org/publications/uist-2018-dynablock-slide.pdf)**]\n[**[Talk](https://www.youtube.com/watch?v=R3FRUtOIiCQ)**]\n -->\n\n# Abstract\n\nThis paper introduces Dynamic 3D Printing, a fast and re- constructable shape formation system. Dynamic 3D Printing assembles an arbitrary three-dimensional shape from a large number of small physical elements. It can also disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbi- trary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and imple- mentation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/dynablock/webm/top.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/dynablock/video/top.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-1-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-1-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-1-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-1-2.jpg\" /></a>\n  </div>\n</div>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-2-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-2-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-2-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-2-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-2-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-2-3.jpg\" /></a>\n  </div>\n</div>\n\n# Dynamic 3D Printing\n\nWhat if 3D printers could form a physical object in seconds? What if the object, once it is no longer needed, could quickly and easily be disassembled and reconstructed as a new object? Todays 3D printers take hours to print objects, and output a single static object. However, we envision a future in which 3D printing could instantly create objects from reusable and reconstructable materials.\n\nThis paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.\n\nWe define Dynamic 3D Printing as a class of systems that have the following properties:\n\n- Immediate: The system can form a physical shape in sec- onds.\n\n- Reconstructable: Rendered shapes can be disassembled and reconstructed by hand or with the system, and the blocks are reusable.\n\n- Arbitrary Shapes: It can create arbitrary three dimensional shapes.\n\n- Graspable: The output shapes and structure are graspable and solid.\n\n# Parallel Assembler\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-3-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-3-1.png\" /></a>\n  </div>\n</div>\n\nDynamic 3D printing deploys a large number of small dis- crete material elements, which are assembled to form arbitrary shaped macro-scale objects. Individual elements are passive, which requires an external actuator to perform the assembly. As illustrated in the above Figure, the assembler consists of an N x N grid of motorized pins and linear actuators. The elements, which are the same size as the pins, are stacked on top of the pins (Figure 3 A). When stacked, the elements are connected in vertical direction, while discon- nected with nearby elements in horizontal direction. Similar to existing pin-based shape displays, the assembler can incrementally generate 2.5D shapes by individually moving pins to push elements to the surface.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/dynablock/webm/mechanism.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/dynablock/video/mechanism.mp4\" type=\"video/mp4\"></source>\n</video>\n\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-4-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-4-1.png\" /></a>\n  </div>\n  <p class=\"column\">\n    This paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.\n  </p>\n</div>\n\n\n# Implementation\n\nThe assembler consists of a 24 x 16 array of motor-driven pins. Each pin moves up and down, driven by a small DC motor (TTMotors TGPP06-D700) and a 3D printed lead screw (2 mm pitch, 4 starts, 120 mm in length). TGPP06-D700 is 6 mm in diameter and 29 mm in length and can rotate 47 rpm with 1:700 gear ratio. The 2 mm 4 starts lead screw can travel 12 mm per second without load, and each motor consumes approximately 60 mA. The pins are 3D printed with a nut at the bottom to travel along the lead screw. Each pin is 120 mm long and has a 7mm square cross section with a 5 mm diameter hole from top to bottom, and an N45 disk magnet ( 3mm x 2.4 mm thickness) is attached at the top. Guide grids at the top prevent pins from rotating and ensure that pins travel vertically. The 24 x 16 guide grids have 7.5 mm square holes with 10.16 mm pitch and are cut from a 5 mm acrylic plate. We fabricated the pins, the lead screws, and blocks with an inkjet 3D printer (Keyence Agilista 3200) with water soluble support material. In total, we fabricated 384 (= 24 x 16) pins and lead screws, and 3,072 (= 24 x 16 x 8 layers) blocks. To create the magnetic blocks, we embedded spherical magnets in each block by hand and inserted disk magnets using a bench vice.\n\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-5-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-5-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-5-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-5-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-5-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-5-3.png\" /></a>\n  </div>\n</div>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-7-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-7-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-7-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-7-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-7-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-7-3.jpg\" /></a>\n  </div>\n</div>\n\n\n# Future Vision\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/dynablock/webm/claytronics.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/dynablock/video/claytronics.mp4\" type=\"video/mp4\"></source>\n</video>\n[Video Credit: Carnegie Mellon University, Claytronics Vision]\n\n<br/>\n\nWith these capabilities, a 3D printer would become an inter- active medium, rather than merely a fabrication device. For example, such a 3D printer could be used in a Virtual Real- ity or Augmented Reality application to dynamically form a tangible object or controller to provide haptic feedback and engage users physically. For children, it could dynamically form a physical educational manipulative, such as a molec- ular or architectural model, to learn and explore topics, for example in a science museum. Designers could use it to ren- der a physical product to present to clients and interactively change the products design through direct manipulation. In this vision, Dynamic 3D printing is an environment in which the user thinks, designs, explores, and communicates through dynamic and interactive physical representation.\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-8-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-8-1.png\" /></a>\n  </div>\n</div>\n\nDynamic 3D printing would enable a new design workflow for digital fabrication. One notable advantage of dynamic 3D printing is the capability of connecting and disconnecting building blocks through direct manipulation. The user can also define variables or abstract attributes for parametric design through direct and gestural interaction. By leveraging this capability, the user could interactively design and fabri- cate in a physical space, similar to the man-machine dialogue proposed by Frazer et al. and later tangible CAD interfaces.","bodyHtml":"<!--\nLinks:\n[**[PDF](http://ryosuzuki.org/publications/uist-2018-dynablock.pdf)**]\n[**[ACM DL](https://dl.acm.org/citation.cfm?id=3242659)**]\n[**[Video](https://www.youtube.com/watch?v=7nPlr3O9xu8)**]\n[**[Slide](http://ryosuzuki.org/publications/uist-2018-dynablock-slide.pdf)**]\n[**[Talk](https://www.youtube.com/watch?v=R3FRUtOIiCQ)**]\n -->\n<h1>Abstract</h1>\n<p>This paper introduces Dynamic 3D Printing, a fast and re- constructable shape formation system. Dynamic 3D Printing assembles an arbitrary three-dimensional shape from a large number of small physical elements. It can also disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbi- trary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and imple- mentation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/dynablock/webm/top.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/dynablock/video/top.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-1-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-1-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-1-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-1-2.jpg\" /></a>\n  </div>\n</div>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-2-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-2-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-2-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-2-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-2-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-2-3.jpg\" /></a>\n  </div>\n</div>\n<h1>Dynamic 3D Printing</h1>\n<p>What if 3D printers could form a physical object in seconds? What if the object, once it is no longer needed, could quickly and easily be disassembled and reconstructed as a new object? Todays 3D printers take hours to print objects, and output a single static object. However, we envision a future in which 3D printing could instantly create objects from reusable and reconstructable materials.</p>\n<p>This paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.</p>\n<p>We define Dynamic 3D Printing as a class of systems that have the following properties:</p>\n<ul>\n<li>\n<p>Immediate: The system can form a physical shape in sec- onds.</p>\n</li>\n<li>\n<p>Reconstructable: Rendered shapes can be disassembled and reconstructed by hand or with the system, and the blocks are reusable.</p>\n</li>\n<li>\n<p>Arbitrary Shapes: It can create arbitrary three dimensional shapes.</p>\n</li>\n<li>\n<p>Graspable: The output shapes and structure are graspable and solid.</p>\n</li>\n</ul>\n<h1>Parallel Assembler</h1>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-3-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-3-1.png\" /></a>\n  </div>\n</div>\n<p>Dynamic 3D printing deploys a large number of small dis- crete material elements, which are assembled to form arbitrary shaped macro-scale objects. Individual elements are passive, which requires an external actuator to perform the assembly. As illustrated in the above Figure, the assembler consists of an N x N grid of motorized pins and linear actuators. The elements, which are the same size as the pins, are stacked on top of the pins (Figure 3 A). When stacked, the elements are connected in vertical direction, while discon- nected with nearby elements in horizontal direction. Similar to existing pin-based shape displays, the assembler can incrementally generate 2.5D shapes by individually moving pins to push elements to the surface.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/dynablock/webm/mechanism.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/dynablock/video/mechanism.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-4-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-4-1.png\" /></a>\n  </div>\n  <p class=\"column\">\n    This paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.\n  </p>\n</div>\n<h1>Implementation</h1>\n<p>The assembler consists of a 24 x 16 array of motor-driven pins. Each pin moves up and down, driven by a small DC motor (TTMotors TGPP06-D700) and a 3D printed lead screw (2 mm pitch, 4 starts, 120 mm in length). TGPP06-D700 is 6 mm in diameter and 29 mm in length and can rotate 47 rpm with 1:700 gear ratio. The 2 mm 4 starts lead screw can travel 12 mm per second without load, and each motor consumes approximately 60 mA. The pins are 3D printed with a nut at the bottom to travel along the lead screw. Each pin is 120 mm long and has a 7mm square cross section with a 5 mm diameter hole from top to bottom, and an N45 disk magnet ( 3mm x 2.4 mm thickness) is attached at the top. Guide grids at the top prevent pins from rotating and ensure that pins travel vertically. The 24 x 16 guide grids have 7.5 mm square holes with 10.16 mm pitch and are cut from a 5 mm acrylic plate. We fabricated the pins, the lead screws, and blocks with an inkjet 3D printer (Keyence Agilista 3200) with water soluble support material. In total, we fabricated 384 (= 24 x 16) pins and lead screws, and 3,072 (= 24 x 16 x 8 layers) blocks. To create the magnetic blocks, we embedded spherical magnets in each block by hand and inserted disk magnets using a bench vice.</p>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-5-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-5-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-5-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-5-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-5-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-5-3.png\" /></a>\n  </div>\n</div>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-7-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-7-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-7-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-7-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-7-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-7-3.jpg\" /></a>\n  </div>\n</div>\n<h1>Future Vision</h1>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/dynablock/webm/claytronics.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/dynablock/video/claytronics.mp4\" type=\"video/mp4\"></source>\n</video>\n[Video Credit: Carnegie Mellon University, Claytronics Vision]\n<br/>\n<p>With these capabilities, a 3D printer would become an inter- active medium, rather than merely a fabrication device. For example, such a 3D printer could be used in a Virtual Real- ity or Augmented Reality application to dynamically form a tangible object or controller to provide haptic feedback and engage users physically. For children, it could dynamically form a physical educational manipulative, such as a molec- ular or architectural model, to learn and explore topics, for example in a science museum. Designers could use it to ren- der a physical product to present to clients and interactively change the products design through direct manipulation. In this vision, Dynamic 3D printing is an environment in which the user thinks, designs, explores, and communicates through dynamic and interactive physical representation.</p>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/dynablock/figure-8-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/dynablock/figure-8-1.png\" /></a>\n  </div>\n</div>\n<p>Dynamic 3D printing would enable a new design workflow for digital fabrication. One notable advantage of dynamic 3D printing is the capability of connecting and disconnecting building blocks through direct manipulation. The user can also define variables or abstract attributes for parametric design through direct and gestural interaction. By leveraging this capability, the user could interactively design and fabri- cate in a physical space, similar to the man-machine dialogue proposed by Frazer et al. and later tangible CAD interfaces.</p>\n","dir":"content/output/projects","base":"dynablock.json","ext":".json","sourceBase":"dynablock.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/electro-voxel.json":
/*!****************************************************!*\
  !*** ./content/output/projects/electro-voxel.json ***!
  \****************************************************/
/*! exports provided: id, name, description, title, authors, year, conference, external, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"electro-voxel","name":"ElectroVoxel","description":"Electromagnetically Actuated Pivoting for Scalable Modular Self-Reconfigurable Robots","title":"ElectroVoxel: Electromagnetically Actuated Pivoting for Scalable Modular Self-Reconfigurable Robots","authors":["Martin Nisser","Leon Cheng","Yashaswini Makaram","Ryo Suzuki","Stefanie Mueller"],"year":2022,"conference":{"name":"ICRA 2022","fullname":"International Conference on Robotics and Automation (ICRA 2022)","url":"https://www.icra2022.org/"},"external":"https://hcie.csail.mit.edu/research/Electrovoxel/electrovoxel.html","bodyContent":"","bodyHtml":"","dir":"content/output/projects","base":"electro-voxel.json","ext":".json","sourceBase":"electro-voxel.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/expandable-robots.json":
/*!********************************************************!*\
  !*** ./content/output/projects/expandable-robots.json ***!
  \********************************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, doi, conference, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"expandable-robots","name":"HRI for Expandable Robots","description":"Designing Expandable-Structure Robots for Human-Robot Interaction","title":"Designing Expandable-Structure Robots for Human-Robot Interaction","authors":["Hooman Hedayati","Ryo Suzuki","Wyatt Rees","Daniel Leithinger","Daniel Szafir"],"year":2022,"booktitle":"Frontiers in Robotics and AI","publisher":"ACM, New York, NY, USA","doi":"https://doi.org/10.3389/frobt.2022.719639","conference":{"name":"Frontiers 2022","fullname":"Frontiers in Robotics and AI (Frontiers 2022)","url":"https://www.frontiersin.org/journals/robotics-and-ai"},"bodyContent":"","bodyHtml":"","dir":"content/output/projects","base":"expandable-robots.json","ext":".json","sourceBase":"expandable-robots.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/flux-marker.json":
/*!**************************************************!*\
  !*** ./content/output/projects/flux-marker.json ***!
  \**************************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, pages, doi, conference, pdf, video, embed, slide, acm-dl, arxiv, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"flux-marker","name":"FluxMarker","description":"Enhancing Tactile Graphics with Dynamic Tactile Markers for Blind People","title":"FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers","authors":["Ryo Suzuki","Abigale Stangl","Mark D. Gross","Tom Yeh"],"year":2017,"booktitle":"In Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '17)","publisher":"ACM, New York, NY, USA","pages":"190-199","doi":"https://doi.org/10.1145/3132525.3132548","conference":{"name":"ASSETS 2017","fullname":"The International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS 2017)","url":"https://assets17.sigaccess.org/"},"pdf":"assets-2017-fluxmarker.pdf","video":"https://www.youtube.com/watch?v=VbwIZ9V6i_g","embed":"https://www.youtube.com/embed/VbwIZ9V6i_g","slide":"assets-2017-fluxmarker-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3132548","arxiv":"https://arxiv.org/abs/1708.03783","pageCount":10,"slideCount":53,"bodyContent":"# Abstract\n\nFor people with visual impairments, tactile graphics are an impor- tant means to learn and explore information. However, raised line tactile graphics created with traditional materials such as emboss- ing are static. While available refreshable displays can dynamically change the content, they are still too expensive for many users, and are limited in size. These factors limit wide-spread adoption and the representation of large graphics or data sets. In this paper, we present FluxMaker, an inexpensive scalable system that renders dy- namic information on top of static tactile graphics with movable tactile markers. These dynamic tactile markers can be easily re- configured and used to annotate static raised line tactile graphics, including maps, graphs, and diagrams. We developed a hardware prototype that actuates magnetic tactile markers driven by low-cost and scalable electromagnetic coil arrays, which can be fabricated with standard printed circuit board manufacturing. We evaluate our prototype with six participants with visual impairments and found positive results across four application areas: location finding or navigating on tactile maps, data analysis, and physicalization, fea- ture identification for tactile graphics, and drawing support. The user study confirms advantages in application domains such as ed- ucation and data exploration.","bodyHtml":"<h1>Abstract</h1>\n<p>For people with visual impairments, tactile graphics are an impor- tant means to learn and explore information. However, raised line tactile graphics created with traditional materials such as emboss- ing are static. While available refreshable displays can dynamically change the content, they are still too expensive for many users, and are limited in size. These factors limit wide-spread adoption and the representation of large graphics or data sets. In this paper, we present FluxMaker, an inexpensive scalable system that renders dy- namic information on top of static tactile graphics with movable tactile markers. These dynamic tactile markers can be easily re- configured and used to annotate static raised line tactile graphics, including maps, graphs, and diagrams. We developed a hardware prototype that actuates magnetic tactile markers driven by low-cost and scalable electromagnetic coil arrays, which can be fabricated with standard printed circuit board manufacturing. We evaluate our prototype with six participants with visual impairments and found positive results across four application areas: location finding or navigating on tactile maps, data analysis, and physicalization, fea- ture identification for tactile graphics, and drawing support. The user study confirms advantages in application domains such as ed- ucation and data exploration.</p>\n","dir":"content/output/projects","base":"flux-marker.json","ext":".json","sourceBase":"flux-marker.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/hapticbots.json":
/*!*************************************************!*\
  !*** ./content/output/projects/hapticbots.json ***!
  \*************************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, pages, doi, conference, pdf, slide, acm-dl, video, short-video, embed, github, arxiv, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"hapticbots","name":"HapticBots","description":"Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots","title":"HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots","authors":["Ryo Suzuki","Eyal Ofek","Mike Sinclair","Daniel Leithinger","Mar Gonzalez-Franco"],"year":2021,"booktitle":"In Proceedings of the 34th Annual ACM Symposium on User Interface Software and Technology (UIST '21)","publisher":"ACM, New York, NY, USA","pages":"1-13","doi":"https://doi.org/10.1145/3472749.3474821","conference":{"name":"UIST 2021","fullname":"The ACM Symposium on User Interface Software and Technology (UIST 2021)","url":"http://uist.acm.org/uist2021"},"pdf":"uist-2021-hapticbots.pdf","slide":"uist-2021-hapticbots-slide.pdf","acm-dl":"https://doi.org/10.1145/3472749.3474821","video":"https://www.youtube.com/watch?v=s5NVJMYhfjk","short-video":"https://www.youtube.com/watch?v=HTiZgOESJyQ","embed":"https://www.youtube.com/embed/s5NVJMYhfjk","github":"https://github.com/ryosuzuki/hapticbots","arxiv":"https://arxiv.org/abs/2108.10829","pageCount":13,"slideCount":21,"bodyContent":"# Abstract\n\nHapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic approaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability---these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in- time touch-points on the users hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various ap- plications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.\n\n<video poster=\"/static/projects/hapticbots/video-poster/top.jpg\" preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/hapticbots/video/top.mp4\" type=\"video/mp4\"></source>\n</video>\n\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-1.jpg\" /></a>\n  </div>\n</div>\n\n\n# Introduction\n\nEfective haptic feedback promises to enrich Virtual Reality (VR) experiences in many application domains [17], but supporting general-purpose haptic feedback is still a difcult challenge. A common approach to providing haptic feedback for VR is to use a hand-held or wearable device. However, these wearable hand- grounded devices are inherently limited in their ability to render a world grounded force, such as surfaces that can be touched or pushed with the users hand.\n\nTo fll this gap, **encountered-type haptics** are introduced as an alternative approach. In contrast to hand-held or wearable devices, the encountered-type haptics provide haptic sensations through actuated physical environments by dynamically moving physical objects or transforming the physical shape when the user encounters the virtual object.\n\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-2-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-2-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-2-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-2-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-2-4.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-2-4.jpg\" /></a>\n  </div>\n</div>\n\nDiferent approaches have been developed for encountered-type haptics: from grounded robotic arms (e.g., Snake Charmer, VR- Robot) to shape displays (e.g., shapeShift, Feelex, inForce). However, the current approaches still face a number of challenges and limitations. For example, shape displays often require large, heavy, and mechanically complex devices, reducing reliability and deployability of the system for use outside research labs. Also, the resolution fdelity and the displays size are still limited, making it difcult to render smooth and continuous surfaces across a large interaction area. Alternately, robotic arms can bring a small piece of a surface to meet the user hand on demand, but the speed at which humans move challenges the ability to cover just in time large interaction spaces with a single device. Scaling the number of robotic arms is also a challenge as complex 3D path planning is required to avoid unnecessary collision with both the user and the other arms.\n\n\n# Distributed Encountered-type Haptics\n\n\n<video poster=\"/static/projects/hapticbots/video-poster/surface.jpg\" preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/hapticbots/video/surface.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-3-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-3-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-3-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-3-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-3-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-3-3.jpg\" /></a>\n  </div>\n</div>\n\n\n\nThis paper introduces a novel encountered-type haptics approach, which we call **distributed encountered-type haptics**. Distributed encountered-type haptics employ multiple shape-changing mobile robots to simulate a consistent physical object that the user can encounter through hands or fngers. By synchronously controlling multiple robots, these robots can approximate diferent objects and surfaces distributed in a large interaction area.\n\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-4-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-4-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-4-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-4-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-4-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-4-3.jpg\" /></a>\n  </div>\n</div>\n\n\n\nOur proposed approach enables deployable, scalable, and general- purpose encountered-type haptics for VR, providing a number of advantages compared to the existing approaches, including shape displays, robotic arms, and non-transformable mobile robots.\n\n1. **Deployable**: Each mobile robot is light and compact, making the system portable and easy to deploy.\n\n2. **Scalable**: Since each robot is simple and modular, it can scale to increase the number of touch-points and covered area. Moreover, the use of multiple robots can reduce the average distance that a robot needs to travel, which reduces the robots speed requirements.\n\n3. **General-purpose**: Finally, the shape-changing capability of each robot can signifcantly increase the expressive- ness of haptic rendering by transforming itself to closely match with the virtual object on-demand and in real-time. This allows for greater fexibility needed for general-purpose applications.\n\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-6.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-6.jpg\" /></a>\n  </div>\n</div>\n\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-5-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-5-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-5-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-5-2.jpg\" /></a>\n  </div>\n</div>\n\n\n\n# HapticBots\n\nTo demonstrate this idea, we built HapticBots, an open source 1 tabletop shape- changing mobile robots that are specifcally designed for distributed encountered-type haptics. HapticBots consists of of-the-shelf mo- bile robots (Sony TOIO), and custom height-changing mechanisms to haptically render general large surfaces with varying normal directions (-60 to 60 degrees). It can cover a large space (55 cm  55 cm) above the table (a dynamic range of 24 cm elevation) at high speed (24 cm/sec and 2.8 cm/sec for horizontal and vertical speed, respectively). Each robot is compact (4.7  4.7  8 cm, 135 g) and its tracking system consists of an expandable, pattern-printed paper mat; thus, it is portable and deployable.\n\nOur HapticBots hardware design is inspired by ShapeBots, but as far as we know, our system is the frst exploration of using multiple tabletop shape-changing robots for VR haptics. Apply- ing to VR haptics introduces a set of challenging requirements, which led to a new distributed haptics system design as well as to new hardware for each of the robots: 1) Efcient path planning integrated with real-time hand tracking: The system coordinates the movements of all robots with the users hand. We track and anticipate potential touch points at a high frame rate (60 FPS) and guide the robots to encounter the users hands in a just in time fashion. 2) Precise height and tilt control: In contrast to ShapeBots open-loop system, HapticBots enables more precise height and tilt control with embedded encoders and closed-loop control system to render surfaces with varying normal angles. 3) Actuator robust- ness: We vastly improved actuator force by around 70x (21.8N vs. 0.3N holding force of ShapeBots) to provide meaningful force feedback. In addition to these technical contributions, we developed various VR applications to demonstrate the new possibilities for encoun- tered haptics, including remote collaboration, medical training, 3D modeling, and entertainment.\n\n\n<video poster=\"/static/projects/hapticbots/video-poster/robot.jpg\" preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/hapticbots/video/robot.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-7-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-7-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-7-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-7-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-7-4.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-7-4.jpg\" /></a>\n  </div>\n</div>\n\n\nOur HapticBots linear actuator is designed to achieve all of these requirements with reasonable force capabilities. Figure 5 illustrates the mechanical design of a linear actuator. In our design, the two retractable metal tapes on motorized reels occupy a small footprint but extend and hold their shape while resisting modest loads in certain directions. Our reel-based linear actuator uses compact DC motors (Pololu 1000:1 Micro Metal Gearmotor HP 6V, Product No. 2373). This motor has a cross-section of 1.0  1.2 cm and its length is 2.9 cm. The no-load speed of the geared motor is 31 rpm, which extends the metal tape at 2.8 cm/sec. The motors maximum stall torque is 12 kgcm. We accommodate two motors placed side by side to minimize the overall footprint size.\n\nFor the reel, we use an of-the-shelf metal tape measure reel (Crescent Lufkin CS8506 1/2 x 6 inch metal tape measure). The material choice of this reel is one of the key design considerations as it determines the vertical load-bearing capability. On the other hand, a strong material makes it more difcult for this small DC motor to successfully rotate and rotate the reel. After the test of eight diferent tape measures devices with various materials, stifnesses, thicknesses, and widths, we determined the Crescent Lufkin CS8506 tape measure to work most reliably in our setting. The tape has 0.15 mm thickness and is 1.2 cm (1/2 inch) width wide, and slightly curved to avoid buckling. We cut this tape measure to 36 cm and drilled a 3 mm hole at the end to fx it to the shaft with an M3 screw.\n\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-11-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-11-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-11-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-11-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-11-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-11-3.jpg\" /></a>\n  </div>\n</div>\n\n<video poster=\"/static/projects/hapticbots/video-poster/surface-3.jpg\" preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/hapticbots/video/surface-3.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-12-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-12-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-12-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-12-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-12-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-12-3.jpg\" /></a>\n  </div>\n</div>\n\nOur system consists of multiple coordinated height-changing robots and the associated VR software. Each robot is made of 1) custom-built shape-changing mechanisms with reel-based actuators, and 2) an off-the-shelf mobile robot (Sony Toio) that can move on a mat printed with a pattern for position tracking. For the VR system, we used Oculus Quest HMD and its hand tracking capability for interaction. The software system synchronizes virtual scenes with physical environment (e.g., each robots position, orientation, and height), so that the robots can provide a haptic sensation in a timely manner. This section describes the design and implementation of the both hardware and software systems, then provides technical evaluation of HapticBots prototype.\n\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-9-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-9-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-9-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-9-2.jpg\" /></a>\n  </div>\n</div>\n\nFor the virtual reality environment and gestural tracking, we use an Oculus Quest HMD with its built- in hand tracking mechanism. The main computer (MacBook Pro 16 inch, Core i9, 32GB RAM) runs as the Node.js server. The server controls and communicates with the linear actu- ators and the Unity application on an Oculus Quest through Wi-Fi (with UDP and Websocket protocol, respectively) and Toio robots through Bluetooth. The host computer communicates with seven Toio robots through Bluetooth V4.2 (Bluetooth Low Energy). The HapticBots computer operates at 60 FPS for tracking and control.\n\nWe use the Unity game engine to render a virtual environment. As each robot moves along the planar surface, it constantly changes its height and orien- tation to best ft the virtual surface above it. To obtain the target height and surface normal of the robot, the system uses a vertical ray casting to measure the height of the virtual contact point given its position on the desk.\n\n\n<video poster=\"/static/projects/hapticbots/video-poster/control.jpg\" preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/hapticbots/video/control.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-8-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-8-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-8-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-8-2.jpg\" /></a>\n  </div>\n</div>\n\n\n\n# Applications\n\nVR is an accessible way to create realistic training setups to improve skills or prepare for complex situations before they happen in real life. With its fast encounter-type approach, users of HapticBots can train their muscle memory to learn where diferent physical elements such as the interface of a fight cockpit are located (Figure 2). HapticBots can simulate continuous surfaces, and the robots can follow the users fngers as they move and even elevate them during palpation diagnostics. These features could be relevant for medical education and surgery training.\n\n\n<video poster=\"/static/projects/hapticbots/video-poster/applications.jpg\" preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/hapticbots/video/applications.mp4\" type=\"video/mp4\"></source>\n</video>\n\n\n\n### Design and 3D Modeling\nIn addition to its continuous shape rendering capabilities, the design of HapticBots being based on dual actuators makes the system robust to lateral bending and provides the ability to control diferent tilts to render topography of a terrain surface. This enables activities like map and city exploration or terrain simulation, which can be necessary for architectural design or virtual scene/object modeling\n\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-14-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-14-1.jpg\" /></a>\n  </div>\n</div>\n\n### Remote Collaboration\nTangible interfaces can enrich remote collaboration through shared synchronized physical objects [7]. Using two connected HapticBots setups, we can reproduce remote physical objects, or introduce shared virtual objects. Figure 17 shows an example of a chess game application where the user moves the chess fgures phys- ically through robots. As a user is replacing an opponent piece from the board, she can feel the robots at the correct place on the board. This interaction could extend to multiple end points to create shared, distributed multi-user spaces.\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-14-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-14-2.jpg\" /></a>\n  </div>\n</div>\n\n### Gaming and Entertainment\nWorld-building games like Minecraft often rely on players con- structing terrains and objects. However the lack of haptics distracts from the immersive experience. HapticBots can augment the game experience during construction or game play in these VR games. Apart from the previously mentioned interactions to grab, push, and encounter, multiple robots can act in coordinated ways to simulate larger objects. They can also provide proxy objects that interact with additional props and game controllers, such as an axe in Minecraft\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-14-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-14-3.jpg\" /></a>\n  </div>\n</div>","bodyHtml":"<h1>Abstract</h1>\n<p>HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic approaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability---these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in- time touch-points on the users hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various ap- plications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.</p>\n<video poster=\"/static/projects/hapticbots/video-poster/top.jpg\" preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/hapticbots/video/top.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-1.jpg\" /></a>\n  </div>\n</div>\n<h1>Introduction</h1>\n<p>Efective haptic feedback promises to enrich Virtual Reality (VR) experiences in many application domains [17], but supporting general-purpose haptic feedback is still a difcult challenge. A common approach to providing haptic feedback for VR is to use a hand-held or wearable device. However, these wearable hand- grounded devices are inherently limited in their ability to render a world grounded force, such as surfaces that can be touched or pushed with the users hand.</p>\n<p>To fll this gap, <strong>encountered-type haptics</strong> are introduced as an alternative approach. In contrast to hand-held or wearable devices, the encountered-type haptics provide haptic sensations through actuated physical environments by dynamically moving physical objects or transforming the physical shape when the user encounters the virtual object.</p>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-2-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-2-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-2-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-2-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-2-4.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-2-4.jpg\" /></a>\n  </div>\n</div>\n<p>Diferent approaches have been developed for encountered-type haptics: from grounded robotic arms (e.g., Snake Charmer, VR- Robot) to shape displays (e.g., shapeShift, Feelex, inForce). However, the current approaches still face a number of challenges and limitations. For example, shape displays often require large, heavy, and mechanically complex devices, reducing reliability and deployability of the system for use outside research labs. Also, the resolution fdelity and the displays size are still limited, making it difcult to render smooth and continuous surfaces across a large interaction area. Alternately, robotic arms can bring a small piece of a surface to meet the user hand on demand, but the speed at which humans move challenges the ability to cover just in time large interaction spaces with a single device. Scaling the number of robotic arms is also a challenge as complex 3D path planning is required to avoid unnecessary collision with both the user and the other arms.</p>\n<h1>Distributed Encountered-type Haptics</h1>\n<video poster=\"/static/projects/hapticbots/video-poster/surface.jpg\" preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/hapticbots/video/surface.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-3-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-3-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-3-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-3-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-3-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-3-3.jpg\" /></a>\n  </div>\n</div>\n<p>This paper introduces a novel encountered-type haptics approach, which we call <strong>distributed encountered-type haptics</strong>. Distributed encountered-type haptics employ multiple shape-changing mobile robots to simulate a consistent physical object that the user can encounter through hands or fngers. By synchronously controlling multiple robots, these robots can approximate diferent objects and surfaces distributed in a large interaction area.</p>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-4-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-4-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-4-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-4-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-4-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-4-3.jpg\" /></a>\n  </div>\n</div>\n<p>Our proposed approach enables deployable, scalable, and general- purpose encountered-type haptics for VR, providing a number of advantages compared to the existing approaches, including shape displays, robotic arms, and non-transformable mobile robots.</p>\n<ol>\n<li>\n<p><strong>Deployable</strong>: Each mobile robot is light and compact, making the system portable and easy to deploy.</p>\n</li>\n<li>\n<p><strong>Scalable</strong>: Since each robot is simple and modular, it can scale to increase the number of touch-points and covered area. Moreover, the use of multiple robots can reduce the average distance that a robot needs to travel, which reduces the robots speed requirements.</p>\n</li>\n<li>\n<p><strong>General-purpose</strong>: Finally, the shape-changing capability of each robot can signifcantly increase the expressive- ness of haptic rendering by transforming itself to closely match with the virtual object on-demand and in real-time. This allows for greater fexibility needed for general-purpose applications.</p>\n</li>\n</ol>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-6.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-6.jpg\" /></a>\n  </div>\n</div>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-5-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-5-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-5-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-5-2.jpg\" /></a>\n  </div>\n</div>\n<h1>HapticBots</h1>\n<p>To demonstrate this idea, we built HapticBots, an open source 1 tabletop shape- changing mobile robots that are specifcally designed for distributed encountered-type haptics. HapticBots consists of of-the-shelf mo- bile robots (Sony TOIO), and custom height-changing mechanisms to haptically render general large surfaces with varying normal directions (-60 to 60 degrees). It can cover a large space (55 cm  55 cm) above the table (a dynamic range of 24 cm elevation) at high speed (24 cm/sec and 2.8 cm/sec for horizontal and vertical speed, respectively). Each robot is compact (4.7  4.7  8 cm, 135 g) and its tracking system consists of an expandable, pattern-printed paper mat; thus, it is portable and deployable.</p>\n<p>Our HapticBots hardware design is inspired by ShapeBots, but as far as we know, our system is the frst exploration of using multiple tabletop shape-changing robots for VR haptics. Apply- ing to VR haptics introduces a set of challenging requirements, which led to a new distributed haptics system design as well as to new hardware for each of the robots: 1) Efcient path planning integrated with real-time hand tracking: The system coordinates the movements of all robots with the users hand. We track and anticipate potential touch points at a high frame rate (60 FPS) and guide the robots to encounter the users hands in a just in time fashion. 2) Precise height and tilt control: In contrast to ShapeBots open-loop system, HapticBots enables more precise height and tilt control with embedded encoders and closed-loop control system to render surfaces with varying normal angles. 3) Actuator robust- ness: We vastly improved actuator force by around 70x (21.8N vs. 0.3N holding force of ShapeBots) to provide meaningful force feedback. In addition to these technical contributions, we developed various VR applications to demonstrate the new possibilities for encoun- tered haptics, including remote collaboration, medical training, 3D modeling, and entertainment.</p>\n<video poster=\"/static/projects/hapticbots/video-poster/robot.jpg\" preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/hapticbots/video/robot.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-7-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-7-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-7-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-7-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-7-4.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-7-4.jpg\" /></a>\n  </div>\n</div>\n<p>Our HapticBots linear actuator is designed to achieve all of these requirements with reasonable force capabilities. Figure 5 illustrates the mechanical design of a linear actuator. In our design, the two retractable metal tapes on motorized reels occupy a small footprint but extend and hold their shape while resisting modest loads in certain directions. Our reel-based linear actuator uses compact DC motors (Pololu 1000:1 Micro Metal Gearmotor HP 6V, Product No. 2373). This motor has a cross-section of 1.0  1.2 cm and its length is 2.9 cm. The no-load speed of the geared motor is 31 rpm, which extends the metal tape at 2.8 cm/sec. The motors maximum stall torque is 12 kgcm. We accommodate two motors placed side by side to minimize the overall footprint size.</p>\n<p>For the reel, we use an of-the-shelf metal tape measure reel (Crescent Lufkin CS8506 1/2 x 6 inch metal tape measure). The material choice of this reel is one of the key design considerations as it determines the vertical load-bearing capability. On the other hand, a strong material makes it more difcult for this small DC motor to successfully rotate and rotate the reel. After the test of eight diferent tape measures devices with various materials, stifnesses, thicknesses, and widths, we determined the Crescent Lufkin CS8506 tape measure to work most reliably in our setting. The tape has 0.15 mm thickness and is 1.2 cm (1/2 inch) width wide, and slightly curved to avoid buckling. We cut this tape measure to 36 cm and drilled a 3 mm hole at the end to fx it to the shaft with an M3 screw.</p>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-11-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-11-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-11-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-11-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-11-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-11-3.jpg\" /></a>\n  </div>\n</div>\n<video poster=\"/static/projects/hapticbots/video-poster/surface-3.jpg\" preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/hapticbots/video/surface-3.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-12-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-12-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-12-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-12-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-12-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-12-3.jpg\" /></a>\n  </div>\n</div>\n<p>Our system consists of multiple coordinated height-changing robots and the associated VR software. Each robot is made of 1) custom-built shape-changing mechanisms with reel-based actuators, and 2) an off-the-shelf mobile robot (Sony Toio) that can move on a mat printed with a pattern for position tracking. For the VR system, we used Oculus Quest HMD and its hand tracking capability for interaction. The software system synchronizes virtual scenes with physical environment (e.g., each robots position, orientation, and height), so that the robots can provide a haptic sensation in a timely manner. This section describes the design and implementation of the both hardware and software systems, then provides technical evaluation of HapticBots prototype.</p>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-9-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-9-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-9-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-9-2.jpg\" /></a>\n  </div>\n</div>\n<p>For the virtual reality environment and gestural tracking, we use an Oculus Quest HMD with its built- in hand tracking mechanism. The main computer (MacBook Pro 16 inch, Core i9, 32GB RAM) runs as the Node.js server. The server controls and communicates with the linear actu- ators and the Unity application on an Oculus Quest through Wi-Fi (with UDP and Websocket protocol, respectively) and Toio robots through Bluetooth. The host computer communicates with seven Toio robots through Bluetooth V4.2 (Bluetooth Low Energy). The HapticBots computer operates at 60 FPS for tracking and control.</p>\n<p>We use the Unity game engine to render a virtual environment. As each robot moves along the planar surface, it constantly changes its height and orien- tation to best ft the virtual surface above it. To obtain the target height and surface normal of the robot, the system uses a vertical ray casting to measure the height of the virtual contact point given its position on the desk.</p>\n<video poster=\"/static/projects/hapticbots/video-poster/control.jpg\" preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/hapticbots/video/control.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-8-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-8-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-8-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-8-2.jpg\" /></a>\n  </div>\n</div>\n<h1>Applications</h1>\n<p>VR is an accessible way to create realistic training setups to improve skills or prepare for complex situations before they happen in real life. With its fast encounter-type approach, users of HapticBots can train their muscle memory to learn where diferent physical elements such as the interface of a fight cockpit are located (Figure 2). HapticBots can simulate continuous surfaces, and the robots can follow the users fngers as they move and even elevate them during palpation diagnostics. These features could be relevant for medical education and surgery training.</p>\n<video poster=\"/static/projects/hapticbots/video-poster/applications.jpg\" preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/hapticbots/video/applications.mp4\" type=\"video/mp4\"></source>\n</video>\n<h3>Design and 3D Modeling</h3>\n<p>In addition to its continuous shape rendering capabilities, the design of HapticBots being based on dual actuators makes the system robust to lateral bending and provides the ability to control diferent tilts to render topography of a terrain surface. This enables activities like map and city exploration or terrain simulation, which can be necessary for architectural design or virtual scene/object modeling</p>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-14-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-14-1.jpg\" /></a>\n  </div>\n</div>\n<h3>Remote Collaboration</h3>\n<p>Tangible interfaces can enrich remote collaboration through shared synchronized physical objects [7]. Using two connected HapticBots setups, we can reproduce remote physical objects, or introduce shared virtual objects. Figure 17 shows an example of a chess game application where the user moves the chess fgures phys- ically through robots. As a user is replacing an opponent piece from the board, she can feel the robots at the correct place on the board. This interaction could extend to multiple end points to create shared, distributed multi-user spaces.</p>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-14-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-14-2.jpg\" /></a>\n  </div>\n</div>\n<h3>Gaming and Entertainment</h3>\n<p>World-building games like Minecraft often rely on players con- structing terrains and objects. However the lack of haptics distracts from the immersive experience. HapticBots can augment the game experience during construction or game play in these VR games. Apart from the previously mentioned interactions to grab, push, and encounter, multiple robots can act in coordinated ways to simulate larger objects. They can also provide proxy objects that interact with additional props and game controllers, such as an axe in Minecraft</p>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/hapticbots/figure-14-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/hapticbots/figure-14-3.jpg\" /></a>\n  </div>\n</div>","dir":"content/output/projects","base":"hapticbots.json","ext":".json","sourceBase":"hapticbots.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/lift-tiles.json":
/*!*************************************************!*\
  !*** ./content/output/projects/lift-tiles.json ***!
  \*************************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, pages, doi, conference, video, embed, pdf, slide, poster, acm-dl, pageCount, slideCount, related, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"lift-tiles","name":"LiftTiles","description":"Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces","title":"LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces","authors":["Ryo Suzuki","Ryosuke Nakayama","Dan Liu","Yasuaki Kakehi","Mark D. Gross","Daniel Leithinger"],"year":2020,"booktitle":"In Proceedings of the 14th ACM International Conference on Tangible, Embedded and Embodied Interaction (TEI '20)","publisher":"ACM, New York, NY, USA","pages":"143151","doi":"https://doi.org/10.1145/3374920.3374941","conference":{"name":"TEI 2020","fullname":"The ACM International Conference on Tangible, Embedded and Embodied Interaction (TEI 2020)","url":"https://tei.acm.org/2020/"},"video":"https://www.youtube.com/watch?v=0LHeTkOMR84","embed":"https://www.youtube.com/embed/0LHeTkOMR84","pdf":"tei-2020-lift-tiles.pdf","slide":"tei-2020-lift-tiles-slide.pdf","poster":"uist-2019-lift-tiles-poster.pdf","acm-dl":"https://dl.acm.org/doi/10.1145/3374920.3374941","pageCount":9,"slideCount":37,"related":{"title":"LiftTiles: Modular and Reconfigurable Room-scale Shape Displays through Retractable Inflatable Actuators","authors":["Ryo Suzuki","Ryosuke Nakayama","Dan Liu","Yasuaki Kakehi","Mark D. Gross","Daniel Leithinger"],"year":2019,"booktitle":"In Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST '19)","publisher":"ACM, New York, NY, USA","pages":"1-3","doi":"https://doi.org/10.1145/3332167.3357105","url":"http://uist.acm.org/uist2019","pdf":"uist-2019-lift-tiles.pdf","suffix":"adjunct","pageCount":3},"bodyContent":"# Abstract\n\nLarge-scale shape-changing interfaces have great potential, but creating such systems requires substantial time, cost, space, and efforts, which hinders the research community to explore inter- actions beyond the scale of human hands. We introduce modular inflatable actuators as building blocks for prototyping room-scale shape-changing interfaces. Each actuator can change its height from 15cm to 150cm, actuated and controlled by air pressure. Each unit is low-cost (8 USD), lightweight (10 kg), compact (15 cm), and robust (e.g., can support 10 kg weight) making it well-suited for prototyping room-scale shape transformations. Moreover, our mod- ular and reconfigurable design allows researchers and designers to quickly construct different geometries and to explore various appli- cations. This paper contributes to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/lift-tiles/video/top.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/lift-tiles/video/top.mp4\" type=\"video/mp4\"></source>\n</video>\n\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-1-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-1-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-1-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-1-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-1-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-1-3.jpg\" /></a>\n  </div>\n</div>\n\n<!--\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-2-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-2-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-2-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-2-2.png\" /></a>\n  </div>\n</div>\n -->\n\n\n# LiftTiles: Room-scale Modular Inflatable Actuators\n\nThis paper introduces LiftTiles, modular inflatable actuators for prototyping room-scale shape-changing interfaces. Each inflatable actuator has a large footprint (e.g., 30 cm x 30 cm) and enables large-scale shape transformation. The ac- tuator is fabricated from a flexible plastic tube and constant force springs. It extends when inflated and retracts by the force of its spring when deflated. By controlling the internal air volume, the actuator can change its height from 15 cm to 150 cm. We designed each module as low cost (e.g., 8 USD), lightweight (e.g., 1.8kg), and robust (e.g., with- stand more than 10 kg weight), so that it is suitable for rapid prototyping of room-sized interfaces. Our design utilizes constant force springs to provide greater scalability, simplified fabrication, and stronger retraction force, all essential for large-scale shape-change.\n\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/lift-tiles/video/unit.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/lift-tiles/video/unit.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-3-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-3-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-3-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-3-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-3-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-3-3.jpg\" /></a>\n  </div>\n</div>\n\nOur inflatable actuator leverages an extendable structure similar to a party horn. Each inflatable actuator consists of a flexible plastic tube and two constant force springs, which are rolled at their resting positions (Figure 2). When pumping air into the tube, the actuator extends as the internal air pressure increases and the end of the tube unwinds. When releasing air through a release valve, the inflatable tube retracts due to the force of the embedded spring returning to its resting position.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/lift-tiles/video/unit-animation.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/lift-tiles/video/unit-animation.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-4-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-4-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-4-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-4-2.jpg\" /></a>\n  </div>\n</div>\n\nThe goal of our system is to provide an accessible prototyping building block for room-scale shape-changing interfaces. To achieve this goal, we set the following three design require- ments:\n\n- **Fast**: LIftTiles provide a means to quickly mock up a room-size shape-changing interface\n\n- **Extendable**:  range (from 15cm to 150cm),\n\n- **Low-cost** (8 USD per tile),\n\n- **Light**: (10 kg per tile),\n\n- **Compact**: (Each tile compresses to 15 cm)\n\n- **Modular**: (can reconfigure the arrangement)\n\n- **Strong**: (each tile supports up 10 kg)\n\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-5-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-5-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-5-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-5-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-5-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-5-3.jpg\" /></a>\n  </div>\n</div>\n\n<!--\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/lift-tiles/video/cad.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/lift-tiles/video/cad.mp4\" type=\"video/mp4\"></source>\n</video>\n -->\n\n# Modular Design\n\nEach actuator is modular and can connect with the air supply of neighboring actuators. Each solenoid air intake valve is connected to a T-fitting. Adjacent actua- tors are pneumatically connected with a silicon tube between the T-fittings (Figure 6). This way, an array of actuators is connected to a shared pressurized supply line.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/lift-tiles/video/modular.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/lift-tiles/video/modular.mp4\" type=\"video/mp4\"></source>\n</video>\n\n</br>\n\nAlso, due to the relatively compact size and small weight of the modules, various ar- chitectural surfaces can be considered for installation, such as placed on the floor or installed sideways from a wall. The actuators can be arranged in a grid, a line, or as individ- ual, loosely arranged units. Figure 10 depicts a few example configurations, which can also be reconfigured by the person installing them or the end user. An individual actuator can be picked up and moved, which may let the user handle it like a traditional piece of furniture, but could also open up new possibilities for interaction, such as actuators with a motorized base that reconfigures the arrangement according to the current use case.\n\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/lift-tiles/video/tile-animation.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/lift-tiles/video/tile-animation.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-6-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-6-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-6-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-6-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-6-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-6-3.jpg\" /></a>\n  </div>\n</div>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-7-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-7-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-7-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-7-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-7-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-7-3.jpg\" /></a>\n  </div>\n</div>\n\n\n# Prototyping Applications\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-10.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-10.jpg\" /></a>\n  </div>\n</div>\n\nTo gauge how well LiftTiles is suited to our goal of supporting rapid prototyping and user testing, we built several differ- ent example applications.\n\n- **Adaptive Floor Furniture**: The first application is a recon- figurable floor that creates adaptive furniture, inspired by an adaptive dynamic table (e.g., TRANSFORM [45]) and dy- namic physical affordances (e.g., inFORM [8]). We created a shape-changing floor by arranging 25 modules in a 5 x 5 grid. Each unit is individually actuated to provide a chair, a table, and a bed (Figure 1).\n\n- **Landscape Haptics for VR**: While hand-sized shape displays have rendered haptic sensations for VR [34]), we propose to render furniture-size props, walls, and game elements that users can feel through walking and touching while experi- encing immersive VR scenes. Similar to the applications pro- posed by HapticTurk [3], TurkDeck [4], and TilePop [44], In this application, the haptics increases the feeling of presence in games by providing room-scale dynamic haptic feedback. Also, this can aid the design process by rendering full-scale object mock-ups of large objects like a car.\n\n- **Shape-changing Wall**: By leveraging the blocks compact form factor, we also prototyped a horizontal shape-changing wall to adapt to user needs and display information. The wall becomes a space separator to make a temporary meet- ing space, while it can also act as public signage by displaying an arrow shape to guide a passer-by.\n\n- **Deployable Pop-Up Structure**: As the actuators are compact compared to traditional furniture and walls, they can be deployed in temporary use cases such as festivals, trade shows, concerts, and disaster relief. In these applications, the actuators are shipped in a compact box and laid out in an empty room, similar to tiles. After connecting them to a compressor and control computer, the operator selects a use case and the blocks render a layout of chairs, beds, tables, and partitions.\n\n\n<div class=\"figures ui four column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-8-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-8-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-8-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-8-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-8-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-8-3.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-8-4.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-8-4.jpg\" /></a>\n  </div>\n</div>\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/lift-tiles/video/wall-animation.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/lift-tiles/video/wall-animation.mp4\" type=\"video/mp4\"></source>\n</video>\n\n\n<div class=\"figures ui four column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-2-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-2-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-2-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-2-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-9-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-9-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-9-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-9-2.jpg\" /></a>\n  </div>\n</div>\n\n\n# Conclusion\n\nThis paper introduces constructive building blocks for proto- typing room-scale shape-changing interfaces. To this end, we designed a modular inflatable actuator that is highly extend- able (from 15cm to 150cm), low-cost (8 USD), lightweight (10 kg), compact (15 cm), and strong (e.g., can support 10 kg weight) making it well-suited for prototyping room-scale shape transformations. Moreover, our modular and recon- figurable design allows researchers and designers to quickly construct different geometries and to investigate various ap- plications. In this paper, we contributed to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block. We plan to evaluate the potential of our system by inviting designers to use our blocks in their own designs and see whether and/or how our building blocks enhances their ideation process.","bodyHtml":"<h1>Abstract</h1>\n<p>Large-scale shape-changing interfaces have great potential, but creating such systems requires substantial time, cost, space, and efforts, which hinders the research community to explore inter- actions beyond the scale of human hands. We introduce modular inflatable actuators as building blocks for prototyping room-scale shape-changing interfaces. Each actuator can change its height from 15cm to 150cm, actuated and controlled by air pressure. Each unit is low-cost (8 USD), lightweight (10 kg), compact (15 cm), and robust (e.g., can support 10 kg weight) making it well-suited for prototyping room-scale shape transformations. Moreover, our mod- ular and reconfigurable design allows researchers and designers to quickly construct different geometries and to explore various appli- cations. This paper contributes to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/lift-tiles/video/top.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/lift-tiles/video/top.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-1-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-1-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-1-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-1-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-1-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-1-3.jpg\" /></a>\n  </div>\n</div>\n<!--\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-2-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-2-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-2-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-2-2.png\" /></a>\n  </div>\n</div>\n -->\n<h1>LiftTiles: Room-scale Modular Inflatable Actuators</h1>\n<p>This paper introduces LiftTiles, modular inflatable actuators for prototyping room-scale shape-changing interfaces. Each inflatable actuator has a large footprint (e.g., 30 cm x 30 cm) and enables large-scale shape transformation. The ac- tuator is fabricated from a flexible plastic tube and constant force springs. It extends when inflated and retracts by the force of its spring when deflated. By controlling the internal air volume, the actuator can change its height from 15 cm to 150 cm. We designed each module as low cost (e.g., 8 USD), lightweight (e.g., 1.8kg), and robust (e.g., with- stand more than 10 kg weight), so that it is suitable for rapid prototyping of room-sized interfaces. Our design utilizes constant force springs to provide greater scalability, simplified fabrication, and stronger retraction force, all essential for large-scale shape-change.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/lift-tiles/video/unit.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/lift-tiles/video/unit.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-3-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-3-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-3-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-3-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-3-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-3-3.jpg\" /></a>\n  </div>\n</div>\n<p>Our inflatable actuator leverages an extendable structure similar to a party horn. Each inflatable actuator consists of a flexible plastic tube and two constant force springs, which are rolled at their resting positions (Figure 2). When pumping air into the tube, the actuator extends as the internal air pressure increases and the end of the tube unwinds. When releasing air through a release valve, the inflatable tube retracts due to the force of the embedded spring returning to its resting position.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/lift-tiles/video/unit-animation.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/lift-tiles/video/unit-animation.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-4-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-4-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-4-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-4-2.jpg\" /></a>\n  </div>\n</div>\n<p>The goal of our system is to provide an accessible prototyping building block for room-scale shape-changing interfaces. To achieve this goal, we set the following three design require- ments:</p>\n<ul>\n<li>\n<p><strong>Fast</strong>: LIftTiles provide a means to quickly mock up a room-size shape-changing interface</p>\n</li>\n<li>\n<p><strong>Extendable</strong>:  range (from 15cm to 150cm),</p>\n</li>\n<li>\n<p><strong>Low-cost</strong> (8 USD per tile),</p>\n</li>\n<li>\n<p><strong>Light</strong>: (10 kg per tile),</p>\n</li>\n<li>\n<p><strong>Compact</strong>: (Each tile compresses to 15 cm)</p>\n</li>\n<li>\n<p><strong>Modular</strong>: (can reconfigure the arrangement)</p>\n</li>\n<li>\n<p><strong>Strong</strong>: (each tile supports up 10 kg)</p>\n</li>\n</ul>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-5-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-5-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-5-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-5-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-5-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-5-3.jpg\" /></a>\n  </div>\n</div>\n<!--\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/lift-tiles/video/cad.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/lift-tiles/video/cad.mp4\" type=\"video/mp4\"></source>\n</video>\n -->\n<h1>Modular Design</h1>\n<p>Each actuator is modular and can connect with the air supply of neighboring actuators. Each solenoid air intake valve is connected to a T-fitting. Adjacent actua- tors are pneumatically connected with a silicon tube between the T-fittings (Figure 6). This way, an array of actuators is connected to a shared pressurized supply line.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/lift-tiles/video/modular.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/lift-tiles/video/modular.mp4\" type=\"video/mp4\"></source>\n</video>\n</br>\n<p>Also, due to the relatively compact size and small weight of the modules, various ar- chitectural surfaces can be considered for installation, such as placed on the floor or installed sideways from a wall. The actuators can be arranged in a grid, a line, or as individ- ual, loosely arranged units. Figure 10 depicts a few example configurations, which can also be reconfigured by the person installing them or the end user. An individual actuator can be picked up and moved, which may let the user handle it like a traditional piece of furniture, but could also open up new possibilities for interaction, such as actuators with a motorized base that reconfigures the arrangement according to the current use case.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/lift-tiles/video/tile-animation.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/lift-tiles/video/tile-animation.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-6-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-6-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-6-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-6-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-6-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-6-3.jpg\" /></a>\n  </div>\n</div>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-7-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-7-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-7-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-7-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-7-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-7-3.jpg\" /></a>\n  </div>\n</div>\n<h1>Prototyping Applications</h1>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-10.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-10.jpg\" /></a>\n  </div>\n</div>\n<p>To gauge how well LiftTiles is suited to our goal of supporting rapid prototyping and user testing, we built several differ- ent example applications.</p>\n<ul>\n<li>\n<p><strong>Adaptive Floor Furniture</strong>: The first application is a recon- figurable floor that creates adaptive furniture, inspired by an adaptive dynamic table (e.g., TRANSFORM [45]) and dy- namic physical affordances (e.g., inFORM [8]). We created a shape-changing floor by arranging 25 modules in a 5 x 5 grid. Each unit is individually actuated to provide a chair, a table, and a bed (Figure 1).</p>\n</li>\n<li>\n<p><strong>Landscape Haptics for VR</strong>: While hand-sized shape displays have rendered haptic sensations for VR [34]), we propose to render furniture-size props, walls, and game elements that users can feel through walking and touching while experi- encing immersive VR scenes. Similar to the applications pro- posed by HapticTurk [3], TurkDeck [4], and TilePop [44], In this application, the haptics increases the feeling of presence in games by providing room-scale dynamic haptic feedback. Also, this can aid the design process by rendering full-scale object mock-ups of large objects like a car.</p>\n</li>\n<li>\n<p><strong>Shape-changing Wall</strong>: By leveraging the blocks compact form factor, we also prototyped a horizontal shape-changing wall to adapt to user needs and display information. The wall becomes a space separator to make a temporary meet- ing space, while it can also act as public signage by displaying an arrow shape to guide a passer-by.</p>\n</li>\n<li>\n<p><strong>Deployable Pop-Up Structure</strong>: As the actuators are compact compared to traditional furniture and walls, they can be deployed in temporary use cases such as festivals, trade shows, concerts, and disaster relief. In these applications, the actuators are shipped in a compact box and laid out in an empty room, similar to tiles. After connecting them to a compressor and control computer, the operator selects a use case and the blocks render a layout of chairs, beds, tables, and partitions.</p>\n</li>\n</ul>\n<div class=\"figures ui four column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-8-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-8-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-8-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-8-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-8-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-8-3.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-8-4.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-8-4.jpg\" /></a>\n  </div>\n</div>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/lift-tiles/video/wall-animation.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/lift-tiles/video/wall-animation.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui four column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-2-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-2-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-2-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-2-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-9-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-9-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/lift-tiles/figure-9-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/lift-tiles/figure-9-2.jpg\" /></a>\n  </div>\n</div>\n<h1>Conclusion</h1>\n<p>This paper introduces constructive building blocks for proto- typing room-scale shape-changing interfaces. To this end, we designed a modular inflatable actuator that is highly extend- able (from 15cm to 150cm), low-cost (8 USD), lightweight (10 kg), compact (15 cm), and strong (e.g., can support 10 kg weight) making it well-suited for prototyping room-scale shape transformations. Moreover, our modular and recon- figurable design allows researchers and designers to quickly construct different geometries and to investigate various ap- plications. In this paper, we contributed to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block. We plan to evaluate the potential of our system by inviting designers to use our blocks in their own designs and see whether and/or how our building blocks enhances their ideation process.</p>\n","dir":"content/output/projects","base":"lift-tiles.json","ext":".json","sourceBase":"lift-tiles.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/mixed-initiative.json":
/*!*******************************************************!*\
  !*** ./content/output/projects/mixed-initiative.json ***!
  \*******************************************************/
/*! exports provided: id, name, description, title, authors, note, year, booktitle, publisher, pages, doi, conference, pdf, slide, acm-dl, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"mixed-initiative","name":"Mixed-Initiative Code Feedback","description":"Writing Reusable Code Feedback at Scale with Mixed-Initiative Program Synthesis","title":"Writing Reusable Code Feedback at Scale with Mixed-Initiative Program Synthesis","authors":["Andrew Head","Elena Glassman","Gustavo Soares","Ryo Suzuki","Lucas Figueredo","Loris DAntoni","Bjrn Hartmann"],"note":"(the first three authors equally contributed)","year":2017,"booktitle":"In Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale (L@S '17)","publisher":"ACM, New York, NY, USA","pages":"89-98","doi":"https://doi.org/10.1145/3051457.3051467","conference":{"name":"L@S 2017","fullname":"The ACM Conference on Learning at Scale (L@S 2017)","url":"http://learningatscale.acm.org/las2017"},"pdf":"las-2017-mixed.pdf","slide":"las-2017-mixed-slide.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=3051467","pageCount":10,"slideCount":62,"bodyContent":"","bodyHtml":"","dir":"content/output/projects","base":"mixed-initiative.json","ext":".json","sourceBase":"mixed-initiative.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/mixels.json":
/*!*********************************************!*\
  !*** ./content/output/projects/mixels.json ***!
  \*********************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, conference, external, doi, acm-dl, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"mixels","name":"Mixels","description":"Fabricating Interfaces using Programmable Magnetic Pixels","title":"Mixels: Fabricating Interfaces using Programmable Magnetic Pixels","authors":["Martin Nisser","Yashaswini Makaram","Lucian Covarrubias","Amadou Yaye Bah","Faraz Faruqi","Ryo Suzuki","Stefanie Mueller"],"year":2022,"booktitle":"In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology (UIST '22)","publisher":"ACM, New York, NY, USA","conference":{"name":"UIST 2022","fullname":"The ACM Symposium on User Interface Software and Technology (UIST 2022)","url":"http://uist.acm.org/uist2022"},"external":"https://hcie.csail.mit.edu/research/mixels/mixels.html","doi":"https://doi.org/10.1145/3526114.3558654","acm-dl":"https://dl.acm.org/doi/10.1145/3526114.3558654","bodyContent":"","bodyHtml":"","dir":"content/output/projects","base":"mixels.json","ext":".json","sourceBase":"mixels.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/morphio.json":
/*!**********************************************!*\
  !*** ./content/output/projects/morphio.json ***!
  \**********************************************/
/*! exports provided: id, name, description, title, authors, note, year, booktitle, publisher, pages, doi, conference, pdf, slide, video, embed, acm-dl, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"morphio","name":"MorphIO","description":"Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction","title":"MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction","authors":["Ryosuke Nakayama*","Ryo Suzuki*","Satoshi Nakamaru","Ryuma Niiyama","Yoshihiro Kawahara","Yasuaki Kakehi"],"note":"(the first two authors equally contributed)","year":2018,"booktitle":"In Proceedings of the 2019 on Designing Interactive Systems Conference (DIS '19)","publisher":"ACM, New York, NY, USA","pages":"975-986","doi":"https://doi.org/10.1145/3322276.3322337","conference":{"name":"DIS 2019","fullname":"The ACM conference on Designing Interactive Systems (DIS 2019) - Best Paper Award","url":"https://dis2019.com/"},"pdf":"dis-2019-morphio.pdf","slide":"dis-2019-morphio-slide.pdf","video":"https://www.youtube.com/watch?v=ZkCcazfFD-M","embed":"https://www.youtube.com/embed/ZkCcazfFD-M","acm-dl":"https://dl.acm.org/citation.cfm?id=3322337","pageCount":12,"slideCount":52,"bodyContent":"# Abstract\n\nWe introduce **MorphIO, entirely soft sensing and actuation modules** for programming by demonstration of soft robots and shape-changing interfaces. MorphIOs hardware consists of a **soft pneumatic actuator containing a conductive sponge sensor**. This allows both input and output of three-dimensional deformation of a soft material. Leveraging this capability, MorphIO enables a user to **record and later playback physical motion** of programmable shape-changing materials. In addition, the modular design of MorphIOs unit allows the user to construct various shapes and topologies through magnetic connection. We demonstrate several application scenarios, including tangible character animation, locomotion experiment of a soft robot, and prototyping tools for animated soft objects. Our user study with six participants confirms the benefits of MorphIO, as compared to the existing programming paradigm.\n\n\n<video poster=\"/static/projects/morphio/video-poster/top.png\" preload=\"metadata\" autoplay loop muted playsinline webkit-playsinline=\"\">\n  <source src=\"/static/projects/morphio/webm/top.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/morphio/video/top.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-1-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-1-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-1-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-1-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-1-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-1-3.png\" /></a>\n  </div>\n</div>\n\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-1-4.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-1-4.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-1-5.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-1-5.png\" /></a>\n  </div>\n</div>\n\n# Introduction\n\n**Programmable soft materials** have a great potential for many application domains, such as soft robotics, material interfaces, accessibility, and haptic interfaces.\n**However, programming of such materials is hard.**\nThe dominant programming paradigm of soft robots and material interfaces is largely confined within a digital screen, leaving little room for users to interactively explore physical motion through tangible interaction. In such a workflowcompiling code on a digital screen then trans- ferring it into the physical objectusers need to repeatedly switch between the digital and physical worlds. This leaves a large gulf of execution in their programming experiences.\nThus, the traditional programming paradigm significantly limits the users ability to experiment with the design of expressive motion. Moreover, due to this barrier, such an opportunity is largely limited to highly skilled programmers and researchers who are proficient in hardware programming.\n\n\n# MorphIO\n\nThis paper introduces **MorphIO, entirely soft sensing and actuation modules** for programming by demonstration of soft robots and shape-changing interfaces.\nMorphIOs hardware consists of a soft pneumatic actuator containing a conductive sponge sensor. This allows for integrated and entirely soft shape-changing modules that can both sense and actuate a variety of three-dimensional deformations. Leveraging this capability, MorphIO enables the user to program behaviors by **recording and later playing back physical motions** through tangible interaction. In addition, the modular design of MorphIOs unit allows the user to construct various shapes and topologies through magnetic connection, then **synthesize multiple recorded motions to achieve more complex behaviors**, such as bending, gripping, and walking.\n\n\n<video poster=\"/static/projects/morphio/video-poster/module.png\" preload=\"metadata\" autoplay loop muted playsinline webkit-playsinline=\"\">\n  <source src=\"/static/projects/morphio/webm/module.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/morphio/video/module.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<br/>\n\n<video poster=\"/static/projects/morphio/video-poster/bear.png\" preload=\"metadata\" autoplay loop muted playsinline webkit-playsinline=\"\">\n  <source src=\"/static/projects/morphio/webm/bear.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/morphio/video/bear.mp4\" type=\"video/mp4\"></source>\n</video>\n\n\n# System Overview\n\nThe programming workflow with MorphIO is the following:\n\n- **Step 1:** A user starts manipulating the MorphIO unit.\n\n- **Step 2:** The demonstrated motion is detected and recorded through internal sensors, and the recorded sensor values are stored in the software.\n\n- **Step 3:** Once the user clicks play in the graphical user interface, the pneumatic pump starts supplying air.\n\n- **Step 4:** By controlling the air flow through switching on and off the solenoid valves, the system can control the behavior of the pneumatic actuator as it plays back the recorded motion.\n\nThe MorphIO system consists of the following components: A sensor and actuation unit, a sensing and actuation control unit, a microcontroller, software to control these units, and a visual interface for users to control behaviors. Figure illustrates the overview architecture of MorphIO.\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-6-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-6-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-6-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-6-2.png\" /></a>\n  </div>\n</div>\n\n\n# Entirely Soft Sensing and Actuation Modules\n\nOur main contribution is a design and fabrication method for **a conductive sponge sensor** that can be embedded into an air chamber in the pneumatic actuator. The conductive sponge sensor leverages the porous structure to **sense the three-dimensional deformation by measuring the internal resistance value**; when contracted, the resistance value be- tween the top and bottom surfaces drops, and when extended, it increases. In contrast to existing sensing techniques, an elastic sponge allows for a higher degree of freedom in sensing capability (e.g., stretching, bending, and compression) without sacrificing the softness of the interface.\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-2-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-2-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-2-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-2-2.png\" /></a>\n  </div>\n</div>\n\n\n<video poster=\"/static/projects/morphio/video-poster/mechanism.png\" preload=\"metadata\" autoplay loop muted playsinline webkit-playsinline=\"\">\n  <source src=\"/static/projects/morphio/webm/mechanism.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/morphio/video/mechanism.mp4\" type=\"video/mp4\"></source>\n</video>\n\n\n<br />\n\nMoreover, our **modular design** and **graphical interface** allows for easy experiments involving multiple units. For example, the system can visualize multiple recorded sensor values, so that the user can see, customize, and synthesize recorded motion to construct more complex behaviors. These hardware and software designs were informed by our formative study, wherein we interviewed five experienced researchers from the robotics and HCI communities.\n\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-3-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-3-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-3-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-3-2.png\" /></a>\n  </div>\n</div>\n\n<video poster=\"/static/projects/morphio/video-poster/unit-x2.png\" preload=\"metadata\" autoplay loop muted playsinline webkit-playsinline=\"\">\n  <source src=\"/static/projects/morphio/webm/unit-x2.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/morphio/video/unit-x2.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<video poster=\"/static/projects/morphio/video-poster/unit-x3.png\" preload=\"metadata\" autoplay loop muted playsinline webkit-playsinline=\"\">\n  <source src=\"/static/projects/morphio/webm/unit-x3.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/morphio/video/unit-x3.mp4\" type=\"video/mp4\"></source>\n</video>\n\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-4-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-4-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-4-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-4-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-4-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-4-3.png\" /></a>\n  </div>\n</div>\n\n\n\n\n# Fabrication Process\n\nThe fabrication process follows three steps: 1) Fabricate an elastic sponge, 2) impregnate into conductive ink, and 3) attach electrodes and wires.\nTo fabricate an elastic sponge, we first prepare 6.0 g of elastomer prepolymer solution and 29.1 g of sodium-chloride, then mix them together by using a planetary centrifugal mixer. The mixed solution is injected into a 3D printed cylindrical mold (16mm diameter, 40mm height). Then we dry the material with an oven at 100 C degrees for one hour. Once dried, we immerse the sponge in water, so that the sodium chloride can melt, leaving a porous structure within the elastomer sponge.\n\n<video poster=\"/static/projects/morphio/video-poster/fabrication.png\" preload=\"metadata\" autoplay loop muted playsinline webkit-playsinline=\"\">\n  <source src=\"/static/projects/morphio/webm/fabrication.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/morphio/video/fabrication.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui four column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-5-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-5-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-5-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-5-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-5-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-5-3.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-5-4.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-5-4.png\" /></a>\n  </div>\n</div>\n\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-5-5.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-5-5.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-5-6.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-5-6.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-5-7.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-5-7.png\" /></a>\n  </div>\n</div>\n\n\n\n# Applications\n\nWe demonstrate several possible applications scenarios with MorphIO. 1) Tangible character animation, 2) Animating existing soft objects, 3) Remote manipulation of soft grippers, 4) Locomotion experimentation with soft robots.\n\n<div class=\"figures ui four column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-10-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-10-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-10-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-10-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-10-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-10-3.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-10-4.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-10-4.png\" /></a>\n  </div>\n</div>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-11-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-11-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-11-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-11-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-11-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-11-3.png\" /></a>\n  </div>\n</div>\n\n<video poster=\"/static/projects/morphio/video-poster/locomotion.png\" preload=\"metadata\" autoplay loop muted playsinline webkit-playsinline=\"\">\n  <source src=\"/static/projects/morphio/webm/locomotion.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/morphio/video/locomotion.mp4\" type=\"video/mp4\"></source>\n</video>\n\n\n# Evaluation\n\nWe conducted a user evaluation study to understand the bene- fits and limitations of MorphIO. In this study, we focused on answering the following research questions:\n\n- **RQ1:** Does MorphIO save time and reduce the number of iterations to program the target behavior, compared to the existing approach?\n\n- **RQ2:** Does MorphIO increase the expressiveness of the physical motion?\n\nTo answer these questions, we conducted a controlled experiment where we compared MorphIO (left) with the current programming approach. We chose Arduino IDE (right) as a base condition for the comparison, as this is the most common programming approach identified through our formative study. We provide three basic tasks to construct a program. For each task, the participants were asked to program three differ- ent emotionshappiness, anger, and sadnessof an animated character. We chose these emotions based on Ekmans basic emotions for communication.\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-12-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-12-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-12-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-12-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-12-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-12-3.png\" /></a>\n  </div>\n</div>\n\nThe average time of task completion time of MorphIO was 2m 19s, compared to 5m 21s with the control condition. The average number of iterations of MorphIO was 4.4 times, compared to 6.4 with the control condition, which confirms that MorphIO is significantly efficient in terms of task completion time and the number of iterations. When asked about the achievement of the expressions using a 9-point Likert scale, the average score with MorphIO was 6.5, compared to 6.3 with the control condition. We did not find differences between the two conditions. Thus, we conclude the result of our study as follows: **RQ1: Yes, RQ2: No**.\n\nBased on our post interviews, we discuss the benefits and limitations of our approach: **1) tangible interactions are suitable for sculpting rough motion**, **2) programming allows for fine-tuning more precise adjustments**. Thus, for future research, systems might allow users to quickly make a rough motion, which can automatically be converted into digital parameters so that the user can also precisely control and adjust the motion. The same human- computer cooperation approach can be applied to other design domains: For example, when designing an object, the user can quickly make rough shapes with clay, while letting a machine finish the details. We believe this insight can lead the HCI community to further explore design approaches wherein users and machines cooperate for enhanced interaction design.\n\n\n# Future Vision\nWe believe this approachs potential for lowering the barrier and opening new opportunities for a larger community to begin designing, prototyping, and exploring soft material motionnot by coding on a screen, but by sculpting behaviors in the physical world.\nWe envision the future where people can interactively explore various behaviors through tangible interactions, **just like sculpting behaviors with clay**.\n\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-14.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-14.png\" /></a>\n  </div>\n</div>","bodyHtml":"<h1>Abstract</h1>\n<p>We introduce <strong>MorphIO, entirely soft sensing and actuation modules</strong> for programming by demonstration of soft robots and shape-changing interfaces. MorphIOs hardware consists of a <strong>soft pneumatic actuator containing a conductive sponge sensor</strong>. This allows both input and output of three-dimensional deformation of a soft material. Leveraging this capability, MorphIO enables a user to <strong>record and later playback physical motion</strong> of programmable shape-changing materials. In addition, the modular design of MorphIOs unit allows the user to construct various shapes and topologies through magnetic connection. We demonstrate several application scenarios, including tangible character animation, locomotion experiment of a soft robot, and prototyping tools for animated soft objects. Our user study with six participants confirms the benefits of MorphIO, as compared to the existing programming paradigm.</p>\n<video poster=\"/static/projects/morphio/video-poster/top.png\" preload=\"metadata\" autoplay loop muted playsinline webkit-playsinline=\"\">\n  <source src=\"/static/projects/morphio/webm/top.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/morphio/video/top.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-1-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-1-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-1-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-1-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-1-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-1-3.png\" /></a>\n  </div>\n</div>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-1-4.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-1-4.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-1-5.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-1-5.png\" /></a>\n  </div>\n</div>\n<h1>Introduction</h1>\n<p><strong>Programmable soft materials</strong> have a great potential for many application domains, such as soft robotics, material interfaces, accessibility, and haptic interfaces.\n<strong>However, programming of such materials is hard.</strong>\nThe dominant programming paradigm of soft robots and material interfaces is largely confined within a digital screen, leaving little room for users to interactively explore physical motion through tangible interaction. In such a workflowcompiling code on a digital screen then trans- ferring it into the physical objectusers need to repeatedly switch between the digital and physical worlds. This leaves a large gulf of execution in their programming experiences.\nThus, the traditional programming paradigm significantly limits the users ability to experiment with the design of expressive motion. Moreover, due to this barrier, such an opportunity is largely limited to highly skilled programmers and researchers who are proficient in hardware programming.</p>\n<h1>MorphIO</h1>\n<p>This paper introduces <strong>MorphIO, entirely soft sensing and actuation modules</strong> for programming by demonstration of soft robots and shape-changing interfaces.\nMorphIOs hardware consists of a soft pneumatic actuator containing a conductive sponge sensor. This allows for integrated and entirely soft shape-changing modules that can both sense and actuate a variety of three-dimensional deformations. Leveraging this capability, MorphIO enables the user to program behaviors by <strong>recording and later playing back physical motions</strong> through tangible interaction. In addition, the modular design of MorphIOs unit allows the user to construct various shapes and topologies through magnetic connection, then <strong>synthesize multiple recorded motions to achieve more complex behaviors</strong>, such as bending, gripping, and walking.</p>\n<video poster=\"/static/projects/morphio/video-poster/module.png\" preload=\"metadata\" autoplay loop muted playsinline webkit-playsinline=\"\">\n  <source src=\"/static/projects/morphio/webm/module.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/morphio/video/module.mp4\" type=\"video/mp4\"></source>\n</video>\n<br/>\n<video poster=\"/static/projects/morphio/video-poster/bear.png\" preload=\"metadata\" autoplay loop muted playsinline webkit-playsinline=\"\">\n  <source src=\"/static/projects/morphio/webm/bear.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/morphio/video/bear.mp4\" type=\"video/mp4\"></source>\n</video>\n<h1>System Overview</h1>\n<p>The programming workflow with MorphIO is the following:</p>\n<ul>\n<li>\n<p><strong>Step 1:</strong> A user starts manipulating the MorphIO unit.</p>\n</li>\n<li>\n<p><strong>Step 2:</strong> The demonstrated motion is detected and recorded through internal sensors, and the recorded sensor values are stored in the software.</p>\n</li>\n<li>\n<p><strong>Step 3:</strong> Once the user clicks play in the graphical user interface, the pneumatic pump starts supplying air.</p>\n</li>\n<li>\n<p><strong>Step 4:</strong> By controlling the air flow through switching on and off the solenoid valves, the system can control the behavior of the pneumatic actuator as it plays back the recorded motion.</p>\n</li>\n</ul>\n<p>The MorphIO system consists of the following components: A sensor and actuation unit, a sensing and actuation control unit, a microcontroller, software to control these units, and a visual interface for users to control behaviors. Figure illustrates the overview architecture of MorphIO.</p>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-6-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-6-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-6-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-6-2.png\" /></a>\n  </div>\n</div>\n<h1>Entirely Soft Sensing and Actuation Modules</h1>\n<p>Our main contribution is a design and fabrication method for <strong>a conductive sponge sensor</strong> that can be embedded into an air chamber in the pneumatic actuator. The conductive sponge sensor leverages the porous structure to <strong>sense the three-dimensional deformation by measuring the internal resistance value</strong>; when contracted, the resistance value be- tween the top and bottom surfaces drops, and when extended, it increases. In contrast to existing sensing techniques, an elastic sponge allows for a higher degree of freedom in sensing capability (e.g., stretching, bending, and compression) without sacrificing the softness of the interface.</p>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-2-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-2-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-2-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-2-2.png\" /></a>\n  </div>\n</div>\n<video poster=\"/static/projects/morphio/video-poster/mechanism.png\" preload=\"metadata\" autoplay loop muted playsinline webkit-playsinline=\"\">\n  <source src=\"/static/projects/morphio/webm/mechanism.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/morphio/video/mechanism.mp4\" type=\"video/mp4\"></source>\n</video>\n<br />\n<p>Moreover, our <strong>modular design</strong> and <strong>graphical interface</strong> allows for easy experiments involving multiple units. For example, the system can visualize multiple recorded sensor values, so that the user can see, customize, and synthesize recorded motion to construct more complex behaviors. These hardware and software designs were informed by our formative study, wherein we interviewed five experienced researchers from the robotics and HCI communities.</p>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-3-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-3-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-3-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-3-2.png\" /></a>\n  </div>\n</div>\n<video poster=\"/static/projects/morphio/video-poster/unit-x2.png\" preload=\"metadata\" autoplay loop muted playsinline webkit-playsinline=\"\">\n  <source src=\"/static/projects/morphio/webm/unit-x2.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/morphio/video/unit-x2.mp4\" type=\"video/mp4\"></source>\n</video>\n<video poster=\"/static/projects/morphio/video-poster/unit-x3.png\" preload=\"metadata\" autoplay loop muted playsinline webkit-playsinline=\"\">\n  <source src=\"/static/projects/morphio/webm/unit-x3.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/morphio/video/unit-x3.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-4-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-4-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-4-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-4-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-4-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-4-3.png\" /></a>\n  </div>\n</div>\n<h1>Fabrication Process</h1>\n<p>The fabrication process follows three steps: 1) Fabricate an elastic sponge, 2) impregnate into conductive ink, and 3) attach electrodes and wires.\nTo fabricate an elastic sponge, we first prepare 6.0 g of elastomer prepolymer solution and 29.1 g of sodium-chloride, then mix them together by using a planetary centrifugal mixer. The mixed solution is injected into a 3D printed cylindrical mold (16mm diameter, 40mm height). Then we dry the material with an oven at 100 C degrees for one hour. Once dried, we immerse the sponge in water, so that the sodium chloride can melt, leaving a porous structure within the elastomer sponge.</p>\n<video poster=\"/static/projects/morphio/video-poster/fabrication.png\" preload=\"metadata\" autoplay loop muted playsinline webkit-playsinline=\"\">\n  <source src=\"/static/projects/morphio/webm/fabrication.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/morphio/video/fabrication.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui four column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-5-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-5-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-5-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-5-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-5-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-5-3.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-5-4.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-5-4.png\" /></a>\n  </div>\n</div>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-5-5.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-5-5.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-5-6.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-5-6.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-5-7.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-5-7.png\" /></a>\n  </div>\n</div>\n<h1>Applications</h1>\n<p>We demonstrate several possible applications scenarios with MorphIO. 1) Tangible character animation, 2) Animating existing soft objects, 3) Remote manipulation of soft grippers, 4) Locomotion experimentation with soft robots.</p>\n<div class=\"figures ui four column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-10-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-10-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-10-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-10-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-10-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-10-3.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-10-4.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-10-4.png\" /></a>\n  </div>\n</div>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-11-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-11-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-11-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-11-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-11-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-11-3.png\" /></a>\n  </div>\n</div>\n<video poster=\"/static/projects/morphio/video-poster/locomotion.png\" preload=\"metadata\" autoplay loop muted playsinline webkit-playsinline=\"\">\n  <source src=\"/static/projects/morphio/webm/locomotion.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/morphio/video/locomotion.mp4\" type=\"video/mp4\"></source>\n</video>\n<h1>Evaluation</h1>\n<p>We conducted a user evaluation study to understand the bene- fits and limitations of MorphIO. In this study, we focused on answering the following research questions:</p>\n<ul>\n<li>\n<p><strong>RQ1:</strong> Does MorphIO save time and reduce the number of iterations to program the target behavior, compared to the existing approach?</p>\n</li>\n<li>\n<p><strong>RQ2:</strong> Does MorphIO increase the expressiveness of the physical motion?</p>\n</li>\n</ul>\n<p>To answer these questions, we conducted a controlled experiment where we compared MorphIO (left) with the current programming approach. We chose Arduino IDE (right) as a base condition for the comparison, as this is the most common programming approach identified through our formative study. We provide three basic tasks to construct a program. For each task, the participants were asked to program three differ- ent emotionshappiness, anger, and sadnessof an animated character. We chose these emotions based on Ekmans basic emotions for communication.</p>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-12-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-12-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-12-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-12-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-12-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-12-3.png\" /></a>\n  </div>\n</div>\n<p>The average time of task completion time of MorphIO was 2m 19s, compared to 5m 21s with the control condition. The average number of iterations of MorphIO was 4.4 times, compared to 6.4 with the control condition, which confirms that MorphIO is significantly efficient in terms of task completion time and the number of iterations. When asked about the achievement of the expressions using a 9-point Likert scale, the average score with MorphIO was 6.5, compared to 6.3 with the control condition. We did not find differences between the two conditions. Thus, we conclude the result of our study as follows: <strong>RQ1: Yes, RQ2: No</strong>.</p>\n<p>Based on our post interviews, we discuss the benefits and limitations of our approach: <strong>1) tangible interactions are suitable for sculpting rough motion</strong>, <strong>2) programming allows for fine-tuning more precise adjustments</strong>. Thus, for future research, systems might allow users to quickly make a rough motion, which can automatically be converted into digital parameters so that the user can also precisely control and adjust the motion. The same human- computer cooperation approach can be applied to other design domains: For example, when designing an object, the user can quickly make rough shapes with clay, while letting a machine finish the details. We believe this insight can lead the HCI community to further explore design approaches wherein users and machines cooperate for enhanced interaction design.</p>\n<h1>Future Vision</h1>\n<p>We believe this approachs potential for lowering the barrier and opening new opportunities for a larger community to begin designing, prototyping, and exploring soft material motionnot by coding on a screen, but by sculpting behaviors in the physical world.\nWe envision the future where people can interactively explore various behaviors through tangible interactions, <strong>just like sculpting behaviors with clay</strong>.</p>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/morphio/figure-14.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/morphio/figure-14.png\" /></a>\n  </div>\n</div>","dir":"content/output/projects","base":"morphio.json","ext":".json","sourceBase":"morphio.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/pep.json":
/*!******************************************!*\
  !*** ./content/output/projects/pep.json ***!
  \******************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, pages, doi, conference, pdf, video, embed, short-video, acm-dl, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"pep","name":"PEP","description":"3D Printed Electronic Papercrafts - An Integrated Approach for 3D Sculpting Paper-based Electronic Devices","title":"PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-based Electronic Devices","authors":["Hyunjoo Oh","Tung D. Ta","Ryo Suzuki","Mark D. Gross","Yoshihiro Kawahara","Lining Yao"],"year":2018,"booktitle":"In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18)","publisher":"ACM, New York, NY, USA","pages":"Paper 441, 12 pages","doi":"https://doi.org/10.1145/3173574.3174015","conference":{"name":"CHI 2018","fullname":"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2018)","url":"https://chi2018.acm.org/"},"pdf":"chi-2018-pep.pdf","video":"https://vimeo.com/252080903","embed":"https://www.youtube.com/embed/DTd863suDN0","short-video":"https://www.youtube.com/watch?v=DTd863suDN0","acm-dl":"https://dl.acm.org/citation.cfm?id=3174015","pageCount":12,"slideCount":0,"bodyContent":"","bodyHtml":"","dir":"content/output/projects","base":"pep.json","ext":".json","sourceBase":"pep.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/phd-thesis.json":
/*!*************************************************!*\
  !*** ./content/output/projects/phd-thesis.json ***!
  \*************************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, pages, conference, pdf, slide, talk, pageCount, slideCount, related, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"phd-thesis","name":"Collective Shape-changing Interfaces","description":"Dynamic Shape Construction and Transformation with Collective Elements","title":"Dynamic Shape Construction and Transformation with Collective Elements","authors":["Ryo Suzuki"],"year":2020,"booktitle":"PhD Dissertation","publisher":"University of Colorado Boulder","pages":"1-289","conference":{"name":"PhD Dissertation","fullname":"PhD Dissertation"},"pdf":"phd-dissertation.pdf","slide":"phd-defense.pdf","talk":"https://www.youtube.com/watch?v=FHmp7BIhXJI","pageCount":250,"slideCount":198,"related":{"title":"Collective Shape-changing Interfaces","authors":["Ryo Suzuki"],"year":2019,"booktitle":"In Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST '19 Doctoral Consortium)","publisher":"ACM, New York, NY, USA","pages":"154157","doi":"https://doi.org/10.1145/3332167.3356877","pdf":"uist-2019-collective.pdf","suffix":"dc","pageCount":4},"bodyContent":"# Abstract\nThis thesis explores dynamic and collective shape construction as a new way to **physicalize** digital information for interactive physical displays --- i.e., **shape-changing displays enabled by a swarm of collective elements**. Through physical form of digital objects, the user can directly touch, grasp, and manipulate digital information through rich tangible and embodied interactions, but at the same time, such physical objects can dynamically change their shape for an interactive computer display and interface through collective shape construction and transformation with a swarm of elements. The goal of this thesis is to envision and illustrate how such an interface might support human activities by transforming physical forms at various sizes, from millimeter to meter scale.\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-1.jpg\" /></a>\n  </div>\n</div>\n\nTo achieve this goal, this thesis introduces **collective shape-changing interface**, a new type of shape-changing interfaces constructed by discrete, collective, physical elements. This proposed approach promises to address the current limitations of shape-changing interfaces --- wherein a swarm of modular elements enables us to decompose the large, monolithic shape-changing objects into a set of simple, distributed elements. At the same time, their swarm behaviors enable us to make an unbounded shape transformation for expressive representation. This thesis contributes to the first exploration of this new class of shape-changing interfaces and proposes two approaches: active and passive shape construction. In active shape construction, collective elements can dynamically move and reconfigure themselves to construct a three-dimensional shape. Passive shape construction instead leverages external actuation to assemble and transform collective passive objects for dynamic shape creation. I explore and demonstrate how active and passive collective shape construction can be used as a future of computer interfaces, by developing various prototypes built on top of novel hardware and software platforms. Given these investigations, I discuss the design implications and possible research directions towards the future of collective shape-changing user interfaces.\n\n\n# Introduction\n\nWhat if computer displays can represent information not only *graphically* but also *physically*? What if such physical forms of information could be as malleable and programmable as the pixels on a computer screen? If so, it could be used as a dynamic physical medium to interact with the digital world.\n\nIvan Sutherland, a founder of virtual and augmented reality, once envisioned that the future of computer displays would be *\"a room within which the computer can control the existence of matter\"*\nThis radical vision has inspired many researchers over the decades, as such technologies could open up a new\nparadigm of human-machine interfaces.\n\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-2.jpg\" /></a>\n  </div>\n</div>\n\nHowever, we are still far from this exciting future. Today's computer interfaces mostly focus on *screen-based* interaction, where the screens serve as a ``looking window'' of the digital world --- the user can see digital information through the glass, but a barrier between what is inside (digital world) and what is outside (physical world) confines how we interact with the digital world. Current technologies do not allow us to directly touch, feel, grasp, and manipulate digital objects, in the same way that humans have done with physical objects for hundreds of thousands of years.\n\n\n# Thesis Statement\n\nThe goal of this thesis is to bring Sutherland's vision closer to reality by developing a new form of interactive and dynamic physical displays, and to illustrate how such an interface might support human activities by transforming physical forms and environments at various scales.\n\nAs a step toward this vision, this thesis explores dynamic and collective shape construction as a new way to **physicalize** digital information for interactive physical displays --- i.e., **shape-changing displays** enabled by a swarm of collective elements.\nCollective elements refer to discrete physical objects that can construct a physical, three-dimensional shape.\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-3.jpg\" /></a>\n  </div>\n</div>\n\nEach individual element can dynamically change its shape, position, and other physical properties through internal or external actuation, as to collectively construct and transform the overall physical shape.\nThis enables a new way of representing digital information.\nSuch physical shapes allow the user not only to see information, but to touch, feel, grasp, construct, and manipulate it, in the same way that interact with physical objects.\nAt the same time, these physical objects must also embody dynamic computation. Collective elements can dynamically and programmatically reconfigure themselves, as if they are rendered in an interactive computer display and interface.\n\nIn contrast to shape changes made of monolithic materials, constructing shapes out of discrete elements enables rich expressiveness in representing information.\nLike pixels on a screen, they make shapes by collectively transforming the overall structure.\nAdditionally, their components can be simple and interchangeable, thus allowing for scale.\nThese elements can also interact with existing environments, and they make everyday objects and environments more dynamic, adaptive, and interactive by collectively actuating and reconfiguring them in a programmable fashion.\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-4.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-4.jpg\" /></a>\n  </div>\n</div>\n\nMaking shapes out of discrete collective elements is not a new idea.\nThere is a long history of modular self-reconfigurable robots and swarm robotics.\nThese areas of research have explored the idea of collective and general-purpose shape transformation for robotic applications, such as space exploration, rescue, and navigation.\nHowever, there are many critical challenges when we apply this approach for **interactive interfaces**.\n\nFor example, the speed of transformation needs to be much faster than for robotic applications, as the interactive system must change and respond to the user in real-time (e.g., in seconds, not minutes or hours). Another consideration is scalability.\nTo display meaningful information, it may require a relatively large number of elements, which often introduces implementation problems.\nFinally, unlike autonomous systems, interactive systems must consider the interaction between humans and objects --- there remains work to be done in understanding how we might interact with such collective elements and how these interfaces could support everyday human activities.\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-5.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-5.jpg\" /></a>\n  </div>\n</div>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-7.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-7.jpg\" /></a>\n  </div>\n</div>\n\nThis thesis addresses these questions by investigating how collective shape construction and transformation can be used for *interactive computer interfaces*.\nTo this end, this thesis introduces **collective shape-changing interfaces**, a new class of shape-changing interfaces constructed by a swarm of discrete physical elements.\nThe main contribution of this thesis is the first exploration of this new class of shape-changing interfaces in the following four domains:\n\n1. **Shape representation**: <br/>\nexplore what types of shape representations are possible to display information\n\n\n2. **Reconfiguration methods**: <br/>\nexplore how both active and passive elements can be used to construct a shape for interactive interfaces\n\n\n3. **Interaction**: <br/>\nexplore how the user can interact with many collective elements through direct physical manipulation,\n\n\n4. **Applications**: <br/>\nexplore, illustrate, and demonstrate what kind of applications are achievable for human-computer interaction.\n\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-1.jpg\" /></a>\n  </div>\n</div>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-2-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-2-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-9.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-9.jpg\" /></a>\n  </div>\n</div>\n\n\nThis new class of shape-changing interfaces promises to address some of the limitations of the current shape-changing interfaces. For example, a swarm of modular elements enables us to decompose the large, monolithic shape-changing objects into a set of simple, distributed elements. This significantly contributes to the deployability of the system in everyday environments. In addition, the swarm behaviors enable us to make unbounded shape transformations for expressive representation.\nThrough my explorations of various proof-of-concept prototypes, I demonstrate how we can push the boundary of the current shape-changing interfaces by leveraging the collective behaviors of both active and passive elements.\nI also demonstrate how these dynamic shapes can support a range of application scenarios, such as interactive displays, adaptive environments, dynamic data physicalization, and accessibility support for people with visual impairments.\nFinally, I discuss the challenges and opportunities for using this approach towards the future of dynamic physical media.\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-6.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-6.jpg\" /></a>\n  </div>\n</div>\n\n\n# Thesis Contributions\nThis thesis makes contributions to the field of Human-Computer Interaction in the following areas:\n\n1. A design space exploration of dynamic shape construction with collective elements\n\n\n2. A new taxonomy and investigation of active and passive shape construction and transformation with collective elements\n\n\n3. A novel technique for creating a dynamic shape with active shape-transformable swarm robots (e.g., ShapeBots, LiftTiles)\n\n\n4. A novel technique for constructing 3D shapes with an assembly of passive magnetically connectable blocks (e.g., Dynablock)\n\n\n5. A novel technique for actuating passive magnetic markers with scalable electro-magnetic actuation (e.g., FluxMarker, Reactile)\n\n\n6. A novel technique for actuating existing objects to reconfigure spatial layouts (e.g., RoomShift)\n\n\n7. A novel interaction technique for programming the dynamic shape construction on a 2D surface with direct physical manipulation (e.g., Reactile)\n\n\n<br/>\n\n# Future Vision\n\n<div class=\"video-container\" style=\"display: block\">\n  <iframe\n    class=\"embed\"\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/_huMQVBAJsY\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\"\n    allowFullScreen={true}\n    mozAllowFullScreen={true}\n    msAllowFullScreen={true}\n    oAllowFullScreen={true}\n    webkitAllowFullScreen={true}\n  ></iframe>\n</div>\n\n<div class=\"figures ui four column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-3.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-4.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-4.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-5.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-5.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-6.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-6.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-7.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-7.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-8.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-8.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-9.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-9.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-10.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-10.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-11.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-11.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-12.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-12.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-13.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-13.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-14.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-14.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-15.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-15.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-16.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-16.jpg\" /></a>\n  </div>\n</div>\n\n# PhD Dissertation Defense\n\n<div class=\"video-container\" style=\"display: block\">\n  <iframe\n    class=\"embed\"\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/FHmp7BIhXJI\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\"\n    allowFullScreen={true}\n    mozAllowFullScreen={true}\n    msAllowFullScreen={true}\n    oAllowFullScreen={true}\n    webkitAllowFullScreen={true}\n  ></iframe>\n</div>\n\n<br/>\n\nThesis Committee:\n\n- **Daniel Leithinger** (CU Boulder, chair)\n\n\n- **Mark Gross** (CU Boulder)\n\n\n- **Tom Yeh** (CU Boulder)\n\n\n- **Hiroshi Ishii** (MIT Media Lab)\n\n\n- **Takeo Igarashi** (The University of Tokyo)\n\n<br/>\n\nDefense Date: May 13th, 2020","bodyHtml":"<h1>Abstract</h1>\n<p>This thesis explores dynamic and collective shape construction as a new way to <strong>physicalize</strong> digital information for interactive physical displays --- i.e., <strong>shape-changing displays enabled by a swarm of collective elements</strong>. Through physical form of digital objects, the user can directly touch, grasp, and manipulate digital information through rich tangible and embodied interactions, but at the same time, such physical objects can dynamically change their shape for an interactive computer display and interface through collective shape construction and transformation with a swarm of elements. The goal of this thesis is to envision and illustrate how such an interface might support human activities by transforming physical forms at various sizes, from millimeter to meter scale.</p>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-1.jpg\" /></a>\n  </div>\n</div>\n<p>To achieve this goal, this thesis introduces <strong>collective shape-changing interface</strong>, a new type of shape-changing interfaces constructed by discrete, collective, physical elements. This proposed approach promises to address the current limitations of shape-changing interfaces --- wherein a swarm of modular elements enables us to decompose the large, monolithic shape-changing objects into a set of simple, distributed elements. At the same time, their swarm behaviors enable us to make an unbounded shape transformation for expressive representation. This thesis contributes to the first exploration of this new class of shape-changing interfaces and proposes two approaches: active and passive shape construction. In active shape construction, collective elements can dynamically move and reconfigure themselves to construct a three-dimensional shape. Passive shape construction instead leverages external actuation to assemble and transform collective passive objects for dynamic shape creation. I explore and demonstrate how active and passive collective shape construction can be used as a future of computer interfaces, by developing various prototypes built on top of novel hardware and software platforms. Given these investigations, I discuss the design implications and possible research directions towards the future of collective shape-changing user interfaces.</p>\n<h1>Introduction</h1>\n<p>What if computer displays can represent information not only <em>graphically</em> but also <em>physically</em>? What if such physical forms of information could be as malleable and programmable as the pixels on a computer screen? If so, it could be used as a dynamic physical medium to interact with the digital world.</p>\n<p>Ivan Sutherland, a founder of virtual and augmented reality, once envisioned that the future of computer displays would be <em>&quot;a room within which the computer can control the existence of matter&quot;</em>\nThis radical vision has inspired many researchers over the decades, as such technologies could open up a new\nparadigm of human-machine interfaces.</p>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-2.jpg\" /></a>\n  </div>\n</div>\n<p>However, we are still far from this exciting future. Today's computer interfaces mostly focus on <em>screen-based</em> interaction, where the screens serve as a ``looking window'' of the digital world --- the user can see digital information through the glass, but a barrier between what is inside (digital world) and what is outside (physical world) confines how we interact with the digital world. Current technologies do not allow us to directly touch, feel, grasp, and manipulate digital objects, in the same way that humans have done with physical objects for hundreds of thousands of years.</p>\n<h1>Thesis Statement</h1>\n<p>The goal of this thesis is to bring Sutherland's vision closer to reality by developing a new form of interactive and dynamic physical displays, and to illustrate how such an interface might support human activities by transforming physical forms and environments at various scales.</p>\n<p>As a step toward this vision, this thesis explores dynamic and collective shape construction as a new way to <strong>physicalize</strong> digital information for interactive physical displays --- i.e., <strong>shape-changing displays</strong> enabled by a swarm of collective elements.\nCollective elements refer to discrete physical objects that can construct a physical, three-dimensional shape.</p>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-3.jpg\" /></a>\n  </div>\n</div>\n<p>Each individual element can dynamically change its shape, position, and other physical properties through internal or external actuation, as to collectively construct and transform the overall physical shape.\nThis enables a new way of representing digital information.\nSuch physical shapes allow the user not only to see information, but to touch, feel, grasp, construct, and manipulate it, in the same way that interact with physical objects.\nAt the same time, these physical objects must also embody dynamic computation. Collective elements can dynamically and programmatically reconfigure themselves, as if they are rendered in an interactive computer display and interface.</p>\n<p>In contrast to shape changes made of monolithic materials, constructing shapes out of discrete elements enables rich expressiveness in representing information.\nLike pixels on a screen, they make shapes by collectively transforming the overall structure.\nAdditionally, their components can be simple and interchangeable, thus allowing for scale.\nThese elements can also interact with existing environments, and they make everyday objects and environments more dynamic, adaptive, and interactive by collectively actuating and reconfiguring them in a programmable fashion.</p>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-4.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-4.jpg\" /></a>\n  </div>\n</div>\n<p>Making shapes out of discrete collective elements is not a new idea.\nThere is a long history of modular self-reconfigurable robots and swarm robotics.\nThese areas of research have explored the idea of collective and general-purpose shape transformation for robotic applications, such as space exploration, rescue, and navigation.\nHowever, there are many critical challenges when we apply this approach for <strong>interactive interfaces</strong>.</p>\n<p>For example, the speed of transformation needs to be much faster than for robotic applications, as the interactive system must change and respond to the user in real-time (e.g., in seconds, not minutes or hours). Another consideration is scalability.\nTo display meaningful information, it may require a relatively large number of elements, which often introduces implementation problems.\nFinally, unlike autonomous systems, interactive systems must consider the interaction between humans and objects --- there remains work to be done in understanding how we might interact with such collective elements and how these interfaces could support everyday human activities.</p>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-5.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-5.jpg\" /></a>\n  </div>\n</div>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-7.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-7.jpg\" /></a>\n  </div>\n</div>\n<p>This thesis addresses these questions by investigating how collective shape construction and transformation can be used for <em>interactive computer interfaces</em>.\nTo this end, this thesis introduces <strong>collective shape-changing interfaces</strong>, a new class of shape-changing interfaces constructed by a swarm of discrete physical elements.\nThe main contribution of this thesis is the first exploration of this new class of shape-changing interfaces in the following four domains:</p>\n<ol>\n<li>\n<p><strong>Shape representation</strong>: <br/>\nexplore what types of shape representations are possible to display information</p>\n</li>\n<li>\n<p><strong>Reconfiguration methods</strong>: <br/>\nexplore how both active and passive elements can be used to construct a shape for interactive interfaces</p>\n</li>\n<li>\n<p><strong>Interaction</strong>: <br/>\nexplore how the user can interact with many collective elements through direct physical manipulation,</p>\n</li>\n<li>\n<p><strong>Applications</strong>: <br/>\nexplore, illustrate, and demonstrate what kind of applications are achievable for human-computer interaction.</p>\n</li>\n</ol>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-1.jpg\" /></a>\n  </div>\n</div>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-2-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-2-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-9.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-9.jpg\" /></a>\n  </div>\n</div>\n<p>This new class of shape-changing interfaces promises to address some of the limitations of the current shape-changing interfaces. For example, a swarm of modular elements enables us to decompose the large, monolithic shape-changing objects into a set of simple, distributed elements. This significantly contributes to the deployability of the system in everyday environments. In addition, the swarm behaviors enable us to make unbounded shape transformations for expressive representation.\nThrough my explorations of various proof-of-concept prototypes, I demonstrate how we can push the boundary of the current shape-changing interfaces by leveraging the collective behaviors of both active and passive elements.\nI also demonstrate how these dynamic shapes can support a range of application scenarios, such as interactive displays, adaptive environments, dynamic data physicalization, and accessibility support for people with visual impairments.\nFinally, I discuss the challenges and opportunities for using this approach towards the future of dynamic physical media.</p>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-6.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-6.jpg\" /></a>\n  </div>\n</div>\n<h1>Thesis Contributions</h1>\n<p>This thesis makes contributions to the field of Human-Computer Interaction in the following areas:</p>\n<ol>\n<li>\n<p>A design space exploration of dynamic shape construction with collective elements</p>\n</li>\n<li>\n<p>A new taxonomy and investigation of active and passive shape construction and transformation with collective elements</p>\n</li>\n<li>\n<p>A novel technique for creating a dynamic shape with active shape-transformable swarm robots (e.g., ShapeBots, LiftTiles)</p>\n</li>\n<li>\n<p>A novel technique for constructing 3D shapes with an assembly of passive magnetically connectable blocks (e.g., Dynablock)</p>\n</li>\n<li>\n<p>A novel technique for actuating passive magnetic markers with scalable electro-magnetic actuation (e.g., FluxMarker, Reactile)</p>\n</li>\n<li>\n<p>A novel technique for actuating existing objects to reconfigure spatial layouts (e.g., RoomShift)</p>\n</li>\n<li>\n<p>A novel interaction technique for programming the dynamic shape construction on a 2D surface with direct physical manipulation (e.g., Reactile)</p>\n</li>\n</ol>\n<br/>\n<h1>Future Vision</h1>\n<div class=\"video-container\" style=\"display: block\">\n  <iframe\n    class=\"embed\"\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/_huMQVBAJsY\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\"\n    allowFullScreen={true}\n    mozAllowFullScreen={true}\n    msAllowFullScreen={true}\n    oAllowFullScreen={true}\n    webkitAllowFullScreen={true}\n  ></iframe>\n</div>\n<div class=\"figures ui four column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-3.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-4.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-4.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-5.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-5.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-6.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-6.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-7.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-7.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-8.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-8.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-9.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-9.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-10.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-10.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-11.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-11.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-12.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-12.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-13.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-13.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-14.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-14.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-15.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-15.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/phd-thesis/figure-10-16.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/phd-thesis/figure-10-16.jpg\" /></a>\n  </div>\n</div>\n<h1>PhD Dissertation Defense</h1>\n<div class=\"video-container\" style=\"display: block\">\n  <iframe\n    class=\"embed\"\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/FHmp7BIhXJI\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\"\n    allowFullScreen={true}\n    mozAllowFullScreen={true}\n    msAllowFullScreen={true}\n    oAllowFullScreen={true}\n    webkitAllowFullScreen={true}\n  ></iframe>\n</div>\n<br/>\n<p>Thesis Committee:</p>\n<ul>\n<li>\n<p><strong>Daniel Leithinger</strong> (CU Boulder, chair)</p>\n</li>\n<li>\n<p><strong>Mark Gross</strong> (CU Boulder)</p>\n</li>\n<li>\n<p><strong>Tom Yeh</strong> (CU Boulder)</p>\n</li>\n<li>\n<p><strong>Hiroshi Ishii</strong> (MIT Media Lab)</p>\n</li>\n<li>\n<p><strong>Takeo Igarashi</strong> (The University of Tokyo)</p>\n</li>\n</ul>\n<br/>\n<p>Defense Date: May 13th, 2020</p>\n","dir":"content/output/projects","base":"phd-thesis.json","ext":".json","sourceBase":"phd-thesis.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/pufferbot.json":
/*!************************************************!*\
  !*** ./content/output/projects/pufferbot.json ***!
  \************************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, pages, conference, pdf, video, embed, arxiv, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"pufferbot","name":"PufferBot","description":"Actuated Expandable Structures for Aerial Robots","title":"PufferBot: Actuated Expandable Structures for Aerial Robots","authors":["Hooman Hedayati","Ryo Suzuki","Daniel Leithinger","Daniel Szafir"],"year":2020,"booktitle":"In Proceedings of the 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS '20)","publisher":"IEEE, New York, NY, USA","pages":"1-6","conference":{"name":"IROS 2020","fullname":"The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2020)","url":"https://www.iros2020.org/"},"pdf":"iros-2020-pufferbot.pdf","video":"https://www.youtube.com/watch?v=XtPepCxWcBg","embed":"https://www.youtube.com/embed/XtPepCxWcBg","arxiv":"https://arxiv.org/abs/2008.07615","pageCount":6,"slideCount":0,"bodyContent":"# Abstract\n\nWe present PufferBot, an aerial robot with an expandable structure that may expand to protect a drones propellers when the robot is close to obstacles or collocated humans. PufferBot is made of a custom 3D-printed expandable scissor structure, which utilizes a one degree of freedom actuator with rack and pinion mechanism. We propose four designs for the expandable structure, each with unique characterizations for different situations. Finally, we present three motivating scenarios in which PufferBot may extend the utility of existing static propeller guard structures.\n\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/pufferbot/video/top.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-1-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-1-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-1-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-1-2.jpg\" /></a>\n  </div>\n</div>\n\n\n# Introduction\n\nAerial robots are increasingly used in a wide variety of applications, such as search and rescue, journalism, structural inspection, and environmental data collection. When used indoors, aerial robots have traditionally been isolated from humans through cages or operated in an entirely separated space, but they are increasingly entering into environments with collocated humans (e.g., construction sites). In such situations, there is an increasing demand to reduce the danger and unpredictability of robots, as well as increase safety for nearby people.\n\nTo address issues surrounding propeller damage/safety, many aerial robots use propeller guardsfixed structures that may prevent the propellers from hitting an obstacle or person in the event of a collision. However, many guards do not fully cover the robots propellers (for instance, only providing cover for the horizontal size of a propeller), leaving other parts of the propellers (e.g., the top) exposed and vulnerable to damage. On the other hand, guards that do provide full coverage surrounding the propellers, such as in the Zero Zero Robotics HoverCam and the Flyability GimBall, significantly increase the size and rigidity of the robot, potentially making the robot less maneuverable. This can pose a problem if the robot operates in narrow spaces (e.g., search and rescue in a collapsed building), as the robot cannot navigate tight spaces and can become stuck between obstacles.\n\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/pufferbot/video/collision-1.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-7-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-7-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-7-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-7-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-7-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-7-3.jpg\" /></a>\n  </div>\n</div>\n\n\n# PufferBot\nIn this paper, we introduce PufferBot, the concept of an ex- pandable aerial robot that can dynamically change its shape to reduce damage in the event of collisions with collocated humans and/or the environment. PufferBot consists of an aerial robot with a mounted expandable structure that can be actuated to expand in order to reduce the collision damage or create an enlarged buffer zone surrounding the robot. The PufferBot concept is inspired by both natural designs (e.g., pufferfish) and mechanical systems (e.g., vehicle airbags). When in danger, a pufferfish (Tetraodontidae) inflates its body by taking water or air into portions of its digestive tract to increase its size. Similarly, vehicle airbag systems also inflate to protect humans when crashes occur.\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-2.png\" /></a>\n  </div>\n</div>\n\nBy taking an inspiration from such metaphors, we propose an expandable structure for an aerial robot that may reduce the risk of crashing and protect the robots propellers when the robot is in danger of falling on the ground, crashing into an object, or navigating cluttered spaces. One advan- tage of our system is that the expandable structure can dynamically change its shape in order to reduce the overall size in the non-expanded state, making it easier for the robot to navigate in narrow spaces and avoid unnecessary contact with the surrounding environment. In addition, such expandable structures may open up a new design element for future work examining user interaction.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/pufferbot/video/human-2.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-10-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-10-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-10-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-10-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-10-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-10-3.jpg\" /></a>\n  </div>\n</div>\n\n\n\n# Design and Implementation\n\n\n## Aerial Robot\nIn this work, We used a DJI Flame Wheel F450 frame for our aerial robot. The base frame weight is 282 g. After mounting additional components (motors, battery, flight controller, etc.), the weight of the aerial robot accumulates to 1.2 Kg. Based on the specification document, the robot is capable of lifting up to 1.6 Kg of payload. which is enough for our expandable structure (in our setting, the weight of expandable structure and the actuator combined is 1.6 Kg). The diagonal length of the robot (motor to motor) is 45 cm. We used 4.5 inch propellers (11.43 cm), which make the total length of the aerial robot 70cm. We used a 4S Lithium-ion Polymer (LiPo) battery as the power source, which gives the robot a flight time of approximately 18 minutes. We built a plate on top of the aerial robot which gives us enough surface to mount and secure the expandable structure and actuator. The plate also allows us to avoid direct contact with the inertial measurement unit (IMU) and onboard sensors in the flight controller.\n\n\n## Expandable Structure\nThe expandable structure is made of three parts: (1) actuator joints, (2) regular joints, and (3) scissor units. 74 pieces were used in total to make the expandable structure. Revolvable press fit joints are used for secure but rotatable connections\n\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-3-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-3-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-3-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-3-3.png\" /></a>\n    <a href=\"/static/projects/pufferbot/figure-3-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-3-2.png\" /></a>\n  </div>\n</div>\n\n\n## Actuation\n\nWe actuate the expandable structure with a one degree of freedom actuation mechanism based on rack and pinion. The pinion gear located in the center rotates the four individual rack planes at the same time, so that the actuated racks can evenly apply the expansion force in four different directions at the same rate. The actuator joint attached to the end of the rack can expand and collapse the expandable structure by pushing and pulling the connected points. All the actuator parts are 3D printed with PLA. We laser cut the racks with 3mm plywood. We used plywood after testing with different materials (e.g., acrylic) and found that plywood was the most robust in terms of holding its shape against bending forces over time. We used a FeeTech FS5103R as a servo motor, controlled by a Wemos D1 mini ESP8266 micro-controller.\n\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-4-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-4-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-4-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-4-2.jpg\" /></a>\n  </div>\n</div>\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-5.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-5.jpg\" /></a>\n  </div>\n</div>\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/pufferbot/video/expand.mp4\" type=\"video/mp4\"></source>\n</video>\n\n\n\n## Control\n\nThere are two components that need to be controlled in the PufferBot: the aerial robot and the expandable structure actuator. Both can be controlled autonomously by a central computer or manually by a teleoperator. To implement au- tonomous control, we developed a linear PID controller that controls the position and altitude of the aerial robot. As the aerial robot we used lacks on-board sensing capabilities suf- ficient for accurate localization, our PID controller currently relies on a Vicon motion capture system with 200Hz motion tracking cameras embedded in the environment to track the physical robot. There is a trajectory planner built on top of the PID controller so the robot can traverse the space as planned. The local computer that runs the Vicon system also continuously detects nearby obstacles, and based on this information, it wirelessly communicates with the actuators microcontroller to programmatically control the size of the expandable structure. In the manual mode, a teleoperator is in charge of controlling the robot as well as the expandable structure.\n\n## Performance\n\nThe Pufferfish weights 600 grams and can expand or collapse in 6 seconds (we tested the expanding structure 50 times and the number reported is the mean of the trials). PufferBot can handle 6-9 N of force: 6 N against the parts furthest from the actuation racks and up to 9 N applied to the links directly connected to the racks.\n\n\n# Application Scenarios\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-6.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-6.png\" /></a>\n  </div>\n</div>\n\n## Protecting Humans / Human-Robot Communication\n\nAerial robots and humans are increasingly occupying shared spaces, whether through collaboration in the work- place or users partaking in leisure activities (e.g., a hobbyist piloting an aerial robot in their neighbourhood). While human safety is critical in these situations, developing autonomous/semi-autonomous systems capable of mitigating all risk of collisions remains an open problem, with some risks arising from humans themselves (e.g., a wandering human who is not paying attention or one who is attempting to test the robots behaviors might incite a collision with the robot). In these scenarios, PufferBot may reduce the risk of human injury in contacting the robots spinning propellers, disperse the force of the robot during a collision over a wider surface area, and provide a compliance mechanism that helps mitigate impact force.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/pufferbot/video/human.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-11-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-11-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-11-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-11-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-11-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-11-3.jpg\" /></a>\n  </div>\n</div>\n\n\n## Protecting Drones\n\nIn recent years, there has been an increasing trend in utilizing aerial robots for inspecting bridges, powerlines, pipelines, and other infrastructure elements. For these tasks, aerial robots must operate close to the target of interest, increasing the chance of the robot hitting obstacles due to operator error, loss of power, and/or unexpected elements of weather like gusts of wind. In these situations, PufferBot may protect the robot from dangerous objects in the environment. In the worst case, PufferBot may reduce damage to the robot when in free-fall by expanding to leverage the scissor structure, which acts like a spring\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/pufferbot/video/crash.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-9-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-9-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-9-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-9-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-9-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-9-3.jpg\" /></a>\n  </div>\n</div>\n\n\n## Sensing and Navigating Complex Environments\n\nWhen drones navigate in complex indoor environments, they usually rely on some form of perception (e.g., SLAM) to avoid colliding with walls and obstacles. However, many conditions may impair perception algorithms, including smoke, glare, dirty camera lenses, etc. We propose to use the expanding scissor structure as a guiding mechanism in collision-tolerant navigation, similar to the whiskers of a cat, the use of white canes by people who are blind or visually impaired, or the bumper bar of wheeled robots like Roomba. When the drone is in a complex environment, it may expand in order to locate obstacles by bumping into them. We can further mount sensors into the structure to enhance this navigation and even provide haptic feedback to a teleoperator to indicate when the presence of nearby obstacles.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/pufferbot/video/collision-2.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-8-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-8-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-8-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-8-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-8-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-8-3.jpg\" /></a>\n  </div>\n</div>","bodyHtml":"<h1>Abstract</h1>\n<p>We present PufferBot, an aerial robot with an expandable structure that may expand to protect a drones propellers when the robot is close to obstacles or collocated humans. PufferBot is made of a custom 3D-printed expandable scissor structure, which utilizes a one degree of freedom actuator with rack and pinion mechanism. We propose four designs for the expandable structure, each with unique characterizations for different situations. Finally, we present three motivating scenarios in which PufferBot may extend the utility of existing static propeller guard structures.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/pufferbot/video/top.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-1-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-1-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-1-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-1-2.jpg\" /></a>\n  </div>\n</div>\n<h1>Introduction</h1>\n<p>Aerial robots are increasingly used in a wide variety of applications, such as search and rescue, journalism, structural inspection, and environmental data collection. When used indoors, aerial robots have traditionally been isolated from humans through cages or operated in an entirely separated space, but they are increasingly entering into environments with collocated humans (e.g., construction sites). In such situations, there is an increasing demand to reduce the danger and unpredictability of robots, as well as increase safety for nearby people.</p>\n<p>To address issues surrounding propeller damage/safety, many aerial robots use propeller guardsfixed structures that may prevent the propellers from hitting an obstacle or person in the event of a collision. However, many guards do not fully cover the robots propellers (for instance, only providing cover for the horizontal size of a propeller), leaving other parts of the propellers (e.g., the top) exposed and vulnerable to damage. On the other hand, guards that do provide full coverage surrounding the propellers, such as in the Zero Zero Robotics HoverCam and the Flyability GimBall, significantly increase the size and rigidity of the robot, potentially making the robot less maneuverable. This can pose a problem if the robot operates in narrow spaces (e.g., search and rescue in a collapsed building), as the robot cannot navigate tight spaces and can become stuck between obstacles.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/pufferbot/video/collision-1.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-7-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-7-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-7-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-7-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-7-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-7-3.jpg\" /></a>\n  </div>\n</div>\n<h1>PufferBot</h1>\n<p>In this paper, we introduce PufferBot, the concept of an ex- pandable aerial robot that can dynamically change its shape to reduce damage in the event of collisions with collocated humans and/or the environment. PufferBot consists of an aerial robot with a mounted expandable structure that can be actuated to expand in order to reduce the collision damage or create an enlarged buffer zone surrounding the robot. The PufferBot concept is inspired by both natural designs (e.g., pufferfish) and mechanical systems (e.g., vehicle airbags). When in danger, a pufferfish (Tetraodontidae) inflates its body by taking water or air into portions of its digestive tract to increase its size. Similarly, vehicle airbag systems also inflate to protect humans when crashes occur.</p>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-2.png\" /></a>\n  </div>\n</div>\n<p>By taking an inspiration from such metaphors, we propose an expandable structure for an aerial robot that may reduce the risk of crashing and protect the robots propellers when the robot is in danger of falling on the ground, crashing into an object, or navigating cluttered spaces. One advan- tage of our system is that the expandable structure can dynamically change its shape in order to reduce the overall size in the non-expanded state, making it easier for the robot to navigate in narrow spaces and avoid unnecessary contact with the surrounding environment. In addition, such expandable structures may open up a new design element for future work examining user interaction.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/pufferbot/video/human-2.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-10-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-10-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-10-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-10-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-10-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-10-3.jpg\" /></a>\n  </div>\n</div>\n<h1>Design and Implementation</h1>\n<h2>Aerial Robot</h2>\n<p>In this work, We used a DJI Flame Wheel F450 frame for our aerial robot. The base frame weight is 282 g. After mounting additional components (motors, battery, flight controller, etc.), the weight of the aerial robot accumulates to 1.2 Kg. Based on the specification document, the robot is capable of lifting up to 1.6 Kg of payload. which is enough for our expandable structure (in our setting, the weight of expandable structure and the actuator combined is 1.6 Kg). The diagonal length of the robot (motor to motor) is 45 cm. We used 4.5 inch propellers (11.43 cm), which make the total length of the aerial robot 70cm. We used a 4S Lithium-ion Polymer (LiPo) battery as the power source, which gives the robot a flight time of approximately 18 minutes. We built a plate on top of the aerial robot which gives us enough surface to mount and secure the expandable structure and actuator. The plate also allows us to avoid direct contact with the inertial measurement unit (IMU) and onboard sensors in the flight controller.</p>\n<h2>Expandable Structure</h2>\n<p>The expandable structure is made of three parts: (1) actuator joints, (2) regular joints, and (3) scissor units. 74 pieces were used in total to make the expandable structure. Revolvable press fit joints are used for secure but rotatable connections</p>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-3-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-3-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-3-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-3-3.png\" /></a>\n    <a href=\"/static/projects/pufferbot/figure-3-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-3-2.png\" /></a>\n  </div>\n</div>\n<h2>Actuation</h2>\n<p>We actuate the expandable structure with a one degree of freedom actuation mechanism based on rack and pinion. The pinion gear located in the center rotates the four individual rack planes at the same time, so that the actuated racks can evenly apply the expansion force in four different directions at the same rate. The actuator joint attached to the end of the rack can expand and collapse the expandable structure by pushing and pulling the connected points. All the actuator parts are 3D printed with PLA. We laser cut the racks with 3mm plywood. We used plywood after testing with different materials (e.g., acrylic) and found that plywood was the most robust in terms of holding its shape against bending forces over time. We used a FeeTech FS5103R as a servo motor, controlled by a Wemos D1 mini ESP8266 micro-controller.</p>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-4-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-4-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-4-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-4-2.jpg\" /></a>\n  </div>\n</div>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-5.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-5.jpg\" /></a>\n  </div>\n</div>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/pufferbot/video/expand.mp4\" type=\"video/mp4\"></source>\n</video>\n<h2>Control</h2>\n<p>There are two components that need to be controlled in the PufferBot: the aerial robot and the expandable structure actuator. Both can be controlled autonomously by a central computer or manually by a teleoperator. To implement au- tonomous control, we developed a linear PID controller that controls the position and altitude of the aerial robot. As the aerial robot we used lacks on-board sensing capabilities suf- ficient for accurate localization, our PID controller currently relies on a Vicon motion capture system with 200Hz motion tracking cameras embedded in the environment to track the physical robot. There is a trajectory planner built on top of the PID controller so the robot can traverse the space as planned. The local computer that runs the Vicon system also continuously detects nearby obstacles, and based on this information, it wirelessly communicates with the actuators microcontroller to programmatically control the size of the expandable structure. In the manual mode, a teleoperator is in charge of controlling the robot as well as the expandable structure.</p>\n<h2>Performance</h2>\n<p>The Pufferfish weights 600 grams and can expand or collapse in 6 seconds (we tested the expanding structure 50 times and the number reported is the mean of the trials). PufferBot can handle 6-9 N of force: 6 N against the parts furthest from the actuation racks and up to 9 N applied to the links directly connected to the racks.</p>\n<h1>Application Scenarios</h1>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-6.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-6.png\" /></a>\n  </div>\n</div>\n<h2>Protecting Humans / Human-Robot Communication</h2>\n<p>Aerial robots and humans are increasingly occupying shared spaces, whether through collaboration in the work- place or users partaking in leisure activities (e.g., a hobbyist piloting an aerial robot in their neighbourhood). While human safety is critical in these situations, developing autonomous/semi-autonomous systems capable of mitigating all risk of collisions remains an open problem, with some risks arising from humans themselves (e.g., a wandering human who is not paying attention or one who is attempting to test the robots behaviors might incite a collision with the robot). In these scenarios, PufferBot may reduce the risk of human injury in contacting the robots spinning propellers, disperse the force of the robot during a collision over a wider surface area, and provide a compliance mechanism that helps mitigate impact force.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/pufferbot/video/human.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-11-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-11-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-11-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-11-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-11-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-11-3.jpg\" /></a>\n  </div>\n</div>\n<h2>Protecting Drones</h2>\n<p>In recent years, there has been an increasing trend in utilizing aerial robots for inspecting bridges, powerlines, pipelines, and other infrastructure elements. For these tasks, aerial robots must operate close to the target of interest, increasing the chance of the robot hitting obstacles due to operator error, loss of power, and/or unexpected elements of weather like gusts of wind. In these situations, PufferBot may protect the robot from dangerous objects in the environment. In the worst case, PufferBot may reduce damage to the robot when in free-fall by expanding to leverage the scissor structure, which acts like a spring</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/pufferbot/video/crash.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-9-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-9-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-9-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-9-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-9-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-9-3.jpg\" /></a>\n  </div>\n</div>\n<h2>Sensing and Navigating Complex Environments</h2>\n<p>When drones navigate in complex indoor environments, they usually rely on some form of perception (e.g., SLAM) to avoid colliding with walls and obstacles. However, many conditions may impair perception algorithms, including smoke, glare, dirty camera lenses, etc. We propose to use the expanding scissor structure as a guiding mechanism in collision-tolerant navigation, similar to the whiskers of a cat, the use of white canes by people who are blind or visually impaired, or the bumper bar of wheeled robots like Roomba. When the drone is in a complex environment, it may expand in order to locate obstacles by bumping into them. We can further mount sensors into the structure to enhance this navigation and even provide haptic feedback to a teleoperator to indicate when the presence of nearby obstacles.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/pufferbot/video/collision-2.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-8-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-8-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-8-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-8-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/pufferbot/figure-8-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/pufferbot/figure-8-3.jpg\" /></a>\n  </div>\n</div>","dir":"content/output/projects","base":"pufferbot.json","ext":".json","sourceBase":"pufferbot.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/reactile.json":
/*!***********************************************!*\
  !*** ./content/output/projects/reactile.json ***!
  \***********************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, pages, doi, conference, pdf, video, embed, short-video, slide, acm-dl, github, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"reactile","name":"Reactile","description":"Programming Swarm User Interfaces through Direct Physical Manipulation","title":"Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation","authors":["Ryo Suzuki","Jun Kato","Mark D. Gross","Tom Yeh"],"year":2018,"booktitle":"In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18)","publisher":"ACM, New York, NY, USA","pages":"Paper 199, 13 pages","doi":"https://doi.org/10.1145/3173574.3173773","conference":{"name":"CHI 2018","fullname":"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2018)","url":"https://chi2018.acm.org/"},"pdf":"chi-2018-reactile.pdf","video":"https://www.youtube.com/watch?v=Gb7brajKCVE","embed":"https://www.youtube.com/embed/Gb7brajKCVE","short-video":"https://www.youtube.com/watch?v=YT7vMJZjohU","slide":"chi-2018-reactile-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3173773","github":"https://github.com/ryosuzuki/reactile","pageCount":12,"slideCount":56,"bodyContent":"# Abstract\n\nWe explore a new approach to programming swarm user interfaces (Swarm UI) by leveraging direct physical manipulation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high- level interface design. Inspired by current UI programming practices, we introduce a four-step workflowcreate elements, abstract attributes, specify behaviors, and propagate changesfor Swarm UI programming. We propose a set of direct physi- cal manipulation techniques to support each step in this work- flow. To demonstrate these concepts, we developed Reac- tile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studiesan in-class survey with 148 students and a lab interview with eight participantsconfirm that our approach is intuitive and understandable for programming Swarm UIs.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/reactile/top.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-1-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-1-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-1-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-1-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-1-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-1-3.png\" /></a>\n  </div>\n</div>\n\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-2-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-2-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-2-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-2-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-2-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-2-3.png\" /></a>\n  </div>\n</div>\n\n\n# Introduction\n\nIn recent years, **Swarm User Interfaces** have emerged as a new paradigm of human-computer interaction. While the idea of coordinated miniature robots was originally proposed in the literature of swarm and micro-robotic systems, HCI researchers have explored the use of these robots as a user interface.\nHowever, this opportunity is currently limited to highly skilled programmers who are proficient in robot programming. For typical programmers inexperienced in robot programming who wish to build a Swarm UI application, it is unclear if the robot programming approach is the most appropriate for UI programming. To design interactive UI applications, pro- grammers often must think in terms of higher-level design for user interaction, whereas robot programming tends to focus on low-level controls of sensors and actuators. Historically, a novel UI platform is adopted only after the advent of an effective programming tool that empowers a larger developer community, and even end-users, to create many applications for the platform; for example, HyperCard for interactive hyper- media, Phidgets for physical interfaces, and Interface Builder for GUI applications. We stipulate that current approaches to programming Swarm UI are too robot-centric to be effec- tive for building rich and interactive applications. Then, what would be a better alternative?\n\n\n# Reactile\n\nThis paper introduces Reactile, a programming environment for Swarm UI applications.\nThe goal of Reactile is to explore a new approach to programming Swarm UI applications. To design an appropriate workflow for Swarm UI programming, we look into existing UI programming paradigm for inspiration. The common workflow of UI programming can be decomposed into four basic steps: create elements, abstract attributes, specify behaviors, and propagate changes. Based on these insights, we propose the following four-step workflow for Swarm UI programming: 1) creates shapes, 2) abstracts shape attributes as variables, 3) specifies data-bindings be- tween dynamic attributes, and 4) the system changes shapes in response to user inputs. With this workflow, a programmer can think in terms of high-level interface and interaction design to build interactive Swarm UI appli- cations, compared to existing, low-level, robot programming approaches.\n\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-3.png\" /></a>\n  </div>\n</div>\n\nThe workflow of Swarm UI programming is inspired by the existing UI programming paradigm. We first review the common workflow of UI programming and decompose it into four basic elements that represent high-level steps. Then we discuss how to apply this workflow to Swarm UI programming.\nAs we see in well-known design patterns for interactive UI ap- plications such as reactive programming paradigm, the Model-View-Controller, and the observer pattern, they share a com- mon workflow consisting of four basic elements: 1) create elements, 2) abstract attributes, 3) specify behaviors, and 4) propagate changes.\n\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-1-4.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-1-4.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-1-5.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-1-5.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-1-6.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-1-6.png\" /></a>\n  </div>\n</div>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-2-4.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-2-4.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-2-5.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-2-5.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-2-6.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-2-6.png\" /></a>\n  </div>\n</div>\n\n# Implementation\n\nReactile actuates a swarm of small magnetic markers to move on a 2D canvas with electromagnetic force. We designed and fabricated a board of electromagnetic coil arrays (3,200 coils), which covers an 80 cm x 40 cm area. Reactile tracks the marker positions and detects interactions between a user and swarm markers using a standard RGB camera and computer vision techniques. The system displays spatial information using a DLP projector to allow a programmer to see program states in the same physical context.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/reactile/coil.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<br />\n\nIn Reactile, a user interface consists of a swarm of passive magnetic markers which move on a 2D workspace driven by electromagnetic forces. Reactile uses a grid of electromagnetic coils to actuate these magnetic markers. Running current through the circuit coils generates a local magnetic field so that each coil can attract a single magnet located within its area. Each coil is aligned with a certain offset in both horizontal and vertical direction with an effective area overlap, which allows the coil to attract the magnet located in the adjacent coil. We design electromagnetic coil arrays to be fabricated with a standard printed circuit board (PCB) manufacturing. This reduces the cost and fabrication complexity, making it easy for the actuation area to scale up.\n\nOur PCB design is a 4-layer board, and each layer contains a set of coils, each of which has an identical circular shape with a 15 mm diameter and a 2.5 mm overlap between nearby coils. Each coil has 15 turns with 0.203 mm (8 mils) spacing between lines, and the distance between centers of two coils is approximately 10 mm, which makes a 10 mm grid for attractive points. The final prototype covers an 80 cm x 40 cm area with 80 x 40 coils by aligning five identical boards horizontally. The fabrication of each board costs approximately $80 USD, including manufacturing of PCB and electronic components.\n\n<br />\n\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/reactile/mechanism.mp4\" type=\"video/mp4\"></source>\n</video>","bodyHtml":"<h1>Abstract</h1>\n<p>We explore a new approach to programming swarm user interfaces (Swarm UI) by leveraging direct physical manipulation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high- level interface design. Inspired by current UI programming practices, we introduce a four-step workflowcreate elements, abstract attributes, specify behaviors, and propagate changesfor Swarm UI programming. We propose a set of direct physi- cal manipulation techniques to support each step in this work- flow. To demonstrate these concepts, we developed Reac- tile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studiesan in-class survey with 148 students and a lab interview with eight participantsconfirm that our approach is intuitive and understandable for programming Swarm UIs.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/reactile/top.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-1-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-1-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-1-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-1-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-1-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-1-3.png\" /></a>\n  </div>\n</div>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-2-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-2-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-2-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-2-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-2-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-2-3.png\" /></a>\n  </div>\n</div>\n<h1>Introduction</h1>\n<p>In recent years, <strong>Swarm User Interfaces</strong> have emerged as a new paradigm of human-computer interaction. While the idea of coordinated miniature robots was originally proposed in the literature of swarm and micro-robotic systems, HCI researchers have explored the use of these robots as a user interface.\nHowever, this opportunity is currently limited to highly skilled programmers who are proficient in robot programming. For typical programmers inexperienced in robot programming who wish to build a Swarm UI application, it is unclear if the robot programming approach is the most appropriate for UI programming. To design interactive UI applications, pro- grammers often must think in terms of higher-level design for user interaction, whereas robot programming tends to focus on low-level controls of sensors and actuators. Historically, a novel UI platform is adopted only after the advent of an effective programming tool that empowers a larger developer community, and even end-users, to create many applications for the platform; for example, HyperCard for interactive hyper- media, Phidgets for physical interfaces, and Interface Builder for GUI applications. We stipulate that current approaches to programming Swarm UI are too robot-centric to be effec- tive for building rich and interactive applications. Then, what would be a better alternative?</p>\n<h1>Reactile</h1>\n<p>This paper introduces Reactile, a programming environment for Swarm UI applications.\nThe goal of Reactile is to explore a new approach to programming Swarm UI applications. To design an appropriate workflow for Swarm UI programming, we look into existing UI programming paradigm for inspiration. The common workflow of UI programming can be decomposed into four basic steps: create elements, abstract attributes, specify behaviors, and propagate changes. Based on these insights, we propose the following four-step workflow for Swarm UI programming: 1) creates shapes, 2) abstracts shape attributes as variables, 3) specifies data-bindings be- tween dynamic attributes, and 4) the system changes shapes in response to user inputs. With this workflow, a programmer can think in terms of high-level interface and interaction design to build interactive Swarm UI appli- cations, compared to existing, low-level, robot programming approaches.</p>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-3.png\" /></a>\n  </div>\n</div>\n<p>The workflow of Swarm UI programming is inspired by the existing UI programming paradigm. We first review the common workflow of UI programming and decompose it into four basic elements that represent high-level steps. Then we discuss how to apply this workflow to Swarm UI programming.\nAs we see in well-known design patterns for interactive UI ap- plications such as reactive programming paradigm, the Model-View-Controller, and the observer pattern, they share a com- mon workflow consisting of four basic elements: 1) create elements, 2) abstract attributes, 3) specify behaviors, and 4) propagate changes.</p>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-1-4.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-1-4.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-1-5.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-1-5.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-1-6.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-1-6.png\" /></a>\n  </div>\n</div>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-2-4.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-2-4.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-2-5.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-2-5.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/reactile/figure-2-6.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/reactile/figure-2-6.png\" /></a>\n  </div>\n</div>\n<h1>Implementation</h1>\n<p>Reactile actuates a swarm of small magnetic markers to move on a 2D canvas with electromagnetic force. We designed and fabricated a board of electromagnetic coil arrays (3,200 coils), which covers an 80 cm x 40 cm area. Reactile tracks the marker positions and detects interactions between a user and swarm markers using a standard RGB camera and computer vision techniques. The system displays spatial information using a DLP projector to allow a programmer to see program states in the same physical context.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/reactile/coil.mp4\" type=\"video/mp4\"></source>\n</video>\n<br />\n<p>In Reactile, a user interface consists of a swarm of passive magnetic markers which move on a 2D workspace driven by electromagnetic forces. Reactile uses a grid of electromagnetic coils to actuate these magnetic markers. Running current through the circuit coils generates a local magnetic field so that each coil can attract a single magnet located within its area. Each coil is aligned with a certain offset in both horizontal and vertical direction with an effective area overlap, which allows the coil to attract the magnet located in the adjacent coil. We design electromagnetic coil arrays to be fabricated with a standard printed circuit board (PCB) manufacturing. This reduces the cost and fabrication complexity, making it easy for the actuation area to scale up.</p>\n<p>Our PCB design is a 4-layer board, and each layer contains a set of coils, each of which has an identical circular shape with a 15 mm diameter and a 2.5 mm overlap between nearby coils. Each coil has 15 turns with 0.203 mm (8 mils) spacing between lines, and the distance between centers of two coils is approximately 10 mm, which makes a 10 mm grid for attractive points. The final prototype covers an 80 cm x 40 cm area with 80 x 40 coils by aligning five identical boards horizontally. The fabrication of each board costs approximately $80 USD, including manufacturing of PCB and electronic components.</p>\n<br />\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/reactile/mechanism.mp4\" type=\"video/mp4\"></source>\n</video>","dir":"content/output/projects","base":"reactile.json","ext":".json","sourceBase":"reactile.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/realitysketch.json":
/*!****************************************************!*\
  !*** ./content/output/projects/realitysketch.json ***!
  \****************************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, pages, doi, conference, pdf, video, embed, arxiv, acm-dl, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"realitysketch","name":"RealitySketch","description":"Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching","title":"RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching","authors":["Ryo Suzuki","Rubaiat Habib Kazi","Li-Yi Wei","Stephen DiVerdi","Wilmot Li","Daniel Leithinger"],"year":2020,"booktitle":"In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology (UIST '20)","publisher":"ACM, New York, NY, USA","pages":"1-16","doi":"https://doi.org/10.1145/3379337.3415892","conference":{"name":"UIST 2020","fullname":"The ACM Symposium on User Interface Software and Technology (UIST 2020) - Honorable Mention Award","url":"https://uist.acm.org/uist2020/"},"pdf":"uist-2020-realitysketch.pdf","video":"https://www.youtube.com/watch?v=L0p-BNU9rXU","embed":"https://www.youtube.com/embed/L0p-BNU9rXU","arxiv":"https://arxiv.org/abs/2008.08688","acm-dl":"https://dl.acm.org/doi/10.1145/3379337.3415892","pageCount":16,"slideCount":0,"bodyContent":"# Abstract\n\nWe present RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, floating in mid-air without responding to the real world. This paper introduces a new way to embed *dynamic* and *responsive* graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-defined programs and configurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/top.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-1-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-1-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-1-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-1-3.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-1-4.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-1-4.jpg\" /></a>\n  </div>\n</div>\n\n\n# Introduction\n\nOver the last decades, interactive and dynamic sketching has been one of the central themes in human-computer interaction (HCI) research. Since Sutherland first demonstrated the power of interactive sketching for computer-aided design, HCI researchers have explored ways to sketch dynamic contents that can interactively respond to user input, thus enabling us to think and communicate through a dynamic visual medium. The applications of such tools are vast, including mathematics and physics education, animated art creation, and interactive data visualization. Through these research outcomes, we have now developed a rich vocabulary of dynamic sketching techniques to fluidly create interactive, animated content in real-time.\n\nWith the advent of augmented and mixed reality interfaces, we now have a unique opportunity to expand such practices beyond screen-based interactions towards reality-based interactions. In fact, there is an increasing number of tools that provide sketching interfaces for augmented reality, from commercial products like Just a Line, Vuforia Chalk, and DoodleLens to research projects like SymbiosisSketch and Mobi3DSketch. These tools allow users to sketch digital elements and embed them in the real world. However, a key limitation is that the sketched content is *static* --- it does not respond, change, and animate based on user actions or real-world phenomena.\n\n\nWhat if, instead, these sketched elements could *dynamically respond* when the real world changes? For example, imagine a line sketched onto a physical pendulum that moves as the pendulum swings. This capability would allow us to directly manipulate the sketch through tangible interactions or capture and visualize the pendulum motion to understand the underlying phenomenon.\n\nAs a first step toward this goal, we present **RealitySketch, an end-user sketching tool to support real-time creation and sharing of embedded interactive graphics and visualization in AR**. To create graphics, the user sketches on a phone or tablet screen, which embeds interactive visualizations into a scene. The sketched elements can be bound to physical objects such that they respond dynamically as the real-world changes.\n\n# RealitySketch: How it works\n\n## Design Space and Basic Setup\n\nThe goal of this paper is to provide a way to embed dynamic and responsive graphics through dynamic sketching. To better understand the scope, we first define the terminology:\n\n1. **Embedded**: graphics are presented and *spatially integrated within the real-world*\n\n\n2. **Responsive**: *graphics change and animate* based on the users interactions\n\n\nThe important aspect of embedded and responsive graphics is that graphics must interact with the real-world. Here, the real- world means either the environment itself, a physical phenomenon, or a users tangible and gestural interactions.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/sketch.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<br/>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-3-4.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-3-4.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-3-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-3-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-3-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-3-2.jpg\" /></a>\n  </div>\n</div>\n\nTo enable that, RealitySketch leverages mobile augmented reality (ARKit) to embed sketches in the real world. The user sketches with a finger or a pen on a touchscreen, where the sketched elements are overlaid onto a camera view of the real world.\n\nOur approach proposes the following four-step workflow:\n\n1. **Object tracking**: the user specifies a visual entity (e.g., a physical object, a skeletal joint) to track in the real-world scene\n\n\n2. **Parameterization**: the user parameterizes tracked entities by drawing lines or arcs that define specific variables of interest\n\n\n3. **Parameter binding**: the user binds these variables to the graphical properties of sketched elements (e.g., length, angle, etc.) to define their dynamic behavior as the real-world variables change\n\n\n4. **Visualization**: the user can also interact, analyze, and visualize real-world movements through several visualization effects.\n\nWe contextualize our approach in the larger design space of dynamic graphics authoring approaches.\nThe main contribution of this paper is a set of interaction techniques that enable these steps without *pre-defined programs* and configurations --- rather, the system lets the user perform in real-time and improvisational ways.\n\n\n\n## Step 1: Track an Object\n\nFor embedded and responsive graphics, the graphical elements need to be tightly coupled with physical objects and environments. Thus, capturing and tracking an object is vital to make the graphics dynamically change and move.\nTo specify an object, the user enters the selection mode and then taps an object on the screen. Once selected, our system highlights the selected object with a white contour line and starts tracking the object in the 3D scene.\n\nIn our current implementation, the system tracks objects based on color tracking implemented with OpenCV. When the user taps an object on the screen, the algorithm gets the HSV value at the x and y position. Then the system captures similar colors at each frame based on a certain upper and lower threshold range. This color tracking was fast enough for most of our applications (e.g., 20-30 FPS with iPad Pro 11 inch 2018).\n\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/tracking.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-4-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-4-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-4-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-4-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-4-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-4-3.jpg\" /></a>\n  </div>\n</div>\n\n\n## Step 2: Sketch to Parameterize\n\nNext, the user parameterizes the real world to define and capture the dynamic value of interest. In this paper, we specifically focus on parameterization that can be done through simple sketching interactions using line segments.\n\nFirst, when entering the sketching mode, the user can start drawing a line segment onto the scene. All the sketched lines are projected onto a 2D surface within the 3D scene. The system takes the two end-points to create the line segment. This creates a variable that defines the distance between two points on the surface. To create a dynamic line segment, the user draws a line whose endpoint is attached to the selected object. As one end of this dynamic line segment is bound to the selected object, if the user moves the object in the real world, the line segment and its parameter (e.g., distance value) will change accordingly. The system visually renders the line segment values using labels.\n\nRealitySketch employs simple heuristics to determine the nature (e.g., static vs. dynamic, distance vs angle, free move vs constraints, etc) of the line segment. If the start or end point of the line segment is close to an existing tracked object, then the system binds the end point to the tracked object. Thus, for example, if the user draws a line between two tracked objects, then both ends attach to an object. In that case, the line segment captures the distance between those two objects.\n\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/sketch.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-5-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-5-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-5-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-5-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-5-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-5-3.jpg\" /></a>\n  </div>\n</div>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-6-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-6-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-6-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-6-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-6-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-6-3.jpg\" /></a>\n  </div>\n</div>\n\n## Step 3: Bind Parameters to Make Them Responsive\n\nTo make the existing line segments responsive, the user can bind variables between two elements. The parameter of a static line segment can be bound to another variable. For example, suppose the user has a variable named angle for a dynamic line segment. When the user taps the label of another angle of the static line segment, the system shows a popup to let the user enter the variable name. If the entered variable name matches an existing name, the angle of the static line segment will be dynamically bound to the existing parameter\n\n\nSimilarly, the user can define a constraint by typing a function of an existing variable. For example, consider the user wants to draw the bisector of the angle formed by a dynamic line segment. The user can first draw a line and an arc between the line and the base line.\n\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/bind.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-7-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-7-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-7-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-7-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-7-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-7-3.jpg\" /></a>\n  </div>\n</div>\n\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-8-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-8-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-8-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-8-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-8-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-8-3.jpg\" /></a>\n  </div>\n</div>\n\n## Step 4: Move and Animate\n\nRather than 2D sketches that are based on the device screen coordinates, all sketched elements have a 3D geometry and position in real world coordinates, anchored in 3D space. This way, the user can move the device to see from a different perspective and the sketches stay correctly anchored to the real objects.\n\nTo enable this functionality, our system leverages ARKit and SceneKit for both surface detection and object placement in the 3D scene.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/multi-angle.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-9-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-9-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-9-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-9-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-9-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-9-3.jpg\" /></a>\n  </div>\n</div>\n\n\n## Step 5: Record and Visualize\n\nRealitySketch can also make responsive visualization based on graph plotting of a parameter. In the graph placing mode, the user can place a 2D graph and change its size by dragging and dropping onto the surface. By default, the x-axis of the graph is time. By binding an existing variable to the graph, it starts visualizing the time series data of the variable.\n\nTo bind the variable, the user can simply tap a label of the graph, and then, enter the variable the user-defined. For example, if the user binds the angle variable of the pendulum to the y-axis of the chart, the graph will dynamically plot the angle of the pendulum when it starts swinging. By adding multiple variables separated with a comma (e.g., a,b,c), the user can also plot multiple parameters in the same graph. The user can also change the x-axis from time to a variable by tapping the x-axis and entering a second variable. For example, the user can define an angle and y distance of a point of a circle. By binding x-axis as the angle and y-axis as the perpendicular length, the system can plot the relationship between the angle and corresponding sine value.\n\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/visualizations.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-16-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-16-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-16-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-16-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-16-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-16-3.jpg\" /></a>\n  </div>\n</div>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-10-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-10-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-10-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-10-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-10-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-10-3.jpg\" /></a>\n  </div>\n</div>\n\n\n<br/>\n<div class=\"ui divider\"></div>\n\n\n# Application Scenarios\n\nWe have explored and demonstrated the following application scenarios\n\n1. **Augmented Physics Experiments**\n\n\n2. **Interactive and Explorable Concept Explanation**\n\n\n3. **Improvised Visualization for Sports and Exercises**\n\n\n4. **In-situ Tangible User Interfaces**\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-2.png\" /></a>\n  </div>\n</div>\n\n<br/>\n\n\n## Application 1: Augmented Physics Experiments\n\nIn science education, a classroom experiment is an integral part of learning physics because the real-world experiment provides students an opportunity to connect their knowledge with physical phenomena. RealitySketch can become a powerful tool to support these experiments by leveraging real-time visualization capability. Figure 15 illustrates how our tool can help students understand the pulley system. In this scenario, by tracking the movement of two points (i.e., the point of the person grabs and the point of the load), the students can visualize the traveling distance of each point. In this way, they can see the load distance movement is shorter than the distance the person pulls.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/physics.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-17-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-17-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-17-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-17-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-17-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-17-3.jpg\" /></a>\n  </div>\n</div>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-19-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-19-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-19-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-19-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-19-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-19-3.jpg\" /></a>\n  </div>\n</div>\n\n<br/>\n\n## Application 2: Interactive and Explorable Concept Explanation\n\nRealitySketch is also useful to help teachers explain concepts that may be difficult to understand with static graphs, and to let students explore them through tangible interactions. Some examples are shown in the above Figure (Top: demonstrating an area of a triangle remains the same with horizontal movement; a bisector line of a triangle always intersect at the middle point. Bottom: showing how a sine curve is generated from plotting the angle and perpendicular distance of a rotating point.)\n\n\nFor educational applications, we can think of the following three setups of how students and teachers can use our tool:\n\n1. **Classroom presentation**: In this case, a teacher or an assistant sketches and visualizes the motion, which can be shared through a connected large display or projector, so that the students can see and understand.\n\n2. **Collaborative learning**: In this case, students can form a group and interact with their own devices. Since mobile AR is accessible for almost all smartphones, every student should be able to play by themselves, which can lead to an interesting exploration and discoveries.\n\n3. **Remote learning**: In this case, a teacher only records the video of the experiment, then share the recorded video with the students. The student can download and visualize with their own app, so that it provides a fun and interactive experiment to support online and remote learning.\n\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/concept.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-18-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-18-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-18-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-18-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-18-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-18-3.jpg\" /></a>\n  </div>\n</div>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-20-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-20-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-20-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-20-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-20-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-20-3.jpg\" /></a>\n  </div>\n</div>\n\n<br/>\n\n\n## Application 3: Improvised Visualization for Sports and Exercises\n\nRealitySketch could be also useful to analyze and visualize the motion for sports training and exercises because they often employ the physical movements. For example, sports practices, like a golf shot, baseball pitching, and basketball shooting, would be an interesting example to visualize the trajectory of the ball. Similar to the previous example of showing the trajectory of a ball, it is useful to quickly see the path through stroboscopic effects. In addition to showing the trajectory, the system can also capture and compare multiple attempts to let the user analyze what works and what does not.\n\nAlso, many sports and exercises may require proper form and posture. For example, in baseball batting, golf shot, and a tennis swing, a players form, such as a body angle, can be important, which the tool can help visualize through sketched guided lines. These features could be particularly useful for exercise instructions. For example, in yoga practice, bike riding, or weight lifting training, there are recommended angles to be followed to maximize the performance benefits. In these cases, the improvisational measurement of the posture can help the user to see and check if correctly done.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/sports.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-14-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-14-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-14-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-14-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-14-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-14-3.jpg\" /></a>\n  </div>\n</div>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-15-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-15-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-15-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-15-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-15-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-15-3.jpg\" /></a>\n  </div>\n</div>\n\n\n<br/>\n\n## Application 4: In-situ Tangible User Interfaces\nThe parameterized value can be used for many different pur- poses to enable responsive visual outputs. So far, we have mostly focused on animation of the simple basic geometry (e.g., line segments, arc) or build-in visualization outputs (e.g., graph plot). However, the dynamic parameter value can be also used for other outputs via binding, as we discussed in the previous sections (e.g., pre-defined vs user-defined section).\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/dino.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-11-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-11-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-11-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-11-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-11-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-11-3.jpg\" /></a>\n  </div>\n</div>\n\n\nOne promising application domain of this is to use these dynamic parameter values as an input of changing a property of existing virtual objects. For example, if one can connect a parameter value to a size property of a virtual 3D model, then the size of the model can dynamically change when the value changes. This use case is particularly useful for in-situ creation of tangible controllers. For example, a colored token can become a tangible slider to change the size of the object. The system could bind these values based on a proper naming rule\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/shark.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-12-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-12-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-12-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-12-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-12-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-12-3.jpg\" /></a>\n  </div>\n</div>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-13-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-13-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-13-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-13-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-13-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-13-3.jpg\" /></a>\n  </div>\n</div>\n\n<br/>\n\n# Future Vision\n\nWe believe there are a lot of future work and a broad design space of embedded and responsive sketching.\nIn general, crafting responsive and embedded graphics in the real-world can be a continuum between two approaches: pre-defined behavior and user-defined behavior.\nFor example, pre-defined behavior refers to a behavior specification given in advance. For example, one can think of a system that specifies all of the sketched elements to follow the law of physics, so that as long as a user draws a virtual element, it automatically falls and bounces on the ground. In this case, the behavior of sketched elements is pre-defined, based on the physics simulation, and the user can only control the shape of the sketches. Similarly, one can imagine a sketched character that starts walking around or interacting with the physical environment. In this case, the behavior of the sketched character should also be defined in advance (by programming or design tools), as it is hard to specify such complex behaviors in real-time.\nThese practices are often utilized in the screen-based sketching interfaces. For example, PhysInk uses a physics engine and ChalkTalk leverages pre-programmed behavior to animate the sketched elements in real-time.\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-22.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-22.jpg\" /></a>\n  </div>\n</div>\n\nOn the other end of the spectrum, user-defined behavior lets the user decide how the sketched elements move, behave, and animate on the fly. For example, consider an example of visualizing pendulum motion. In this example, the user should be able to specify how and which parameter (e.g., angle) will be visualized. In the previous works, Apparatus leverages the user-defined behavior to create interactive diagrams. In this example, the user has full control of how it behaves, based on the user-defined constraints and parameter bindings, which is also known as constraint-based programming. These practices are also utilized to create interactive 2D animation, design exploration, and dynamic data visualization, as it is useful to let the user explicitly specify how it behaves.\n\n\nWe envision the future where the user can draw these dynamic pictures by sketching, like *Harold's purple crayon*.\nWe hope this paper opens up new opportunities for embedded and responsive sketching and inspires the HCI community to further explore such interactions to realize the full potential of augmented- and mixed-reality (AR/MR) as a dynamic, computational medium.\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-21-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-21-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-21-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-21-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-21-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-21-3.jpg\" /></a>\n  </div>\n  <p style=\"width: 100%; text-align: center\">\n    Image Credit: <a href=\"https://en.wikipedia.org/wiki/Harold_and_the_Purple_Crayon\" target=\"_blank\">Harold and the Purple Crayon</a>\n  </p>\n</div>","bodyHtml":"<h1>Abstract</h1>\n<p>We present RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, floating in mid-air without responding to the real world. This paper introduces a new way to embed <em>dynamic</em> and <em>responsive</em> graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-defined programs and configurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/top.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-1-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-1-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-1-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-1-3.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-1-4.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-1-4.jpg\" /></a>\n  </div>\n</div>\n<h1>Introduction</h1>\n<p>Over the last decades, interactive and dynamic sketching has been one of the central themes in human-computer interaction (HCI) research. Since Sutherland first demonstrated the power of interactive sketching for computer-aided design, HCI researchers have explored ways to sketch dynamic contents that can interactively respond to user input, thus enabling us to think and communicate through a dynamic visual medium. The applications of such tools are vast, including mathematics and physics education, animated art creation, and interactive data visualization. Through these research outcomes, we have now developed a rich vocabulary of dynamic sketching techniques to fluidly create interactive, animated content in real-time.</p>\n<p>With the advent of augmented and mixed reality interfaces, we now have a unique opportunity to expand such practices beyond screen-based interactions towards reality-based interactions. In fact, there is an increasing number of tools that provide sketching interfaces for augmented reality, from commercial products like Just a Line, Vuforia Chalk, and DoodleLens to research projects like SymbiosisSketch and Mobi3DSketch. These tools allow users to sketch digital elements and embed them in the real world. However, a key limitation is that the sketched content is <em>static</em> --- it does not respond, change, and animate based on user actions or real-world phenomena.</p>\n<p>What if, instead, these sketched elements could <em>dynamically respond</em> when the real world changes? For example, imagine a line sketched onto a physical pendulum that moves as the pendulum swings. This capability would allow us to directly manipulate the sketch through tangible interactions or capture and visualize the pendulum motion to understand the underlying phenomenon.</p>\n<p>As a first step toward this goal, we present <strong>RealitySketch, an end-user sketching tool to support real-time creation and sharing of embedded interactive graphics and visualization in AR</strong>. To create graphics, the user sketches on a phone or tablet screen, which embeds interactive visualizations into a scene. The sketched elements can be bound to physical objects such that they respond dynamically as the real-world changes.</p>\n<h1>RealitySketch: How it works</h1>\n<h2>Design Space and Basic Setup</h2>\n<p>The goal of this paper is to provide a way to embed dynamic and responsive graphics through dynamic sketching. To better understand the scope, we first define the terminology:</p>\n<ol>\n<li>\n<p><strong>Embedded</strong>: graphics are presented and <em>spatially integrated within the real-world</em></p>\n</li>\n<li>\n<p><strong>Responsive</strong>: <em>graphics change and animate</em> based on the users interactions</p>\n</li>\n</ol>\n<p>The important aspect of embedded and responsive graphics is that graphics must interact with the real-world. Here, the real- world means either the environment itself, a physical phenomenon, or a users tangible and gestural interactions.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/sketch.mp4\" type=\"video/mp4\"></source>\n</video>\n<br/>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-3-4.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-3-4.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-3-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-3-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-3-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-3-2.jpg\" /></a>\n  </div>\n</div>\n<p>To enable that, RealitySketch leverages mobile augmented reality (ARKit) to embed sketches in the real world. The user sketches with a finger or a pen on a touchscreen, where the sketched elements are overlaid onto a camera view of the real world.</p>\n<p>Our approach proposes the following four-step workflow:</p>\n<ol>\n<li>\n<p><strong>Object tracking</strong>: the user specifies a visual entity (e.g., a physical object, a skeletal joint) to track in the real-world scene</p>\n</li>\n<li>\n<p><strong>Parameterization</strong>: the user parameterizes tracked entities by drawing lines or arcs that define specific variables of interest</p>\n</li>\n<li>\n<p><strong>Parameter binding</strong>: the user binds these variables to the graphical properties of sketched elements (e.g., length, angle, etc.) to define their dynamic behavior as the real-world variables change</p>\n</li>\n<li>\n<p><strong>Visualization</strong>: the user can also interact, analyze, and visualize real-world movements through several visualization effects.</p>\n</li>\n</ol>\n<p>We contextualize our approach in the larger design space of dynamic graphics authoring approaches.\nThe main contribution of this paper is a set of interaction techniques that enable these steps without <em>pre-defined programs</em> and configurations --- rather, the system lets the user perform in real-time and improvisational ways.</p>\n<h2>Step 1: Track an Object</h2>\n<p>For embedded and responsive graphics, the graphical elements need to be tightly coupled with physical objects and environments. Thus, capturing and tracking an object is vital to make the graphics dynamically change and move.\nTo specify an object, the user enters the selection mode and then taps an object on the screen. Once selected, our system highlights the selected object with a white contour line and starts tracking the object in the 3D scene.</p>\n<p>In our current implementation, the system tracks objects based on color tracking implemented with OpenCV. When the user taps an object on the screen, the algorithm gets the HSV value at the x and y position. Then the system captures similar colors at each frame based on a certain upper and lower threshold range. This color tracking was fast enough for most of our applications (e.g., 20-30 FPS with iPad Pro 11 inch 2018).</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/tracking.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-4-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-4-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-4-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-4-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-4-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-4-3.jpg\" /></a>\n  </div>\n</div>\n<h2>Step 2: Sketch to Parameterize</h2>\n<p>Next, the user parameterizes the real world to define and capture the dynamic value of interest. In this paper, we specifically focus on parameterization that can be done through simple sketching interactions using line segments.</p>\n<p>First, when entering the sketching mode, the user can start drawing a line segment onto the scene. All the sketched lines are projected onto a 2D surface within the 3D scene. The system takes the two end-points to create the line segment. This creates a variable that defines the distance between two points on the surface. To create a dynamic line segment, the user draws a line whose endpoint is attached to the selected object. As one end of this dynamic line segment is bound to the selected object, if the user moves the object in the real world, the line segment and its parameter (e.g., distance value) will change accordingly. The system visually renders the line segment values using labels.</p>\n<p>RealitySketch employs simple heuristics to determine the nature (e.g., static vs. dynamic, distance vs angle, free move vs constraints, etc) of the line segment. If the start or end point of the line segment is close to an existing tracked object, then the system binds the end point to the tracked object. Thus, for example, if the user draws a line between two tracked objects, then both ends attach to an object. In that case, the line segment captures the distance between those two objects.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/sketch.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-5-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-5-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-5-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-5-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-5-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-5-3.jpg\" /></a>\n  </div>\n</div>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-6-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-6-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-6-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-6-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-6-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-6-3.jpg\" /></a>\n  </div>\n</div>\n<h2>Step 3: Bind Parameters to Make Them Responsive</h2>\n<p>To make the existing line segments responsive, the user can bind variables between two elements. The parameter of a static line segment can be bound to another variable. For example, suppose the user has a variable named angle for a dynamic line segment. When the user taps the label of another angle of the static line segment, the system shows a popup to let the user enter the variable name. If the entered variable name matches an existing name, the angle of the static line segment will be dynamically bound to the existing parameter</p>\n<p>Similarly, the user can define a constraint by typing a function of an existing variable. For example, consider the user wants to draw the bisector of the angle formed by a dynamic line segment. The user can first draw a line and an arc between the line and the base line.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/bind.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-7-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-7-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-7-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-7-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-7-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-7-3.jpg\" /></a>\n  </div>\n</div>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-8-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-8-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-8-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-8-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-8-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-8-3.jpg\" /></a>\n  </div>\n</div>\n<h2>Step 4: Move and Animate</h2>\n<p>Rather than 2D sketches that are based on the device screen coordinates, all sketched elements have a 3D geometry and position in real world coordinates, anchored in 3D space. This way, the user can move the device to see from a different perspective and the sketches stay correctly anchored to the real objects.</p>\n<p>To enable this functionality, our system leverages ARKit and SceneKit for both surface detection and object placement in the 3D scene.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/multi-angle.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-9-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-9-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-9-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-9-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-9-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-9-3.jpg\" /></a>\n  </div>\n</div>\n<h2>Step 5: Record and Visualize</h2>\n<p>RealitySketch can also make responsive visualization based on graph plotting of a parameter. In the graph placing mode, the user can place a 2D graph and change its size by dragging and dropping onto the surface. By default, the x-axis of the graph is time. By binding an existing variable to the graph, it starts visualizing the time series data of the variable.</p>\n<p>To bind the variable, the user can simply tap a label of the graph, and then, enter the variable the user-defined. For example, if the user binds the angle variable of the pendulum to the y-axis of the chart, the graph will dynamically plot the angle of the pendulum when it starts swinging. By adding multiple variables separated with a comma (e.g., a,b,c), the user can also plot multiple parameters in the same graph. The user can also change the x-axis from time to a variable by tapping the x-axis and entering a second variable. For example, the user can define an angle and y distance of a point of a circle. By binding x-axis as the angle and y-axis as the perpendicular length, the system can plot the relationship between the angle and corresponding sine value.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/visualizations.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-16-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-16-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-16-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-16-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-16-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-16-3.jpg\" /></a>\n  </div>\n</div>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-10-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-10-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-10-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-10-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-10-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-10-3.jpg\" /></a>\n  </div>\n</div>\n<br/>\n<div class=\"ui divider\"></div>\n<h1>Application Scenarios</h1>\n<p>We have explored and demonstrated the following application scenarios</p>\n<ol>\n<li>\n<p><strong>Augmented Physics Experiments</strong></p>\n</li>\n<li>\n<p><strong>Interactive and Explorable Concept Explanation</strong></p>\n</li>\n<li>\n<p><strong>Improvised Visualization for Sports and Exercises</strong></p>\n</li>\n<li>\n<p><strong>In-situ Tangible User Interfaces</strong></p>\n</li>\n</ol>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-2.png\" /></a>\n  </div>\n</div>\n<br/>\n<h2>Application 1: Augmented Physics Experiments</h2>\n<p>In science education, a classroom experiment is an integral part of learning physics because the real-world experiment provides students an opportunity to connect their knowledge with physical phenomena. RealitySketch can become a powerful tool to support these experiments by leveraging real-time visualization capability. Figure 15 illustrates how our tool can help students understand the pulley system. In this scenario, by tracking the movement of two points (i.e., the point of the person grabs and the point of the load), the students can visualize the traveling distance of each point. In this way, they can see the load distance movement is shorter than the distance the person pulls.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/physics.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-17-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-17-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-17-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-17-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-17-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-17-3.jpg\" /></a>\n  </div>\n</div>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-19-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-19-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-19-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-19-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-19-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-19-3.jpg\" /></a>\n  </div>\n</div>\n<br/>\n<h2>Application 2: Interactive and Explorable Concept Explanation</h2>\n<p>RealitySketch is also useful to help teachers explain concepts that may be difficult to understand with static graphs, and to let students explore them through tangible interactions. Some examples are shown in the above Figure (Top: demonstrating an area of a triangle remains the same with horizontal movement; a bisector line of a triangle always intersect at the middle point. Bottom: showing how a sine curve is generated from plotting the angle and perpendicular distance of a rotating point.)</p>\n<p>For educational applications, we can think of the following three setups of how students and teachers can use our tool:</p>\n<ol>\n<li>\n<p><strong>Classroom presentation</strong>: In this case, a teacher or an assistant sketches and visualizes the motion, which can be shared through a connected large display or projector, so that the students can see and understand.</p>\n</li>\n<li>\n<p><strong>Collaborative learning</strong>: In this case, students can form a group and interact with their own devices. Since mobile AR is accessible for almost all smartphones, every student should be able to play by themselves, which can lead to an interesting exploration and discoveries.</p>\n</li>\n<li>\n<p><strong>Remote learning</strong>: In this case, a teacher only records the video of the experiment, then share the recorded video with the students. The student can download and visualize with their own app, so that it provides a fun and interactive experiment to support online and remote learning.</p>\n</li>\n</ol>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/concept.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-18-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-18-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-18-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-18-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-18-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-18-3.jpg\" /></a>\n  </div>\n</div>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-20-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-20-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-20-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-20-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-20-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-20-3.jpg\" /></a>\n  </div>\n</div>\n<br/>\n<h2>Application 3: Improvised Visualization for Sports and Exercises</h2>\n<p>RealitySketch could be also useful to analyze and visualize the motion for sports training and exercises because they often employ the physical movements. For example, sports practices, like a golf shot, baseball pitching, and basketball shooting, would be an interesting example to visualize the trajectory of the ball. Similar to the previous example of showing the trajectory of a ball, it is useful to quickly see the path through stroboscopic effects. In addition to showing the trajectory, the system can also capture and compare multiple attempts to let the user analyze what works and what does not.</p>\n<p>Also, many sports and exercises may require proper form and posture. For example, in baseball batting, golf shot, and a tennis swing, a players form, such as a body angle, can be important, which the tool can help visualize through sketched guided lines. These features could be particularly useful for exercise instructions. For example, in yoga practice, bike riding, or weight lifting training, there are recommended angles to be followed to maximize the performance benefits. In these cases, the improvisational measurement of the posture can help the user to see and check if correctly done.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/sports.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-14-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-14-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-14-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-14-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-14-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-14-3.jpg\" /></a>\n  </div>\n</div>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-15-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-15-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-15-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-15-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-15-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-15-3.jpg\" /></a>\n  </div>\n</div>\n<br/>\n<h2>Application 4: In-situ Tangible User Interfaces</h2>\n<p>The parameterized value can be used for many different pur- poses to enable responsive visual outputs. So far, we have mostly focused on animation of the simple basic geometry (e.g., line segments, arc) or build-in visualization outputs (e.g., graph plot). However, the dynamic parameter value can be also used for other outputs via binding, as we discussed in the previous sections (e.g., pre-defined vs user-defined section).</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/dino.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-11-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-11-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-11-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-11-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-11-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-11-3.jpg\" /></a>\n  </div>\n</div>\n<p>One promising application domain of this is to use these dynamic parameter values as an input of changing a property of existing virtual objects. For example, if one can connect a parameter value to a size property of a virtual 3D model, then the size of the model can dynamically change when the value changes. This use case is particularly useful for in-situ creation of tangible controllers. For example, a colored token can become a tangible slider to change the size of the object. The system could bind these values based on a proper naming rule</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/realitysketch/video/shark.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-12-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-12-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-12-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-12-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-12-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-12-3.jpg\" /></a>\n  </div>\n</div>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-13-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-13-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-13-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-13-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-13-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-13-3.jpg\" /></a>\n  </div>\n</div>\n<br/>\n<h1>Future Vision</h1>\n<p>We believe there are a lot of future work and a broad design space of embedded and responsive sketching.\nIn general, crafting responsive and embedded graphics in the real-world can be a continuum between two approaches: pre-defined behavior and user-defined behavior.\nFor example, pre-defined behavior refers to a behavior specification given in advance. For example, one can think of a system that specifies all of the sketched elements to follow the law of physics, so that as long as a user draws a virtual element, it automatically falls and bounces on the ground. In this case, the behavior of sketched elements is pre-defined, based on the physics simulation, and the user can only control the shape of the sketches. Similarly, one can imagine a sketched character that starts walking around or interacting with the physical environment. In this case, the behavior of the sketched character should also be defined in advance (by programming or design tools), as it is hard to specify such complex behaviors in real-time.\nThese practices are often utilized in the screen-based sketching interfaces. For example, PhysInk uses a physics engine and ChalkTalk leverages pre-programmed behavior to animate the sketched elements in real-time.</p>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-22.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-22.jpg\" /></a>\n  </div>\n</div>\n<p>On the other end of the spectrum, user-defined behavior lets the user decide how the sketched elements move, behave, and animate on the fly. For example, consider an example of visualizing pendulum motion. In this example, the user should be able to specify how and which parameter (e.g., angle) will be visualized. In the previous works, Apparatus leverages the user-defined behavior to create interactive diagrams. In this example, the user has full control of how it behaves, based on the user-defined constraints and parameter bindings, which is also known as constraint-based programming. These practices are also utilized to create interactive 2D animation, design exploration, and dynamic data visualization, as it is useful to let the user explicitly specify how it behaves.</p>\n<p>We envision the future where the user can draw these dynamic pictures by sketching, like <em>Harold's purple crayon</em>.\nWe hope this paper opens up new opportunities for embedded and responsive sketching and inspires the HCI community to further explore such interactions to realize the full potential of augmented- and mixed-reality (AR/MR) as a dynamic, computational medium.</p>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-21-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-21-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-21-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-21-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/realitysketch/figure-21-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/realitysketch/figure-21-3.jpg\" /></a>\n  </div>\n  <p style=\"width: 100%; text-align: center\">\n    Image Credit: <a href=\"https://en.wikipedia.org/wiki/Harold_and_the_Purple_Crayon\" target=\"_blank\">Harold and the Purple Crayon</a>\n  </p>\n</div>","dir":"content/output/projects","base":"realitysketch.json","ext":".json","sourceBase":"realitysketch.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/realitytalk.json":
/*!**************************************************!*\
  !*** ./content/output/projects/realitytalk.json ***!
  \**************************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, doi, conference, pdf, video, embed, arxiv, acm-dl, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"realitytalk","name":"RealityTalk","description":"Real-time Speech-driven Augmented Presentation for AR Live Storytelling","title":"RealityTalk: Real-time Speech-driven Augmented Presentation for AR Live Storytelling","authors":["Jian Liao","Adnan Karim","Shivesh Jadon","Rubaiat Habib Kazi","Ryo Suzuki"],"year":2022,"booktitle":"In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology (UIST '22)","publisher":"ACM, New York, NY, USA","doi":"https://doi.org/10.1145/3526113.3545702","conference":{"name":"UIST 2022","fullname":"The ACM Symposium on User Interface Software and Technology (UIST 2022)","url":"http://uist.acm.org/uist2022"},"pdf":"uist-2022-realitytalk.pdf","video":"https://www.youtube.com/watch?v=vfIMeICV-7c","embed":"https://www.youtube.com/embed/vfIMeICV-7c","arxiv":"https://arxiv.org/abs/2208.06350","acm-dl":"https://dl.acm.org/doi/10.1145/3526113.3545702","pageCount":12,"slideCount":0,"bodyContent":"# Abstract\n\nWe present RealityTalk, a system that augments real-time live presentations with speech-driven interactive virtual elements. Augmented presentations leverage embedded visuals and animation for engaging and expressive storytelling. However, existing tools for live presentations often lack interactivity and improvisation, while creating such effects in video editing tools require significant time and expertise. RealityTalk enables users to create live augmented presentations with real-time speech-driven interactions. The user can interactively prompt, move, and manipulate graphical elements through real-time speech and supporting modalities. Based on our analysis of 177 existing video-edited augmented presentations ([https://ilab.ucalgary.ca/realitytalk/](https://ilab.ucalgary.ca/realitytalk/)), we propose a novel set of interaction techniques and then incorporated them into RealityTalk. We evaluate our tool from a presenter's perspective to demonstrate the effectiveness of our system.","bodyHtml":"<h1>Abstract</h1>\n<p>We present RealityTalk, a system that augments real-time live presentations with speech-driven interactive virtual elements. Augmented presentations leverage embedded visuals and animation for engaging and expressive storytelling. However, existing tools for live presentations often lack interactivity and improvisation, while creating such effects in video editing tools require significant time and expertise. RealityTalk enables users to create live augmented presentations with real-time speech-driven interactions. The user can interactively prompt, move, and manipulate graphical elements through real-time speech and supporting modalities. Based on our analysis of 177 existing video-edited augmented presentations (<a href=\"https://ilab.ucalgary.ca/realitytalk/\">https://ilab.ucalgary.ca/realitytalk/</a>), we propose a novel set of interaction techniques and then incorporated them into RealityTalk. We evaluate our tool from a presenter's perspective to demonstrate the effectiveness of our system.</p>\n","dir":"content/output/projects","base":"realitytalk.json","ext":".json","sourceBase":"realitytalk.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/refazer.json":
/*!**********************************************!*\
  !*** ./content/output/projects/refazer.json ***!
  \**********************************************/
/*! exports provided: id, name, description, title, authors, yera, booktitle, publisher, pages, conference, pdf, acm-dl, arxiv, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"refazer","name":"Refazer","description":"Learning Syntactic Program Transformations from Examples","title":"Learning Syntactic Program Transformations from Examples","authors":["Reudismam Rolim","Gustavo Soares","Loris D'Antoni","Oleksandr Polozov","Sumit Gulwani","Rohit Gheyi","Ryo Suzuki","Bjrn Hartmann"],"yera":2017,"booktitle":"In Proceedings of the 39th International Conference on Software Engineering (ICSE '17)","publisher":"IEEE Press, Piscataway, NJ, USA","pages":"404-415","conference":{"name":"ICSE 2017","fullname":"The International Conference on Software Engineering (ICSE 2017)","url":"http://icse2017.gatech.edu/"},"pdf":"icse-2017-refazer.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=3097417","arxiv":"https://arxiv.org/abs/1608.09000","pageCount":12,"slideCount":0,"bodyContent":"","bodyHtml":"","dir":"content/output/projects","base":"refazer.json","ext":".json","sourceBase":"refazer.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/roomshift.json":
/*!************************************************!*\
  !*** ./content/output/projects/roomshift.json ***!
  \************************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, pages, doi, conference, video, embed, short-video, pdf, arxiv, acm-dl, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"roomshift","name":"RoomShift","description":"Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots","title":"RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots","authors":["Ryo Suzuki","Hooman Hedayati","Clement Zheng","James Bohn","Daniel Szafir","Ellen Yi-Luen Do","Mark D Gross","Daniel Leithinger"],"year":2020,"booktitle":"In Proceedings of the ACM CHI Conference on Human Factors in Computing Systems (CHI '20)","publisher":"ACM, New York, NY, USA","pages":"Paper 396, 11 pages","doi":"https://doi.org/10.1145/3313831.3376523","conference":{"name":"CHI 2020","fullname":"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2020)","url":"https://chi2020.acm.org/"},"video":"https://www.youtube.com/watch?v=6_PAirnlDnk","embed":"https://www.youtube.com/embed/6_PAirnlDnk","short-video":"https://www.youtube.com/watch?v=4OWU60gTOFE","pdf":"chi-2020-roomshift.pdf","arxiv":"https://arxiv.org/abs/2008.08695","acm-dl":"https://dl.acm.org/doi/10.1145/3313831.3376523","pageCount":11,"slideCount":0,"bodyContent":"# Abstract\n\nThis paper presents RoomShift, a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/roomshift/video/top.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-1-1.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-1-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-1-2.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-1-2.jpg\" /></a>\n  </div>\n</div>\n\n# Introduction\n\nThere is a clear need to provide haptic sensations in virtual environments. Recent advances in display and tracking technologies promise immersive experience in virtual reality, but objects seen in VR such as walls and furniture are only visual: the user cannot touch, feel, sit on, or place objects on them. This limits the sense of full immersion in the virtual world. To overcome these limitations, various haptic interfaces have been explored. In the previous work, most haptic interfaces focus on finger-tip haptic feedback with actuated controllers or on-body haptic sensations with wearable devices. In contrast, encountered-type haptic feedback with a dynamic environment promises to increase the immersion of virtual experiences, which are difficult to achieve using an only handheld or wearable haptic devices. Through a dynamic haptic environment, users can touch and interact with the whole virtual scene with their bodies --- they can walk, sit on, and lean against objects in the VR environment. Existing approaches for actuated environments, however, are often limited in speed of transformation (e.g., slow transformation with inflatables) and the range of supported interactions (e.g., only walking).\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/roomshift/video/wall.mp4\" type=\"video/mp4\"></source>\n</video>\n\n\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-9-1.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-9-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-9-2.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-9-2.jpg\" /></a>\n  </div>\n</div>\n\nThis paper introduces RoomShift, a room-scale dynamic haptic environment for virtual reality. RoomShift provides haptic sensations by reconfiguring physical environments using a small swarm of robot assistants. Inspired by shelf-moving robots that are used in robotic warehouses, we developed a swarm of shape-changing robots that can move a range of existing furniture. Each robot has a mechanical lift that extends from 30 cm to 100 cm to pick up, carry, and place objects such as chairs, tables, and walls. This way, users can touch, sit, place, and lean against objects in the virtual environment.\n\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-2-1.png\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-2-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-2-2.png\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-2-2.png\" /></a>\n  </div>\n</div>\n\n\n# RoomShift: Furniture-moving Robots\n\nRoomShift consists of a small swarm of shape-changing robots; each robot uses a Roomba as a mobile base. On this base is mounted a custom mechanical scissor lift made of two linear actuators and a metal drying rack. As the mechanical lift is compact in its closed state, the robot can move under a table or chair with 30 cm clearance, and extend the scissor lift to pick it up.\n\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/roomshift/video/carry.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-3-1.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-3-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-3-2.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-3-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-3-3.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-3-3.jpg\" /></a>\n  </div>\n</div>\n\nRoomShift comprises nine shape-changing swarm robots based on the Roomba Create 2. For the mechanical lift structure, we repurposed an off-the-shelf expandable laundry rack (Room Essentials Compact Drying Rack) and attached two linear actuators (Homend DC12V 8 inch Stroke Linear Actuator, which extends from 32 cm to 52 cm) at the base of the rack. The linear actuators are fixed to the endpoints of the scissor structure with 8 mm steel rods, so that when the actuator contracts, the mounted scissor structure extends vertically (from 30 cm to 100 cm). The scissor structure moves at a speed of 1.3 cm / sec. To mount the scissor structure, we fixed a 6mm acrylic bottom plate (35 cm x 35 cm) and four omni-directional casters (Dorhea Ball Transfer Bearing Unit) to relieve the Roomba of most of the weight that the robot carries. Each robot moves at 20 cm / sec. Figure 3 illustrates the mechanical design of each RoomShift robot.\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-4-1.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-4-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-4-2.png\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-4-2.png\" /></a>\n  </div>\n</div>\n\nOne advantage of our approach is that the robot need not support the weight of the user. Once the robot places the furniture, it serves as a static object. Thus, when a user sits on or puts weight on it, all of the weight goes to the furniture, instead of the robot, which significantly reduces the possibility of a mechanical breakdown.\nAlthough the maximum load for the Roomba is 9 kg, the corner-mounted casters distribute and carry heavier loads. Thus, our robots can lift and carry heavier objects than an unmodified Roomba. The maximum weight the robot can lift and carry is 22 kg. When we put a heavier object than 23 kg, we observed the scissor structure started to break. The strength of the scissor structure suffices to lift lightweight chairs and tables, such as the IKEA honeycomb furniture used in our prototypes. The weight of the furniture we have tested (depicted in Figure) ranges from 3.5 to 11.2 kg. For heavier objects, multiple robots can also coordinate to lift a piece together if there is sufficient space under the furniture. Also, with a more robust scissor structure, we can carry heavier objects, as we observed the Roomba base itself (with the corner-mounted casters) can carry up to 30 kg load.\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-5-1.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-5-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-5-2.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-5-2.jpg\" /></a>\n  </div>\n</div>\n\nThis approach also increases flexibility because different types of furniture can be actuated with the height-adjustable scissor lift. For example, Figure illustrates various static props that the RoomShift robot can actuate. These objects include furniture such as a desk, a long table, different chairs, and a side table. Note that due to the robots minimum collapsed size, objects must have at least 30 cm clearance below them, and enough horizontal space to fit the robot. A designer can also create custom props for specific applications, for instance, the styrofoam wall mounted to a side table seen in the Figure.\n\n\n# Tracking and Control\n\nTo accurately control the RoomShift robots, we require precise motion tracking that can cover the play area in which a user walks. We use an optical tracking system with 20 IR cameras (Qualisys Miqus 5) that can track objects in a 10 m  10 m space. The system tracks six degrees of freedom (DOF) position of the objects\nwith retro-reflective spherical markers at 60 FPS frame rate.\nTo track the robots as well as physical props, we attached five 30 mm spherical retroreflective markers. For the robot, we attached markers to a pair of parallel bars, so that the markers relative positions remain constant regardless of the height of the scissor lift. We can also estimate the height of the scissor structure by measuring the orientation of the marker pattern (the pink plane surface depicted in the Figure).\n\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/roomshift/video/tracking.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-7-3.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-7-3.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-7-1.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-7-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-7-2.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-7-2.jpg\" /></a>\n  </div>\n</div>\n\nTo control the robots movements, we use a simple path planning algorithm. The input is 1) the current positions of the robots, 2) the positions of obstacles (e.g., furniture, other robots, and users), and 3) the target locations. The algorithm outputs the goal of each robot at the next time step. The system continuously updates the path and drives them to their target locations. The main server continuously tracks the robot positions, calculates their wheel speeds, and sends commands at 30 Hz over WiFi.\n\nTo pick up and place these, the robot follows a predefined sequence, approaching the object from an angle where it will not collide with the objects legs. To avoid the collision with the legs of furniture, each object has a user-defined entry and exit point (Figure 8). We also register the height of target furniture before the system starts (e.g., 70 cm for Table_A, 40 cm for Chair_B), so that it can extend the scissor lift to certain target height. We could also put a simple sensor on top of the scissor structure to make it a closed-loop system.\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-6-1.png\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-6-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-6-2.png\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-6-2.png\" /></a>\n  </div>\n</div>\n\nThe main computer runs a Node.js server and the Qualisys tracking software. The 6DOF tracking data that the Qualisys tracking system captures is streamed to the Node.js server through the WebSocket protocol. Based on the tracking data, a web browser client renders the VR scene with A-Frame. The user experiences the VR scene using an Oculus Go head mounted display and its built-in VR browser. We synchronize the desktop computer and the Oculus Go browser with real-time communication through WebSocket. When the virtual scene changes, the system moves the robots to dynamically reconfigure the physical scene. First, the system computes the types of props and each target position based on the relative position from the user.\n\n\n# Interactions and Applications\n\nIn this paper, we specifically focus on architectural application scenarios, such as rendering physical room interiors for virtual real estate tours and collaborative architectural design, two increasingly common application areas for VR. Virtual real estate tours reduce the time and cost compared to on-site viewings, but currently lack the bodily experience of being able to touch surfaces and sit down. In architectural design, VR aids the communication between architects and clients, where proposed designs can be experienced, discussed and modified before building them. We are motivated by how RoomShift can enable people with various physical abilities to experience, test and co-design these environments with their bodies. Most of the elements in these applications can be covered with a finite set of furniture and props (e.g., chairs, desks, and walls). We discuss some of the basic interactions to support these applications.\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-8.png\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-8.png\" /></a>\n  </div>\n</div>\n\nTo support these scenarios, we propose four types of basic interactions RoomShift can support, with the spectrum between embodied interactions and controller-based interactions, as illustrated in the Figure.\n\n1. Experiencing Architectural Spaces: Walking and Touching\n\n2. Architectural Co-Design: Physically Moving Furniture\n\n3. Navigating Large Spaces: Teleporting in VR\n\n4. Virtual Scene Editing: Virtually Moving Furniture\n\nEmbodied interactions refer to interaction with virtual scenes through physical movements and manipulation. The user can implicitly interact with the system by walking around or explicitly interact with the virtual scene by physically moving furniture. On the other hand, the user can also interact with the virtual scene with controller-based gestural interactions. An example is when the user relocates a distant piece of furniture or remove the wall in the room. The user can also virtually teleport their location to navigate through space.\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-10-1.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-10-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-10-2.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-10-2.jpg\" /></a>\n  </div>\n</div>\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-11-1.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-11-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-11-2.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-11-2.jpg\" /></a>\n  </div>\n</div>\n\nFor example, the most basic interaction is to render an architectural space that the user can walk around in and touch.  As the user walks around the space, the robots move the props to maintain the illusion of a larger number of objects.\nIn addition, the system can mimic larger objects with a single moving robot. For example, when the user is interacting with a large table, either new physical table segments can be added or a single robot can continually move the current table according to the users position to simulate touching a larger one.\n\nAlso, RoomShift supports teleportation by reconfiguring the room layout to match the new view location. When the user teleports to a new location in the VR scene, the system calculates the positions of the virtual objects relative to the new location and moves the furniture and robots in and out of the play area to enable a fast scene reconfiguration and to avoid collisions with the user and each other.","bodyHtml":"<h1>Abstract</h1>\n<p>This paper presents RoomShift, a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/roomshift/video/top.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-1-1.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-1-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-1-2.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-1-2.jpg\" /></a>\n  </div>\n</div>\n<h1>Introduction</h1>\n<p>There is a clear need to provide haptic sensations in virtual environments. Recent advances in display and tracking technologies promise immersive experience in virtual reality, but objects seen in VR such as walls and furniture are only visual: the user cannot touch, feel, sit on, or place objects on them. This limits the sense of full immersion in the virtual world. To overcome these limitations, various haptic interfaces have been explored. In the previous work, most haptic interfaces focus on finger-tip haptic feedback with actuated controllers or on-body haptic sensations with wearable devices. In contrast, encountered-type haptic feedback with a dynamic environment promises to increase the immersion of virtual experiences, which are difficult to achieve using an only handheld or wearable haptic devices. Through a dynamic haptic environment, users can touch and interact with the whole virtual scene with their bodies --- they can walk, sit on, and lean against objects in the VR environment. Existing approaches for actuated environments, however, are often limited in speed of transformation (e.g., slow transformation with inflatables) and the range of supported interactions (e.g., only walking).</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/roomshift/video/wall.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-9-1.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-9-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-9-2.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-9-2.jpg\" /></a>\n  </div>\n</div>\n<p>This paper introduces RoomShift, a room-scale dynamic haptic environment for virtual reality. RoomShift provides haptic sensations by reconfiguring physical environments using a small swarm of robot assistants. Inspired by shelf-moving robots that are used in robotic warehouses, we developed a swarm of shape-changing robots that can move a range of existing furniture. Each robot has a mechanical lift that extends from 30 cm to 100 cm to pick up, carry, and place objects such as chairs, tables, and walls. This way, users can touch, sit, place, and lean against objects in the virtual environment.</p>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-2-1.png\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-2-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-2-2.png\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-2-2.png\" /></a>\n  </div>\n</div>\n<h1>RoomShift: Furniture-moving Robots</h1>\n<p>RoomShift consists of a small swarm of shape-changing robots; each robot uses a Roomba as a mobile base. On this base is mounted a custom mechanical scissor lift made of two linear actuators and a metal drying rack. As the mechanical lift is compact in its closed state, the robot can move under a table or chair with 30 cm clearance, and extend the scissor lift to pick it up.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/roomshift/video/carry.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-3-1.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-3-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-3-2.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-3-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-3-3.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-3-3.jpg\" /></a>\n  </div>\n</div>\n<p>RoomShift comprises nine shape-changing swarm robots based on the Roomba Create 2. For the mechanical lift structure, we repurposed an off-the-shelf expandable laundry rack (Room Essentials Compact Drying Rack) and attached two linear actuators (Homend DC12V 8 inch Stroke Linear Actuator, which extends from 32 cm to 52 cm) at the base of the rack. The linear actuators are fixed to the endpoints of the scissor structure with 8 mm steel rods, so that when the actuator contracts, the mounted scissor structure extends vertically (from 30 cm to 100 cm). The scissor structure moves at a speed of 1.3 cm / sec. To mount the scissor structure, we fixed a 6mm acrylic bottom plate (35 cm x 35 cm) and four omni-directional casters (Dorhea Ball Transfer Bearing Unit) to relieve the Roomba of most of the weight that the robot carries. Each robot moves at 20 cm / sec. Figure 3 illustrates the mechanical design of each RoomShift robot.</p>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-4-1.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-4-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-4-2.png\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-4-2.png\" /></a>\n  </div>\n</div>\n<p>One advantage of our approach is that the robot need not support the weight of the user. Once the robot places the furniture, it serves as a static object. Thus, when a user sits on or puts weight on it, all of the weight goes to the furniture, instead of the robot, which significantly reduces the possibility of a mechanical breakdown.\nAlthough the maximum load for the Roomba is 9 kg, the corner-mounted casters distribute and carry heavier loads. Thus, our robots can lift and carry heavier objects than an unmodified Roomba. The maximum weight the robot can lift and carry is 22 kg. When we put a heavier object than 23 kg, we observed the scissor structure started to break. The strength of the scissor structure suffices to lift lightweight chairs and tables, such as the IKEA honeycomb furniture used in our prototypes. The weight of the furniture we have tested (depicted in Figure) ranges from 3.5 to 11.2 kg. For heavier objects, multiple robots can also coordinate to lift a piece together if there is sufficient space under the furniture. Also, with a more robust scissor structure, we can carry heavier objects, as we observed the Roomba base itself (with the corner-mounted casters) can carry up to 30 kg load.</p>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-5-1.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-5-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-5-2.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-5-2.jpg\" /></a>\n  </div>\n</div>\n<p>This approach also increases flexibility because different types of furniture can be actuated with the height-adjustable scissor lift. For example, Figure illustrates various static props that the RoomShift robot can actuate. These objects include furniture such as a desk, a long table, different chairs, and a side table. Note that due to the robots minimum collapsed size, objects must have at least 30 cm clearance below them, and enough horizontal space to fit the robot. A designer can also create custom props for specific applications, for instance, the styrofoam wall mounted to a side table seen in the Figure.</p>\n<h1>Tracking and Control</h1>\n<p>To accurately control the RoomShift robots, we require precise motion tracking that can cover the play area in which a user walks. We use an optical tracking system with 20 IR cameras (Qualisys Miqus 5) that can track objects in a 10 m  10 m space. The system tracks six degrees of freedom (DOF) position of the objects\nwith retro-reflective spherical markers at 60 FPS frame rate.\nTo track the robots as well as physical props, we attached five 30 mm spherical retroreflective markers. For the robot, we attached markers to a pair of parallel bars, so that the markers relative positions remain constant regardless of the height of the scissor lift. We can also estimate the height of the scissor structure by measuring the orientation of the marker pattern (the pink plane surface depicted in the Figure).</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/roomshift/video/tracking.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-7-3.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-7-3.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-7-1.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-7-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-7-2.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-7-2.jpg\" /></a>\n  </div>\n</div>\n<p>To control the robots movements, we use a simple path planning algorithm. The input is 1) the current positions of the robots, 2) the positions of obstacles (e.g., furniture, other robots, and users), and 3) the target locations. The algorithm outputs the goal of each robot at the next time step. The system continuously updates the path and drives them to their target locations. The main server continuously tracks the robot positions, calculates their wheel speeds, and sends commands at 30 Hz over WiFi.</p>\n<p>To pick up and place these, the robot follows a predefined sequence, approaching the object from an angle where it will not collide with the objects legs. To avoid the collision with the legs of furniture, each object has a user-defined entry and exit point (Figure 8). We also register the height of target furniture before the system starts (e.g., 70 cm for Table_A, 40 cm for Chair_B), so that it can extend the scissor lift to certain target height. We could also put a simple sensor on top of the scissor structure to make it a closed-loop system.</p>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-6-1.png\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-6-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-6-2.png\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-6-2.png\" /></a>\n  </div>\n</div>\n<p>The main computer runs a Node.js server and the Qualisys tracking software. The 6DOF tracking data that the Qualisys tracking system captures is streamed to the Node.js server through the WebSocket protocol. Based on the tracking data, a web browser client renders the VR scene with A-Frame. The user experiences the VR scene using an Oculus Go head mounted display and its built-in VR browser. We synchronize the desktop computer and the Oculus Go browser with real-time communication through WebSocket. When the virtual scene changes, the system moves the robots to dynamically reconfigure the physical scene. First, the system computes the types of props and each target position based on the relative position from the user.</p>\n<h1>Interactions and Applications</h1>\n<p>In this paper, we specifically focus on architectural application scenarios, such as rendering physical room interiors for virtual real estate tours and collaborative architectural design, two increasingly common application areas for VR. Virtual real estate tours reduce the time and cost compared to on-site viewings, but currently lack the bodily experience of being able to touch surfaces and sit down. In architectural design, VR aids the communication between architects and clients, where proposed designs can be experienced, discussed and modified before building them. We are motivated by how RoomShift can enable people with various physical abilities to experience, test and co-design these environments with their bodies. Most of the elements in these applications can be covered with a finite set of furniture and props (e.g., chairs, desks, and walls). We discuss some of the basic interactions to support these applications.</p>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-8.png\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-8.png\" /></a>\n  </div>\n</div>\n<p>To support these scenarios, we propose four types of basic interactions RoomShift can support, with the spectrum between embodied interactions and controller-based interactions, as illustrated in the Figure.</p>\n<ol>\n<li>\n<p>Experiencing Architectural Spaces: Walking and Touching</p>\n</li>\n<li>\n<p>Architectural Co-Design: Physically Moving Furniture</p>\n</li>\n<li>\n<p>Navigating Large Spaces: Teleporting in VR</p>\n</li>\n<li>\n<p>Virtual Scene Editing: Virtually Moving Furniture</p>\n</li>\n</ol>\n<p>Embodied interactions refer to interaction with virtual scenes through physical movements and manipulation. The user can implicitly interact with the system by walking around or explicitly interact with the virtual scene by physically moving furniture. On the other hand, the user can also interact with the virtual scene with controller-based gestural interactions. An example is when the user relocates a distant piece of furniture or remove the wall in the room. The user can also virtually teleport their location to navigate through space.</p>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-10-1.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-10-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-10-2.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-10-2.jpg\" /></a>\n  </div>\n</div>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-11-1.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-11-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/roomshift/figure-11-2.jpg\" data-lightbox=\"lightbox\">\n      <img src=\"/static/projects/roomshift/figure-11-2.jpg\" /></a>\n  </div>\n</div>\n<p>For example, the most basic interaction is to render an architectural space that the user can walk around in and touch.  As the user walks around the space, the robots move the props to maintain the illusion of a larger number of objects.\nIn addition, the system can mimic larger objects with a single moving robot. For example, when the user is interacting with a large table, either new physical table segments can be added or a single robot can continually move the current table according to the users position to simulate touching a larger one.</p>\n<p>Also, RoomShift supports teleportation by reconfiguring the room layout to match the new view location. When the user teleports to a new location in the VR scene, the system calculates the positions of the virtual objects relative to the new location and moves the furniture and robots in and out of the play area to enable a fast scene reconfiguration and to avoid collisions with the user and each other.</p>\n","dir":"content/output/projects","base":"roomshift.json","ext":".json","sourceBase":"roomshift.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/selective-self-assembly.json":
/*!**************************************************************!*\
  !*** ./content/output/projects/selective-self-assembly.json ***!
  \**************************************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, conference, external, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"selective-self-assembly","name":"Selective Self-Assembly","description":"Selective Self-Assembly using Re-Programmable Magnetic Pixels","title":"Selective Self-Assembly using Re-Programmable Magnetic Pixels","authors":["Martin Nisser","Yashaswini Makaram","Faraz Faruqi","Ryo Suzuki","Stefanie Mueller"],"year":2022,"booktitle":"In Proceedings of 2022 IEEE/RSJ In- ternational Conference on Intelligent Robots and Systems (IROS '22)","publisher":"ACM, New York, NY, USA","conference":{"name":"IROS 2022","fullname":"In Proceedings of 2022 IEEE/RSJ In- ternational Conference on Intelligent Robots and Systems (IROS 2022)","url":"https://iros2022.org/"},"external":"https://hcie.csail.mit.edu/research/selective/selective.html","bodyContent":"","bodyHtml":"","dir":"content/output/projects","base":"selective-self-assembly.json","ext":".json","sourceBase":"selective-self-assembly.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/shapebots.json":
/*!************************************************!*\
  !*** ./content/output/projects/shapebots.json ***!
  \************************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, pages, doi, conference, pdf, slide, video, embed, github, poster, demo, arxiv, acm-dl, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"shapebots","name":"ShapeBots","description":"Shape-changing Swarm Robots","title":"ShapeBots: Shape-changing Swarm Robots","authors":["Ryo Suzuki","Clement Zheng","Yasuaki Kakehi","Tom Yeh","Ellen Yi-Luen Do","Mark D. Gross","Daniel Leithinger"],"year":2019,"booktitle":"In Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST '19)","publisher":"ACM, New York, NY, USA","pages":"1-13","doi":"https://doi.org/10.1145/3332165.3347911","conference":{"name":"UIST 2019","fullname":"The ACM Symposium on User Interface Software and Technology (UIST 2019)","url":"http://uist.acm.org/uist2019"},"pdf":"uist-2019-shapebots.pdf","slide":"uist-2019-shapebots-slide.pdf","video":"https://www.youtube.com/watch?v=cwPaof0kKdM","embed":"https://www.youtube.com/embed/cwPaof0kKdM","github":"https://github.com/ryosuzuki/shapebots","poster":"uist-2019-shapebots-poster.pdf","demo":"https://ryosuzuki.github.io/shapebots-simulator/","arxiv":"https://arxiv.org/abs/1909.03372","acm-dl":"https://dl.acm.org/doi/10.1145/3332165.3347911","pageCount":13,"slideCount":53,"bodyContent":"# Abstract\n\nWe introduce *shape-changing swarm robots*. A swarm of self-transformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions. The modular design of each actuator enables various shapes and geometries of self-transformation. We illustrate potential application scenarios and discuss how this type of interface opens up possibilities for the future of ubiquitous and distributed shape-changing interfaces.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/top.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/shapebots/video/top.mp4\" type=\"video/mp4\"></source>\n</video>\n\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-1-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-1-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-1-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-1-1.jpg\" /></a>\n  </div>\n</div>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-2-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-2-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-2-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-2-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-2-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-2-3.jpg\" /></a>\n  </div>\n</div>\n\n\n# Shape-changing Swarm Robots\n\nThis paper introduces **shape-changing swarm robots** for dis- tributed shape-changing interfaces. Shape-changing swarm robots can both **individually** and **collectively** change their shape, so that they can collectively present information, act as controllers, actuate objects, represent data, and provide dynamic physical affordances.\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-3.png\" /></a>\n  </div>\n</div>\n\nThis paper specifically focuses on the user interface aspect of such systems, which we refer to shape-changing swarm user interfaces. We identified three core aspects of shape-changing swarm robots: 1) locomotion, 2) self-transformation, and 3) collective behaviors of many individual elements.\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-4.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-4.png\" /></a>\n  </div>\n</div>\n\n# ShapeBots\n\n**ShapeBots are self-transformable swarm robots** with modular linear actuators. To enable a large deformation capability of tiny swarm robots, we developed a miniature reel-based linear actuator that is thin (2.5 cm) and fits into the small footprint (3 cm x 3 cm), while able to extend up to 20 cm in both horizontal and vertical directions.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/unit.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/shapebots/video/unit.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-5-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-5-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-5-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-5-2.png\" /></a>\n  </div>\n</div>\n\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-6-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-6-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-6-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-6-2.png\" /></a>\n  </div>\n</div>\n\nThe modular design of each linear actuator unit enables the construction of various shapes and geometries of individual shape transformation (e.g., horizontal lines, vertical lines, curved lines, 2D area expan- sion, and 3D volumetric change). Based on these capabilities, we demonstrate application scenarios showing how a swarm of distributed self-transformable robots can support everyday interactions.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/transformation.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/shapebots/video/transformation.mp4\" type=\"video/mp4\"></source>\n</video>\n\n\n# Tracking and Control\n\nTo track the position and orientation of the swarm robots, we used computer vision and a fiducial marker attached to the bottom of the robot. We used the ArUco fiducial marker printed on a sheet of paper and taped to the bottom of the robot. Our prototype used a 1.5 cm x 1.5 cm size marker with a 4 x 4 grid pattern, which can provide up to 50 unique patterns. For tracking software, we used the OpenCV library and ArUco python module. It can track the position of the markers at 60 frames per second.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/tracking.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/shapebots/video/tracking.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-7-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-7-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-7-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-7-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-7-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-7-3.png\" /></a>\n  </div>\n</div>\n\nTo enable the user to easily specify a target shape, we created a web-based interface where users draw a shape or upload an SVG image (Figure 10). The user draws a set of lines, then the main computer calculates target positions, orientations, and actuator lengths to start sending commands.\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-8.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-8.png\" /></a>\n  </div>\n</div>\n\n\nWe can use the same mechanism to track user input. The system supports four different types of user interaction that our system supports: place, move, orient, and pick-up.\nThe system recognizes as user inputs movement or rotation of a marker that it did not generate.\n\n\n\n# Applications: Robots as Dynamic Physical Media\nWe explore potential application scenarios for the future of human-robot interactions.\nOne interesting application area is to use these **robots as dynamic physical media**, such as showing **data visualization in the physical world**.\nFor example, ShapeBots on the USA map physicalize map data; each robot changes its height to show the population of the state it is on. Users can interact with the dataset by placing a new robot or moving a robot to a different state, and the robots update their physical forms to represent the respective population.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/dataphys.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/shapebots/video/dataphys.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-9-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-9-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-9-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-9-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-9-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-9-3.jpg\" /></a>\n  </div>\n</div>\n\nSimilarly, ShapeBots can provide a physical preview of a CAD design. ShapeBots physicalizes the actual size of the box. The design and physical rendering are tightly coupled; as the user changes the height of the box in CAD software, the ShapeBots change heights accordingly. The user can change the parameters of the design by moving robots in the physical space, and these changes are reflected in the CAD design.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/cad.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/shapebots/video/cad.mp4\" type=\"video/mp4\"></source>\n</video>\n\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-9-4.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-9-4.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-9-5.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-9-5.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-9-6.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-9-6.jpg\" /></a>\n  </div>\n</div>\n\n# Applications: Robots as Ambient Assistants\n\nAnother practical aspect of ShapeBots is the ability to actuate objects and act as physical constraints. As an example, the video shows two robots extending their linear actuators to wipe debris off a table, clearing a workspace for the user.\nIn these scenarios, these robots can help as an **ambient assistant for everyday life**.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/cleaning.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/shapebots/video/cleaning.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-11-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-11-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-11-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-11-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-11-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-11-3.jpg\" /></a>\n  </div>\n</div>\n\nBy leveraging the capability of locomotion and height change of each robot, ShapeBots can create a dynamic fence to hide or encompass existing objects for affordances. For example, when the user pours hot coffee into a cup, the robots surround the cup and change their heights to create a vertical fence. The vertical fence visually and physically provides the affordance to indicate that the coffee is too hot and not ready to drink. Once it is ready, the robots start dispersing and allow the user to grab it. These scenarios illustrate how the distributed shape-changing robots can provide a new type of affordance, which we call **distributed dynamic physical affordances**.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/affordance.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/shapebots/video/affordance.mp4\" type=\"video/mp4\"></source>\n</video>\n\n\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-10-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-10-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-10-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-10-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-10-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-10-3.jpg\" /></a>\n  </div>\n</div>\n\nShapeBots can also act as an interactive physical display. The following figures show how ShapeBots can render different shapes.\nWe highlight the advantage of ShapeBots for rendering contours compared to non self-transformable swarm robots. Using a software simulation, we demonstrate how ShapeBots renders an SVG input at different swarm sizes. You can also play with the [**explorable online simulator**](https://ryosuzuki.github.io/shapebots-simulator/) to see how these robots can render the shape \n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/explorable.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/shapebots/video/explorable.mp4\" type=\"video/mp4\"></source>\n</video>\n\n\n\n# Future Work\n\nShapeBots is just a single example of shape-changing swarm robots.\nThere is a broader design space of shape- changing swarm user interfaces.\nAs future work, we are interested in exploring the different aspct of shape-changing swarm robots.\n\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-12.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-12.png\" /></a>\n  </div>\n</div>\n\n<!--\n# Appendix\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/fabrication.mp4\" type=\"video/mp4\"></source>\n</video>\n\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/explorable.mp4\" type=\"video/mp4\"></source>\n</video>\n -->","bodyHtml":"<h1>Abstract</h1>\n<p>We introduce <em>shape-changing swarm robots</em>. A swarm of self-transformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions. The modular design of each actuator enables various shapes and geometries of self-transformation. We illustrate potential application scenarios and discuss how this type of interface opens up possibilities for the future of ubiquitous and distributed shape-changing interfaces.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/top.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/shapebots/video/top.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-1-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-1-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-1-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-1-1.jpg\" /></a>\n  </div>\n</div>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-2-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-2-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-2-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-2-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-2-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-2-3.jpg\" /></a>\n  </div>\n</div>\n<h1>Shape-changing Swarm Robots</h1>\n<p>This paper introduces <strong>shape-changing swarm robots</strong> for dis- tributed shape-changing interfaces. Shape-changing swarm robots can both <strong>individually</strong> and <strong>collectively</strong> change their shape, so that they can collectively present information, act as controllers, actuate objects, represent data, and provide dynamic physical affordances.</p>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-3.png\" /></a>\n  </div>\n</div>\n<p>This paper specifically focuses on the user interface aspect of such systems, which we refer to shape-changing swarm user interfaces. We identified three core aspects of shape-changing swarm robots: 1) locomotion, 2) self-transformation, and 3) collective behaviors of many individual elements.</p>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-4.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-4.png\" /></a>\n  </div>\n</div>\n<h1>ShapeBots</h1>\n<p><strong>ShapeBots are self-transformable swarm robots</strong> with modular linear actuators. To enable a large deformation capability of tiny swarm robots, we developed a miniature reel-based linear actuator that is thin (2.5 cm) and fits into the small footprint (3 cm x 3 cm), while able to extend up to 20 cm in both horizontal and vertical directions.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/unit.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/shapebots/video/unit.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-5-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-5-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-5-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-5-2.png\" /></a>\n  </div>\n</div>\n<div class=\"figures ui two column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-6-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-6-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-6-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-6-2.png\" /></a>\n  </div>\n</div>\n<p>The modular design of each linear actuator unit enables the construction of various shapes and geometries of individual shape transformation (e.g., horizontal lines, vertical lines, curved lines, 2D area expan- sion, and 3D volumetric change). Based on these capabilities, we demonstrate application scenarios showing how a swarm of distributed self-transformable robots can support everyday interactions.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/transformation.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/shapebots/video/transformation.mp4\" type=\"video/mp4\"></source>\n</video>\n<h1>Tracking and Control</h1>\n<p>To track the position and orientation of the swarm robots, we used computer vision and a fiducial marker attached to the bottom of the robot. We used the ArUco fiducial marker printed on a sheet of paper and taped to the bottom of the robot. Our prototype used a 1.5 cm x 1.5 cm size marker with a 4 x 4 grid pattern, which can provide up to 50 unique patterns. For tracking software, we used the OpenCV library and ArUco python module. It can track the position of the markers at 60 frames per second.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/tracking.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/shapebots/video/tracking.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-7-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-7-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-7-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-7-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-7-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-7-3.png\" /></a>\n  </div>\n</div>\n<p>To enable the user to easily specify a target shape, we created a web-based interface where users draw a shape or upload an SVG image (Figure 10). The user draws a set of lines, then the main computer calculates target positions, orientations, and actuator lengths to start sending commands.</p>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-8.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-8.png\" /></a>\n  </div>\n</div>\n<p>We can use the same mechanism to track user input. The system supports four different types of user interaction that our system supports: place, move, orient, and pick-up.\nThe system recognizes as user inputs movement or rotation of a marker that it did not generate.</p>\n<h1>Applications: Robots as Dynamic Physical Media</h1>\n<p>We explore potential application scenarios for the future of human-robot interactions.\nOne interesting application area is to use these <strong>robots as dynamic physical media</strong>, such as showing <strong>data visualization in the physical world</strong>.\nFor example, ShapeBots on the USA map physicalize map data; each robot changes its height to show the population of the state it is on. Users can interact with the dataset by placing a new robot or moving a robot to a different state, and the robots update their physical forms to represent the respective population.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/dataphys.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/shapebots/video/dataphys.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-9-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-9-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-9-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-9-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-9-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-9-3.jpg\" /></a>\n  </div>\n</div>\n<p>Similarly, ShapeBots can provide a physical preview of a CAD design. ShapeBots physicalizes the actual size of the box. The design and physical rendering are tightly coupled; as the user changes the height of the box in CAD software, the ShapeBots change heights accordingly. The user can change the parameters of the design by moving robots in the physical space, and these changes are reflected in the CAD design.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/cad.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/shapebots/video/cad.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-9-4.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-9-4.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-9-5.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-9-5.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-9-6.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-9-6.jpg\" /></a>\n  </div>\n</div>\n<h1>Applications: Robots as Ambient Assistants</h1>\n<p>Another practical aspect of ShapeBots is the ability to actuate objects and act as physical constraints. As an example, the video shows two robots extending their linear actuators to wipe debris off a table, clearing a workspace for the user.\nIn these scenarios, these robots can help as an <strong>ambient assistant for everyday life</strong>.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/cleaning.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/shapebots/video/cleaning.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-11-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-11-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-11-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-11-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-11-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-11-3.jpg\" /></a>\n  </div>\n</div>\n<p>By leveraging the capability of locomotion and height change of each robot, ShapeBots can create a dynamic fence to hide or encompass existing objects for affordances. For example, when the user pours hot coffee into a cup, the robots surround the cup and change their heights to create a vertical fence. The vertical fence visually and physically provides the affordance to indicate that the coffee is too hot and not ready to drink. Once it is ready, the robots start dispersing and allow the user to grab it. These scenarios illustrate how the distributed shape-changing robots can provide a new type of affordance, which we call <strong>distributed dynamic physical affordances</strong>.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/affordance.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/shapebots/video/affordance.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui three column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-10-1.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-10-1.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-10-2.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-10-2.jpg\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-10-3.jpg\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-10-3.jpg\" /></a>\n  </div>\n</div>\n<p>ShapeBots can also act as an interactive physical display. The following figures show how ShapeBots can render different shapes.\nWe highlight the advantage of ShapeBots for rendering contours compared to non self-transformable swarm robots. Using a software simulation, we demonstrate how ShapeBots renders an SVG input at different swarm sizes. You can also play with the <a href=\"https://ryosuzuki.github.io/shapebots-simulator/\"><strong>explorable online simulator</strong></a> to see how these robots can render the shape </p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/explorable.webm\" type=\"video/webm\"></source>\n  <source src=\"/static/projects/shapebots/video/explorable.mp4\" type=\"video/mp4\"></source>\n</video>\n<h1>Future Work</h1>\n<p>ShapeBots is just a single example of shape-changing swarm robots.\nThere is a broader design space of shape- changing swarm user interfaces.\nAs future work, we are interested in exploring the different aspct of shape-changing swarm robots.</p>\n<div class=\"figures ui one column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/shapebots/figure-12.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/shapebots/figure-12.png\" /></a>\n  </div>\n</div>\n<!--\n# Appendix\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/fabrication.mp4\" type=\"video/mp4\"></source>\n</video>\n\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/shapebots/video/explorable.mp4\" type=\"video/mp4\"></source>\n</video>\n -->","dir":"content/output/projects","base":"shapebots.json","ext":".json","sourceBase":"shapebots.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/sketched-reality.json":
/*!*******************************************************!*\
  !*** ./content/output/projects/sketched-reality.json ***!
  \*******************************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, doi, conference, pdf, video, embed, arxiv, acm-dl, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"sketched-reality","name":"Sketched Reality","description":"Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI","title":"Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI","authors":["Hiroki Kaimoto","Kyzyl Monteiro","Mehrad Faridan","Jiatong Li","Samin Farajian","Yasuaki Kakehi","Ken Nakagaki","Ryo Suzuki"],"year":2022,"booktitle":"In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology (UIST '22)","publisher":"ACM, New York, NY, USA","doi":"https://doi.org/10.1145/3526113.3545626","conference":{"name":"UIST 2022","fullname":"The ACM Symposium on User Interface Software and Technology (UIST 2022)","url":"http://uist.acm.org/uist2022"},"pdf":"uist-2022-sketched-reality.pdf","video":"https://www.youtube.com/watch?v=xy-IeVgoEpY","embed":"https://www.youtube.com/embed/xy-IeVgoEpY","arxiv":"https://arxiv.org/abs/2208.06341","acm-dl":"https://dl.acm.org/doi/10.1145/3526113.3545626","pageCount":12,"slideCount":0,"bodyContent":"# Abstract\n\nThis paper introduces Sketched Reality, an approach that com- bines AR sketching and actuated tangible user interfaces (TUI) for bi-directional sketching interaction. Bi-directional sketching enables virtual sketches and physical objects to \"affect\" each other through physical actuation and digital computation. In the existing AR sketching, the relationship between virtual and physical worlds is only one-directional --- while physical interaction can affect virtual sketches, virtual sketches have no return effect on the physical objects or environment. In contrast, bi-directional sketching interaction allows the seamless coupling between sketches and actuated TUIs. In this paper, we employ tabletop-size small robots (Sony Toio) and an iPad-based AR sketching tool to demonstrate the concept. In our system, virtual sketches drawn and simulated on an iPad (e.g., lines, walls, pendulums, and springs) can move, actuate, collide, and constrain physical Toio robots, as if virtual sketches and the physical objects exist in the same space through seamless coupling between AR and robot motion. This paper contributes a set of novel interactions and a design space of bi-directional AR sketching. We demonstrate a series of potential applications, such as tangible physics education, explorable mechanism, tangible gaming for children, and in-situ robot programming via sketching.","bodyHtml":"<h1>Abstract</h1>\n<p>This paper introduces Sketched Reality, an approach that com- bines AR sketching and actuated tangible user interfaces (TUI) for bi-directional sketching interaction. Bi-directional sketching enables virtual sketches and physical objects to &quot;affect&quot; each other through physical actuation and digital computation. In the existing AR sketching, the relationship between virtual and physical worlds is only one-directional --- while physical interaction can affect virtual sketches, virtual sketches have no return effect on the physical objects or environment. In contrast, bi-directional sketching interaction allows the seamless coupling between sketches and actuated TUIs. In this paper, we employ tabletop-size small robots (Sony Toio) and an iPad-based AR sketching tool to demonstrate the concept. In our system, virtual sketches drawn and simulated on an iPad (e.g., lines, walls, pendulums, and springs) can move, actuate, collide, and constrain physical Toio robots, as if virtual sketches and the physical objects exist in the same space through seamless coupling between AR and robot motion. This paper contributes a set of novel interactions and a design space of bi-directional AR sketching. We demonstrate a series of potential applications, such as tangible physics education, explorable mechanism, tangible gaming for children, and in-situ robot programming via sketching.</p>\n","dir":"content/output/projects","base":"sketched-reality.json","ext":".json","sourceBase":"sketched-reality.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/tabby.json":
/*!********************************************!*\
  !*** ./content/output/projects/tabby.json ***!
  \********************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, pages, doi, conference, pdf, video, embed, acm-dl, slide, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"tabby","name":"Tabby","description":"Explorable Design for 3D Printing Textures","title":"Tabby: Explorable Design for 3D Printing Textures","authors":["Ryo Suzuki","Koji Yatani","Mark D. Gross","Tom Yeh"],"year":2018,"booktitle":"The Pacific Conference on Computer Graphics and Applications (Pacific Graphics 2018)","publisher":"The Eurographics Association","pages":"1-4","doi":"https://doi.org/10.2312/pg.20181273","conference":{"name":"Pacific Graphics 2018","fullname":"The Pacific Conference on Computer Graphics and Applications (Pacific Graphics 2018)","url":"http://sweb.cityu.edu.hk/pg2018/"},"pdf":"pg-2018-tabby.pdf","video":"https://www.youtube.com/watch?v=rRgw8lH74CA","embed":"https://www.youtube.com/embed/rRgw8lH74CA","acm-dl":"https://diglib.eg.org/handle/10.2312/pg20181273","slide":"pg-2018-tabby-slide.pdf","pageCount":4,"slideCount":40,"bodyContent":"# Abstract\nThis paper presents **Tabby, an interactive and explorable design tool for 3D printing textures**. Tabby allows texture design with direct manipulation in the following workflow: 1) select a target surface, 2) sketch and manipulate a texture with 2D drawings, and then 3) generate 3D printing textures onto an arbitrary curved surface. To enable efficient texture creation, Tabby leverages an auto-completion approach which automates the tedious, repetitive process of applying texture, while allowing flexible customization. Our user evaluation study with seven participants confirms that Tabby can effectively support the design exploration of different patterns for both novice and experienced users.\n\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/tabby/top.mp4\" type=\"video/mp4\"></source>\n</video>\n\n<div class=\"figures ui four column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/tabby/figure-1-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/tabby/figure-1-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/tabby/figure-1-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/tabby/figure-1-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/tabby/figure-1-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/tabby/figure-1-3.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/tabby/figure-1-4.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/tabby/figure-1-4.png\" /></a>\n  </div>\n</div>\n\n<div class=\"figures ui four column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/tabby/figure-1-5.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/tabby/figure-1-5.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/tabby/figure-1-6.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/tabby/figure-1-6.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/tabby/figure-1-7.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/tabby/figure-1-7.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/tabby/figure-1-8.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/tabby/figure-1-8.png\" /></a>\n  </div>\n</div>","bodyHtml":"<h1>Abstract</h1>\n<p>This paper presents <strong>Tabby, an interactive and explorable design tool for 3D printing textures</strong>. Tabby allows texture design with direct manipulation in the following workflow: 1) select a target surface, 2) sketch and manipulate a texture with 2D drawings, and then 3) generate 3D printing textures onto an arbitrary curved surface. To enable efficient texture creation, Tabby leverages an auto-completion approach which automates the tedious, repetitive process of applying texture, while allowing flexible customization. Our user evaluation study with seven participants confirms that Tabby can effectively support the design exploration of different patterns for both novice and experienced users.</p>\n<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/tabby/top.mp4\" type=\"video/mp4\"></source>\n</video>\n<div class=\"figures ui four column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/tabby/figure-1-1.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/tabby/figure-1-1.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/tabby/figure-1-2.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/tabby/figure-1-2.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/tabby/figure-1-3.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/tabby/figure-1-3.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/tabby/figure-1-4.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/tabby/figure-1-4.png\" /></a>\n  </div>\n</div>\n<div class=\"figures ui four column grid\">\n  <div class=\"figure column\">\n    <a href=\"/static/projects/tabby/figure-1-5.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/tabby/figure-1-5.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/tabby/figure-1-6.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/tabby/figure-1-6.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/tabby/figure-1-7.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/tabby/figure-1-7.png\" /></a>\n  </div>\n  <div class=\"figure column\">\n    <a href=\"/static/projects/tabby/figure-1-8.png\" data-lightbox=\"lightbox\"><img src=\"/static/projects/tabby/figure-1-8.png\" /></a>\n  </div>\n</div>","dir":"content/output/projects","base":"tabby.json","ext":".json","sourceBase":"tabby.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/teachable-reality.json":
/*!********************************************************!*\
  !*** ./content/output/projects/teachable-reality.json ***!
  \********************************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, doi, conference, pdf, video, embed, arxiv, acm-dl, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"teachable-reality","name":"Teachable Reality","description":"Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching","title":"Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching","authors":["Kyzyl Monteiro","Ritik Vatsal","Neil Chulpongsatorn","Aman Parnami","Ryo Suzuki"],"year":2023,"booktitle":"In Proceedings of the ACM CHI Conference on Human Factors in Computing Systems (CHI '23)","publisher":"ACM, New York, NY, USA","doi":"https://doi.org/10.1145/3544548.3581449","conference":{"name":"CHI 2023","fullname":"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2023)","url":"https://chi2023.acm.org/"},"pdf":"chi-2023-teachable-reality.pdf","video":"https://www.youtube.com/watch?v=JssiyfrhIJw","embed":"https://www.youtube.com/embed/JssiyfrhIJw","arxiv":"https://arxiv.org/abs/2302.11046","acm-dl":"https://dl.acm.org/doi/10.1145/3544548.3581449","pageCount":15,"slideCount":0,"bodyContent":"# Abstract\n\nThis paper introduces Teachable Reality, an augmented reality (AR) prototyping tool for creating interactive tangible AR applications with arbitrary everyday objects. Teachable Reality leverages vision-based interactive machine teaching (e.g., Teachable Machine), which captures real-world interactions for AR prototyping. It identifies the user-defined tangible and gestural interactions using an on-demand computer vision model. Based on this, the user can easily create functional AR prototypes without programming, enabled by a trigger-action authoring interface. Therefore, our approach allows the flexibility, customizability, and generalizability of tangible AR applications that can address the limitation of current marker-based approaches. We explore the design space and demonstrate various AR prototypes, which include tangible and deformable interfaces, context-aware assistants, and body-driven AR applications. The results of our user study and expert interviews confirm that our approach can lower the barrier to creating functional AR prototypes while also allowing flexible and general-purpose prototyping experiences.","bodyHtml":"<h1>Abstract</h1>\n<p>This paper introduces Teachable Reality, an augmented reality (AR) prototyping tool for creating interactive tangible AR applications with arbitrary everyday objects. Teachable Reality leverages vision-based interactive machine teaching (e.g., Teachable Machine), which captures real-world interactions for AR prototyping. It identifies the user-defined tangible and gestural interactions using an on-demand computer vision model. Based on this, the user can easily create functional AR prototypes without programming, enabled by a trigger-action authoring interface. Therefore, our approach allows the flexibility, customizability, and generalizability of tangible AR applications that can address the limitation of current marker-based approaches. We explore the design space and demonstrate various AR prototypes, which include tangible and deformable interfaces, context-aware assistants, and body-driven AR applications. The results of our user study and expert interviews confirm that our approach can lower the barrier to creating functional AR prototypes while also allowing flexible and general-purpose prototyping experiences.</p>\n","dir":"content/output/projects","base":"teachable-reality.json","ext":".json","sourceBase":"teachable-reality.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/trace-diff.json":
/*!*************************************************!*\
  !*** ./content/output/projects/trace-diff.json ***!
  \*************************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, pages, doi, conference, pdf, slide, github, demo, ieee, arxiv, related, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"trace-diff","name":"TraceDiff","description":"Debugging Unexpected Code Behavior Using Trace Divergences","title":"TraceDiff: Debugging Unexpected Code Behavior Using Trace Divergences","authors":["Ryo Suzuki","Gustavo Soares","Andrew Head","Elena Glassman","Ruan Reis","Melina Mongiovi","Loris DAntoni","Bjern Hartmann"],"year":2017,"booktitle":"In Proceedings of 2017 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC '17)","publisher":"IEEE Press, Piscataway, NJ, USA","pages":"107-115","doi":"https://doi.org/10.1109/VLHCC.2017.8103457","conference":{"name":"VL/HCC 2017","fullname":"IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC 2017)","url":"https://sites.google.com/site/vlhcc2017/"},"pdf":"vlhcc-2017-tracediff.pdf","slide":"vlhcc-2017-tracediff-slide.pdf","github":"https://github.com/ryosuzuki/trace-diff","demo":"https://ryosuzuki.github.io/trace-diff/","ieee":"http://ieeexplore.ieee.org/document/8103457/","arxiv":"https://arxiv.org/abs/1708.03786a","related":{"title":"Exploring the Design Space of Automatically Synthesized Hints for Introductory Programming Assignments","authors":["Ryo Suzuki","Gustavo Soares","Elena Glassman","Andrew Head","Loris D'Antoni","Bjrn Hartmann"],"year":2017,"booktitle":"In Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA '17)","publisher":"ACM, New York, NY, USA","pages":"2951-2958","doi":"https://doi.org/10.1145/3027063.3053187","pdf":"chi-2017-lbw.pdf","suffix":"lbw","pageCount":6},"pageCount":9,"slideCount":77,"bodyContent":"<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/trace-diff/top.mp4\" type=\"video/mp4\"></source>\n</video>\n\n# Abstract\n\nRecent advances in program synthesis offer means to automatically debug student submissions and generate personalized feedback in massive programming classrooms. When automatically generating feedback for programming assignments, a key challenge is designing pedagogically useful hints that are as effective as the manual feedback given by teachers. Through an analysis of teachers hint-giving practices in 132 online Q&A posts, we establish three design guidelines that an effective feedback design should follow. Based on these guidelines, we develop a feedback system that leverages both program synthesis and visualization techniques. Our system compares the dynamic code execution of both incorrect and fixed code and highlights how the error leads to a difference in behavior and where the incorrect code trace diverges from the expected solution. Results from our study suggest that our system enables students to detect and fix bugs that are not caught by students using another existing visual debugging tool.","bodyHtml":"<video preload=\"metadata\" autoPlay loop muted playsInline webkit-playsinline=\"\">\n  <source src=\"/static/projects/trace-diff/top.mp4\" type=\"video/mp4\"></source>\n</video>\n<h1>Abstract</h1>\n<p>Recent advances in program synthesis offer means to automatically debug student submissions and generate personalized feedback in massive programming classrooms. When automatically generating feedback for programming assignments, a key challenge is designing pedagogically useful hints that are as effective as the manual feedback given by teachers. Through an analysis of teachers hint-giving practices in 132 online Q&amp;A posts, we establish three design guidelines that an effective feedback design should follow. Based on these guidelines, we develop a feedback system that leverages both program synthesis and visualization techniques. Our system compares the dynamic code execution of both incorrect and fixed code and highlights how the error leads to a difference in behavior and where the incorrect code trace diverges from the expected solution. Results from our study suggest that our system enables students to detect and fix bugs that are not caught by students using another existing visual debugging tool.</p>\n","dir":"content/output/projects","base":"trace-diff.json","ext":".json","sourceBase":"trace-diff.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/publications.json":
/*!******************************************!*\
  !*** ./content/output/publications.json ***!
  \******************************************/
/*! exports provided: 0, 1, 2, 3, 4, 5, default */
/***/ (function(module) {

module.exports = [{"author":"Ryo Suzuki, Jun Kato, Mark D. Gross, Tom Yeh","title":"Reactile: Programming Swarm User Interfaces Through Direct Physical Manipulation","booktitle":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","series":"CHI '18","year":2018,"isbn":"978-1-4503-5620-6","location":"Montreal QC, Canada","pages":"199:1--199:13","articleno":199,"numpages":13,"url":"http://doi.acm.org/10.1145/3173574.3173773","doi":"10.1145/3173574.3173773","acmid":3173773,"publisher":"ACM","address":"New York, NY, USA","keywords":"direct manipulation, programming by demonstration, swarm user interfaces, tangible programming"},{"author":"Hyunjoo Oh, Tung D. Ta, Ryo Suzuki, Mark D. Gross, Yoshihiro Kawahara, and Lining Yao.","title":"PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-Based Electronic Devices","booktitle":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","series":"CHI '18","year":2018,"isbn":"978-1-4503-5620-6","location":"Montreal QC, Canada","pages":"441:1--441:12","articleno":441,"numpages":12,"url":"http://doi.acm.org/10.1145/3173574.3174015","doi":"10.1145/3173574.3174015","acmid":3174015,"publisher":"ACM","address":"New York, NY, USA","keywords":"3d sculpting, fabrication techniques, paper craft, paper electronics, prototyping"},{"author":"Ryo Suzuki, Abigale Stangl, Mark D. Gross, and Tom Yeh","title":"FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers","booktitle":"Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility","series":"ASSETS '17","year":2017,"isbn":"978-1-4503-4926-0","location":"Baltimore, Maryland, USA","pages":"190--199","numpages":10,"url":"http://doi.acm.org/10.1145/3132525.3132548","doi":"10.1145/3132525.3132548","acmid":3132548,"publisher":"ACM","address":"New York, NY, USA","keywords":"dynamic tactile markers, interactive tactile graphics, tangible interfaces, visual impairment"},{"author":"Andrew Head, Elena Glassman, Gustavo Soares, Ryo Suzuki, Lucas Figueredo, Loris D'Antoni, and Bjoern Hartmann","title":"Writing Reusable Code Feedback at Scale with Mixed-Initiative Program Synthesis","booktitle":"Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale","series":"L@S '17","year":2017,"isbn":"978-1-4503-4450-0","location":"Cambridge, Massachusetts, USA","pages":"89--98","numpages":10,"url":"http://doi.acm.org/10.1145/3051457.3051467","doi":"10.1145/3051457.3051467","acmid":3051467,"publisher":"ACM","address":"New York, NY, USA","keywords":"program synthesis, programming education"},{"author":"Reudismam Rolim, Gustavo Soares, Loris D'Antoni, Oleksandr Polozov, Sumit Gulwani, Rohit Gheyi, Ryo Suzuki, and Bjoern Hartmann","title":"Learning Syntactic Program Transformations from Examples","booktitle":"Proceedings of the 39th International Conference on Software Engineering","series":"ICSE '17","year":2017,"isbn":"978-1-5386-3868-2","location":"Buenos Aires, Argentina","pages":"404--415","numpages":12,"url":"https://doi.org/10.1109/ICSE.2017.44","doi":"10.1109/ICSE.2017.44","acmid":3097417,"publisher":"IEEE Press","address":"Piscataway, NJ, USA","keywords":"program synthesis, program transformation, refactoring, tutoring systems"},{"author":"Ryo Suzuki, Niloufar Salehi, Michelle S. Lam, Juan C. Marroquin, and Michael S. Bernstein","title":"Atelier: Repurposing Expert Crowdsourcing Tasks As Micro-internships","booktitle":"Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems","series":"CHI '16","year":2016,"isbn":"978-1-4503-3362-7","location":"San Jose, California, USA","pages":"2645--2656","numpages":12,"url":"http://doi.acm.org/10.1145/2858036.2858121","doi":"10.1145/2858036.2858121","acmid":2858121,"publisher":"ACM","address":"New York, NY, USA","keywords":"crowd work, crowdsourcing, micro-internships"}];

/***/ }),

/***/ "./content/output/summary.json":
/*!*************************************!*\
  !*** ./content/output/summary.json ***!
  \*************************************/
/*! exports provided: fileMap, sourceFileArray, default */
/***/ (function(module) {

module.exports = {"fileMap":{"content/output/projects/atelier.json":{"id":"atelier","name":"Atelier","description":"Repurposing Expert Crowdsourcing Tasks as Micro-internships","title":"Atelier: Repurposing Expert Crowdsourcing Tasks as Micro-internships","authors":["Ryo Suzuki","Niloufar Salehi","Michelle S. Lam","Juan C. Marroquin","Michael S. Bernstein"],"year":2016,"booktitle":"In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16)","publisher":"ACM, New York, NY, USA","pages":"2645-2656","conference":{"name":"CHI 2016","fullname":"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2016)","url":"https://chi2016.acm.org/wp/"},"pdf":"chi-2016-atelier.pdf","video":"https://www.youtube.com/watch?v=tBojZejtFQo","embed":"https://www.youtube.com/embed/tBojZejtFQo","slide":"chi-2016-atelier-slide.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=2858121","arxiv":"https://arxiv.org/abs/1602.06634","pageCount":12,"slideCount":56,"image":"atelier.jpg","dir":"content/output/projects","base":"atelier.json","ext":".json","sourceBase":"atelier.md","sourceExt":".md"},"content/output/activities.json":{"title":"<strong>Program Committee","dir":"content/output","base":"activities.json","ext":".json","sourceBase":"activities.md","sourceExt":".md"},"content/output/fellowship.json":{"title":"Funding","dir":"content/output","base":"fellowship.json","ext":".json","sourceBase":"fellowship.md","sourceExt":".md"},"content/output/posters.json":[{"author":"Ryo Suzuki,","title":"Collective Shape-changing Interfaces","pdf":"uist-2019-collective.pdf","booktitle":"Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software & Technology","series":"UIST '19 Doctoral Consortium","year":2019,"isbn":"978-1-4503-4656-6","location":"New Orleans, Louisiana, USA","pages":"2951--2958","numpages":2,"doi":"xxx/xxx","acmid":3053187,"publisher":"ACM","address":"New York, NY, USA","keywords":"shape-changing interfaces"},{"author":"Ryo Suzuki, Ryosuke Nakayama, Dan Liu, Yasuaki Kakehi, Mark D. Gross, and Daniel Leithinger,","title":"LiftTiles: Modular and Reconfigurable Room-scale Shape Displays through Retractable Inflatable Actuators","pdf":"uist-2019-lift-tiles.pdf","poster":"uist-2019-lift-tiles-poster.pdf","booktitle":"Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software & Technology","series":"UIST '19 Adjunct","year":2019,"isbn":"978-1-4503-4656-6","location":"New Orleans, Louisiana, USA","pages":"2951--2958","numpages":2,"doi":"xxx/xxx","acmid":3053187,"publisher":"ACM","address":"New York, NY, USA","keywords":"inflatables, shape-changing interfaces,large-scale interactions"},{"author":"Ryo Suzuki, Gustavo Soares, Elena Glassman, Andrew Head, Loris D'Antoni, and Bjoern Hartmann,","title":"Exploring the Design Space of Automatically Synthesized Hints for Introductory Programming Assignments","pdf":"chi-2017-lbw.pdf","poster":"chi-2017-lbw-poster.pdf","booktitle":"Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems","series":"CHI EA '17","year":2017,"isbn":"978-1-4503-4656-6","location":"Denver, Colorado, USA","pages":"2951--2958","numpages":8,"url":"http://doi.acm.org/10.1145/3027063.3053187","doi":"10.1145/3027063.3053187","acmid":3053187,"publisher":"ACM","address":"New York, NY, USA","keywords":"automated feedback, program synthesis, programming education"},{"author":"Stanford Crowd Research Collective","title":"Daemo: A Self-Governed Crowdsourcing Marketplace","pdf":"uist-2015-daemo.pdf","booktitle":"Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology","series":"UIST '15 Adjunct","year":2015,"isbn":"978-1-4503-3780-9","location":"Daegu, Kyungpook, Republic of Korea","pages":"101--102","numpages":2,"url":"http://doi.acm.org/10.1145/2815585.2815739","doi":"10.1145/2815585.2815739","acmid":2815739,"publisher":"ACM","address":"New York, NY, USA","keywords":"crowd research, crowd work., crowdsourcing"},{"author":"Ryo Suzuki,","title":"Toward a Community Enhanced Programming Education","pdf":"chi-2015-workshop.pdf","slide":"chi-2015-workshop-slide.pdf","booktitle":"ACM CHI 2015 Symposium on Emerging Japanese HCI Research Collection","series":"CHI '15 Workshop","year":2015,"location":"Seoul, Korea","publisher":"ACM","address":"New York, NY, USA"},{"author":"Ryo Suzuki,","title":"Interactive and Collaborative Source Code Annotation","pdf":"icse-2015-cumiki.pdf","poster":"icse-2015-cumiki-poster.pdf","booktitle":"Proceedings of the 37th International Conference on Software Engineering - Volume 2","series":"ICSE '15 Poster","year":2015,"location":"Florence, Italy","pages":"799--800","numpages":2,"url":"http://dl.acm.org/citation.cfm?id=2819009.2819173","acmid":2819173,"publisher":"IEEE Press","address":"Piscataway, NJ, USA"},{"author":"Ryo Suzuki,","title":"Network Thresholds and Multiple Equilibria in the Diffusion of Content-Based Platforms","pdf":"wine-2014-network.pdf","poster":"wine-2014-network-poster.pdf","booktitle":"International Conference on Web and Internet Economics","series":"WINE '14 Poster","year":2014,"location":"Beijing, China","publisher":"Springer","address":"New York, NY, USA"}],"content/output/experience.json":[{"period":"August, 2020 --- Current","role":"Assistant Professor","logo":"ucalgary.png","institute":{"name":"University of Calgary","url":"https://www.ucalgary.ca/"},"lab":{"name":"HCI Group","url":"https://ilab.cpsc.ucalgary.ca/"},"advisors":[{"name":"Computer Science","url":"https://science.ucalgary.ca/computer-science"}]},{"period":"August, 2015 --- August, 2020","role":"Research Assistnat","logo":"cu-boulder.png","institute":{"name":"CU Boulder","url":"https://colorado.edu"},"lab":{"name":"THING Lab","url":"https://www.colorado.edu/atlas/thing-lab"},"advisors":[{"name":"Daniel Lightinger","url":"http://leithinger.com/"},{"name":"Mark D. Gross","url":"http://mdgross.net/"},{"name":"Tom Yeh","url":"http://tomyeh.info/"}]},{"period":"May, 2020 --- August, 2020","role":"Research Intern","logo":"microsoft.png","institute":{"name":"Microsoft Research","url":"https://www.microsoft.com/en-us/research/"},"lab":{"name":"EPIC Group","url":"https://www.microsoft.com/en-us/research/group/epic/"},"advisors":[{"name":"Mar Gonzalez Franco","url":"https://www.microsoft.com/en-us/research/people/margon/"},{"name":"Eyal Ofek","url":"https://www.microsoft.com/en-us/research/people/eyalofek/"},{"name":"Mike Sinclair","url":"https://www.microsoft.com/en-us/research/people/sinclair/"},{"name":"Ken Hinckley","url":"https://www.microsoft.com/en-us/research/people/kenh/"}]},{"period":"May, 2019 --- August, 2019","role":"Research Intern","logo":"adobe.png","institute":{"name":"Adobe Research","url":"https://colorado.edu"},"lab":{"name":"Creative Intelligence Lab","url":"https://research.adobe.com/"},"advisors":[{"name":"Rubaiat Habib","url":"https://rubaiathabib.me/"},{"name":"Li-Yi Wei","url":"https://www.liyiwei.org/"},{"name":"Stephen Diverdi","url":"http://www.stephendiverdi.com/"},{"name":"Danny Kaufman","url":"http://dannykaufman.io/"}]},{"period":"December, 2017 --- October, 2018","role":"Visiting Researcher","logo":"ut.png","institute":{"name":"University of Tokyo","url":"https://colorado.edu"},"lab":{"name":"ERATO UIN","url":"http://www.jst.go.jp/erato/kawahara"},"advisors":[{"name":"Yasuaki Kakehi","url":"http://xlab.iii.u-tokyo.ac.jp/"},{"name":"Yoshihiro Kawahara","url":"http://www.akg.t.u-tokyo.ac.jp/"},{"name":"Ryuma Niiyama","url":"https://scholar.google.co.jp/citations?user=0NMf5sgAAAAJ&hl=en"}]},{"period":"May, 2016 --- August, 2016","role":"Research Intern","logo":"uc-berkeley.png","institute":{"name":"UC Berkeley","url":null},"lab":{"name":"BiD Lab","url":"http://bid.berkeley.edu/"},"advisors":[{"name":"Bjoern Hartmann","url":"http://people.eecs.berkeley.edu/~bjoern/"}]},{"period":"May, 2015 --- August, 2015","role":"Research Intern","logo":"stanford-2.png","institute":{"name":"Stanford University","url":"https://stanford.edu"},"lab":{"name":"HCI Group","url":"http://hci.stanford.edu/"},"advisors":[{"name":"Michael Bernstein","url":"http://hci.stanford.edu/msb/"}]},{"period":"October, 2014 --- May, 2015","role":"Research Assistant","logo":"ut.png","institute":{"name":"University of Tokyo","url":null},"lab":{"name":"IIS Lab","url":"http://iis-lab.org/"},"advisors":[{"name":"Koji Yatani","url":"http://iis-lab.org/member/koji-yatani/"}]},{"period":"December, 2014 --- March, 2015","role":"Research Intern","logo":"aist.png","institute":{"name":"AIST","url":null},"lab":{"name":"Media Interaction","url":"https://staff.aist.go.jp/m.goto/MIG/index-j.html"},"advisors":[{"name":"Jun Kato","url":"http://junkato.jp/"}]}],"content/output/projects/flux-marker.json":{"id":"flux-marker","name":"FluxMarker","description":"Enhancing Tactile Graphics with Dynamic Tactile Markers for Blind People","title":"FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers","authors":["Ryo Suzuki","Abigale Stangl","Mark D. Gross","Tom Yeh"],"year":2017,"booktitle":"In Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '17)","publisher":"ACM, New York, NY, USA","pages":"190-199","doi":"https://doi.org/10.1145/3132525.3132548","conference":{"name":"ASSETS 2017","fullname":"The International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS 2017)","url":"https://assets17.sigaccess.org/"},"pdf":"assets-2017-fluxmarker.pdf","video":"https://www.youtube.com/watch?v=VbwIZ9V6i_g","embed":"https://www.youtube.com/embed/VbwIZ9V6i_g","slide":"assets-2017-fluxmarker-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3132548","arxiv":"https://arxiv.org/abs/1708.03783","pageCount":10,"slideCount":53,"dir":"content/output/projects","base":"flux-marker.json","ext":".json","sourceBase":"flux-marker.md","sourceExt":".md"},"content/output/press.json":[{"date":"2020-01","tag":"lift-tiles","media":"Arduino Blog","title":"Prototype room-scale, shape-changing interfaces with LiftTiles","url":"https://blog.arduino.cc/2020/01/27/prototype-room-scale-shape-changing-interfaces-with-lifttiles/"},{"date":"2020-01","tag":"lift-tiles","media":"TechXplore","title":"LiftTiles: Actuator-based Building Blocks for Shape-changing Interfaces","url":"https://techxplore.com/news/2020-01-lifttiles-actuator-based-blocks-shape-changing-interfaces.html"},{"date":"2020-01","tag":"shapebots","media":"ITMedia News","title":"A Swarm of Self-transforming Robots to Assist People","url":"https://www.itmedia.co.jp/news/articles/2001/15/news032.html"},{"date":"2019-10","tag":"lift-tiles","media":"Hackster.io","title":"LiftTiles Turn Walls and Floors Into Reconfigurable Structures on Demand","url":"https://www.hackster.io/news/lifttiles-turn-walls-and-floors-into-reconfigurable-structures-on-demand-4a226d58bc74"},{"date":"2019-10","tag":"lift-tiles","media":"Element 14","title":"Engineers Develop LiftTiles, a Scale Shape-changing Interface","url":"https://www.element14.com/community/community/applications/industrial-automation-space/blog/2019/10/25/engineers-develop-lifttiles-a-scale-shape-changing-interface?CMP=SOM-PRG-TWITTER-BLOG-CATWELL-LIFT-TILES-COMM"},{"date":"2019-11","tag":"shapebots","media":"Bouncy","title":"Swarm Robots that can Change Shape to Visualize Data","url":"https://bouncy.news/53532?fbclid=IwAR0jyfBKo8LJ3aiUidDfZUsQqJ5-oSMxRuiZyJju0g_F6A_hi1tOeboPM4E"},{"date":"2019-10","tag":"shapebots","media":"Hackster.io","title":"Swarming Robots Can Change Their Configuration to Handle Different Tasks","url":"https://www.hackster.io/news/shapebots-swarming-robots-can-change-their-configuration-to-handle-different-tasks-59a5ae926e1d"},{"date":"2019-09","tag":"shapebots","media":"TechXplore","title":"ShapeBots: A Swarm of Shape-shifting Robots that Visually Display Data","url":"https://techxplore.com/news/2019-09-shapebots-swarm-shape-shifting-robots-visually.html"},{"date":"2019-09","tag":"shapebots","media":"Hackaday","title":"Tiny Robots that Grow Taller and Wider","url":"https://hackaday.com/2019/10/04/tiny-robots-that-grow-taller-and-wider/"},{"date":"2019-09","media":"Robotic Gizmo","title":"ShapeBots: Shape-changing Swarm Robots","url":"https://www.roboticgizmos.com/shapebots/"},{"date":"2019-09","tag":"shapebots","media":"Gadgetify","title":"ShapeBots: Shape Changing Swarm Robots","url":"http://www.gadgetify.com/shapebots/"},{"date":"2018-10","tag":"dynablock","media":"3DPrint.com","title":"Dynablock: 3D Prints That Assemble and Disassemble in Seconds","url":"https://3dprint.com/227781/3d-prints-that-assemble-in-seconds/"},{"date":"2018-10","tag":"dynablock","media":"Hackster.io","title":"The Dynamic 3D Printing That Assembles and Disassembles Objects in Seconds","url":"https://www.hackster.io/news/dynablock-the-dynamic-3d-printing-that-assembles-and-disassembles-objects-in-seconds-a7f7d4bf6cad"},{"date":"2018-10","tag":"dynablock","media":"Arduino Blog","title":"Create Shapes Over and Over with the Dynablock 3D Printer","url":"https://blog.arduino.cc/2018/10/22/create-shapes-over-and-over-with-the-dynablock-3d-printer/"},{"date":"2018-10","tag":"dynablock","media":"3DRuck.com","title":"Dynablock: Dynamic 3D Printer Creates Objects in Seconds","url":"https://3druck.com/forschung/dynablock-dynamischer-3d-drucker-erstellt-objekte-in-sekunden-2776738/"},{"date":"2018-10","tag":"dynablock","media":"World Business Satellite","title":"Repeatable 3D Printer","url":"https://txbiz.tv-tokyo.co.jp/wbs/trend_tamago/post_168589/"},{"date":"2018-10","tag":"dynablock","media":"Nikkei Newspaper","title":"Modeling 3D Objects with Magnet-Embedded Blocks","url":"https://active.nikkeibp.co.jp/atclact/active/17/071100318/101600559/"},{"date":"2016-06","tag":"atelier","media":"Wired","title":"Its Not Just Robots: Skilled Jobs Are Going to Meatware","url":"https://www.wired.com/2016/06/its-not-just-robots-skilled-jobs-are-going-to-meatware/"}],"content/output/projects/pep.json":{"id":"pep","name":"PEP","description":"3D Printed Electronic Papercrafts - An Integrated Approach for 3D Sculpting Paper-based Electronic Devices","title":"PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-based Electronic Devices","authors":["Hyunjoo Oh","Tung D. Ta","Ryo Suzuki","Mark D. Gross","Yoshihiro Kawahara","Lining Yao"],"year":2018,"booktitle":"In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18)","publisher":"ACM, New York, NY, USA","pages":"Paper 441, 12 pages","doi":"https://doi.org/10.1145/3173574.3174015","conference":{"name":"CHI 2018","fullname":"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2018)","url":"https://chi2018.acm.org/"},"pdf":"chi-2018-pep.pdf","video":"https://vimeo.com/252080903","embed":"https://www.youtube.com/embed/DTd863suDN0","short-video":"https://www.youtube.com/watch?v=DTd863suDN0","acm-dl":"https://dl.acm.org/citation.cfm?id=3174015","pageCount":12,"slideCount":0,"dir":"content/output/projects","base":"pep.json","ext":".json","sourceBase":"pep.md","sourceExt":".md"},"content/output/news.json":[{"date":"2020-08-28","text":"Our [**UIST paper**](https://ryosuzuki.org/realitysketch/) won  **Honorable Mention Awarad**."},{"date":"2020-08-07","text":"Joining the [**University of Calgary**](https://www.ucalgary.ca/) as an Assisatnat Professor in CS and [**HCI Group**](https://ilab.cpsc.ucalgary.ca/) starting in Fall 2020."},{"date":"2020-07-28","text":"[**PhD Dissertation**](https://ryosuzuki.org/publications/phd-dissertation.pdf) is submitted and officially graduated! :)"},{"date":"2020-07-01","text":"One [**IROS 2020**](https://www.iros2020.org/) full-paper is accepted (47%)."},{"date":"2020-06-24","text":"One [**UIST 2020**](http://uist.acm.org/uist2020/) full-paper is accepted (21%)."},{"date":"2020-05-18","text":"Start intern at [**Microsoft Research**](https://www.microsoft.com/en-us/research/) (mentor: [**Mar Gonzalez Franco**](https://www.microsoft.com/en-us/research/people/margon/))."},{"date":"2020-05-13","text":"**[PhD Defense](https://ryosuzuki.org/phd-thesis) ... and Passed! :)** (committee: D Leithinger, M Gross, T Yeh, H Ishii, T Igarashi)"},{"date":"2020-02-14---05-13","text":"Traveling for on-site job interviews (UW, UCSB, Boston, Virginia Tech, Calgary)"},{"date":"2019-12-12","text":"One [**CHI 2020**](https://chi2020.acm.org/) full-paper is accepted (24%)."},{"date":"2019-11-25","text":"Traveling to Boston to visit **MIT CSAIL** and **Media Lab**"},{"date":"2019-10-19","image":"uist-2019.png","text":"Traveling to New Orleans for [**UIST 2019**](http://uist.acm.org/uist2019/)"},{"date":"2019-10-10","image":"uist-2019.png","text":"Became a **PhD candidate** (committee: D Leithinger, M Gross, T Yeh, H Ishii, T Igarashi, E Do)"},{"date":"2019-10-08","image":"uist-2019.png","text":"One [**TEI 2020**](https://tei.acm.org/2020/) full-paper is accepted (28%)."},{"date":"2019-08-02","image":"uist-2019.png","text":"One [**UIST 2019**](http://uist.acm.org/uist2019/) poster and and doctoral consortium paper are accepted."},{"date":"2019-07-01","image":"uist-2019.png","text":"One [**UIST 2019**](http://uist.acm.org/uist2019/) full-paper is accepted (24%)."},{"date":"2019-06-27","image":"uist-2019.png","text":"Won [**Best Paper Award**](https://dis2019.com/overview/#track-4-shape-changing-interfaces) for [**DIS 2019**](https://dis2019.com/) (Top 1%)."},{"date":"2019-05-20","image":"uist-2019.png","text":"Start intern at [**Adobe Research**](https://research.adobe.com/) (mentor: [**Rubaiat Habib**](https://rubaiathabib.me/))."},{"date":"2019-03-26","image":"dis-2019.png","text":"One [**DIS 2019**](https://dis2019.com/) full-paper is accepted (25%)."},{"date":"2018-08-15","text":"Award [**JST ACT-I**](https://www.jst.go.jp/kisoken/act-i/en/project/111C001/111C001_2018.html) funding (mentor [**Takeo Igarashi**](https://www-ui.is.s.u-tokyo.ac.jp/~takeo/))."},{"date":"2018-08-06","image":"uist-2018.png","text":"One [**UIST 2018**](https://uist.acm.org/uist2018/) full-paper is accepted (21%)."},{"date":"2018-08-03","image":"pg-2018.png","text":"One [**Pacific Graphics 2018**](http://sweb.cityu.edu.hk/pg2018/) short-paper is accepted (26%)."},{"date":"2017-12-14","text":"Start intern at [**The University of Tokyo**](http://www.jst.go.jp/erato/kawahara/) (mentor: [**Yasuaki Kakehi**](http://xlab.iii.u-tokyo.ac.jp/))."},{"date":"2017-12-11","image":"chi-2018.png","text":"Two [**CHI 2018**](https://chi2018.acm.org/) full-papers are accepted (25%)."},{"date":"2017-06-27","image":"vlhcc-2017.png","text":"One [**VL/HCC 2017**](https://sites.google.com/site/vlhcc2017/) full-paper are accepted (29%)."},{"date":"2017-06-21","image":"assets-2017.png","text":"One [**ASSETS 2017**](https://assets17.sigaccess.org/) full-paper are accepted (26%)."},{"date":"2017-02-11","image":"chi-2017.png","text":"One [**CHI 2017**](https://chi2017.acm.org/) LBW paper is accepted (38%)."},{"date":"2016-12-16","image":"","text":"One [**L@S 2017**](http://learningatscale.acm.org/las2017/) full paper is accepted (22%)."},{"date":"2016-12-12","image":"icse-2017.png","text":"One [**ICSE 2017**](http://icse2017.gatech.edu/) full-paper is accepted (19%)."},{"date":"2016-05-23","text":"Start intern at [**UC Berkeley**](http://bid.berkeley.edu/) (mentor: [**Bjoern Hartmann**](http://people.eecs.berkeley.edu/~bjoern/))."},{"date":"2016-01-15","image":"chi-2016.png","text":"One [**CHI 2016**](https://chi2016.acm.org/wp/) full-paper is accepted (23%)."},{"date":"2015-10-01","image":"uist-2016.jpg","text":"I will serve as a web and social media chair for [**UIST 2016**](http://uist.acm.org/uist2016/)"},{"date":"2015-05-18","text":"Start intern at [**Stanford**](https://hci.stanford.edu/) (mentor: [**Michael Bernstein**](https://hci.stanford.edu/msb/))."},{"date":"2014-10-15","text":"The very first day of starting my HCI research (mentor: [**Koji Yatani**](http://iis-lab.org/member/koji-yatani/))"}],"content/output/projects/mixed-initiative.json":{"id":"mixed-initiative","name":"Mixed-Initiative Code Feedback","description":"Writing Reusable Code Feedback at Scale with Mixed-Initiative Program Synthesis","title":"Writing Reusable Code Feedback at Scale with Mixed-Initiative Program Synthesis","authors":["Andrew Head","Elena Glassman","Gustavo Soares","Ryo Suzuki","Lucas Figueredo","Loris DAntoni","Bjrn Hartmann"],"note":"(the first three authors equally contributed)","year":2017,"booktitle":"In Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale (L@S '17)","publisher":"ACM, New York, NY, USA","pages":"89-98","doi":"https://doi.org/10.1145/3051457.3051467","conference":{"name":"L@S 2017","fullname":"The ACM Conference on Learning at Scale (L@S 2017)","url":"http://learningatscale.acm.org/las2017"},"pdf":"las-2017-mixed.pdf","slide":"las-2017-mixed-slide.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=3051467","pageCount":10,"slideCount":62,"image":"mixed-initiative.png","dir":"content/output/projects","base":"mixed-initiative.json","ext":".json","sourceBase":"mixed-initiative.md","sourceExt":".md"},"content/output/projects/refazer.json":{"id":"refazer","name":"Refazer","description":"Learning Syntactic Program Transformations from Examples","title":"Learning Syntactic Program Transformations from Examples","authors":["Reudismam Rolim","Gustavo Soares","Loris D'Antoni","Oleksandr Polozov","Sumit Gulwani","Rohit Gheyi","Ryo Suzuki","Bjrn Hartmann"],"yera":2017,"booktitle":"In Proceedings of the 39th International Conference on Software Engineering (ICSE '17)","publisher":"IEEE Press, Piscataway, NJ, USA","pages":"404-415","conference":{"name":"ICSE 2017","fullname":"The International Conference on Software Engineering (ICSE 2017)","url":"http://icse2017.gatech.edu/"},"pdf":"icse-2017-refazer.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=3097417","arxiv":"https://arxiv.org/abs/1608.09000","pageCount":12,"slideCount":0,"image":"refazer.png","dir":"content/output/projects","base":"refazer.json","ext":".json","sourceBase":"refazer.md","sourceExt":".md"},"content/output/publications.json":[{"author":"Ryo Suzuki, Jun Kato, Mark D. Gross, Tom Yeh","title":"Reactile: Programming Swarm User Interfaces Through Direct Physical Manipulation","booktitle":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","series":"CHI '18","year":2018,"isbn":"978-1-4503-5620-6","location":"Montreal QC, Canada","pages":"199:1--199:13","articleno":199,"numpages":13,"url":"http://doi.acm.org/10.1145/3173574.3173773","doi":"10.1145/3173574.3173773","acmid":3173773,"publisher":"ACM","address":"New York, NY, USA","keywords":"direct manipulation, programming by demonstration, swarm user interfaces, tangible programming"},{"author":"Hyunjoo Oh, Tung D. Ta, Ryo Suzuki, Mark D. Gross, Yoshihiro Kawahara, and Lining Yao.","title":"PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-Based Electronic Devices","booktitle":"Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","series":"CHI '18","year":2018,"isbn":"978-1-4503-5620-6","location":"Montreal QC, Canada","pages":"441:1--441:12","articleno":441,"numpages":12,"url":"http://doi.acm.org/10.1145/3173574.3174015","doi":"10.1145/3173574.3174015","acmid":3174015,"publisher":"ACM","address":"New York, NY, USA","keywords":"3d sculpting, fabrication techniques, paper craft, paper electronics, prototyping"},{"author":"Ryo Suzuki, Abigale Stangl, Mark D. Gross, and Tom Yeh","title":"FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers","booktitle":"Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility","series":"ASSETS '17","year":2017,"isbn":"978-1-4503-4926-0","location":"Baltimore, Maryland, USA","pages":"190--199","numpages":10,"url":"http://doi.acm.org/10.1145/3132525.3132548","doi":"10.1145/3132525.3132548","acmid":3132548,"publisher":"ACM","address":"New York, NY, USA","keywords":"dynamic tactile markers, interactive tactile graphics, tangible interfaces, visual impairment"},{"author":"Andrew Head, Elena Glassman, Gustavo Soares, Ryo Suzuki, Lucas Figueredo, Loris D'Antoni, and Bjoern Hartmann","title":"Writing Reusable Code Feedback at Scale with Mixed-Initiative Program Synthesis","booktitle":"Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale","series":"L@S '17","year":2017,"isbn":"978-1-4503-4450-0","location":"Cambridge, Massachusetts, USA","pages":"89--98","numpages":10,"url":"http://doi.acm.org/10.1145/3051457.3051467","doi":"10.1145/3051457.3051467","acmid":3051467,"publisher":"ACM","address":"New York, NY, USA","keywords":"program synthesis, programming education"},{"author":"Reudismam Rolim, Gustavo Soares, Loris D'Antoni, Oleksandr Polozov, Sumit Gulwani, Rohit Gheyi, Ryo Suzuki, and Bjoern Hartmann","title":"Learning Syntactic Program Transformations from Examples","booktitle":"Proceedings of the 39th International Conference on Software Engineering","series":"ICSE '17","year":2017,"isbn":"978-1-5386-3868-2","location":"Buenos Aires, Argentina","pages":"404--415","numpages":12,"url":"https://doi.org/10.1109/ICSE.2017.44","doi":"10.1109/ICSE.2017.44","acmid":3097417,"publisher":"IEEE Press","address":"Piscataway, NJ, USA","keywords":"program synthesis, program transformation, refactoring, tutoring systems"},{"author":"Ryo Suzuki, Niloufar Salehi, Michelle S. Lam, Juan C. Marroquin, and Michael S. Bernstein","title":"Atelier: Repurposing Expert Crowdsourcing Tasks As Micro-internships","booktitle":"Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems","series":"CHI '16","year":2016,"isbn":"978-1-4503-3362-7","location":"San Jose, California, USA","pages":"2645--2656","numpages":12,"url":"http://doi.acm.org/10.1145/2858036.2858121","doi":"10.1145/2858036.2858121","acmid":2858121,"publisher":"ACM","address":"New York, NY, USA","keywords":"crowd work, crowdsourcing, micro-internships"}],"content/output/projects/tabby.json":{"id":"tabby","name":"Tabby","description":"Explorable Design for 3D Printing Textures","title":"Tabby: Explorable Design for 3D Printing Textures","authors":["Ryo Suzuki","Koji Yatani","Mark D. Gross","Tom Yeh"],"year":2018,"booktitle":"The Pacific Conference on Computer Graphics and Applications (Pacific Graphics 2018)","publisher":"The Eurographics Association","pages":"1-4","doi":"https://doi.org/10.2312/pg.20181273","conference":{"name":"Pacific Graphics 2018","fullname":"The Pacific Conference on Computer Graphics and Applications (Pacific Graphics 2018)","url":"http://sweb.cityu.edu.hk/pg2018/"},"pdf":"pg-2018-tabby.pdf","video":"https://www.youtube.com/watch?v=rRgw8lH74CA","embed":"https://www.youtube.com/embed/rRgw8lH74CA","acm-dl":"https://diglib.eg.org/handle/10.2312/pg20181273","slide":"pg-2018-tabby-slide.pdf","pageCount":4,"slideCount":40,"dir":"content/output/projects","base":"tabby.json","ext":".json","sourceBase":"tabby.md","sourceExt":".md"},"content/output/projects/trace-diff.json":{"id":"trace-diff","name":"TraceDiff","description":"Debugging Unexpected Code Behavior Using Trace Divergences","title":"TraceDiff: Debugging Unexpected Code Behavior Using Trace Divergences","authors":["Ryo Suzuki","Gustavo Soares","Andrew Head","Elena Glassman","Ruan Reis","Melina Mongiovi","Loris DAntoni","Bjern Hartmann"],"year":2017,"booktitle":"In Proceedings of 2017 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC '17)","publisher":"IEEE Press, Piscataway, NJ, USA","pages":"107-115","doi":"https://doi.org/10.1109/VLHCC.2017.8103457","conference":{"name":"VL/HCC 2017","fullname":"IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC 2017)","url":"https://sites.google.com/site/vlhcc2017/"},"pdf":"vlhcc-2017-tracediff.pdf","slide":"vlhcc-2017-tracediff-slide.pdf","github":"https://github.com/ryosuzuki/trace-diff","demo":"https://ryosuzuki.github.io/trace-diff/","ieee":"http://ieeexplore.ieee.org/document/8103457/","arxiv":"https://arxiv.org/abs/1708.03786a","related":{"title":"Exploring the Design Space of Automatically Synthesized Hints for Introductory Programming Assignments","authors":["Ryo Suzuki","Gustavo Soares","Elena Glassman","Andrew Head","Loris D'Antoni","Bjrn Hartmann"],"year":2017,"booktitle":"In Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA '17)","publisher":"ACM, New York, NY, USA","pages":"2951-2958","doi":"https://doi.org/10.1145/3027063.3053187","pdf":"chi-2017-lbw.pdf","suffix":"lbw","pageCount":6},"pageCount":9,"slideCount":77,"dir":"content/output/projects","base":"trace-diff.json","ext":".json","sourceBase":"trace-diff.md","sourceExt":".md"},"content/output/projects/dynablock.json":{"id":"dynablock","name":"Dynablock","description":"Dynamic 3D Printing for Instant and Reconstructable Shape Formation","title":"Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation","authors":["Ryo Suzuki","Junichi Yamaoka","Daniel Leithinger","Tom Yeh","Mark D. Gross","Yoshihiro Kawahara","Yasuaki Kakehi"],"year":2018,"booktitle":"In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology (UIST '18)","publisher":"ACM, New York, NY, USA","pages":"99-111","doi":"https://doi.org/10.1145/3242587.3242659","conference":{"name":"UIST 2018","fullname":"The ACM Symposium on User Interface Software and Technology (UIST 2018)","url":"http://uist.acm.org/uist2018"},"pdf":"uist-2018-dynablock.pdf","video":"https://www.youtube.com/watch?v=7nPlr3O9xu8","embed":"https://www.youtube.com/embed/7nPlr3O9xu8","short-video":"https://www.youtube.com/watch?v=92eGI-gYYc4","slide":"uist-2018-dynablock-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3242659","talk":"https://www.youtube.com/watch?v=R3FRUtOIiCQ","poster":"uist-2018-dynablock-poster.pdf","pageCount":12,"slideCount":53,"dir":"content/output/projects","base":"dynablock.json","ext":".json","sourceBase":"dynablock.md","sourceExt":".md"},"content/output/projects/reactile.json":{"id":"reactile","name":"Reactile","description":"Programming Swarm User Interfaces through Direct Physical Manipulation","title":"Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation","authors":["Ryo Suzuki","Jun Kato","Mark D. Gross","Tom Yeh"],"year":2018,"booktitle":"In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18)","publisher":"ACM, New York, NY, USA","pages":"Paper 199, 13 pages","doi":"https://doi.org/10.1145/3173574.3173773","conference":{"name":"CHI 2018","fullname":"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2018)","url":"https://chi2018.acm.org/"},"pdf":"chi-2018-reactile.pdf","video":"https://www.youtube.com/watch?v=Gb7brajKCVE","embed":"https://www.youtube.com/embed/Gb7brajKCVE","short-video":"https://www.youtube.com/watch?v=YT7vMJZjohU","slide":"chi-2018-reactile-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3173773","github":"https://github.com/ryosuzuki/reactile","pageCount":12,"slideCount":56,"dir":"content/output/projects","base":"reactile.json","ext":".json","sourceBase":"reactile.md","sourceExt":".md"},"content/output/projects/lift-tiles.json":{"id":"lift-tiles","name":"LiftTiles","description":"Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces","title":"LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces","authors":["Ryo Suzuki","Ryosuke Nakayama","Dan Liu","Yasuaki Kakehi","Mark D. Gross","Daniel Leithinger"],"year":2020,"booktitle":"In Proceedings of the 14th ACM International Conference on Tangible, Embedded and Embodied Interaction (TEI '20)","publisher":"ACM, New York, NY, USA","pages":"143151","doi":"https://doi.org/10.1145/3374920.3374941","conference":{"name":"TEI 2020","fullname":"The ACM International Conference on Tangible, Embedded and Embodied Interaction (TEI 2020)","url":"https://tei.acm.org/2020/"},"video":"https://www.youtube.com/watch?v=0LHeTkOMR84","embed":"https://www.youtube.com/embed/0LHeTkOMR84","pdf":"tei-2020-lift-tiles.pdf","slide":"tei-2020-lift-tiles-slide.pdf","poster":"uist-2019-lift-tiles-poster.pdf","pageCount":9,"slideCount":37,"related":{"title":"LiftTiles: Modular and Reconfigurable Room-scale Shape Displays through Retractable Inflatable Actuators","authors":["Ryo Suzuki","Ryosuke Nakayama","Dan Liu","Yasuaki Kakehi","Mark D. Gross","Daniel Leithinger"],"year":2019,"booktitle":"In Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST '19)","publisher":"ACM, New York, NY, USA","pages":"1-3","doi":"https://doi.org/10.1145/3332167.3357105","url":"http://uist.acm.org/uist2019","pdf":"uist-2019-lift-tiles.pdf","suffix":"adjunct","pageCount":3},"dir":"content/output/projects","base":"lift-tiles.json","ext":".json","sourceBase":"lift-tiles.md","sourceExt":".md"},"content/output/projects/shapebots.json":{"id":"shapebots","name":"ShapeBots","description":"Shape-changing Swarm Robots","title":"ShapeBots: Shape-changing Swarm Robots","authors":["Ryo Suzuki","Clement Zheng","Yasuaki Kakehi","Tom Yeh","Ellen Yi-Luen Do","Mark D. Gross","Daniel Leithinger"],"year":2019,"booktitle":"In Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST '19)","publisher":"ACM, New York, NY, USA","pages":"1-13","doi":"https://doi.org/10.1145/3332165.3347911","conference":{"name":"UIST 2019","fullname":"The ACM Symposium on User Interface Software and Technology (UIST 2019)","url":"http://uist.acm.org/uist2019"},"pdf":"uist-2019-shapebots.pdf","slide":"uist-2019-shapebots-slide.pdf","video":"https://www.youtube.com/watch?v=cwPaof0kKdM","embed":"https://www.youtube.com/embed/cwPaof0kKdM","github":"https://github.com/ryosuzuki/shapebots","poster":"uist-2019-shapebots-poster.pdf","demo":"https://ryosuzuki.github.io/shapebots-simulator/","arxiv":"https://arxiv.org/abs/1909.03372","pageCount":13,"slideCount":53,"dir":"content/output/projects","base":"shapebots.json","ext":".json","sourceBase":"shapebots.md","sourceExt":".md"},"content/output/projects/morphio.json":{"id":"morphio","name":"MorphIO","description":"Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction","title":"MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction","authors":["Ryosuke Nakayama*","Ryo Suzuki*","Satoshi Nakamaru","Ryuma Niiyama","Yoshihiro Kawahara","Yasuaki Kakehi"],"note":"(the first two authors equally contributed)","year":2018,"booktitle":"In Proceedings of the 2019 on Designing Interactive Systems Conference (DIS '19)","publisher":"ACM, New York, NY, USA","pages":"975-986","doi":"https://doi.org/10.1145/3322276.3322337","conference":{"name":"DIS 2019","fullname":"The ACM conference on Designing Interactive Systems (DIS 2019) - Best Paper Award","url":"https://dis2019.com/"},"pdf":"dis-2019-morphio.pdf","slide":"dis-2019-morphio-slide.pdf","video":"https://www.youtube.com/watch?v=ZkCcazfFD-M","embed":"https://www.youtube.com/embed/ZkCcazfFD-M","acm-dl":"https://dl.acm.org/citation.cfm?id=3322337","pageCount":12,"slideCount":52,"dir":"content/output/projects","base":"morphio.json","ext":".json","sourceBase":"morphio.md","sourceExt":".md"},"content/output/projects/roomshift.json":{"id":"roomshift","name":"RoomShift","description":"Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots","title":"RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots","authors":["Ryo Suzuki","Hooman Hedayati","Clement Zheng","James Bohn","Daniel Szafir","Ellen Yi-Luen Do","Mark D Gross","Daniel Leithinger"],"year":2020,"booktitle":"In Proceedings of the ACM CHI Conference on Human Factors in Computing Systems (CHI '20)","publisher":"ACM, New York, NY, USA","pages":"Paper 396, 11 pages","doi":"https://doi.org/10.1145/3313831.3376523","conference":{"name":"CHI 2020","fullname":"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2020)","url":"https://chi2020.acm.org/"},"pdf":"chi-2020-roomshift.pdf","arxiv":"https://arxiv.org/abs/2008.08695","pageCount":11,"slideCount":0,"dir":"content/output/projects","base":"roomshift.json","ext":".json","sourceBase":"roomshift.md","sourceExt":".md"},"content/output/projects/phd-thesis.json":{"id":"phd-thesis","name":"Collective Shape-changing Interfaces","description":"Dynamic Shape Construction and Transformation with Collective Elements","title":"Dynamic Shape Construction and Transformation with Collective Elements","authors":["Ryo Suzuki"],"year":2020,"booktitle":"PhD Dissertation","publisher":"University of Colorado Boulder","pages":"1-289","conference":{"name":"PhD Dissertation","fullname":"PhD Dissertation"},"pdf":"phd-dissertation.pdf","slide":"phd-defense.pdf","talk":"https://www.youtube.com/watch?v=FHmp7BIhXJI","pageCount":250,"slideCount":198,"related":{"title":"Collective Shape-changing Interfaces","authors":["Ryo Suzuki"],"year":2019,"booktitle":"In Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST '19 Doctoral Consortium)","publisher":"ACM, New York, NY, USA","pages":"154157","doi":"https://doi.org/10.1145/3332167.3356877","pdf":"uist-2019-collective.pdf","suffix":"dc","pageCount":4},"dir":"content/output/projects","base":"phd-thesis.json","ext":".json","sourceBase":"phd-thesis.md","sourceExt":".md"},"content/output/projects/pufferbot.json":{"id":"pufferbot","name":"PufferBot","description":"Actuated Expandable Structures for Aerial Robots","title":"PufferBot: Actuated Expandable Structures for Aerial Robots","authors":["Hooman Hedayati","Ryo Suzuki","Daniel Leithinger","Daniel Szafir"],"year":2020,"booktitle":"In Proceedings of the 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS '20)","publisher":"IEEE, New York, NY, USA","pages":"1-6","conference":{"name":"IROS 2020","fullname":"The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2020)","url":"https://www.iros2020.org/"},"pdf":"iros-2020-pufferbot.pdf","video":"https://www.youtube.com/watch?v=XtPepCxWcBg","embed":"https://www.youtube.com/embed/XtPepCxWcBg","arxiv":"https://arxiv.org/abs/2008.07615","pageCount":6,"slideCount":0,"dir":"content/output/projects","base":"pufferbot.json","ext":".json","sourceBase":"pufferbot.md","sourceExt":".md"},"content/output/projects/realitysketch.json":{"id":"realitysketch","name":"RealitySketch","description":"Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching","title":"RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching","authors":["Ryo Suzuki","Rubaiat Habib Kazi","Li-Yi Wei","Stephen DiVerdi","Wilmot Li","Daniel Leithinger"],"year":2020,"booktitle":"In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology (UIST '20)","publisher":"ACM, New York, NY, USA","pages":"1-16","doi":"https://doi.org/10.1145/3379337.3415892","conference":{"name":"UIST 2020","fullname":"The ACM Symposium on User Interface Software and Technology (UIST 2020) - Honorable Mention Award","url":"https://uist.acm.org/uist2020/"},"pdf":"uist-2020-realitysketch.pdf","video":"https://www.youtube.com/watch?v=L0p-BNU9rXU","embed":"https://www.youtube.com/embed/L0p-BNU9rXU","arxiv":"https://arxiv.org/abs/2008.08688","pageCount":16,"slideCount":0,"dir":"content/output/projects","base":"realitysketch.json","ext":".json","sourceBase":"realitysketch.md","sourceExt":".md"}},"sourceFileArray":["content/activities.md","content/experience.yaml","content/fellowship.md","content/news.yaml","content/posters.yaml","content/press.yaml","content/projects/atelier.md","content/projects/dynablock.md","content/projects/flux-marker.md","content/projects/lift-tiles.md","content/projects/mixed-initiative.md","content/projects/morphio.md","content/projects/pep.md","content/projects/phd-thesis.md","content/projects/pufferbot.md","content/projects/reactile.md","content/projects/realitysketch.md","content/projects/refazer.md","content/projects/roomshift.md","content/projects/shapebots.md","content/projects/tabby.md","content/projects/trace-diff.md","content/publications.yaml"]};

/***/ }),

/***/ "./node_modules/@babel/runtime-corejs2/core-js/object/create.js":
/*!**********************************************************************!*\
  !*** ./node_modules/@babel/runtime-corejs2/core-js/object/create.js ***!
  \**********************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

module.exports = __webpack_require__(/*! core-js/library/fn/object/create */ "./node_modules/core-js/library/fn/object/create.js");

/***/ }),

/***/ "./node_modules/@babel/runtime-corejs2/core-js/object/define-property.js":
/*!*******************************************************************************!*\
  !*** ./node_modules/@babel/runtime-corejs2/core-js/object/define-property.js ***!
  \*******************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

module.exports = __webpack_require__(/*! core-js/library/fn/object/define-property */ "./node_modules/core-js/library/fn/object/define-property.js");

/***/ }),

/***/ "./node_modules/@babel/runtime-corejs2/core-js/object/get-prototype-of.js":
/*!********************************************************************************!*\
  !*** ./node_modules/@babel/runtime-corejs2/core-js/object/get-prototype-of.js ***!
  \********************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

module.exports = __webpack_require__(/*! core-js/library/fn/object/get-prototype-of */ "./node_modules/core-js/library/fn/object/get-prototype-of.js");

/***/ }),

/***/ "./node_modules/@babel/runtime-corejs2/core-js/object/set-prototype-of.js":
/*!********************************************************************************!*\
  !*** ./node_modules/@babel/runtime-corejs2/core-js/object/set-prototype-of.js ***!
  \********************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

module.exports = __webpack_require__(/*! core-js/library/fn/object/set-prototype-of */ "./node_modules/core-js/library/fn/object/set-prototype-of.js");

/***/ }),

/***/ "./node_modules/@babel/runtime-corejs2/core-js/symbol.js":
/*!***************************************************************!*\
  !*** ./node_modules/@babel/runtime-corejs2/core-js/symbol.js ***!
  \***************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

module.exports = __webpack_require__(/*! core-js/library/fn/symbol */ "./node_modules/core-js/library/fn/symbol/index.js");

/***/ }),

/***/ "./node_modules/@babel/runtime-corejs2/core-js/symbol/iterator.js":
/*!************************************************************************!*\
  !*** ./node_modules/@babel/runtime-corejs2/core-js/symbol/iterator.js ***!
  \************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

module.exports = __webpack_require__(/*! core-js/library/fn/symbol/iterator */ "./node_modules/core-js/library/fn/symbol/iterator.js");

/***/ }),

/***/ "./node_modules/@babel/runtime-corejs2/helpers/esm/assertThisInitialized.js":
/*!**********************************************************************************!*\
  !*** ./node_modules/@babel/runtime-corejs2/helpers/esm/assertThisInitialized.js ***!
  \**********************************************************************************/
/*! exports provided: default */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
__webpack_require__.r(__webpack_exports__);
/* harmony export (binding) */ __webpack_require__.d(__webpack_exports__, "default", function() { return _assertThisInitialized; });
function _assertThisInitialized(self) {
  if (self === void 0) {
    throw new ReferenceError("this hasn't been initialised - super() hasn't been called");
  }

  return self;
}

/***/ }),

/***/ "./node_modules/@babel/runtime-corejs2/helpers/esm/classCallCheck.js":
/*!***************************************************************************!*\
  !*** ./node_modules/@babel/runtime-corejs2/helpers/esm/classCallCheck.js ***!
  \***************************************************************************/
/*! exports provided: default */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
__webpack_require__.r(__webpack_exports__);
/* harmony export (binding) */ __webpack_require__.d(__webpack_exports__, "default", function() { return _classCallCheck; });
function _classCallCheck(instance, Constructor) {
  if (!(instance instanceof Constructor)) {
    throw new TypeError("Cannot call a class as a function");
  }
}

/***/ }),

/***/ "./node_modules/@babel/runtime-corejs2/helpers/esm/createClass.js":
/*!************************************************************************!*\
  !*** ./node_modules/@babel/runtime-corejs2/helpers/esm/createClass.js ***!
  \************************************************************************/
/*! exports provided: default */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
__webpack_require__.r(__webpack_exports__);
/* harmony export (binding) */ __webpack_require__.d(__webpack_exports__, "default", function() { return _createClass; });
/* harmony import */ var _core_js_object_define_property__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../core-js/object/define-property */ "./node_modules/@babel/runtime-corejs2/core-js/object/define-property.js");
/* harmony import */ var _core_js_object_define_property__WEBPACK_IMPORTED_MODULE_0___default = /*#__PURE__*/__webpack_require__.n(_core_js_object_define_property__WEBPACK_IMPORTED_MODULE_0__);


function _defineProperties(target, props) {
  for (var i = 0; i < props.length; i++) {
    var descriptor = props[i];
    descriptor.enumerable = descriptor.enumerable || false;
    descriptor.configurable = true;
    if ("value" in descriptor) descriptor.writable = true;

    _core_js_object_define_property__WEBPACK_IMPORTED_MODULE_0___default()(target, descriptor.key, descriptor);
  }
}

function _createClass(Constructor, protoProps, staticProps) {
  if (protoProps) _defineProperties(Constructor.prototype, protoProps);
  if (staticProps) _defineProperties(Constructor, staticProps);
  return Constructor;
}

/***/ }),

/***/ "./node_modules/@babel/runtime-corejs2/helpers/esm/getPrototypeOf.js":
/*!***************************************************************************!*\
  !*** ./node_modules/@babel/runtime-corejs2/helpers/esm/getPrototypeOf.js ***!
  \***************************************************************************/
/*! exports provided: default */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
__webpack_require__.r(__webpack_exports__);
/* harmony export (binding) */ __webpack_require__.d(__webpack_exports__, "default", function() { return _getPrototypeOf; });
/* harmony import */ var _core_js_object_get_prototype_of__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../core-js/object/get-prototype-of */ "./node_modules/@babel/runtime-corejs2/core-js/object/get-prototype-of.js");
/* harmony import */ var _core_js_object_get_prototype_of__WEBPACK_IMPORTED_MODULE_0___default = /*#__PURE__*/__webpack_require__.n(_core_js_object_get_prototype_of__WEBPACK_IMPORTED_MODULE_0__);
/* harmony import */ var _core_js_object_set_prototype_of__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../../core-js/object/set-prototype-of */ "./node_modules/@babel/runtime-corejs2/core-js/object/set-prototype-of.js");
/* harmony import */ var _core_js_object_set_prototype_of__WEBPACK_IMPORTED_MODULE_1___default = /*#__PURE__*/__webpack_require__.n(_core_js_object_set_prototype_of__WEBPACK_IMPORTED_MODULE_1__);


function _getPrototypeOf(o) {
  _getPrototypeOf = _core_js_object_set_prototype_of__WEBPACK_IMPORTED_MODULE_1___default.a ? _core_js_object_get_prototype_of__WEBPACK_IMPORTED_MODULE_0___default.a : function _getPrototypeOf(o) {
    return o.__proto__ || _core_js_object_get_prototype_of__WEBPACK_IMPORTED_MODULE_0___default()(o);
  };
  return _getPrototypeOf(o);
}

/***/ }),

/***/ "./node_modules/@babel/runtime-corejs2/helpers/esm/inherits.js":
/*!*********************************************************************!*\
  !*** ./node_modules/@babel/runtime-corejs2/helpers/esm/inherits.js ***!
  \*********************************************************************/
/*! exports provided: default */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
__webpack_require__.r(__webpack_exports__);
/* harmony export (binding) */ __webpack_require__.d(__webpack_exports__, "default", function() { return _inherits; });
/* harmony import */ var _core_js_object_create__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../core-js/object/create */ "./node_modules/@babel/runtime-corejs2/core-js/object/create.js");
/* harmony import */ var _core_js_object_create__WEBPACK_IMPORTED_MODULE_0___default = /*#__PURE__*/__webpack_require__.n(_core_js_object_create__WEBPACK_IMPORTED_MODULE_0__);
/* harmony import */ var _setPrototypeOf__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./setPrototypeOf */ "./node_modules/@babel/runtime-corejs2/helpers/esm/setPrototypeOf.js");


function _inherits(subClass, superClass) {
  if (typeof superClass !== "function" && superClass !== null) {
    throw new TypeError("Super expression must either be null or a function");
  }

  subClass.prototype = _core_js_object_create__WEBPACK_IMPORTED_MODULE_0___default()(superClass && superClass.prototype, {
    constructor: {
      value: subClass,
      writable: true,
      configurable: true
    }
  });
  if (superClass) Object(_setPrototypeOf__WEBPACK_IMPORTED_MODULE_1__["default"])(subClass, superClass);
}

/***/ }),

/***/ "./node_modules/@babel/runtime-corejs2/helpers/esm/possibleConstructorReturn.js":
/*!**************************************************************************************!*\
  !*** ./node_modules/@babel/runtime-corejs2/helpers/esm/possibleConstructorReturn.js ***!
  \**************************************************************************************/
/*! exports provided: default */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
__webpack_require__.r(__webpack_exports__);
/* harmony export (binding) */ __webpack_require__.d(__webpack_exports__, "default", function() { return _possibleConstructorReturn; });
/* harmony import */ var _helpers_esm_typeof__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../helpers/esm/typeof */ "./node_modules/@babel/runtime-corejs2/helpers/esm/typeof.js");
/* harmony import */ var _assertThisInitialized__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./assertThisInitialized */ "./node_modules/@babel/runtime-corejs2/helpers/esm/assertThisInitialized.js");


function _possibleConstructorReturn(self, call) {
  if (call && (Object(_helpers_esm_typeof__WEBPACK_IMPORTED_MODULE_0__["default"])(call) === "object" || typeof call === "function")) {
    return call;
  }

  return Object(_assertThisInitialized__WEBPACK_IMPORTED_MODULE_1__["default"])(self);
}

/***/ }),

/***/ "./node_modules/@babel/runtime-corejs2/helpers/esm/setPrototypeOf.js":
/*!***************************************************************************!*\
  !*** ./node_modules/@babel/runtime-corejs2/helpers/esm/setPrototypeOf.js ***!
  \***************************************************************************/
/*! exports provided: default */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
__webpack_require__.r(__webpack_exports__);
/* harmony export (binding) */ __webpack_require__.d(__webpack_exports__, "default", function() { return _setPrototypeOf; });
/* harmony import */ var _core_js_object_set_prototype_of__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../core-js/object/set-prototype-of */ "./node_modules/@babel/runtime-corejs2/core-js/object/set-prototype-of.js");
/* harmony import */ var _core_js_object_set_prototype_of__WEBPACK_IMPORTED_MODULE_0___default = /*#__PURE__*/__webpack_require__.n(_core_js_object_set_prototype_of__WEBPACK_IMPORTED_MODULE_0__);

function _setPrototypeOf(o, p) {
  _setPrototypeOf = _core_js_object_set_prototype_of__WEBPACK_IMPORTED_MODULE_0___default.a || function _setPrototypeOf(o, p) {
    o.__proto__ = p;
    return o;
  };

  return _setPrototypeOf(o, p);
}

/***/ }),

/***/ "./node_modules/@babel/runtime-corejs2/helpers/esm/typeof.js":
/*!*******************************************************************!*\
  !*** ./node_modules/@babel/runtime-corejs2/helpers/esm/typeof.js ***!
  \*******************************************************************/
/*! exports provided: default */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
__webpack_require__.r(__webpack_exports__);
/* harmony export (binding) */ __webpack_require__.d(__webpack_exports__, "default", function() { return _typeof; });
/* harmony import */ var _core_js_symbol_iterator__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../core-js/symbol/iterator */ "./node_modules/@babel/runtime-corejs2/core-js/symbol/iterator.js");
/* harmony import */ var _core_js_symbol_iterator__WEBPACK_IMPORTED_MODULE_0___default = /*#__PURE__*/__webpack_require__.n(_core_js_symbol_iterator__WEBPACK_IMPORTED_MODULE_0__);
/* harmony import */ var _core_js_symbol__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../../core-js/symbol */ "./node_modules/@babel/runtime-corejs2/core-js/symbol.js");
/* harmony import */ var _core_js_symbol__WEBPACK_IMPORTED_MODULE_1___default = /*#__PURE__*/__webpack_require__.n(_core_js_symbol__WEBPACK_IMPORTED_MODULE_1__);



function _typeof2(obj) { if (typeof _core_js_symbol__WEBPACK_IMPORTED_MODULE_1___default.a === "function" && typeof _core_js_symbol_iterator__WEBPACK_IMPORTED_MODULE_0___default.a === "symbol") { _typeof2 = function _typeof2(obj) { return typeof obj; }; } else { _typeof2 = function _typeof2(obj) { return obj && typeof _core_js_symbol__WEBPACK_IMPORTED_MODULE_1___default.a === "function" && obj.constructor === _core_js_symbol__WEBPACK_IMPORTED_MODULE_1___default.a && obj !== _core_js_symbol__WEBPACK_IMPORTED_MODULE_1___default.a.prototype ? "symbol" : typeof obj; }; } return _typeof2(obj); }

function _typeof(obj) {
  if (typeof _core_js_symbol__WEBPACK_IMPORTED_MODULE_1___default.a === "function" && _typeof2(_core_js_symbol_iterator__WEBPACK_IMPORTED_MODULE_0___default.a) === "symbol") {
    _typeof = function _typeof(obj) {
      return _typeof2(obj);
    };
  } else {
    _typeof = function _typeof(obj) {
      return obj && typeof _core_js_symbol__WEBPACK_IMPORTED_MODULE_1___default.a === "function" && obj.constructor === _core_js_symbol__WEBPACK_IMPORTED_MODULE_1___default.a && obj !== _core_js_symbol__WEBPACK_IMPORTED_MODULE_1___default.a.prototype ? "symbol" : _typeof2(obj);
    };
  }

  return _typeof(obj);
}

/***/ }),

/***/ "./node_modules/bail/index.js":
/*!************************************!*\
  !*** ./node_modules/bail/index.js ***!
  \************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = bail

function bail(err) {
  if (err) {
    throw err
  }
}


/***/ }),

/***/ "./node_modules/character-entities-legacy/index.json":
/*!***********************************************************!*\
  !*** ./node_modules/character-entities-legacy/index.json ***!
  \***********************************************************/
/*! exports provided: AElig, AMP, Aacute, Acirc, Agrave, Aring, Atilde, Auml, COPY, Ccedil, ETH, Eacute, Ecirc, Egrave, Euml, GT, Iacute, Icirc, Igrave, Iuml, LT, Ntilde, Oacute, Ocirc, Ograve, Oslash, Otilde, Ouml, QUOT, REG, THORN, Uacute, Ucirc, Ugrave, Uuml, Yacute, aacute, acirc, acute, aelig, agrave, amp, aring, atilde, auml, brvbar, ccedil, cedil, cent, copy, curren, deg, divide, eacute, ecirc, egrave, eth, euml, frac12, frac14, frac34, gt, iacute, icirc, iexcl, igrave, iquest, iuml, laquo, lt, macr, micro, middot, nbsp, not, ntilde, oacute, ocirc, ograve, ordf, ordm, oslash, otilde, ouml, para, plusmn, pound, quot, raquo, reg, sect, shy, sup1, sup2, sup3, szlig, thorn, times, uacute, ucirc, ugrave, uml, uuml, yacute, yen, yuml, default */
/***/ (function(module) {

module.exports = {"AElig":"","AMP":"&","Aacute":"","Acirc":"","Agrave":"","Aring":"","Atilde":"","Auml":"","COPY":"","Ccedil":"","ETH":"","Eacute":"","Ecirc":"","Egrave":"","Euml":"","GT":">","Iacute":"","Icirc":"","Igrave":"","Iuml":"","LT":"<","Ntilde":"","Oacute":"","Ocirc":"","Ograve":"","Oslash":"","Otilde":"","Ouml":"","QUOT":"\"","REG":"","THORN":"","Uacute":"","Ucirc":"","Ugrave":"","Uuml":"","Yacute":"","aacute":"","acirc":"","acute":"","aelig":"","agrave":"","amp":"&","aring":"","atilde":"","auml":"","brvbar":"","ccedil":"","cedil":"","cent":"","copy":"","curren":"","deg":"","divide":"","eacute":"","ecirc":"","egrave":"","eth":"","euml":"","frac12":"","frac14":"","frac34":"","gt":">","iacute":"","icirc":"","iexcl":"","igrave":"","iquest":"","iuml":"","laquo":"","lt":"<","macr":"","micro":"","middot":"","nbsp":"","not":"","ntilde":"","oacute":"","ocirc":"","ograve":"","ordf":"","ordm":"","oslash":"","otilde":"","ouml":"","para":"","plusmn":"","pound":"","quot":"\"","raquo":"","reg":"","sect":"","shy":"","sup1":"","sup2":"","sup3":"","szlig":"","thorn":"","times":"","uacute":"","ucirc":"","ugrave":"","uml":"","uuml":"","yacute":"","yen":"","yuml":""};

/***/ }),

/***/ "./node_modules/character-reference-invalid/index.json":
/*!*************************************************************!*\
  !*** ./node_modules/character-reference-invalid/index.json ***!
  \*************************************************************/
/*! exports provided: 0, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 142, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 158, 159, default */
/***/ (function(module) {

module.exports = {"0":"","128":"","130":"","131":"","132":"","133":"","134":"","135":"","136":"","137":"","138":"","139":"","140":"","142":"","145":"","146":"","147":"","148":"","149":"","150":"","151":"","152":"","153":"","154":"","155":"","156":"","158":"","159":""};

/***/ }),

/***/ "./node_modules/collapse-white-space/index.js":
/*!****************************************************!*\
  !*** ./node_modules/collapse-white-space/index.js ***!
  \****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = collapse

/* collapse(' \t\nbar \nbaz\t'); // ' bar baz ' */
function collapse(value) {
  return String(value).replace(/\s+/g, ' ')
}


/***/ }),

/***/ "./node_modules/core-js/library/fn/object/create.js":
/*!**********************************************************!*\
  !*** ./node_modules/core-js/library/fn/object/create.js ***!
  \**********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

__webpack_require__(/*! ../../modules/es6.object.create */ "./node_modules/core-js/library/modules/es6.object.create.js");
var $Object = __webpack_require__(/*! ../../modules/_core */ "./node_modules/core-js/library/modules/_core.js").Object;
module.exports = function create(P, D) {
  return $Object.create(P, D);
};


/***/ }),

/***/ "./node_modules/core-js/library/fn/object/define-property.js":
/*!*******************************************************************!*\
  !*** ./node_modules/core-js/library/fn/object/define-property.js ***!
  \*******************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

__webpack_require__(/*! ../../modules/es6.object.define-property */ "./node_modules/core-js/library/modules/es6.object.define-property.js");
var $Object = __webpack_require__(/*! ../../modules/_core */ "./node_modules/core-js/library/modules/_core.js").Object;
module.exports = function defineProperty(it, key, desc) {
  return $Object.defineProperty(it, key, desc);
};


/***/ }),

/***/ "./node_modules/core-js/library/fn/object/get-prototype-of.js":
/*!********************************************************************!*\
  !*** ./node_modules/core-js/library/fn/object/get-prototype-of.js ***!
  \********************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

__webpack_require__(/*! ../../modules/es6.object.get-prototype-of */ "./node_modules/core-js/library/modules/es6.object.get-prototype-of.js");
module.exports = __webpack_require__(/*! ../../modules/_core */ "./node_modules/core-js/library/modules/_core.js").Object.getPrototypeOf;


/***/ }),

/***/ "./node_modules/core-js/library/fn/object/set-prototype-of.js":
/*!********************************************************************!*\
  !*** ./node_modules/core-js/library/fn/object/set-prototype-of.js ***!
  \********************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

__webpack_require__(/*! ../../modules/es6.object.set-prototype-of */ "./node_modules/core-js/library/modules/es6.object.set-prototype-of.js");
module.exports = __webpack_require__(/*! ../../modules/_core */ "./node_modules/core-js/library/modules/_core.js").Object.setPrototypeOf;


/***/ }),

/***/ "./node_modules/core-js/library/fn/symbol/index.js":
/*!*********************************************************!*\
  !*** ./node_modules/core-js/library/fn/symbol/index.js ***!
  \*********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

__webpack_require__(/*! ../../modules/es6.symbol */ "./node_modules/core-js/library/modules/es6.symbol.js");
__webpack_require__(/*! ../../modules/es6.object.to-string */ "./node_modules/core-js/library/modules/es6.object.to-string.js");
__webpack_require__(/*! ../../modules/es7.symbol.async-iterator */ "./node_modules/core-js/library/modules/es7.symbol.async-iterator.js");
__webpack_require__(/*! ../../modules/es7.symbol.observable */ "./node_modules/core-js/library/modules/es7.symbol.observable.js");
module.exports = __webpack_require__(/*! ../../modules/_core */ "./node_modules/core-js/library/modules/_core.js").Symbol;


/***/ }),

/***/ "./node_modules/core-js/library/fn/symbol/iterator.js":
/*!************************************************************!*\
  !*** ./node_modules/core-js/library/fn/symbol/iterator.js ***!
  \************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

__webpack_require__(/*! ../../modules/es6.string.iterator */ "./node_modules/core-js/library/modules/es6.string.iterator.js");
__webpack_require__(/*! ../../modules/web.dom.iterable */ "./node_modules/core-js/library/modules/web.dom.iterable.js");
module.exports = __webpack_require__(/*! ../../modules/_wks-ext */ "./node_modules/core-js/library/modules/_wks-ext.js").f('iterator');


/***/ }),

/***/ "./node_modules/core-js/library/modules/_a-function.js":
/*!*************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_a-function.js ***!
  \*************************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

module.exports = function (it) {
  if (typeof it != 'function') throw TypeError(it + ' is not a function!');
  return it;
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_add-to-unscopables.js":
/*!*********************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_add-to-unscopables.js ***!
  \*********************************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

module.exports = function () { /* empty */ };


/***/ }),

/***/ "./node_modules/core-js/library/modules/_an-object.js":
/*!************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_an-object.js ***!
  \************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var isObject = __webpack_require__(/*! ./_is-object */ "./node_modules/core-js/library/modules/_is-object.js");
module.exports = function (it) {
  if (!isObject(it)) throw TypeError(it + ' is not an object!');
  return it;
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_array-includes.js":
/*!*****************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_array-includes.js ***!
  \*****************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

// false -> Array#indexOf
// true  -> Array#includes
var toIObject = __webpack_require__(/*! ./_to-iobject */ "./node_modules/core-js/library/modules/_to-iobject.js");
var toLength = __webpack_require__(/*! ./_to-length */ "./node_modules/core-js/library/modules/_to-length.js");
var toAbsoluteIndex = __webpack_require__(/*! ./_to-absolute-index */ "./node_modules/core-js/library/modules/_to-absolute-index.js");
module.exports = function (IS_INCLUDES) {
  return function ($this, el, fromIndex) {
    var O = toIObject($this);
    var length = toLength(O.length);
    var index = toAbsoluteIndex(fromIndex, length);
    var value;
    // Array#includes uses SameValueZero equality algorithm
    // eslint-disable-next-line no-self-compare
    if (IS_INCLUDES && el != el) while (length > index) {
      value = O[index++];
      // eslint-disable-next-line no-self-compare
      if (value != value) return true;
    // Array#indexOf ignores holes, Array#includes - not
    } else for (;length > index; index++) if (IS_INCLUDES || index in O) {
      if (O[index] === el) return IS_INCLUDES || index || 0;
    } return !IS_INCLUDES && -1;
  };
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_cof.js":
/*!******************************************************!*\
  !*** ./node_modules/core-js/library/modules/_cof.js ***!
  \******************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

var toString = {}.toString;

module.exports = function (it) {
  return toString.call(it).slice(8, -1);
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_core.js":
/*!*******************************************************!*\
  !*** ./node_modules/core-js/library/modules/_core.js ***!
  \*******************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

var core = module.exports = { version: '2.6.5' };
if (typeof __e == 'number') __e = core; // eslint-disable-line no-undef


/***/ }),

/***/ "./node_modules/core-js/library/modules/_ctx.js":
/*!******************************************************!*\
  !*** ./node_modules/core-js/library/modules/_ctx.js ***!
  \******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

// optional / simple context binding
var aFunction = __webpack_require__(/*! ./_a-function */ "./node_modules/core-js/library/modules/_a-function.js");
module.exports = function (fn, that, length) {
  aFunction(fn);
  if (that === undefined) return fn;
  switch (length) {
    case 1: return function (a) {
      return fn.call(that, a);
    };
    case 2: return function (a, b) {
      return fn.call(that, a, b);
    };
    case 3: return function (a, b, c) {
      return fn.call(that, a, b, c);
    };
  }
  return function (/* ...args */) {
    return fn.apply(that, arguments);
  };
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_defined.js":
/*!**********************************************************!*\
  !*** ./node_modules/core-js/library/modules/_defined.js ***!
  \**********************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

// 7.2.1 RequireObjectCoercible(argument)
module.exports = function (it) {
  if (it == undefined) throw TypeError("Can't call method on  " + it);
  return it;
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_descriptors.js":
/*!**************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_descriptors.js ***!
  \**************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

// Thank's IE8 for his funny defineProperty
module.exports = !__webpack_require__(/*! ./_fails */ "./node_modules/core-js/library/modules/_fails.js")(function () {
  return Object.defineProperty({}, 'a', { get: function () { return 7; } }).a != 7;
});


/***/ }),

/***/ "./node_modules/core-js/library/modules/_dom-create.js":
/*!*************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_dom-create.js ***!
  \*************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var isObject = __webpack_require__(/*! ./_is-object */ "./node_modules/core-js/library/modules/_is-object.js");
var document = __webpack_require__(/*! ./_global */ "./node_modules/core-js/library/modules/_global.js").document;
// typeof document.createElement is 'object' in old IE
var is = isObject(document) && isObject(document.createElement);
module.exports = function (it) {
  return is ? document.createElement(it) : {};
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_enum-bug-keys.js":
/*!****************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_enum-bug-keys.js ***!
  \****************************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

// IE 8- don't enum bug keys
module.exports = (
  'constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf'
).split(',');


/***/ }),

/***/ "./node_modules/core-js/library/modules/_enum-keys.js":
/*!************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_enum-keys.js ***!
  \************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

// all enumerable object keys, includes symbols
var getKeys = __webpack_require__(/*! ./_object-keys */ "./node_modules/core-js/library/modules/_object-keys.js");
var gOPS = __webpack_require__(/*! ./_object-gops */ "./node_modules/core-js/library/modules/_object-gops.js");
var pIE = __webpack_require__(/*! ./_object-pie */ "./node_modules/core-js/library/modules/_object-pie.js");
module.exports = function (it) {
  var result = getKeys(it);
  var getSymbols = gOPS.f;
  if (getSymbols) {
    var symbols = getSymbols(it);
    var isEnum = pIE.f;
    var i = 0;
    var key;
    while (symbols.length > i) if (isEnum.call(it, key = symbols[i++])) result.push(key);
  } return result;
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_export.js":
/*!*********************************************************!*\
  !*** ./node_modules/core-js/library/modules/_export.js ***!
  \*********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var global = __webpack_require__(/*! ./_global */ "./node_modules/core-js/library/modules/_global.js");
var core = __webpack_require__(/*! ./_core */ "./node_modules/core-js/library/modules/_core.js");
var ctx = __webpack_require__(/*! ./_ctx */ "./node_modules/core-js/library/modules/_ctx.js");
var hide = __webpack_require__(/*! ./_hide */ "./node_modules/core-js/library/modules/_hide.js");
var has = __webpack_require__(/*! ./_has */ "./node_modules/core-js/library/modules/_has.js");
var PROTOTYPE = 'prototype';

var $export = function (type, name, source) {
  var IS_FORCED = type & $export.F;
  var IS_GLOBAL = type & $export.G;
  var IS_STATIC = type & $export.S;
  var IS_PROTO = type & $export.P;
  var IS_BIND = type & $export.B;
  var IS_WRAP = type & $export.W;
  var exports = IS_GLOBAL ? core : core[name] || (core[name] = {});
  var expProto = exports[PROTOTYPE];
  var target = IS_GLOBAL ? global : IS_STATIC ? global[name] : (global[name] || {})[PROTOTYPE];
  var key, own, out;
  if (IS_GLOBAL) source = name;
  for (key in source) {
    // contains in native
    own = !IS_FORCED && target && target[key] !== undefined;
    if (own && has(exports, key)) continue;
    // export native or passed
    out = own ? target[key] : source[key];
    // prevent global pollution for namespaces
    exports[key] = IS_GLOBAL && typeof target[key] != 'function' ? source[key]
    // bind timers to global for call from export context
    : IS_BIND && own ? ctx(out, global)
    // wrap global constructors for prevent change them in library
    : IS_WRAP && target[key] == out ? (function (C) {
      var F = function (a, b, c) {
        if (this instanceof C) {
          switch (arguments.length) {
            case 0: return new C();
            case 1: return new C(a);
            case 2: return new C(a, b);
          } return new C(a, b, c);
        } return C.apply(this, arguments);
      };
      F[PROTOTYPE] = C[PROTOTYPE];
      return F;
    // make static versions for prototype methods
    })(out) : IS_PROTO && typeof out == 'function' ? ctx(Function.call, out) : out;
    // export proto methods to core.%CONSTRUCTOR%.methods.%NAME%
    if (IS_PROTO) {
      (exports.virtual || (exports.virtual = {}))[key] = out;
      // export proto methods to core.%CONSTRUCTOR%.prototype.%NAME%
      if (type & $export.R && expProto && !expProto[key]) hide(expProto, key, out);
    }
  }
};
// type bitmap
$export.F = 1;   // forced
$export.G = 2;   // global
$export.S = 4;   // static
$export.P = 8;   // proto
$export.B = 16;  // bind
$export.W = 32;  // wrap
$export.U = 64;  // safe
$export.R = 128; // real proto method for `library`
module.exports = $export;


/***/ }),

/***/ "./node_modules/core-js/library/modules/_fails.js":
/*!********************************************************!*\
  !*** ./node_modules/core-js/library/modules/_fails.js ***!
  \********************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

module.exports = function (exec) {
  try {
    return !!exec();
  } catch (e) {
    return true;
  }
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_global.js":
/*!*********************************************************!*\
  !*** ./node_modules/core-js/library/modules/_global.js ***!
  \*********************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

// https://github.com/zloirock/core-js/issues/86#issuecomment-115759028
var global = module.exports = typeof window != 'undefined' && window.Math == Math
  ? window : typeof self != 'undefined' && self.Math == Math ? self
  // eslint-disable-next-line no-new-func
  : Function('return this')();
if (typeof __g == 'number') __g = global; // eslint-disable-line no-undef


/***/ }),

/***/ "./node_modules/core-js/library/modules/_has.js":
/*!******************************************************!*\
  !*** ./node_modules/core-js/library/modules/_has.js ***!
  \******************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

var hasOwnProperty = {}.hasOwnProperty;
module.exports = function (it, key) {
  return hasOwnProperty.call(it, key);
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_hide.js":
/*!*******************************************************!*\
  !*** ./node_modules/core-js/library/modules/_hide.js ***!
  \*******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var dP = __webpack_require__(/*! ./_object-dp */ "./node_modules/core-js/library/modules/_object-dp.js");
var createDesc = __webpack_require__(/*! ./_property-desc */ "./node_modules/core-js/library/modules/_property-desc.js");
module.exports = __webpack_require__(/*! ./_descriptors */ "./node_modules/core-js/library/modules/_descriptors.js") ? function (object, key, value) {
  return dP.f(object, key, createDesc(1, value));
} : function (object, key, value) {
  object[key] = value;
  return object;
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_html.js":
/*!*******************************************************!*\
  !*** ./node_modules/core-js/library/modules/_html.js ***!
  \*******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var document = __webpack_require__(/*! ./_global */ "./node_modules/core-js/library/modules/_global.js").document;
module.exports = document && document.documentElement;


/***/ }),

/***/ "./node_modules/core-js/library/modules/_ie8-dom-define.js":
/*!*****************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_ie8-dom-define.js ***!
  \*****************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

module.exports = !__webpack_require__(/*! ./_descriptors */ "./node_modules/core-js/library/modules/_descriptors.js") && !__webpack_require__(/*! ./_fails */ "./node_modules/core-js/library/modules/_fails.js")(function () {
  return Object.defineProperty(__webpack_require__(/*! ./_dom-create */ "./node_modules/core-js/library/modules/_dom-create.js")('div'), 'a', { get: function () { return 7; } }).a != 7;
});


/***/ }),

/***/ "./node_modules/core-js/library/modules/_iobject.js":
/*!**********************************************************!*\
  !*** ./node_modules/core-js/library/modules/_iobject.js ***!
  \**********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

// fallback for non-array-like ES3 and non-enumerable old V8 strings
var cof = __webpack_require__(/*! ./_cof */ "./node_modules/core-js/library/modules/_cof.js");
// eslint-disable-next-line no-prototype-builtins
module.exports = Object('z').propertyIsEnumerable(0) ? Object : function (it) {
  return cof(it) == 'String' ? it.split('') : Object(it);
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_is-array.js":
/*!***********************************************************!*\
  !*** ./node_modules/core-js/library/modules/_is-array.js ***!
  \***********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

// 7.2.2 IsArray(argument)
var cof = __webpack_require__(/*! ./_cof */ "./node_modules/core-js/library/modules/_cof.js");
module.exports = Array.isArray || function isArray(arg) {
  return cof(arg) == 'Array';
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_is-object.js":
/*!************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_is-object.js ***!
  \************************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

module.exports = function (it) {
  return typeof it === 'object' ? it !== null : typeof it === 'function';
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_iter-create.js":
/*!**************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_iter-create.js ***!
  \**************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";

var create = __webpack_require__(/*! ./_object-create */ "./node_modules/core-js/library/modules/_object-create.js");
var descriptor = __webpack_require__(/*! ./_property-desc */ "./node_modules/core-js/library/modules/_property-desc.js");
var setToStringTag = __webpack_require__(/*! ./_set-to-string-tag */ "./node_modules/core-js/library/modules/_set-to-string-tag.js");
var IteratorPrototype = {};

// 25.1.2.1.1 %IteratorPrototype%[@@iterator]()
__webpack_require__(/*! ./_hide */ "./node_modules/core-js/library/modules/_hide.js")(IteratorPrototype, __webpack_require__(/*! ./_wks */ "./node_modules/core-js/library/modules/_wks.js")('iterator'), function () { return this; });

module.exports = function (Constructor, NAME, next) {
  Constructor.prototype = create(IteratorPrototype, { next: descriptor(1, next) });
  setToStringTag(Constructor, NAME + ' Iterator');
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_iter-define.js":
/*!**************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_iter-define.js ***!
  \**************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";

var LIBRARY = __webpack_require__(/*! ./_library */ "./node_modules/core-js/library/modules/_library.js");
var $export = __webpack_require__(/*! ./_export */ "./node_modules/core-js/library/modules/_export.js");
var redefine = __webpack_require__(/*! ./_redefine */ "./node_modules/core-js/library/modules/_redefine.js");
var hide = __webpack_require__(/*! ./_hide */ "./node_modules/core-js/library/modules/_hide.js");
var Iterators = __webpack_require__(/*! ./_iterators */ "./node_modules/core-js/library/modules/_iterators.js");
var $iterCreate = __webpack_require__(/*! ./_iter-create */ "./node_modules/core-js/library/modules/_iter-create.js");
var setToStringTag = __webpack_require__(/*! ./_set-to-string-tag */ "./node_modules/core-js/library/modules/_set-to-string-tag.js");
var getPrototypeOf = __webpack_require__(/*! ./_object-gpo */ "./node_modules/core-js/library/modules/_object-gpo.js");
var ITERATOR = __webpack_require__(/*! ./_wks */ "./node_modules/core-js/library/modules/_wks.js")('iterator');
var BUGGY = !([].keys && 'next' in [].keys()); // Safari has buggy iterators w/o `next`
var FF_ITERATOR = '@@iterator';
var KEYS = 'keys';
var VALUES = 'values';

var returnThis = function () { return this; };

module.exports = function (Base, NAME, Constructor, next, DEFAULT, IS_SET, FORCED) {
  $iterCreate(Constructor, NAME, next);
  var getMethod = function (kind) {
    if (!BUGGY && kind in proto) return proto[kind];
    switch (kind) {
      case KEYS: return function keys() { return new Constructor(this, kind); };
      case VALUES: return function values() { return new Constructor(this, kind); };
    } return function entries() { return new Constructor(this, kind); };
  };
  var TAG = NAME + ' Iterator';
  var DEF_VALUES = DEFAULT == VALUES;
  var VALUES_BUG = false;
  var proto = Base.prototype;
  var $native = proto[ITERATOR] || proto[FF_ITERATOR] || DEFAULT && proto[DEFAULT];
  var $default = $native || getMethod(DEFAULT);
  var $entries = DEFAULT ? !DEF_VALUES ? $default : getMethod('entries') : undefined;
  var $anyNative = NAME == 'Array' ? proto.entries || $native : $native;
  var methods, key, IteratorPrototype;
  // Fix native
  if ($anyNative) {
    IteratorPrototype = getPrototypeOf($anyNative.call(new Base()));
    if (IteratorPrototype !== Object.prototype && IteratorPrototype.next) {
      // Set @@toStringTag to native iterators
      setToStringTag(IteratorPrototype, TAG, true);
      // fix for some old engines
      if (!LIBRARY && typeof IteratorPrototype[ITERATOR] != 'function') hide(IteratorPrototype, ITERATOR, returnThis);
    }
  }
  // fix Array#{values, @@iterator}.name in V8 / FF
  if (DEF_VALUES && $native && $native.name !== VALUES) {
    VALUES_BUG = true;
    $default = function values() { return $native.call(this); };
  }
  // Define iterator
  if ((!LIBRARY || FORCED) && (BUGGY || VALUES_BUG || !proto[ITERATOR])) {
    hide(proto, ITERATOR, $default);
  }
  // Plug for library
  Iterators[NAME] = $default;
  Iterators[TAG] = returnThis;
  if (DEFAULT) {
    methods = {
      values: DEF_VALUES ? $default : getMethod(VALUES),
      keys: IS_SET ? $default : getMethod(KEYS),
      entries: $entries
    };
    if (FORCED) for (key in methods) {
      if (!(key in proto)) redefine(proto, key, methods[key]);
    } else $export($export.P + $export.F * (BUGGY || VALUES_BUG), NAME, methods);
  }
  return methods;
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_iter-step.js":
/*!************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_iter-step.js ***!
  \************************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

module.exports = function (done, value) {
  return { value: value, done: !!done };
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_iterators.js":
/*!************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_iterators.js ***!
  \************************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

module.exports = {};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_library.js":
/*!**********************************************************!*\
  !*** ./node_modules/core-js/library/modules/_library.js ***!
  \**********************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

module.exports = true;


/***/ }),

/***/ "./node_modules/core-js/library/modules/_meta.js":
/*!*******************************************************!*\
  !*** ./node_modules/core-js/library/modules/_meta.js ***!
  \*******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var META = __webpack_require__(/*! ./_uid */ "./node_modules/core-js/library/modules/_uid.js")('meta');
var isObject = __webpack_require__(/*! ./_is-object */ "./node_modules/core-js/library/modules/_is-object.js");
var has = __webpack_require__(/*! ./_has */ "./node_modules/core-js/library/modules/_has.js");
var setDesc = __webpack_require__(/*! ./_object-dp */ "./node_modules/core-js/library/modules/_object-dp.js").f;
var id = 0;
var isExtensible = Object.isExtensible || function () {
  return true;
};
var FREEZE = !__webpack_require__(/*! ./_fails */ "./node_modules/core-js/library/modules/_fails.js")(function () {
  return isExtensible(Object.preventExtensions({}));
});
var setMeta = function (it) {
  setDesc(it, META, { value: {
    i: 'O' + ++id, // object ID
    w: {}          // weak collections IDs
  } });
};
var fastKey = function (it, create) {
  // return primitive with prefix
  if (!isObject(it)) return typeof it == 'symbol' ? it : (typeof it == 'string' ? 'S' : 'P') + it;
  if (!has(it, META)) {
    // can't set metadata to uncaught frozen object
    if (!isExtensible(it)) return 'F';
    // not necessary to add metadata
    if (!create) return 'E';
    // add missing metadata
    setMeta(it);
  // return object ID
  } return it[META].i;
};
var getWeak = function (it, create) {
  if (!has(it, META)) {
    // can't set metadata to uncaught frozen object
    if (!isExtensible(it)) return true;
    // not necessary to add metadata
    if (!create) return false;
    // add missing metadata
    setMeta(it);
  // return hash weak collections IDs
  } return it[META].w;
};
// add metadata on freeze-family methods calling
var onFreeze = function (it) {
  if (FREEZE && meta.NEED && isExtensible(it) && !has(it, META)) setMeta(it);
  return it;
};
var meta = module.exports = {
  KEY: META,
  NEED: false,
  fastKey: fastKey,
  getWeak: getWeak,
  onFreeze: onFreeze
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_object-create.js":
/*!****************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_object-create.js ***!
  \****************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

// 19.1.2.2 / 15.2.3.5 Object.create(O [, Properties])
var anObject = __webpack_require__(/*! ./_an-object */ "./node_modules/core-js/library/modules/_an-object.js");
var dPs = __webpack_require__(/*! ./_object-dps */ "./node_modules/core-js/library/modules/_object-dps.js");
var enumBugKeys = __webpack_require__(/*! ./_enum-bug-keys */ "./node_modules/core-js/library/modules/_enum-bug-keys.js");
var IE_PROTO = __webpack_require__(/*! ./_shared-key */ "./node_modules/core-js/library/modules/_shared-key.js")('IE_PROTO');
var Empty = function () { /* empty */ };
var PROTOTYPE = 'prototype';

// Create object with fake `null` prototype: use iframe Object with cleared prototype
var createDict = function () {
  // Thrash, waste and sodomy: IE GC bug
  var iframe = __webpack_require__(/*! ./_dom-create */ "./node_modules/core-js/library/modules/_dom-create.js")('iframe');
  var i = enumBugKeys.length;
  var lt = '<';
  var gt = '>';
  var iframeDocument;
  iframe.style.display = 'none';
  __webpack_require__(/*! ./_html */ "./node_modules/core-js/library/modules/_html.js").appendChild(iframe);
  iframe.src = 'javascript:'; // eslint-disable-line no-script-url
  // createDict = iframe.contentWindow.Object;
  // html.removeChild(iframe);
  iframeDocument = iframe.contentWindow.document;
  iframeDocument.open();
  iframeDocument.write(lt + 'script' + gt + 'document.F=Object' + lt + '/script' + gt);
  iframeDocument.close();
  createDict = iframeDocument.F;
  while (i--) delete createDict[PROTOTYPE][enumBugKeys[i]];
  return createDict();
};

module.exports = Object.create || function create(O, Properties) {
  var result;
  if (O !== null) {
    Empty[PROTOTYPE] = anObject(O);
    result = new Empty();
    Empty[PROTOTYPE] = null;
    // add "__proto__" for Object.getPrototypeOf polyfill
    result[IE_PROTO] = O;
  } else result = createDict();
  return Properties === undefined ? result : dPs(result, Properties);
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_object-dp.js":
/*!************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_object-dp.js ***!
  \************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var anObject = __webpack_require__(/*! ./_an-object */ "./node_modules/core-js/library/modules/_an-object.js");
var IE8_DOM_DEFINE = __webpack_require__(/*! ./_ie8-dom-define */ "./node_modules/core-js/library/modules/_ie8-dom-define.js");
var toPrimitive = __webpack_require__(/*! ./_to-primitive */ "./node_modules/core-js/library/modules/_to-primitive.js");
var dP = Object.defineProperty;

exports.f = __webpack_require__(/*! ./_descriptors */ "./node_modules/core-js/library/modules/_descriptors.js") ? Object.defineProperty : function defineProperty(O, P, Attributes) {
  anObject(O);
  P = toPrimitive(P, true);
  anObject(Attributes);
  if (IE8_DOM_DEFINE) try {
    return dP(O, P, Attributes);
  } catch (e) { /* empty */ }
  if ('get' in Attributes || 'set' in Attributes) throw TypeError('Accessors not supported!');
  if ('value' in Attributes) O[P] = Attributes.value;
  return O;
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_object-dps.js":
/*!*************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_object-dps.js ***!
  \*************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var dP = __webpack_require__(/*! ./_object-dp */ "./node_modules/core-js/library/modules/_object-dp.js");
var anObject = __webpack_require__(/*! ./_an-object */ "./node_modules/core-js/library/modules/_an-object.js");
var getKeys = __webpack_require__(/*! ./_object-keys */ "./node_modules/core-js/library/modules/_object-keys.js");

module.exports = __webpack_require__(/*! ./_descriptors */ "./node_modules/core-js/library/modules/_descriptors.js") ? Object.defineProperties : function defineProperties(O, Properties) {
  anObject(O);
  var keys = getKeys(Properties);
  var length = keys.length;
  var i = 0;
  var P;
  while (length > i) dP.f(O, P = keys[i++], Properties[P]);
  return O;
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_object-gopd.js":
/*!**************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_object-gopd.js ***!
  \**************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var pIE = __webpack_require__(/*! ./_object-pie */ "./node_modules/core-js/library/modules/_object-pie.js");
var createDesc = __webpack_require__(/*! ./_property-desc */ "./node_modules/core-js/library/modules/_property-desc.js");
var toIObject = __webpack_require__(/*! ./_to-iobject */ "./node_modules/core-js/library/modules/_to-iobject.js");
var toPrimitive = __webpack_require__(/*! ./_to-primitive */ "./node_modules/core-js/library/modules/_to-primitive.js");
var has = __webpack_require__(/*! ./_has */ "./node_modules/core-js/library/modules/_has.js");
var IE8_DOM_DEFINE = __webpack_require__(/*! ./_ie8-dom-define */ "./node_modules/core-js/library/modules/_ie8-dom-define.js");
var gOPD = Object.getOwnPropertyDescriptor;

exports.f = __webpack_require__(/*! ./_descriptors */ "./node_modules/core-js/library/modules/_descriptors.js") ? gOPD : function getOwnPropertyDescriptor(O, P) {
  O = toIObject(O);
  P = toPrimitive(P, true);
  if (IE8_DOM_DEFINE) try {
    return gOPD(O, P);
  } catch (e) { /* empty */ }
  if (has(O, P)) return createDesc(!pIE.f.call(O, P), O[P]);
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_object-gopn-ext.js":
/*!******************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_object-gopn-ext.js ***!
  \******************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

// fallback for IE11 buggy Object.getOwnPropertyNames with iframe and window
var toIObject = __webpack_require__(/*! ./_to-iobject */ "./node_modules/core-js/library/modules/_to-iobject.js");
var gOPN = __webpack_require__(/*! ./_object-gopn */ "./node_modules/core-js/library/modules/_object-gopn.js").f;
var toString = {}.toString;

var windowNames = typeof window == 'object' && window && Object.getOwnPropertyNames
  ? Object.getOwnPropertyNames(window) : [];

var getWindowNames = function (it) {
  try {
    return gOPN(it);
  } catch (e) {
    return windowNames.slice();
  }
};

module.exports.f = function getOwnPropertyNames(it) {
  return windowNames && toString.call(it) == '[object Window]' ? getWindowNames(it) : gOPN(toIObject(it));
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_object-gopn.js":
/*!**************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_object-gopn.js ***!
  \**************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

// 19.1.2.7 / 15.2.3.4 Object.getOwnPropertyNames(O)
var $keys = __webpack_require__(/*! ./_object-keys-internal */ "./node_modules/core-js/library/modules/_object-keys-internal.js");
var hiddenKeys = __webpack_require__(/*! ./_enum-bug-keys */ "./node_modules/core-js/library/modules/_enum-bug-keys.js").concat('length', 'prototype');

exports.f = Object.getOwnPropertyNames || function getOwnPropertyNames(O) {
  return $keys(O, hiddenKeys);
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_object-gops.js":
/*!**************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_object-gops.js ***!
  \**************************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

exports.f = Object.getOwnPropertySymbols;


/***/ }),

/***/ "./node_modules/core-js/library/modules/_object-gpo.js":
/*!*************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_object-gpo.js ***!
  \*************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

// 19.1.2.9 / 15.2.3.2 Object.getPrototypeOf(O)
var has = __webpack_require__(/*! ./_has */ "./node_modules/core-js/library/modules/_has.js");
var toObject = __webpack_require__(/*! ./_to-object */ "./node_modules/core-js/library/modules/_to-object.js");
var IE_PROTO = __webpack_require__(/*! ./_shared-key */ "./node_modules/core-js/library/modules/_shared-key.js")('IE_PROTO');
var ObjectProto = Object.prototype;

module.exports = Object.getPrototypeOf || function (O) {
  O = toObject(O);
  if (has(O, IE_PROTO)) return O[IE_PROTO];
  if (typeof O.constructor == 'function' && O instanceof O.constructor) {
    return O.constructor.prototype;
  } return O instanceof Object ? ObjectProto : null;
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_object-keys-internal.js":
/*!***********************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_object-keys-internal.js ***!
  \***********************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var has = __webpack_require__(/*! ./_has */ "./node_modules/core-js/library/modules/_has.js");
var toIObject = __webpack_require__(/*! ./_to-iobject */ "./node_modules/core-js/library/modules/_to-iobject.js");
var arrayIndexOf = __webpack_require__(/*! ./_array-includes */ "./node_modules/core-js/library/modules/_array-includes.js")(false);
var IE_PROTO = __webpack_require__(/*! ./_shared-key */ "./node_modules/core-js/library/modules/_shared-key.js")('IE_PROTO');

module.exports = function (object, names) {
  var O = toIObject(object);
  var i = 0;
  var result = [];
  var key;
  for (key in O) if (key != IE_PROTO) has(O, key) && result.push(key);
  // Don't enum bug & hidden keys
  while (names.length > i) if (has(O, key = names[i++])) {
    ~arrayIndexOf(result, key) || result.push(key);
  }
  return result;
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_object-keys.js":
/*!**************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_object-keys.js ***!
  \**************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

// 19.1.2.14 / 15.2.3.14 Object.keys(O)
var $keys = __webpack_require__(/*! ./_object-keys-internal */ "./node_modules/core-js/library/modules/_object-keys-internal.js");
var enumBugKeys = __webpack_require__(/*! ./_enum-bug-keys */ "./node_modules/core-js/library/modules/_enum-bug-keys.js");

module.exports = Object.keys || function keys(O) {
  return $keys(O, enumBugKeys);
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_object-pie.js":
/*!*************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_object-pie.js ***!
  \*************************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

exports.f = {}.propertyIsEnumerable;


/***/ }),

/***/ "./node_modules/core-js/library/modules/_object-sap.js":
/*!*************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_object-sap.js ***!
  \*************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

// most Object methods by ES6 should accept primitives
var $export = __webpack_require__(/*! ./_export */ "./node_modules/core-js/library/modules/_export.js");
var core = __webpack_require__(/*! ./_core */ "./node_modules/core-js/library/modules/_core.js");
var fails = __webpack_require__(/*! ./_fails */ "./node_modules/core-js/library/modules/_fails.js");
module.exports = function (KEY, exec) {
  var fn = (core.Object || {})[KEY] || Object[KEY];
  var exp = {};
  exp[KEY] = exec(fn);
  $export($export.S + $export.F * fails(function () { fn(1); }), 'Object', exp);
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_property-desc.js":
/*!****************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_property-desc.js ***!
  \****************************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

module.exports = function (bitmap, value) {
  return {
    enumerable: !(bitmap & 1),
    configurable: !(bitmap & 2),
    writable: !(bitmap & 4),
    value: value
  };
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_redefine.js":
/*!***********************************************************!*\
  !*** ./node_modules/core-js/library/modules/_redefine.js ***!
  \***********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

module.exports = __webpack_require__(/*! ./_hide */ "./node_modules/core-js/library/modules/_hide.js");


/***/ }),

/***/ "./node_modules/core-js/library/modules/_set-proto.js":
/*!************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_set-proto.js ***!
  \************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

// Works with __proto__ only. Old v8 can't work with null proto objects.
/* eslint-disable no-proto */
var isObject = __webpack_require__(/*! ./_is-object */ "./node_modules/core-js/library/modules/_is-object.js");
var anObject = __webpack_require__(/*! ./_an-object */ "./node_modules/core-js/library/modules/_an-object.js");
var check = function (O, proto) {
  anObject(O);
  if (!isObject(proto) && proto !== null) throw TypeError(proto + ": can't set as prototype!");
};
module.exports = {
  set: Object.setPrototypeOf || ('__proto__' in {} ? // eslint-disable-line
    function (test, buggy, set) {
      try {
        set = __webpack_require__(/*! ./_ctx */ "./node_modules/core-js/library/modules/_ctx.js")(Function.call, __webpack_require__(/*! ./_object-gopd */ "./node_modules/core-js/library/modules/_object-gopd.js").f(Object.prototype, '__proto__').set, 2);
        set(test, []);
        buggy = !(test instanceof Array);
      } catch (e) { buggy = true; }
      return function setPrototypeOf(O, proto) {
        check(O, proto);
        if (buggy) O.__proto__ = proto;
        else set(O, proto);
        return O;
      };
    }({}, false) : undefined),
  check: check
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_set-to-string-tag.js":
/*!********************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_set-to-string-tag.js ***!
  \********************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var def = __webpack_require__(/*! ./_object-dp */ "./node_modules/core-js/library/modules/_object-dp.js").f;
var has = __webpack_require__(/*! ./_has */ "./node_modules/core-js/library/modules/_has.js");
var TAG = __webpack_require__(/*! ./_wks */ "./node_modules/core-js/library/modules/_wks.js")('toStringTag');

module.exports = function (it, tag, stat) {
  if (it && !has(it = stat ? it : it.prototype, TAG)) def(it, TAG, { configurable: true, value: tag });
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_shared-key.js":
/*!*************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_shared-key.js ***!
  \*************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var shared = __webpack_require__(/*! ./_shared */ "./node_modules/core-js/library/modules/_shared.js")('keys');
var uid = __webpack_require__(/*! ./_uid */ "./node_modules/core-js/library/modules/_uid.js");
module.exports = function (key) {
  return shared[key] || (shared[key] = uid(key));
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_shared.js":
/*!*********************************************************!*\
  !*** ./node_modules/core-js/library/modules/_shared.js ***!
  \*********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var core = __webpack_require__(/*! ./_core */ "./node_modules/core-js/library/modules/_core.js");
var global = __webpack_require__(/*! ./_global */ "./node_modules/core-js/library/modules/_global.js");
var SHARED = '__core-js_shared__';
var store = global[SHARED] || (global[SHARED] = {});

(module.exports = function (key, value) {
  return store[key] || (store[key] = value !== undefined ? value : {});
})('versions', []).push({
  version: core.version,
  mode: __webpack_require__(/*! ./_library */ "./node_modules/core-js/library/modules/_library.js") ? 'pure' : 'global',
  copyright: ' 2019 Denis Pushkarev (zloirock.ru)'
});


/***/ }),

/***/ "./node_modules/core-js/library/modules/_string-at.js":
/*!************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_string-at.js ***!
  \************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var toInteger = __webpack_require__(/*! ./_to-integer */ "./node_modules/core-js/library/modules/_to-integer.js");
var defined = __webpack_require__(/*! ./_defined */ "./node_modules/core-js/library/modules/_defined.js");
// true  -> String#at
// false -> String#codePointAt
module.exports = function (TO_STRING) {
  return function (that, pos) {
    var s = String(defined(that));
    var i = toInteger(pos);
    var l = s.length;
    var a, b;
    if (i < 0 || i >= l) return TO_STRING ? '' : undefined;
    a = s.charCodeAt(i);
    return a < 0xd800 || a > 0xdbff || i + 1 === l || (b = s.charCodeAt(i + 1)) < 0xdc00 || b > 0xdfff
      ? TO_STRING ? s.charAt(i) : a
      : TO_STRING ? s.slice(i, i + 2) : (a - 0xd800 << 10) + (b - 0xdc00) + 0x10000;
  };
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_to-absolute-index.js":
/*!********************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_to-absolute-index.js ***!
  \********************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var toInteger = __webpack_require__(/*! ./_to-integer */ "./node_modules/core-js/library/modules/_to-integer.js");
var max = Math.max;
var min = Math.min;
module.exports = function (index, length) {
  index = toInteger(index);
  return index < 0 ? max(index + length, 0) : min(index, length);
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_to-integer.js":
/*!*************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_to-integer.js ***!
  \*************************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

// 7.1.4 ToInteger
var ceil = Math.ceil;
var floor = Math.floor;
module.exports = function (it) {
  return isNaN(it = +it) ? 0 : (it > 0 ? floor : ceil)(it);
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_to-iobject.js":
/*!*************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_to-iobject.js ***!
  \*************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

// to indexed object, toObject with fallback for non-array-like ES3 strings
var IObject = __webpack_require__(/*! ./_iobject */ "./node_modules/core-js/library/modules/_iobject.js");
var defined = __webpack_require__(/*! ./_defined */ "./node_modules/core-js/library/modules/_defined.js");
module.exports = function (it) {
  return IObject(defined(it));
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_to-length.js":
/*!************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_to-length.js ***!
  \************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

// 7.1.15 ToLength
var toInteger = __webpack_require__(/*! ./_to-integer */ "./node_modules/core-js/library/modules/_to-integer.js");
var min = Math.min;
module.exports = function (it) {
  return it > 0 ? min(toInteger(it), 0x1fffffffffffff) : 0; // pow(2, 53) - 1 == 9007199254740991
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_to-object.js":
/*!************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_to-object.js ***!
  \************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

// 7.1.13 ToObject(argument)
var defined = __webpack_require__(/*! ./_defined */ "./node_modules/core-js/library/modules/_defined.js");
module.exports = function (it) {
  return Object(defined(it));
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_to-primitive.js":
/*!***************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_to-primitive.js ***!
  \***************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

// 7.1.1 ToPrimitive(input [, PreferredType])
var isObject = __webpack_require__(/*! ./_is-object */ "./node_modules/core-js/library/modules/_is-object.js");
// instead of the ES6 spec version, we didn't implement @@toPrimitive case
// and the second argument - flag - preferred type is a string
module.exports = function (it, S) {
  if (!isObject(it)) return it;
  var fn, val;
  if (S && typeof (fn = it.toString) == 'function' && !isObject(val = fn.call(it))) return val;
  if (typeof (fn = it.valueOf) == 'function' && !isObject(val = fn.call(it))) return val;
  if (!S && typeof (fn = it.toString) == 'function' && !isObject(val = fn.call(it))) return val;
  throw TypeError("Can't convert object to primitive value");
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_uid.js":
/*!******************************************************!*\
  !*** ./node_modules/core-js/library/modules/_uid.js ***!
  \******************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

var id = 0;
var px = Math.random();
module.exports = function (key) {
  return 'Symbol('.concat(key === undefined ? '' : key, ')_', (++id + px).toString(36));
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_wks-define.js":
/*!*************************************************************!*\
  !*** ./node_modules/core-js/library/modules/_wks-define.js ***!
  \*************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var global = __webpack_require__(/*! ./_global */ "./node_modules/core-js/library/modules/_global.js");
var core = __webpack_require__(/*! ./_core */ "./node_modules/core-js/library/modules/_core.js");
var LIBRARY = __webpack_require__(/*! ./_library */ "./node_modules/core-js/library/modules/_library.js");
var wksExt = __webpack_require__(/*! ./_wks-ext */ "./node_modules/core-js/library/modules/_wks-ext.js");
var defineProperty = __webpack_require__(/*! ./_object-dp */ "./node_modules/core-js/library/modules/_object-dp.js").f;
module.exports = function (name) {
  var $Symbol = core.Symbol || (core.Symbol = LIBRARY ? {} : global.Symbol || {});
  if (name.charAt(0) != '_' && !(name in $Symbol)) defineProperty($Symbol, name, { value: wksExt.f(name) });
};


/***/ }),

/***/ "./node_modules/core-js/library/modules/_wks-ext.js":
/*!**********************************************************!*\
  !*** ./node_modules/core-js/library/modules/_wks-ext.js ***!
  \**********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

exports.f = __webpack_require__(/*! ./_wks */ "./node_modules/core-js/library/modules/_wks.js");


/***/ }),

/***/ "./node_modules/core-js/library/modules/_wks.js":
/*!******************************************************!*\
  !*** ./node_modules/core-js/library/modules/_wks.js ***!
  \******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var store = __webpack_require__(/*! ./_shared */ "./node_modules/core-js/library/modules/_shared.js")('wks');
var uid = __webpack_require__(/*! ./_uid */ "./node_modules/core-js/library/modules/_uid.js");
var Symbol = __webpack_require__(/*! ./_global */ "./node_modules/core-js/library/modules/_global.js").Symbol;
var USE_SYMBOL = typeof Symbol == 'function';

var $exports = module.exports = function (name) {
  return store[name] || (store[name] =
    USE_SYMBOL && Symbol[name] || (USE_SYMBOL ? Symbol : uid)('Symbol.' + name));
};

$exports.store = store;


/***/ }),

/***/ "./node_modules/core-js/library/modules/es6.array.iterator.js":
/*!********************************************************************!*\
  !*** ./node_modules/core-js/library/modules/es6.array.iterator.js ***!
  \********************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";

var addToUnscopables = __webpack_require__(/*! ./_add-to-unscopables */ "./node_modules/core-js/library/modules/_add-to-unscopables.js");
var step = __webpack_require__(/*! ./_iter-step */ "./node_modules/core-js/library/modules/_iter-step.js");
var Iterators = __webpack_require__(/*! ./_iterators */ "./node_modules/core-js/library/modules/_iterators.js");
var toIObject = __webpack_require__(/*! ./_to-iobject */ "./node_modules/core-js/library/modules/_to-iobject.js");

// 22.1.3.4 Array.prototype.entries()
// 22.1.3.13 Array.prototype.keys()
// 22.1.3.29 Array.prototype.values()
// 22.1.3.30 Array.prototype[@@iterator]()
module.exports = __webpack_require__(/*! ./_iter-define */ "./node_modules/core-js/library/modules/_iter-define.js")(Array, 'Array', function (iterated, kind) {
  this._t = toIObject(iterated); // target
  this._i = 0;                   // next index
  this._k = kind;                // kind
// 22.1.5.2.1 %ArrayIteratorPrototype%.next()
}, function () {
  var O = this._t;
  var kind = this._k;
  var index = this._i++;
  if (!O || index >= O.length) {
    this._t = undefined;
    return step(1);
  }
  if (kind == 'keys') return step(0, index);
  if (kind == 'values') return step(0, O[index]);
  return step(0, [index, O[index]]);
}, 'values');

// argumentsList[@@iterator] is %ArrayProto_values% (9.4.4.6, 9.4.4.7)
Iterators.Arguments = Iterators.Array;

addToUnscopables('keys');
addToUnscopables('values');
addToUnscopables('entries');


/***/ }),

/***/ "./node_modules/core-js/library/modules/es6.object.create.js":
/*!*******************************************************************!*\
  !*** ./node_modules/core-js/library/modules/es6.object.create.js ***!
  \*******************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var $export = __webpack_require__(/*! ./_export */ "./node_modules/core-js/library/modules/_export.js");
// 19.1.2.2 / 15.2.3.5 Object.create(O [, Properties])
$export($export.S, 'Object', { create: __webpack_require__(/*! ./_object-create */ "./node_modules/core-js/library/modules/_object-create.js") });


/***/ }),

/***/ "./node_modules/core-js/library/modules/es6.object.define-property.js":
/*!****************************************************************************!*\
  !*** ./node_modules/core-js/library/modules/es6.object.define-property.js ***!
  \****************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var $export = __webpack_require__(/*! ./_export */ "./node_modules/core-js/library/modules/_export.js");
// 19.1.2.4 / 15.2.3.6 Object.defineProperty(O, P, Attributes)
$export($export.S + $export.F * !__webpack_require__(/*! ./_descriptors */ "./node_modules/core-js/library/modules/_descriptors.js"), 'Object', { defineProperty: __webpack_require__(/*! ./_object-dp */ "./node_modules/core-js/library/modules/_object-dp.js").f });


/***/ }),

/***/ "./node_modules/core-js/library/modules/es6.object.get-prototype-of.js":
/*!*****************************************************************************!*\
  !*** ./node_modules/core-js/library/modules/es6.object.get-prototype-of.js ***!
  \*****************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

// 19.1.2.9 Object.getPrototypeOf(O)
var toObject = __webpack_require__(/*! ./_to-object */ "./node_modules/core-js/library/modules/_to-object.js");
var $getPrototypeOf = __webpack_require__(/*! ./_object-gpo */ "./node_modules/core-js/library/modules/_object-gpo.js");

__webpack_require__(/*! ./_object-sap */ "./node_modules/core-js/library/modules/_object-sap.js")('getPrototypeOf', function () {
  return function getPrototypeOf(it) {
    return $getPrototypeOf(toObject(it));
  };
});


/***/ }),

/***/ "./node_modules/core-js/library/modules/es6.object.set-prototype-of.js":
/*!*****************************************************************************!*\
  !*** ./node_modules/core-js/library/modules/es6.object.set-prototype-of.js ***!
  \*****************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

// 19.1.3.19 Object.setPrototypeOf(O, proto)
var $export = __webpack_require__(/*! ./_export */ "./node_modules/core-js/library/modules/_export.js");
$export($export.S, 'Object', { setPrototypeOf: __webpack_require__(/*! ./_set-proto */ "./node_modules/core-js/library/modules/_set-proto.js").set });


/***/ }),

/***/ "./node_modules/core-js/library/modules/es6.object.to-string.js":
/*!**********************************************************************!*\
  !*** ./node_modules/core-js/library/modules/es6.object.to-string.js ***!
  \**********************************************************************/
/*! no static exports found */
/***/ (function(module, exports) {



/***/ }),

/***/ "./node_modules/core-js/library/modules/es6.string.iterator.js":
/*!*********************************************************************!*\
  !*** ./node_modules/core-js/library/modules/es6.string.iterator.js ***!
  \*********************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";

var $at = __webpack_require__(/*! ./_string-at */ "./node_modules/core-js/library/modules/_string-at.js")(true);

// 21.1.3.27 String.prototype[@@iterator]()
__webpack_require__(/*! ./_iter-define */ "./node_modules/core-js/library/modules/_iter-define.js")(String, 'String', function (iterated) {
  this._t = String(iterated); // target
  this._i = 0;                // next index
// 21.1.5.2.1 %StringIteratorPrototype%.next()
}, function () {
  var O = this._t;
  var index = this._i;
  var point;
  if (index >= O.length) return { value: undefined, done: true };
  point = $at(O, index);
  this._i += point.length;
  return { value: point, done: false };
});


/***/ }),

/***/ "./node_modules/core-js/library/modules/es6.symbol.js":
/*!************************************************************!*\
  !*** ./node_modules/core-js/library/modules/es6.symbol.js ***!
  \************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";

// ECMAScript 6 symbols shim
var global = __webpack_require__(/*! ./_global */ "./node_modules/core-js/library/modules/_global.js");
var has = __webpack_require__(/*! ./_has */ "./node_modules/core-js/library/modules/_has.js");
var DESCRIPTORS = __webpack_require__(/*! ./_descriptors */ "./node_modules/core-js/library/modules/_descriptors.js");
var $export = __webpack_require__(/*! ./_export */ "./node_modules/core-js/library/modules/_export.js");
var redefine = __webpack_require__(/*! ./_redefine */ "./node_modules/core-js/library/modules/_redefine.js");
var META = __webpack_require__(/*! ./_meta */ "./node_modules/core-js/library/modules/_meta.js").KEY;
var $fails = __webpack_require__(/*! ./_fails */ "./node_modules/core-js/library/modules/_fails.js");
var shared = __webpack_require__(/*! ./_shared */ "./node_modules/core-js/library/modules/_shared.js");
var setToStringTag = __webpack_require__(/*! ./_set-to-string-tag */ "./node_modules/core-js/library/modules/_set-to-string-tag.js");
var uid = __webpack_require__(/*! ./_uid */ "./node_modules/core-js/library/modules/_uid.js");
var wks = __webpack_require__(/*! ./_wks */ "./node_modules/core-js/library/modules/_wks.js");
var wksExt = __webpack_require__(/*! ./_wks-ext */ "./node_modules/core-js/library/modules/_wks-ext.js");
var wksDefine = __webpack_require__(/*! ./_wks-define */ "./node_modules/core-js/library/modules/_wks-define.js");
var enumKeys = __webpack_require__(/*! ./_enum-keys */ "./node_modules/core-js/library/modules/_enum-keys.js");
var isArray = __webpack_require__(/*! ./_is-array */ "./node_modules/core-js/library/modules/_is-array.js");
var anObject = __webpack_require__(/*! ./_an-object */ "./node_modules/core-js/library/modules/_an-object.js");
var isObject = __webpack_require__(/*! ./_is-object */ "./node_modules/core-js/library/modules/_is-object.js");
var toIObject = __webpack_require__(/*! ./_to-iobject */ "./node_modules/core-js/library/modules/_to-iobject.js");
var toPrimitive = __webpack_require__(/*! ./_to-primitive */ "./node_modules/core-js/library/modules/_to-primitive.js");
var createDesc = __webpack_require__(/*! ./_property-desc */ "./node_modules/core-js/library/modules/_property-desc.js");
var _create = __webpack_require__(/*! ./_object-create */ "./node_modules/core-js/library/modules/_object-create.js");
var gOPNExt = __webpack_require__(/*! ./_object-gopn-ext */ "./node_modules/core-js/library/modules/_object-gopn-ext.js");
var $GOPD = __webpack_require__(/*! ./_object-gopd */ "./node_modules/core-js/library/modules/_object-gopd.js");
var $DP = __webpack_require__(/*! ./_object-dp */ "./node_modules/core-js/library/modules/_object-dp.js");
var $keys = __webpack_require__(/*! ./_object-keys */ "./node_modules/core-js/library/modules/_object-keys.js");
var gOPD = $GOPD.f;
var dP = $DP.f;
var gOPN = gOPNExt.f;
var $Symbol = global.Symbol;
var $JSON = global.JSON;
var _stringify = $JSON && $JSON.stringify;
var PROTOTYPE = 'prototype';
var HIDDEN = wks('_hidden');
var TO_PRIMITIVE = wks('toPrimitive');
var isEnum = {}.propertyIsEnumerable;
var SymbolRegistry = shared('symbol-registry');
var AllSymbols = shared('symbols');
var OPSymbols = shared('op-symbols');
var ObjectProto = Object[PROTOTYPE];
var USE_NATIVE = typeof $Symbol == 'function';
var QObject = global.QObject;
// Don't use setters in Qt Script, https://github.com/zloirock/core-js/issues/173
var setter = !QObject || !QObject[PROTOTYPE] || !QObject[PROTOTYPE].findChild;

// fallback for old Android, https://code.google.com/p/v8/issues/detail?id=687
var setSymbolDesc = DESCRIPTORS && $fails(function () {
  return _create(dP({}, 'a', {
    get: function () { return dP(this, 'a', { value: 7 }).a; }
  })).a != 7;
}) ? function (it, key, D) {
  var protoDesc = gOPD(ObjectProto, key);
  if (protoDesc) delete ObjectProto[key];
  dP(it, key, D);
  if (protoDesc && it !== ObjectProto) dP(ObjectProto, key, protoDesc);
} : dP;

var wrap = function (tag) {
  var sym = AllSymbols[tag] = _create($Symbol[PROTOTYPE]);
  sym._k = tag;
  return sym;
};

var isSymbol = USE_NATIVE && typeof $Symbol.iterator == 'symbol' ? function (it) {
  return typeof it == 'symbol';
} : function (it) {
  return it instanceof $Symbol;
};

var $defineProperty = function defineProperty(it, key, D) {
  if (it === ObjectProto) $defineProperty(OPSymbols, key, D);
  anObject(it);
  key = toPrimitive(key, true);
  anObject(D);
  if (has(AllSymbols, key)) {
    if (!D.enumerable) {
      if (!has(it, HIDDEN)) dP(it, HIDDEN, createDesc(1, {}));
      it[HIDDEN][key] = true;
    } else {
      if (has(it, HIDDEN) && it[HIDDEN][key]) it[HIDDEN][key] = false;
      D = _create(D, { enumerable: createDesc(0, false) });
    } return setSymbolDesc(it, key, D);
  } return dP(it, key, D);
};
var $defineProperties = function defineProperties(it, P) {
  anObject(it);
  var keys = enumKeys(P = toIObject(P));
  var i = 0;
  var l = keys.length;
  var key;
  while (l > i) $defineProperty(it, key = keys[i++], P[key]);
  return it;
};
var $create = function create(it, P) {
  return P === undefined ? _create(it) : $defineProperties(_create(it), P);
};
var $propertyIsEnumerable = function propertyIsEnumerable(key) {
  var E = isEnum.call(this, key = toPrimitive(key, true));
  if (this === ObjectProto && has(AllSymbols, key) && !has(OPSymbols, key)) return false;
  return E || !has(this, key) || !has(AllSymbols, key) || has(this, HIDDEN) && this[HIDDEN][key] ? E : true;
};
var $getOwnPropertyDescriptor = function getOwnPropertyDescriptor(it, key) {
  it = toIObject(it);
  key = toPrimitive(key, true);
  if (it === ObjectProto && has(AllSymbols, key) && !has(OPSymbols, key)) return;
  var D = gOPD(it, key);
  if (D && has(AllSymbols, key) && !(has(it, HIDDEN) && it[HIDDEN][key])) D.enumerable = true;
  return D;
};
var $getOwnPropertyNames = function getOwnPropertyNames(it) {
  var names = gOPN(toIObject(it));
  var result = [];
  var i = 0;
  var key;
  while (names.length > i) {
    if (!has(AllSymbols, key = names[i++]) && key != HIDDEN && key != META) result.push(key);
  } return result;
};
var $getOwnPropertySymbols = function getOwnPropertySymbols(it) {
  var IS_OP = it === ObjectProto;
  var names = gOPN(IS_OP ? OPSymbols : toIObject(it));
  var result = [];
  var i = 0;
  var key;
  while (names.length > i) {
    if (has(AllSymbols, key = names[i++]) && (IS_OP ? has(ObjectProto, key) : true)) result.push(AllSymbols[key]);
  } return result;
};

// 19.4.1.1 Symbol([description])
if (!USE_NATIVE) {
  $Symbol = function Symbol() {
    if (this instanceof $Symbol) throw TypeError('Symbol is not a constructor!');
    var tag = uid(arguments.length > 0 ? arguments[0] : undefined);
    var $set = function (value) {
      if (this === ObjectProto) $set.call(OPSymbols, value);
      if (has(this, HIDDEN) && has(this[HIDDEN], tag)) this[HIDDEN][tag] = false;
      setSymbolDesc(this, tag, createDesc(1, value));
    };
    if (DESCRIPTORS && setter) setSymbolDesc(ObjectProto, tag, { configurable: true, set: $set });
    return wrap(tag);
  };
  redefine($Symbol[PROTOTYPE], 'toString', function toString() {
    return this._k;
  });

  $GOPD.f = $getOwnPropertyDescriptor;
  $DP.f = $defineProperty;
  __webpack_require__(/*! ./_object-gopn */ "./node_modules/core-js/library/modules/_object-gopn.js").f = gOPNExt.f = $getOwnPropertyNames;
  __webpack_require__(/*! ./_object-pie */ "./node_modules/core-js/library/modules/_object-pie.js").f = $propertyIsEnumerable;
  __webpack_require__(/*! ./_object-gops */ "./node_modules/core-js/library/modules/_object-gops.js").f = $getOwnPropertySymbols;

  if (DESCRIPTORS && !__webpack_require__(/*! ./_library */ "./node_modules/core-js/library/modules/_library.js")) {
    redefine(ObjectProto, 'propertyIsEnumerable', $propertyIsEnumerable, true);
  }

  wksExt.f = function (name) {
    return wrap(wks(name));
  };
}

$export($export.G + $export.W + $export.F * !USE_NATIVE, { Symbol: $Symbol });

for (var es6Symbols = (
  // 19.4.2.2, 19.4.2.3, 19.4.2.4, 19.4.2.6, 19.4.2.8, 19.4.2.9, 19.4.2.10, 19.4.2.11, 19.4.2.12, 19.4.2.13, 19.4.2.14
  'hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables'
).split(','), j = 0; es6Symbols.length > j;)wks(es6Symbols[j++]);

for (var wellKnownSymbols = $keys(wks.store), k = 0; wellKnownSymbols.length > k;) wksDefine(wellKnownSymbols[k++]);

$export($export.S + $export.F * !USE_NATIVE, 'Symbol', {
  // 19.4.2.1 Symbol.for(key)
  'for': function (key) {
    return has(SymbolRegistry, key += '')
      ? SymbolRegistry[key]
      : SymbolRegistry[key] = $Symbol(key);
  },
  // 19.4.2.5 Symbol.keyFor(sym)
  keyFor: function keyFor(sym) {
    if (!isSymbol(sym)) throw TypeError(sym + ' is not a symbol!');
    for (var key in SymbolRegistry) if (SymbolRegistry[key] === sym) return key;
  },
  useSetter: function () { setter = true; },
  useSimple: function () { setter = false; }
});

$export($export.S + $export.F * !USE_NATIVE, 'Object', {
  // 19.1.2.2 Object.create(O [, Properties])
  create: $create,
  // 19.1.2.4 Object.defineProperty(O, P, Attributes)
  defineProperty: $defineProperty,
  // 19.1.2.3 Object.defineProperties(O, Properties)
  defineProperties: $defineProperties,
  // 19.1.2.6 Object.getOwnPropertyDescriptor(O, P)
  getOwnPropertyDescriptor: $getOwnPropertyDescriptor,
  // 19.1.2.7 Object.getOwnPropertyNames(O)
  getOwnPropertyNames: $getOwnPropertyNames,
  // 19.1.2.8 Object.getOwnPropertySymbols(O)
  getOwnPropertySymbols: $getOwnPropertySymbols
});

// 24.3.2 JSON.stringify(value [, replacer [, space]])
$JSON && $export($export.S + $export.F * (!USE_NATIVE || $fails(function () {
  var S = $Symbol();
  // MS Edge converts symbol values to JSON as {}
  // WebKit converts symbol values to JSON as null
  // V8 throws on boxed symbols
  return _stringify([S]) != '[null]' || _stringify({ a: S }) != '{}' || _stringify(Object(S)) != '{}';
})), 'JSON', {
  stringify: function stringify(it) {
    var args = [it];
    var i = 1;
    var replacer, $replacer;
    while (arguments.length > i) args.push(arguments[i++]);
    $replacer = replacer = args[1];
    if (!isObject(replacer) && it === undefined || isSymbol(it)) return; // IE8 returns string on undefined
    if (!isArray(replacer)) replacer = function (key, value) {
      if (typeof $replacer == 'function') value = $replacer.call(this, key, value);
      if (!isSymbol(value)) return value;
    };
    args[1] = replacer;
    return _stringify.apply($JSON, args);
  }
});

// 19.4.3.4 Symbol.prototype[@@toPrimitive](hint)
$Symbol[PROTOTYPE][TO_PRIMITIVE] || __webpack_require__(/*! ./_hide */ "./node_modules/core-js/library/modules/_hide.js")($Symbol[PROTOTYPE], TO_PRIMITIVE, $Symbol[PROTOTYPE].valueOf);
// 19.4.3.5 Symbol.prototype[@@toStringTag]
setToStringTag($Symbol, 'Symbol');
// 20.2.1.9 Math[@@toStringTag]
setToStringTag(Math, 'Math', true);
// 24.3.3 JSON[@@toStringTag]
setToStringTag(global.JSON, 'JSON', true);


/***/ }),

/***/ "./node_modules/core-js/library/modules/es7.symbol.async-iterator.js":
/*!***************************************************************************!*\
  !*** ./node_modules/core-js/library/modules/es7.symbol.async-iterator.js ***!
  \***************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

__webpack_require__(/*! ./_wks-define */ "./node_modules/core-js/library/modules/_wks-define.js")('asyncIterator');


/***/ }),

/***/ "./node_modules/core-js/library/modules/es7.symbol.observable.js":
/*!***********************************************************************!*\
  !*** ./node_modules/core-js/library/modules/es7.symbol.observable.js ***!
  \***********************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

__webpack_require__(/*! ./_wks-define */ "./node_modules/core-js/library/modules/_wks-define.js")('observable');


/***/ }),

/***/ "./node_modules/core-js/library/modules/web.dom.iterable.js":
/*!******************************************************************!*\
  !*** ./node_modules/core-js/library/modules/web.dom.iterable.js ***!
  \******************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

__webpack_require__(/*! ./es6.array.iterator */ "./node_modules/core-js/library/modules/es6.array.iterator.js");
var global = __webpack_require__(/*! ./_global */ "./node_modules/core-js/library/modules/_global.js");
var hide = __webpack_require__(/*! ./_hide */ "./node_modules/core-js/library/modules/_hide.js");
var Iterators = __webpack_require__(/*! ./_iterators */ "./node_modules/core-js/library/modules/_iterators.js");
var TO_STRING_TAG = __webpack_require__(/*! ./_wks */ "./node_modules/core-js/library/modules/_wks.js")('toStringTag');

var DOMIterables = ('CSSRuleList,CSSStyleDeclaration,CSSValueList,ClientRectList,DOMRectList,DOMStringList,' +
  'DOMTokenList,DataTransferItemList,FileList,HTMLAllCollection,HTMLCollection,HTMLFormElement,HTMLSelectElement,' +
  'MediaList,MimeTypeArray,NamedNodeMap,NodeList,PaintRequestList,Plugin,PluginArray,SVGLengthList,SVGNumberList,' +
  'SVGPathSegList,SVGPointList,SVGStringList,SVGTransformList,SourceBufferList,StyleSheetList,TextTrackCueList,' +
  'TextTrackList,TouchList').split(',');

for (var i = 0; i < DOMIterables.length; i++) {
  var NAME = DOMIterables[i];
  var Collection = global[NAME];
  var proto = Collection && Collection.prototype;
  if (proto && !proto[TO_STRING_TAG]) hide(proto, TO_STRING_TAG, NAME);
  Iterators[NAME] = Iterators.Array;
}


/***/ }),

/***/ "./node_modules/extend/index.js":
/*!**************************************!*\
  !*** ./node_modules/extend/index.js ***!
  \**************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var hasOwn = Object.prototype.hasOwnProperty;
var toStr = Object.prototype.toString;
var defineProperty = Object.defineProperty;
var gOPD = Object.getOwnPropertyDescriptor;

var isArray = function isArray(arr) {
	if (typeof Array.isArray === 'function') {
		return Array.isArray(arr);
	}

	return toStr.call(arr) === '[object Array]';
};

var isPlainObject = function isPlainObject(obj) {
	if (!obj || toStr.call(obj) !== '[object Object]') {
		return false;
	}

	var hasOwnConstructor = hasOwn.call(obj, 'constructor');
	var hasIsPrototypeOf = obj.constructor && obj.constructor.prototype && hasOwn.call(obj.constructor.prototype, 'isPrototypeOf');
	// Not own constructor property must be Object
	if (obj.constructor && !hasOwnConstructor && !hasIsPrototypeOf) {
		return false;
	}

	// Own properties are enumerated firstly, so to speed up,
	// if last one is own, then all properties are own.
	var key;
	for (key in obj) { /**/ }

	return typeof key === 'undefined' || hasOwn.call(obj, key);
};

// If name is '__proto__', and Object.defineProperty is available, define __proto__ as an own property on target
var setProperty = function setProperty(target, options) {
	if (defineProperty && options.name === '__proto__') {
		defineProperty(target, options.name, {
			enumerable: true,
			configurable: true,
			value: options.newValue,
			writable: true
		});
	} else {
		target[options.name] = options.newValue;
	}
};

// Return undefined instead of __proto__ if '__proto__' is not an own property
var getProperty = function getProperty(obj, name) {
	if (name === '__proto__') {
		if (!hasOwn.call(obj, name)) {
			return void 0;
		} else if (gOPD) {
			// In early versions of node, obj['__proto__'] is buggy when obj has
			// __proto__ as an own property. Object.getOwnPropertyDescriptor() works.
			return gOPD(obj, name).value;
		}
	}

	return obj[name];
};

module.exports = function extend() {
	var options, name, src, copy, copyIsArray, clone;
	var target = arguments[0];
	var i = 1;
	var length = arguments.length;
	var deep = false;

	// Handle a deep copy situation
	if (typeof target === 'boolean') {
		deep = target;
		target = arguments[1] || {};
		// skip the boolean and the target
		i = 2;
	}
	if (target == null || (typeof target !== 'object' && typeof target !== 'function')) {
		target = {};
	}

	for (; i < length; ++i) {
		options = arguments[i];
		// Only deal with non-null/undefined values
		if (options != null) {
			// Extend the base object
			for (name in options) {
				src = getProperty(target, name);
				copy = getProperty(options, name);

				// Prevent never-ending loop
				if (target !== copy) {
					// Recurse if we're merging plain objects or arrays
					if (deep && copy && (isPlainObject(copy) || (copyIsArray = isArray(copy)))) {
						if (copyIsArray) {
							copyIsArray = false;
							clone = src && isArray(src) ? src : [];
						} else {
							clone = src && isPlainObject(src) ? src : {};
						}

						// Never move original objects, clone them
						setProperty(target, { name: name, newValue: extend(deep, clone, copy) });

					// Don't bring in undefined values
					} else if (typeof copy !== 'undefined') {
						setProperty(target, { name: name, newValue: copy });
					}
				}
			}
		}
	}

	// Return the modified object
	return target;
};


/***/ }),

/***/ "./node_modules/inherits/inherits_browser.js":
/*!***************************************************!*\
  !*** ./node_modules/inherits/inherits_browser.js ***!
  \***************************************************/
/*! no static exports found */
/***/ (function(module, exports) {

if (typeof Object.create === 'function') {
  // implementation from standard node.js 'util' module
  module.exports = function inherits(ctor, superCtor) {
    ctor.super_ = superCtor
    ctor.prototype = Object.create(superCtor.prototype, {
      constructor: {
        value: ctor,
        enumerable: false,
        writable: true,
        configurable: true
      }
    });
  };
} else {
  // old school shim for old browsers
  module.exports = function inherits(ctor, superCtor) {
    ctor.super_ = superCtor
    var TempCtor = function () {}
    TempCtor.prototype = superCtor.prototype
    ctor.prototype = new TempCtor()
    ctor.prototype.constructor = ctor
  }
}


/***/ }),

/***/ "./node_modules/is-alphabetical/index.js":
/*!***********************************************!*\
  !*** ./node_modules/is-alphabetical/index.js ***!
  \***********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = alphabetical

/* Check if the given character code, or the character
 * code at the first character, is alphabetical. */
function alphabetical(character) {
  var code = typeof character === 'string' ? character.charCodeAt(0) : character

  return (
    (code >= 97 && code <= 122) /* a-z */ ||
    (code >= 65 && code <= 90) /* A-Z */
  )
}


/***/ }),

/***/ "./node_modules/is-alphanumerical/index.js":
/*!*************************************************!*\
  !*** ./node_modules/is-alphanumerical/index.js ***!
  \*************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var alphabetical = __webpack_require__(/*! is-alphabetical */ "./node_modules/is-alphabetical/index.js")
var decimal = __webpack_require__(/*! is-decimal */ "./node_modules/is-decimal/index.js")

module.exports = alphanumerical

/* Check if the given character code, or the character
 * code at the first character, is alphanumerical. */
function alphanumerical(character) {
  return alphabetical(character) || decimal(character)
}


/***/ }),

/***/ "./node_modules/is-buffer/index.js":
/*!*****************************************!*\
  !*** ./node_modules/is-buffer/index.js ***!
  \*****************************************/
/*! no static exports found */
/***/ (function(module, exports) {

/*!
 * Determine if an object is a Buffer
 *
 * @author   Feross Aboukhadijeh <https://feross.org>
 * @license  MIT
 */

// The _isBuffer check is for Safari 5-7 support, because it's missing
// Object.prototype.constructor. Remove this eventually
module.exports = function (obj) {
  return obj != null && (isBuffer(obj) || isSlowBuffer(obj) || !!obj._isBuffer)
}

function isBuffer (obj) {
  return !!obj.constructor && typeof obj.constructor.isBuffer === 'function' && obj.constructor.isBuffer(obj)
}

// For Node v0.10 support. Remove this eventually.
function isSlowBuffer (obj) {
  return typeof obj.readFloatLE === 'function' && typeof obj.slice === 'function' && isBuffer(obj.slice(0, 0))
}


/***/ }),

/***/ "./node_modules/is-decimal/index.js":
/*!******************************************!*\
  !*** ./node_modules/is-decimal/index.js ***!
  \******************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = decimal

/* Check if the given character code, or the character
 * code at the first character, is decimal. */
function decimal(character) {
  var code = typeof character === 'string' ? character.charCodeAt(0) : character

  return code >= 48 && code <= 57 /* 0-9 */
}


/***/ }),

/***/ "./node_modules/is-hexadecimal/index.js":
/*!**********************************************!*\
  !*** ./node_modules/is-hexadecimal/index.js ***!
  \**********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = hexadecimal

/* Check if the given character code, or the character
 * code at the first character, is hexadecimal. */
function hexadecimal(character) {
  var code = typeof character === 'string' ? character.charCodeAt(0) : character

  return (
    (code >= 97 /* a */ && code <= 102) /* z */ ||
    (code >= 65 /* A */ && code <= 70) /* Z */ ||
    (code >= 48 /* A */ && code <= 57) /* Z */
  )
}


/***/ }),

/***/ "./node_modules/is-plain-obj/index.js":
/*!********************************************!*\
  !*** ./node_modules/is-plain-obj/index.js ***!
  \********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";

var toString = Object.prototype.toString;

module.exports = function (x) {
	var prototype;
	return toString.call(x) === '[object Object]' && (prototype = Object.getPrototypeOf(x), prototype === null || prototype === Object.getPrototypeOf({}));
};


/***/ }),

/***/ "./node_modules/is-whitespace-character/index.js":
/*!*******************************************************!*\
  !*** ./node_modules/is-whitespace-character/index.js ***!
  \*******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = whitespace

var fromCode = String.fromCharCode
var re = /\s/

/* Check if the given character code, or the character
 * code at the first character, is a whitespace character. */
function whitespace(character) {
  return re.test(
    typeof character === 'number' ? fromCode(character) : character.charAt(0)
  )
}


/***/ }),

/***/ "./node_modules/is-word-character/index.js":
/*!*************************************************!*\
  !*** ./node_modules/is-word-character/index.js ***!
  \*************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = wordCharacter

var fromCode = String.fromCharCode
var re = /\w/

/* Check if the given character code, or the character
 * code at the first character, is a word character. */
function wordCharacter(character) {
  return re.test(
    typeof character === 'number' ? fromCode(character) : character.charAt(0)
  )
}


/***/ }),

/***/ "./node_modules/markdown-escapes/index.js":
/*!************************************************!*\
  !*** ./node_modules/markdown-escapes/index.js ***!
  \************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = escapes

var defaults = [
  '\\',
  '`',
  '*',
  '{',
  '}',
  '[',
  ']',
  '(',
  ')',
  '#',
  '+',
  '-',
  '.',
  '!',
  '_',
  '>'
]

var gfm = defaults.concat(['~', '|'])

var commonmark = gfm.concat([
  '\n',
  '"',
  '$',
  '%',
  '&',
  "'",
  ',',
  '/',
  ':',
  ';',
  '<',
  '=',
  '?',
  '@',
  '^'
])

escapes.default = defaults
escapes.gfm = gfm
escapes.commonmark = commonmark

/* Get markdown escapes. */
function escapes(options) {
  var settings = options || {}

  if (settings.commonmark) {
    return commonmark
  }

  return settings.gfm ? gfm : defaults
}


/***/ }),

/***/ "./node_modules/mdast-add-list-metadata/index.js":
/*!*******************************************************!*\
  !*** ./node_modules/mdast-add-list-metadata/index.js ***!
  \*******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

var visitWithParents = __webpack_require__(/*! unist-util-visit-parents */ "./node_modules/unist-util-visit-parents/index.js");

function addListMetadata() {
  return function (ast) {
    visitWithParents(ast, 'list', function (listNode, parents) {
      var depth = 0, i, n;
      for (i = 0, n = parents.length; i < n; i++) {
        if (parents[i].type === 'list') depth += 1;
      }
      for (i = 0, n = listNode.children.length; i < n; i++) {
        var child = listNode.children[i];
        child.index = i;
        child.ordered = listNode.ordered;
      }
      listNode.depth = depth;
    });
    return ast;
  };
}

module.exports = addListMetadata;


/***/ }),

/***/ "./node_modules/next/dist/build/webpack/loaders/next-client-pages-loader.js?page=%2F&absolutePagePath=%2FUsers%2Fryosuzuki%2FDocuments%2Fryosuzuki%2Fhomepage%2Fpages%2Findex.js!./":
/*!***************************************************************************************************************************************************************************************!*\
  !*** ./node_modules/next/dist/build/webpack/loaders/next-client-pages-loader.js?page=%2F&absolutePagePath=%2FUsers%2Fryosuzuki%2FDocuments%2Fryosuzuki%2Fhomepage%2Fpages%2Findex.js ***!
  \***************************************************************************************************************************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {


    (window.__NEXT_P=window.__NEXT_P||[]).push(["/", function() {
      var page = __webpack_require__(/*! ./pages/index.js */ "./pages/index.js")
      if(true) {
        module.hot.accept(/*! ./pages/index.js */ "./pages/index.js", function() {
          if(!next.router.components["/"]) return
          var updatedPage = __webpack_require__(/*! ./pages/index.js */ "./pages/index.js")
          next.router.update("/", updatedPage.default || updatedPage)
        })
      }
      return { page: page.default || page }
    }]);
  

/***/ }),

/***/ "./node_modules/object-assign/index.js":
/*!***************************************************************************************************!*\
  !*** delegated ./node_modules/object-assign/index.js from dll-reference dll_1aef2d0bbc0d334d831c ***!
  \***************************************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

module.exports = (__webpack_require__(/*! dll-reference dll_1aef2d0bbc0d334d831c */ "dll-reference dll_1aef2d0bbc0d334d831c"))("./node_modules/object-assign/index.js");

/***/ }),

/***/ "./node_modules/parse-entities/decode-entity.browser.js":
/*!**************************************************************!*\
  !*** ./node_modules/parse-entities/decode-entity.browser.js ***!
  \**************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


/* eslint-env browser */

var el

module.exports = decodeEntity

function decodeEntity(characters) {
  var entity = '&' + characters + ';'
  var char

  el = el || document.createElement('i')
  el.innerHTML = entity
  char = el.textContent

  // Some entities do not require the closing semicolon (&not - for instance),
  // which leads to situations where parsing the assumed entity of &notit; will
  // result in the string `it;`.  When we encounter a trailing semicolon after
  // parsing and the entity to decode was not a semicolon (&semi;), we can
  // assume that the matching was incomplete
  if (char.slice(-1) === ';' && characters !== 'semi') {
    return false
  }

  // If the decoded string is equal to the input, the entity was not valid
  return char === entity ? false : char
}


/***/ }),

/***/ "./node_modules/parse-entities/index.js":
/*!**********************************************!*\
  !*** ./node_modules/parse-entities/index.js ***!
  \**********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var legacy = __webpack_require__(/*! character-entities-legacy */ "./node_modules/character-entities-legacy/index.json")
var invalid = __webpack_require__(/*! character-reference-invalid */ "./node_modules/character-reference-invalid/index.json")
var decimal = __webpack_require__(/*! is-decimal */ "./node_modules/is-decimal/index.js")
var hexadecimal = __webpack_require__(/*! is-hexadecimal */ "./node_modules/is-hexadecimal/index.js")
var alphanumerical = __webpack_require__(/*! is-alphanumerical */ "./node_modules/is-alphanumerical/index.js")
var decodeEntity = __webpack_require__(/*! ./decode-entity */ "./node_modules/parse-entities/decode-entity.browser.js")

module.exports = parseEntities

var own = {}.hasOwnProperty
var fromCharCode = String.fromCharCode
var noop = Function.prototype

/* Default settings. */
var defaults = {
  warning: null,
  reference: null,
  text: null,
  warningContext: null,
  referenceContext: null,
  textContext: null,
  position: {},
  additional: null,
  attribute: false,
  nonTerminated: true
}

/* Reference types. */
var NAMED = 'named'
var HEXADECIMAL = 'hexadecimal'
var DECIMAL = 'decimal'

/* Map of bases. */
var BASE = {}

BASE[HEXADECIMAL] = 16
BASE[DECIMAL] = 10

/* Map of types to tests. Each type of character reference
 * accepts different characters. This test is used to
 * detect whether a reference has ended (as the semicolon
 * is not strictly needed). */
var TESTS = {}

TESTS[NAMED] = alphanumerical
TESTS[DECIMAL] = decimal
TESTS[HEXADECIMAL] = hexadecimal

/* Warning messages. */
var NAMED_NOT_TERMINATED = 1
var NUMERIC_NOT_TERMINATED = 2
var NAMED_EMPTY = 3
var NUMERIC_EMPTY = 4
var NAMED_UNKNOWN = 5
var NUMERIC_DISALLOWED = 6
var NUMERIC_PROHIBITED = 7

var MESSAGES = {}

MESSAGES[NAMED_NOT_TERMINATED] =
  'Named character references must be terminated by a semicolon'
MESSAGES[NUMERIC_NOT_TERMINATED] =
  'Numeric character references must be terminated by a semicolon'
MESSAGES[NAMED_EMPTY] = 'Named character references cannot be empty'
MESSAGES[NUMERIC_EMPTY] = 'Numeric character references cannot be empty'
MESSAGES[NAMED_UNKNOWN] = 'Named character references must be known'
MESSAGES[NUMERIC_DISALLOWED] =
  'Numeric character references cannot be disallowed'
MESSAGES[NUMERIC_PROHIBITED] =
  'Numeric character references cannot be outside the permissible Unicode range'

/* Wrap to ensure clean parameters are given to `parse`. */
function parseEntities(value, options) {
  var settings = {}
  var option
  var key

  if (!options) {
    options = {}
  }

  for (key in defaults) {
    option = options[key]
    settings[key] =
      option === null || option === undefined ? defaults[key] : option
  }

  if (settings.position.indent || settings.position.start) {
    settings.indent = settings.position.indent || []
    settings.position = settings.position.start
  }

  return parse(value, settings)
}

/* Parse entities. */
function parse(value, settings) {
  var additional = settings.additional
  var nonTerminated = settings.nonTerminated
  var handleText = settings.text
  var handleReference = settings.reference
  var handleWarning = settings.warning
  var textContext = settings.textContext
  var referenceContext = settings.referenceContext
  var warningContext = settings.warningContext
  var pos = settings.position
  var indent = settings.indent || []
  var length = value.length
  var index = 0
  var lines = -1
  var column = pos.column || 1
  var line = pos.line || 1
  var queue = ''
  var result = []
  var entityCharacters
  var namedEntity
  var terminated
  var characters
  var character
  var reference
  var following
  var warning
  var reason
  var output
  var entity
  var begin
  var start
  var type
  var test
  var prev
  var next
  var diff
  var end

  /* Cache the current point. */
  prev = now()

  /* Wrap `handleWarning`. */
  warning = handleWarning ? parseError : noop

  /* Ensure the algorithm walks over the first character
   * and the end (inclusive). */
  index--
  length++

  while (++index < length) {
    /* If the previous character was a newline. */
    if (character === '\n') {
      column = indent[lines] || 1
    }

    character = at(index)

    /* Handle anything other than an ampersand,
     * including newlines and EOF. */
    if (character !== '&') {
      if (character === '\n') {
        line++
        lines++
        column = 0
      }

      if (character) {
        queue += character
        column++
      } else {
        flush()
      }
    } else {
      following = at(index + 1)

      /* The behaviour depends on the identity of the next
       * character. */
      if (
        following === '\t' /* Tab */ ||
        following === '\n' /* Newline */ ||
        following === '\f' /* Form feed */ ||
        following === ' ' /* Space */ ||
        following === '<' /* Less-than */ ||
        following === '&' /* Ampersand */ ||
        following === '' ||
        (additional && following === additional)
      ) {
        /* Not a character reference. No characters
         * are consumed, and nothing is returned.
         * This is not an error, either. */
        queue += character
        column++

        continue
      }

      start = index + 1
      begin = start
      end = start

      /* Numerical entity. */
      if (following !== '#') {
        type = NAMED
      } else {
        end = ++begin

        /* The behaviour further depends on the
         * character after the U+0023 NUMBER SIGN. */
        following = at(end)

        if (following === 'x' || following === 'X') {
          /* ASCII hex digits. */
          type = HEXADECIMAL
          end = ++begin
        } else {
          /* ASCII digits. */
          type = DECIMAL
        }
      }

      entityCharacters = ''
      entity = ''
      characters = ''
      test = TESTS[type]
      end--

      while (++end < length) {
        following = at(end)

        if (!test(following)) {
          break
        }

        characters += following

        /* Check if we can match a legacy named
         * reference.  If so, we cache that as the
         * last viable named reference.  This
         * ensures we do not need to walk backwards
         * later. */
        if (type === NAMED && own.call(legacy, characters)) {
          entityCharacters = characters
          entity = legacy[characters]
        }
      }

      terminated = at(end) === ';'

      if (terminated) {
        end++

        namedEntity = type === NAMED ? decodeEntity(characters) : false

        if (namedEntity) {
          entityCharacters = characters
          entity = namedEntity
        }
      }

      diff = 1 + end - start

      if (!terminated && !nonTerminated) {
        /* Empty. */
      } else if (!characters) {
        /* An empty (possible) entity is valid, unless
         * its numeric (thus an ampersand followed by
         * an octothorp). */
        if (type !== NAMED) {
          warning(NUMERIC_EMPTY, diff)
        }
      } else if (type === NAMED) {
        /* An ampersand followed by anything
         * unknown, and not terminated, is invalid. */
        if (terminated && !entity) {
          warning(NAMED_UNKNOWN, 1)
        } else {
          /* If theres something after an entity
           * name which is not known, cap the
           * reference. */
          if (entityCharacters !== characters) {
            end = begin + entityCharacters.length
            diff = 1 + end - begin
            terminated = false
          }

          /* If the reference is not terminated,
           * warn. */
          if (!terminated) {
            reason = entityCharacters ? NAMED_NOT_TERMINATED : NAMED_EMPTY

            if (!settings.attribute) {
              warning(reason, diff)
            } else {
              following = at(end)

              if (following === '=') {
                warning(reason, diff)
                entity = null
              } else if (alphanumerical(following)) {
                entity = null
              } else {
                warning(reason, diff)
              }
            }
          }
        }

        reference = entity
      } else {
        if (!terminated) {
          /* All non-terminated numeric entities are
           * not rendered, and trigger a warning. */
          warning(NUMERIC_NOT_TERMINATED, diff)
        }

        /* When terminated and number, parse as
         * either hexadecimal or decimal. */
        reference = parseInt(characters, BASE[type])

        /* Trigger a warning when the parsed number
         * is prohibited, and replace with
         * replacement character. */
        if (prohibited(reference)) {
          warning(NUMERIC_PROHIBITED, diff)
          reference = '\uFFFD'
        } else if (reference in invalid) {
          /* Trigger a warning when the parsed number
           * is disallowed, and replace by an
           * alternative. */
          warning(NUMERIC_DISALLOWED, diff)
          reference = invalid[reference]
        } else {
          /* Parse the number. */
          output = ''

          /* Trigger a warning when the parsed
           * number should not be used. */
          if (disallowed(reference)) {
            warning(NUMERIC_DISALLOWED, diff)
          }

          /* Stringify the number. */
          if (reference > 0xffff) {
            reference -= 0x10000
            output += fromCharCode((reference >>> (10 & 0x3ff)) | 0xd800)
            reference = 0xdc00 | (reference & 0x3ff)
          }

          reference = output + fromCharCode(reference)
        }
      }

      /* If we could not find a reference, queue the
       * checked characters (as normal characters),
       * and move the pointer to their end. This is
       * possible because we can be certain neither
       * newlines nor ampersands are included. */
      if (!reference) {
        characters = value.slice(start - 1, end)
        queue += characters
        column += characters.length
        index = end - 1
      } else {
        /* Found it! First eat the queued
         * characters as normal text, then eat
         * an entity. */
        flush()

        prev = now()
        index = end - 1
        column += end - start + 1
        result.push(reference)
        next = now()
        next.offset++

        if (handleReference) {
          handleReference.call(
            referenceContext,
            reference,
            {start: prev, end: next},
            value.slice(start - 1, end)
          )
        }

        prev = next
      }
    }
  }

  /* Return the reduced nodes, and any possible warnings. */
  return result.join('')

  /* Get current position. */
  function now() {
    return {
      line: line,
      column: column,
      offset: index + (pos.offset || 0)
    }
  }

  /* Throw a parse-error: a warning. */
  function parseError(code, offset) {
    var position = now()

    position.column += offset
    position.offset += offset

    handleWarning.call(warningContext, MESSAGES[code], position, code)
  }

  /* Get character at position. */
  function at(position) {
    return value.charAt(position)
  }

  /* Flush `queue` (normal text). Macro invoked before
   * each entity and at the end of `value`.
   * Does nothing when `queue` is empty. */
  function flush() {
    if (queue) {
      result.push(queue)

      if (handleText) {
        handleText.call(textContext, queue, {start: prev, end: now()})
      }

      queue = ''
    }
  }
}

/* Check if `character` is outside the permissible unicode range. */
function prohibited(code) {
  return (code >= 0xd800 && code <= 0xdfff) || code > 0x10ffff
}

/* Check if `character` is disallowed. */
function disallowed(code) {
  return (
    (code >= 0x0001 && code <= 0x0008) ||
    code === 0x000b ||
    (code >= 0x000d && code <= 0x001f) ||
    (code >= 0x007f && code <= 0x009f) ||
    (code >= 0xfdd0 && code <= 0xfdef) ||
    (code & 0xffff) === 0xffff ||
    (code & 0xffff) === 0xfffe
  )
}


/***/ }),

/***/ "./node_modules/path-browserify/index.js":
/*!***********************************************!*\
  !*** ./node_modules/path-browserify/index.js ***!
  \***********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

/* WEBPACK VAR INJECTION */(function(process) {// Copyright Joyent, Inc. and other Node contributors.
//
// Permission is hereby granted, free of charge, to any person obtaining a
// copy of this software and associated documentation files (the
// "Software"), to deal in the Software without restriction, including
// without limitation the rights to use, copy, modify, merge, publish,
// distribute, sublicense, and/or sell copies of the Software, and to permit
// persons to whom the Software is furnished to do so, subject to the
// following conditions:
//
// The above copyright notice and this permission notice shall be included
// in all copies or substantial portions of the Software.
//
// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
// OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN
// NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,
// DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR
// OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE
// USE OR OTHER DEALINGS IN THE SOFTWARE.

// resolves . and .. elements in a path array with directory names there
// must be no slashes, empty elements, or device names (c:\) in the array
// (so also no leading and trailing slashes - it does not distinguish
// relative and absolute paths)
function normalizeArray(parts, allowAboveRoot) {
  // if the path tries to go above the root, `up` ends up > 0
  var up = 0;
  for (var i = parts.length - 1; i >= 0; i--) {
    var last = parts[i];
    if (last === '.') {
      parts.splice(i, 1);
    } else if (last === '..') {
      parts.splice(i, 1);
      up++;
    } else if (up) {
      parts.splice(i, 1);
      up--;
    }
  }

  // if the path is allowed to go above the root, restore leading ..s
  if (allowAboveRoot) {
    for (; up--; up) {
      parts.unshift('..');
    }
  }

  return parts;
}

// Split a filename into [root, dir, basename, ext], unix version
// 'root' is just a slash, or nothing.
var splitPathRe =
    /^(\/?|)([\s\S]*?)((?:\.{1,2}|[^\/]+?|)(\.[^.\/]*|))(?:[\/]*)$/;
var splitPath = function(filename) {
  return splitPathRe.exec(filename).slice(1);
};

// path.resolve([from ...], to)
// posix version
exports.resolve = function() {
  var resolvedPath = '',
      resolvedAbsolute = false;

  for (var i = arguments.length - 1; i >= -1 && !resolvedAbsolute; i--) {
    var path = (i >= 0) ? arguments[i] : process.cwd();

    // Skip empty and invalid entries
    if (typeof path !== 'string') {
      throw new TypeError('Arguments to path.resolve must be strings');
    } else if (!path) {
      continue;
    }

    resolvedPath = path + '/' + resolvedPath;
    resolvedAbsolute = path.charAt(0) === '/';
  }

  // At this point the path should be resolved to a full absolute path, but
  // handle relative paths to be safe (might happen when process.cwd() fails)

  // Normalize the path
  resolvedPath = normalizeArray(filter(resolvedPath.split('/'), function(p) {
    return !!p;
  }), !resolvedAbsolute).join('/');

  return ((resolvedAbsolute ? '/' : '') + resolvedPath) || '.';
};

// path.normalize(path)
// posix version
exports.normalize = function(path) {
  var isAbsolute = exports.isAbsolute(path),
      trailingSlash = substr(path, -1) === '/';

  // Normalize the path
  path = normalizeArray(filter(path.split('/'), function(p) {
    return !!p;
  }), !isAbsolute).join('/');

  if (!path && !isAbsolute) {
    path = '.';
  }
  if (path && trailingSlash) {
    path += '/';
  }

  return (isAbsolute ? '/' : '') + path;
};

// posix version
exports.isAbsolute = function(path) {
  return path.charAt(0) === '/';
};

// posix version
exports.join = function() {
  var paths = Array.prototype.slice.call(arguments, 0);
  return exports.normalize(filter(paths, function(p, index) {
    if (typeof p !== 'string') {
      throw new TypeError('Arguments to path.join must be strings');
    }
    return p;
  }).join('/'));
};


// path.relative(from, to)
// posix version
exports.relative = function(from, to) {
  from = exports.resolve(from).substr(1);
  to = exports.resolve(to).substr(1);

  function trim(arr) {
    var start = 0;
    for (; start < arr.length; start++) {
      if (arr[start] !== '') break;
    }

    var end = arr.length - 1;
    for (; end >= 0; end--) {
      if (arr[end] !== '') break;
    }

    if (start > end) return [];
    return arr.slice(start, end - start + 1);
  }

  var fromParts = trim(from.split('/'));
  var toParts = trim(to.split('/'));

  var length = Math.min(fromParts.length, toParts.length);
  var samePartsLength = length;
  for (var i = 0; i < length; i++) {
    if (fromParts[i] !== toParts[i]) {
      samePartsLength = i;
      break;
    }
  }

  var outputParts = [];
  for (var i = samePartsLength; i < fromParts.length; i++) {
    outputParts.push('..');
  }

  outputParts = outputParts.concat(toParts.slice(samePartsLength));

  return outputParts.join('/');
};

exports.sep = '/';
exports.delimiter = ':';

exports.dirname = function(path) {
  var result = splitPath(path),
      root = result[0],
      dir = result[1];

  if (!root && !dir) {
    // No dirname whatsoever
    return '.';
  }

  if (dir) {
    // It has a dirname, strip trailing slash
    dir = dir.substr(0, dir.length - 1);
  }

  return root + dir;
};


exports.basename = function(path, ext) {
  var f = splitPath(path)[2];
  // TODO: make this comparison case-insensitive on windows?
  if (ext && f.substr(-1 * ext.length) === ext) {
    f = f.substr(0, f.length - ext.length);
  }
  return f;
};


exports.extname = function(path) {
  return splitPath(path)[3];
};

function filter (xs, f) {
    if (xs.filter) return xs.filter(f);
    var res = [];
    for (var i = 0; i < xs.length; i++) {
        if (f(xs[i], i, xs)) res.push(xs[i]);
    }
    return res;
}

// String.prototype.substr - negative index don't work in IE8
var substr = 'ab'.substr(-1) === 'b'
    ? function (str, start, len) { return str.substr(start, len) }
    : function (str, start, len) {
        if (start < 0) start = str.length + start;
        return str.substr(start, len);
    }
;

/* WEBPACK VAR INJECTION */}.call(this, __webpack_require__(/*! ./../process/browser.js */ "./node_modules/process/browser.js")))

/***/ }),

/***/ "./node_modules/process/browser.js":
/*!*****************************************!*\
  !*** ./node_modules/process/browser.js ***!
  \*****************************************/
/*! no static exports found */
/***/ (function(module, exports) {

// shim for using process in browser
var process = module.exports = {};

// cached from whatever global is present so that test runners that stub it
// don't break things.  But we need to wrap it in a try catch in case it is
// wrapped in strict mode code which doesn't define any globals.  It's inside a
// function because try/catches deoptimize in certain engines.

var cachedSetTimeout;
var cachedClearTimeout;

function defaultSetTimout() {
    throw new Error('setTimeout has not been defined');
}
function defaultClearTimeout () {
    throw new Error('clearTimeout has not been defined');
}
(function () {
    try {
        if (typeof setTimeout === 'function') {
            cachedSetTimeout = setTimeout;
        } else {
            cachedSetTimeout = defaultSetTimout;
        }
    } catch (e) {
        cachedSetTimeout = defaultSetTimout;
    }
    try {
        if (typeof clearTimeout === 'function') {
            cachedClearTimeout = clearTimeout;
        } else {
            cachedClearTimeout = defaultClearTimeout;
        }
    } catch (e) {
        cachedClearTimeout = defaultClearTimeout;
    }
} ())
function runTimeout(fun) {
    if (cachedSetTimeout === setTimeout) {
        //normal enviroments in sane situations
        return setTimeout(fun, 0);
    }
    // if setTimeout wasn't available but was latter defined
    if ((cachedSetTimeout === defaultSetTimout || !cachedSetTimeout) && setTimeout) {
        cachedSetTimeout = setTimeout;
        return setTimeout(fun, 0);
    }
    try {
        // when when somebody has screwed with setTimeout but no I.E. maddness
        return cachedSetTimeout(fun, 0);
    } catch(e){
        try {
            // When we are in I.E. but the script has been evaled so I.E. doesn't trust the global object when called normally
            return cachedSetTimeout.call(null, fun, 0);
        } catch(e){
            // same as above but when it's a version of I.E. that must have the global object for 'this', hopfully our context correct otherwise it will throw a global error
            return cachedSetTimeout.call(this, fun, 0);
        }
    }


}
function runClearTimeout(marker) {
    if (cachedClearTimeout === clearTimeout) {
        //normal enviroments in sane situations
        return clearTimeout(marker);
    }
    // if clearTimeout wasn't available but was latter defined
    if ((cachedClearTimeout === defaultClearTimeout || !cachedClearTimeout) && clearTimeout) {
        cachedClearTimeout = clearTimeout;
        return clearTimeout(marker);
    }
    try {
        // when when somebody has screwed with setTimeout but no I.E. maddness
        return cachedClearTimeout(marker);
    } catch (e){
        try {
            // When we are in I.E. but the script has been evaled so I.E. doesn't  trust the global object when called normally
            return cachedClearTimeout.call(null, marker);
        } catch (e){
            // same as above but when it's a version of I.E. that must have the global object for 'this', hopfully our context correct otherwise it will throw a global error.
            // Some versions of I.E. have different rules for clearTimeout vs setTimeout
            return cachedClearTimeout.call(this, marker);
        }
    }



}
var queue = [];
var draining = false;
var currentQueue;
var queueIndex = -1;

function cleanUpNextTick() {
    if (!draining || !currentQueue) {
        return;
    }
    draining = false;
    if (currentQueue.length) {
        queue = currentQueue.concat(queue);
    } else {
        queueIndex = -1;
    }
    if (queue.length) {
        drainQueue();
    }
}

function drainQueue() {
    if (draining) {
        return;
    }
    var timeout = runTimeout(cleanUpNextTick);
    draining = true;

    var len = queue.length;
    while(len) {
        currentQueue = queue;
        queue = [];
        while (++queueIndex < len) {
            if (currentQueue) {
                currentQueue[queueIndex].run();
            }
        }
        queueIndex = -1;
        len = queue.length;
    }
    currentQueue = null;
    draining = false;
    runClearTimeout(timeout);
}

process.nextTick = function (fun) {
    var args = new Array(arguments.length - 1);
    if (arguments.length > 1) {
        for (var i = 1; i < arguments.length; i++) {
            args[i - 1] = arguments[i];
        }
    }
    queue.push(new Item(fun, args));
    if (queue.length === 1 && !draining) {
        runTimeout(drainQueue);
    }
};

// v8 likes predictible objects
function Item(fun, array) {
    this.fun = fun;
    this.array = array;
}
Item.prototype.run = function () {
    this.fun.apply(null, this.array);
};
process.title = 'browser';
process.browser = true;
process.env = {};
process.argv = [];
process.version = ''; // empty string to avoid regexp issues
process.versions = {};

function noop() {}

process.on = noop;
process.addListener = noop;
process.once = noop;
process.off = noop;
process.removeListener = noop;
process.removeAllListeners = noop;
process.emit = noop;
process.prependListener = noop;
process.prependOnceListener = noop;

process.listeners = function (name) { return [] }

process.binding = function (name) {
    throw new Error('process.binding is not supported');
};

process.cwd = function () { return '/' };
process.chdir = function (dir) {
    throw new Error('process.chdir is not supported');
};
process.umask = function() { return 0; };


/***/ }),

/***/ "./node_modules/react-is/cjs/react-is.development.js":
/*!***********************************************************!*\
  !*** ./node_modules/react-is/cjs/react-is.development.js ***!
  \***********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
/** @license React v16.8.6
 * react-is.development.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */





if (true) {
  (function() {
'use strict';

Object.defineProperty(exports, '__esModule', { value: true });

// The Symbol used to tag the ReactElement-like types. If there is no native Symbol
// nor polyfill, then a plain number is used for performance.
var hasSymbol = typeof Symbol === 'function' && Symbol.for;

var REACT_ELEMENT_TYPE = hasSymbol ? Symbol.for('react.element') : 0xeac7;
var REACT_PORTAL_TYPE = hasSymbol ? Symbol.for('react.portal') : 0xeaca;
var REACT_FRAGMENT_TYPE = hasSymbol ? Symbol.for('react.fragment') : 0xeacb;
var REACT_STRICT_MODE_TYPE = hasSymbol ? Symbol.for('react.strict_mode') : 0xeacc;
var REACT_PROFILER_TYPE = hasSymbol ? Symbol.for('react.profiler') : 0xead2;
var REACT_PROVIDER_TYPE = hasSymbol ? Symbol.for('react.provider') : 0xeacd;
var REACT_CONTEXT_TYPE = hasSymbol ? Symbol.for('react.context') : 0xeace;
var REACT_ASYNC_MODE_TYPE = hasSymbol ? Symbol.for('react.async_mode') : 0xeacf;
var REACT_CONCURRENT_MODE_TYPE = hasSymbol ? Symbol.for('react.concurrent_mode') : 0xeacf;
var REACT_FORWARD_REF_TYPE = hasSymbol ? Symbol.for('react.forward_ref') : 0xead0;
var REACT_SUSPENSE_TYPE = hasSymbol ? Symbol.for('react.suspense') : 0xead1;
var REACT_MEMO_TYPE = hasSymbol ? Symbol.for('react.memo') : 0xead3;
var REACT_LAZY_TYPE = hasSymbol ? Symbol.for('react.lazy') : 0xead4;

function isValidElementType(type) {
  return typeof type === 'string' || typeof type === 'function' ||
  // Note: its typeof might be other than 'symbol' or 'number' if it's a polyfill.
  type === REACT_FRAGMENT_TYPE || type === REACT_CONCURRENT_MODE_TYPE || type === REACT_PROFILER_TYPE || type === REACT_STRICT_MODE_TYPE || type === REACT_SUSPENSE_TYPE || typeof type === 'object' && type !== null && (type.$$typeof === REACT_LAZY_TYPE || type.$$typeof === REACT_MEMO_TYPE || type.$$typeof === REACT_PROVIDER_TYPE || type.$$typeof === REACT_CONTEXT_TYPE || type.$$typeof === REACT_FORWARD_REF_TYPE);
}

/**
 * Forked from fbjs/warning:
 * https://github.com/facebook/fbjs/blob/e66ba20ad5be433eb54423f2b097d829324d9de6/packages/fbjs/src/__forks__/warning.js
 *
 * Only change is we use console.warn instead of console.error,
 * and do nothing when 'console' is not supported.
 * This really simplifies the code.
 * ---
 * Similar to invariant but only logs a warning if the condition is not met.
 * This can be used to log issues in development environments in critical
 * paths. Removing the logging code for production environments will keep the
 * same logic and follow the same code paths.
 */

var lowPriorityWarning = function () {};

{
  var printWarning = function (format) {
    for (var _len = arguments.length, args = Array(_len > 1 ? _len - 1 : 0), _key = 1; _key < _len; _key++) {
      args[_key - 1] = arguments[_key];
    }

    var argIndex = 0;
    var message = 'Warning: ' + format.replace(/%s/g, function () {
      return args[argIndex++];
    });
    if (typeof console !== 'undefined') {
      console.warn(message);
    }
    try {
      // --- Welcome to debugging React ---
      // This error was thrown as a convenience so that you can use this stack
      // to find the callsite that caused this warning to fire.
      throw new Error(message);
    } catch (x) {}
  };

  lowPriorityWarning = function (condition, format) {
    if (format === undefined) {
      throw new Error('`lowPriorityWarning(condition, format, ...args)` requires a warning ' + 'message argument');
    }
    if (!condition) {
      for (var _len2 = arguments.length, args = Array(_len2 > 2 ? _len2 - 2 : 0), _key2 = 2; _key2 < _len2; _key2++) {
        args[_key2 - 2] = arguments[_key2];
      }

      printWarning.apply(undefined, [format].concat(args));
    }
  };
}

var lowPriorityWarning$1 = lowPriorityWarning;

function typeOf(object) {
  if (typeof object === 'object' && object !== null) {
    var $$typeof = object.$$typeof;
    switch ($$typeof) {
      case REACT_ELEMENT_TYPE:
        var type = object.type;

        switch (type) {
          case REACT_ASYNC_MODE_TYPE:
          case REACT_CONCURRENT_MODE_TYPE:
          case REACT_FRAGMENT_TYPE:
          case REACT_PROFILER_TYPE:
          case REACT_STRICT_MODE_TYPE:
          case REACT_SUSPENSE_TYPE:
            return type;
          default:
            var $$typeofType = type && type.$$typeof;

            switch ($$typeofType) {
              case REACT_CONTEXT_TYPE:
              case REACT_FORWARD_REF_TYPE:
              case REACT_PROVIDER_TYPE:
                return $$typeofType;
              default:
                return $$typeof;
            }
        }
      case REACT_LAZY_TYPE:
      case REACT_MEMO_TYPE:
      case REACT_PORTAL_TYPE:
        return $$typeof;
    }
  }

  return undefined;
}

// AsyncMode is deprecated along with isAsyncMode
var AsyncMode = REACT_ASYNC_MODE_TYPE;
var ConcurrentMode = REACT_CONCURRENT_MODE_TYPE;
var ContextConsumer = REACT_CONTEXT_TYPE;
var ContextProvider = REACT_PROVIDER_TYPE;
var Element = REACT_ELEMENT_TYPE;
var ForwardRef = REACT_FORWARD_REF_TYPE;
var Fragment = REACT_FRAGMENT_TYPE;
var Lazy = REACT_LAZY_TYPE;
var Memo = REACT_MEMO_TYPE;
var Portal = REACT_PORTAL_TYPE;
var Profiler = REACT_PROFILER_TYPE;
var StrictMode = REACT_STRICT_MODE_TYPE;
var Suspense = REACT_SUSPENSE_TYPE;

var hasWarnedAboutDeprecatedIsAsyncMode = false;

// AsyncMode should be deprecated
function isAsyncMode(object) {
  {
    if (!hasWarnedAboutDeprecatedIsAsyncMode) {
      hasWarnedAboutDeprecatedIsAsyncMode = true;
      lowPriorityWarning$1(false, 'The ReactIs.isAsyncMode() alias has been deprecated, ' + 'and will be removed in React 17+. Update your code to use ' + 'ReactIs.isConcurrentMode() instead. It has the exact same API.');
    }
  }
  return isConcurrentMode(object) || typeOf(object) === REACT_ASYNC_MODE_TYPE;
}
function isConcurrentMode(object) {
  return typeOf(object) === REACT_CONCURRENT_MODE_TYPE;
}
function isContextConsumer(object) {
  return typeOf(object) === REACT_CONTEXT_TYPE;
}
function isContextProvider(object) {
  return typeOf(object) === REACT_PROVIDER_TYPE;
}
function isElement(object) {
  return typeof object === 'object' && object !== null && object.$$typeof === REACT_ELEMENT_TYPE;
}
function isForwardRef(object) {
  return typeOf(object) === REACT_FORWARD_REF_TYPE;
}
function isFragment(object) {
  return typeOf(object) === REACT_FRAGMENT_TYPE;
}
function isLazy(object) {
  return typeOf(object) === REACT_LAZY_TYPE;
}
function isMemo(object) {
  return typeOf(object) === REACT_MEMO_TYPE;
}
function isPortal(object) {
  return typeOf(object) === REACT_PORTAL_TYPE;
}
function isProfiler(object) {
  return typeOf(object) === REACT_PROFILER_TYPE;
}
function isStrictMode(object) {
  return typeOf(object) === REACT_STRICT_MODE_TYPE;
}
function isSuspense(object) {
  return typeOf(object) === REACT_SUSPENSE_TYPE;
}

exports.typeOf = typeOf;
exports.AsyncMode = AsyncMode;
exports.ConcurrentMode = ConcurrentMode;
exports.ContextConsumer = ContextConsumer;
exports.ContextProvider = ContextProvider;
exports.Element = Element;
exports.ForwardRef = ForwardRef;
exports.Fragment = Fragment;
exports.Lazy = Lazy;
exports.Memo = Memo;
exports.Portal = Portal;
exports.Profiler = Profiler;
exports.StrictMode = StrictMode;
exports.Suspense = Suspense;
exports.isValidElementType = isValidElementType;
exports.isAsyncMode = isAsyncMode;
exports.isConcurrentMode = isConcurrentMode;
exports.isContextConsumer = isContextConsumer;
exports.isContextProvider = isContextProvider;
exports.isElement = isElement;
exports.isForwardRef = isForwardRef;
exports.isFragment = isFragment;
exports.isLazy = isLazy;
exports.isMemo = isMemo;
exports.isPortal = isPortal;
exports.isProfiler = isProfiler;
exports.isStrictMode = isStrictMode;
exports.isSuspense = isSuspense;
  })();
}


/***/ }),

/***/ "./node_modules/react-is/index.js":
/*!****************************************!*\
  !*** ./node_modules/react-is/index.js ***!
  \****************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


if (false) {} else {
  module.exports = __webpack_require__(/*! ./cjs/react-is.development.js */ "./node_modules/react-is/cjs/react-is.development.js");
}


/***/ }),

/***/ "./node_modules/react-markdown/lib/ast-to-react.js":
/*!*********************************************************!*\
  !*** ./node_modules/react-markdown/lib/ast-to-react.js ***!
  \*********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var React = __webpack_require__(/*! react */ "./node_modules/react/index.js");

var xtend = __webpack_require__(/*! xtend */ "./node_modules/xtend/immutable.js");

function astToReact(node, options) {
  var parent = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};
  var index = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 0;
  var renderer = options.renderers[node.type];
  var pos = node.position.start;
  var key = [node.type, pos.line, pos.column].join('-');

  if (typeof renderer !== 'function' && typeof renderer !== 'string' && !isReactFragment(renderer)) {
    throw new Error("Renderer for type `".concat(node.type, "` not defined or is not renderable"));
  }

  var nodeProps = getNodeProps(node, key, options, renderer, parent, index);
  return React.createElement(renderer, nodeProps, nodeProps.children || resolveChildren() || undefined);

  function resolveChildren() {
    return node.children && node.children.map(function (childNode, i) {
      return astToReact(childNode, options, {
        node: node,
        props: nodeProps
      }, i);
    });
  }
}

function isReactFragment(renderer) {
  return React.Fragment && React.Fragment === renderer;
} // eslint-disable-next-line max-params, complexity


function getNodeProps(node, key, opts, renderer, parent, index) {
  var props = {
    key: key
  };
  var isTagRenderer = typeof renderer === 'string'; // `sourcePos` is true if the user wants source information (line/column info from markdown source)

  if (opts.sourcePos && node.position) {
    props['data-sourcepos'] = flattenPosition(node.position);
  }

  if (opts.rawSourcePos && !isTagRenderer) {
    props.sourcePosition = node.position;
  } // If `includeNodeIndex` is true, pass node index info to all non-tag renderers


  if (opts.includeNodeIndex && parent.node && parent.node.children && !isTagRenderer) {
    props.index = parent.node.children.indexOf(node);
    props.parentChildCount = parent.node.children.length;
  }

  var ref = node.identifier !== null && node.identifier !== undefined ? opts.definitions[node.identifier] || {} : null;

  switch (node.type) {
    case 'root':
      assignDefined(props, {
        className: opts.className
      });
      break;

    case 'text':
      props.nodeKey = key;
      props.children = node.value;
      break;

    case 'heading':
      props.level = node.depth;
      break;

    case 'list':
      props.start = node.start;
      props.ordered = node.ordered;
      props.tight = !node.loose;
      props.depth = node.depth;
      break;

    case 'listItem':
      props.checked = node.checked;
      props.tight = !node.loose;
      props.ordered = node.ordered;
      props.index = node.index;
      props.children = getListItemChildren(node, parent).map(function (childNode, i) {
        return astToReact(childNode, opts, {
          node: node,
          props: props
        }, i);
      });
      break;

    case 'definition':
      assignDefined(props, {
        identifier: node.identifier,
        title: node.title,
        url: node.url
      });
      break;

    case 'code':
      assignDefined(props, {
        language: node.lang && node.lang.split(/\s/, 1)[0]
      });
      break;

    case 'inlineCode':
      props.children = node.value;
      props.inline = true;
      break;

    case 'link':
      assignDefined(props, {
        title: node.title || undefined,
        target: typeof opts.linkTarget === 'function' ? opts.linkTarget(node.url, node.children, node.title) : opts.linkTarget,
        href: opts.transformLinkUri ? opts.transformLinkUri(node.url, node.children, node.title) : node.url
      });
      break;

    case 'image':
      assignDefined(props, {
        alt: node.alt || undefined,
        title: node.title || undefined,
        src: opts.transformImageUri ? opts.transformImageUri(node.url, node.children, node.title, node.alt) : node.url
      });
      break;

    case 'linkReference':
      assignDefined(props, xtend(ref, {
        href: opts.transformLinkUri ? opts.transformLinkUri(ref.href) : ref.href
      }));
      break;

    case 'imageReference':
      assignDefined(props, {
        src: opts.transformImageUri && ref.href ? opts.transformImageUri(ref.href, node.children, ref.title, node.alt) : ref.href,
        title: ref.title || undefined,
        alt: node.alt || undefined
      });
      break;

    case 'table':
    case 'tableHead':
    case 'tableBody':
      props.columnAlignment = node.align;
      break;

    case 'tableRow':
      props.isHeader = parent.node.type === 'tableHead';
      props.columnAlignment = parent.props.columnAlignment;
      break;

    case 'tableCell':
      assignDefined(props, {
        isHeader: parent.props.isHeader,
        align: parent.props.columnAlignment[index]
      });
      break;

    case 'virtualHtml':
      props.tag = node.tag;
      break;

    case 'html':
      // @todo find a better way than this
      props.isBlock = node.position.start.line !== node.position.end.line;
      props.escapeHtml = opts.escapeHtml;
      props.skipHtml = opts.skipHtml;
      break;

    case 'parsedHtml':
      {
        var parsedChildren;

        if (node.children) {
          parsedChildren = node.children.map(function (child, i) {
            return astToReact(child, opts, {
              node: node,
              props: props
            }, i);
          });
        }

        props.escapeHtml = opts.escapeHtml;
        props.skipHtml = opts.skipHtml;
        props.element = mergeNodeChildren(node, parsedChildren);
        break;
      }

    default:
      assignDefined(props, xtend(node, {
        type: undefined,
        position: undefined,
        children: undefined
      }));
  }

  if (!isTagRenderer && node.value) {
    props.value = node.value;
  }

  return props;
}

function assignDefined(target, attrs) {
  for (var key in attrs) {
    if (typeof attrs[key] !== 'undefined') {
      target[key] = attrs[key];
    }
  }
}

function mergeNodeChildren(node, parsedChildren) {
  var el = node.element;

  if (Array.isArray(el)) {
    var Fragment = React.Fragment || 'div';
    return React.createElement(Fragment, null, el);
  }

  if (el.props.children || parsedChildren) {
    var children = React.Children.toArray(el.props.children).concat(parsedChildren);
    return React.cloneElement(el, null, children);
  }

  return React.cloneElement(el, null);
}

function flattenPosition(pos) {
  return [pos.start.line, ':', pos.start.column, '-', pos.end.line, ':', pos.end.column].map(String).join('');
}

function getListItemChildren(node, parent) {
  if (node.loose) {
    return node.children;
  }

  if (parent.node && node.index > 0 && parent.node.children[node.index - 1].loose) {
    return node.children;
  }

  return unwrapParagraphs(node);
}

function unwrapParagraphs(node) {
  return node.children.reduce(function (array, child) {
    return array.concat(child.type === 'paragraph' ? child.children || [] : [child]);
  }, []);
}

module.exports = astToReact;

/***/ }),

/***/ "./node_modules/react-markdown/lib/get-definitions.js":
/*!************************************************************!*\
  !*** ./node_modules/react-markdown/lib/get-definitions.js ***!
  \************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = function getDefinitions(node) {
  var defs = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};
  return (node.children || []).reduce(function (definitions, child) {
    if (child.type === 'definition') {
      definitions[child.identifier] = {
        href: child.url,
        title: child.title
      };
    }

    return getDefinitions(child, definitions);
  }, defs);
};

/***/ }),

/***/ "./node_modules/react-markdown/lib/plugins/disallow-node.js":
/*!******************************************************************!*\
  !*** ./node_modules/react-markdown/lib/plugins/disallow-node.js ***!
  \******************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var visit = __webpack_require__(/*! unist-util-visit */ "./node_modules/unist-util-visit/index.js");

exports.ofType = function (types, mode) {
  return function (node) {
    types.forEach(function (type) {
      return visit(node, type, disallow, true);
    });
    return node;
  };

  function disallow(node, index, parent) {
    if (parent) {
      untangle(node, index, parent, mode);
    }
  }
};

exports.ifNotMatch = function (allowNode, mode) {
  return function (node) {
    visit(node, disallow, true);
    return node;
  };

  function disallow(node, index, parent) {
    if (parent && !allowNode(node, index, parent)) {
      untangle(node, index, parent, mode);
    }
  }
};

function untangle(node, index, parent, mode) {
  if (mode === 'remove') {
    parent.children.splice(index, 1);
  } else if (mode === 'unwrap') {
    var args = [index, 1];

    if (node.children) {
      args = args.concat(node.children);
    }

    Array.prototype.splice.apply(parent.children, args);
  }
}

/***/ }),

/***/ "./node_modules/react-markdown/lib/plugins/naive-html.js":
/*!***************************************************************!*\
  !*** ./node_modules/react-markdown/lib/plugins/naive-html.js ***!
  \***************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


/**
 * Naive, simple plugin to match inline nodes without attributes
 * This allows say <strong>foo</strong>, but not <strong class="very">foo</strong>
 * For proper HTML support, you'll want a different plugin
 **/
var visit = __webpack_require__(/*! unist-util-visit */ "./node_modules/unist-util-visit/index.js");

var type = 'virtualHtml';
var selfClosingRe = /^<(area|base|br|col|embed|hr|img|input|keygen|link|meta|param|source|track|wbr)\s*\/?>$/i;
var simpleTagRe = /^<(\/?)([a-z]+)\s*>$/;

module.exports = function (tree) {
  var open;
  var currentParent;
  visit(tree, 'html', function (node, index, parent) {
    if (currentParent !== parent) {
      open = [];
      currentParent = parent;
    }

    var selfClosing = getSelfClosing(node);

    if (selfClosing) {
      parent.children.splice(index, 1, {
        type: type,
        tag: selfClosing,
        position: node.position
      });
      return true;
    }

    var current = getSimpleTag(node, parent);

    if (!current) {
      return true;
    }

    var matching = findAndPull(open, current.tag);

    if (matching) {
      parent.children.splice(index, 0, virtual(current, matching, parent));
    } else if (!current.opening) {
      open.push(current);
    }

    return true;
  }, true // Iterate in reverse
  );
  return tree;
};

function findAndPull(open, matchingTag) {
  var i = open.length;

  while (i--) {
    if (open[i].tag === matchingTag) {
      return open.splice(i, 1)[0];
    }
  }

  return false;
}

function getSimpleTag(node, parent) {
  var match = node.value.match(simpleTagRe);
  return match ? {
    tag: match[2],
    opening: !match[1],
    node: node
  } : false;
}

function getSelfClosing(node) {
  var match = node.value.match(selfClosingRe);
  return match ? match[1] : false;
}

function virtual(fromNode, toNode, parent) {
  var fromIndex = parent.children.indexOf(fromNode.node);
  var toIndex = parent.children.indexOf(toNode.node);
  var extracted = parent.children.splice(fromIndex, toIndex - fromIndex + 1);
  var children = extracted.slice(1, -1);
  return {
    type: type,
    children: children,
    tag: fromNode.tag,
    position: {
      start: fromNode.node.position.start,
      end: toNode.node.position.end,
      indent: []
    }
  };
}

/***/ }),

/***/ "./node_modules/react-markdown/lib/react-markdown.js":
/*!***********************************************************!*\
  !*** ./node_modules/react-markdown/lib/react-markdown.js ***!
  \***********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


function _toConsumableArray(arr) { return _arrayWithoutHoles(arr) || _iterableToArray(arr) || _nonIterableSpread(); }

function _nonIterableSpread() { throw new TypeError("Invalid attempt to spread non-iterable instance"); }

function _iterableToArray(iter) { if (Symbol.iterator in Object(iter) || Object.prototype.toString.call(iter) === "[object Arguments]") return Array.from(iter); }

function _arrayWithoutHoles(arr) { if (Array.isArray(arr)) { for (var i = 0, arr2 = new Array(arr.length); i < arr.length; i++) { arr2[i] = arr[i]; } return arr2; } }

var xtend = __webpack_require__(/*! xtend */ "./node_modules/xtend/immutable.js");

var unified = __webpack_require__(/*! unified */ "./node_modules/unified/index.js");

var parse = __webpack_require__(/*! remark-parse */ "./node_modules/remark-parse/index.js");

var PropTypes = __webpack_require__(/*! prop-types */ "./node_modules/react-markdown/node_modules/prop-types/index.js");

var addListMetadata = __webpack_require__(/*! mdast-add-list-metadata */ "./node_modules/mdast-add-list-metadata/index.js");

var naiveHtml = __webpack_require__(/*! ./plugins/naive-html */ "./node_modules/react-markdown/lib/plugins/naive-html.js");

var disallowNode = __webpack_require__(/*! ./plugins/disallow-node */ "./node_modules/react-markdown/lib/plugins/disallow-node.js");

var astToReact = __webpack_require__(/*! ./ast-to-react */ "./node_modules/react-markdown/lib/ast-to-react.js");

var wrapTableRows = __webpack_require__(/*! ./wrap-table-rows */ "./node_modules/react-markdown/lib/wrap-table-rows.js");

var getDefinitions = __webpack_require__(/*! ./get-definitions */ "./node_modules/react-markdown/lib/get-definitions.js");

var uriTransformer = __webpack_require__(/*! ./uri-transformer */ "./node_modules/react-markdown/lib/uri-transformer.js");

var defaultRenderers = __webpack_require__(/*! ./renderers */ "./node_modules/react-markdown/lib/renderers.js");

var symbols = __webpack_require__(/*! ./symbols */ "./node_modules/react-markdown/lib/symbols.js");

var allTypes = Object.keys(defaultRenderers);

var ReactMarkdown = function ReactMarkdown(props) {
  var src = props.source || props.children || '';

  if (props.allowedTypes && props.disallowedTypes) {
    throw new Error('Only one of `allowedTypes` and `disallowedTypes` should be defined');
  }

  var renderers = xtend(defaultRenderers, props.renderers);
  var plugins = [parse].concat(props.plugins || []);
  var parser = plugins.reduce(applyParserPlugin, unified());
  var rawAst = parser.parse(src);
  var renderProps = xtend(props, {
    renderers: renderers,
    definitions: getDefinitions(rawAst)
  });
  var astPlugins = determineAstPlugins(props);
  var ast = astPlugins.reduce(function (node, plugin) {
    return plugin(node, renderProps);
  }, rawAst);
  return astToReact(ast, renderProps);
};

function applyParserPlugin(parser, plugin) {
  return Array.isArray(plugin) ? parser.use.apply(parser, _toConsumableArray(plugin)) : parser.use(plugin);
}

function determineAstPlugins(props) {
  var plugins = [wrapTableRows, addListMetadata()];
  var disallowedTypes = props.disallowedTypes;

  if (props.allowedTypes) {
    disallowedTypes = allTypes.filter(function (type) {
      return type !== 'root' && props.allowedTypes.indexOf(type) === -1;
    });
  }

  var removalMethod = props.unwrapDisallowed ? 'unwrap' : 'remove';

  if (disallowedTypes && disallowedTypes.length > 0) {
    plugins.push(disallowNode.ofType(disallowedTypes, removalMethod));
  }

  if (props.allowNode) {
    plugins.push(disallowNode.ifNotMatch(props.allowNode, removalMethod));
  }

  var renderHtml = !props.escapeHtml && !props.skipHtml;
  var hasHtmlParser = (props.astPlugins || []).some(function (item) {
    var plugin = Array.isArray(item) ? item[0] : item;
    return plugin.identity === symbols.HtmlParser;
  });

  if (renderHtml && !hasHtmlParser) {
    plugins.push(naiveHtml);
  }

  return props.astPlugins ? plugins.concat(props.astPlugins) : plugins;
}

ReactMarkdown.defaultProps = {
  renderers: {},
  escapeHtml: true,
  skipHtml: false,
  sourcePos: false,
  rawSourcePos: false,
  transformLinkUri: uriTransformer,
  astPlugins: [],
  plugins: []
};
ReactMarkdown.propTypes = {
  className: PropTypes.string,
  source: PropTypes.string,
  children: PropTypes.string,
  sourcePos: PropTypes.bool,
  rawSourcePos: PropTypes.bool,
  escapeHtml: PropTypes.bool,
  skipHtml: PropTypes.bool,
  allowNode: PropTypes.func,
  allowedTypes: PropTypes.arrayOf(PropTypes.oneOf(allTypes)),
  disallowedTypes: PropTypes.arrayOf(PropTypes.oneOf(allTypes)),
  transformLinkUri: PropTypes.oneOfType([PropTypes.func, PropTypes.bool]),
  linkTarget: PropTypes.oneOfType([PropTypes.func, PropTypes.string]),
  transformImageUri: PropTypes.func,
  astPlugins: PropTypes.arrayOf(PropTypes.func),
  unwrapDisallowed: PropTypes.bool,
  renderers: PropTypes.object,
  plugins: PropTypes.array
};
ReactMarkdown.types = allTypes;
ReactMarkdown.renderers = defaultRenderers;
ReactMarkdown.uriTransformer = uriTransformer;
module.exports = ReactMarkdown;

/***/ }),

/***/ "./node_modules/react-markdown/lib/renderers.js":
/*!******************************************************!*\
  !*** ./node_modules/react-markdown/lib/renderers.js ***!
  \******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
/* eslint-disable react/prop-types, react/no-multi-comp */


var xtend = __webpack_require__(/*! xtend */ "./node_modules/xtend/immutable.js");

var React = __webpack_require__(/*! react */ "./node_modules/react/index.js");

var supportsStringRender = parseInt((React.version || '16').slice(0, 2), 10) >= 16;
var createElement = React.createElement;
module.exports = {
  break: 'br',
  paragraph: 'p',
  emphasis: 'em',
  strong: 'strong',
  thematicBreak: 'hr',
  blockquote: 'blockquote',
  delete: 'del',
  link: 'a',
  image: 'img',
  linkReference: 'a',
  imageReference: 'img',
  table: SimpleRenderer.bind(null, 'table'),
  tableHead: SimpleRenderer.bind(null, 'thead'),
  tableBody: SimpleRenderer.bind(null, 'tbody'),
  tableRow: SimpleRenderer.bind(null, 'tr'),
  tableCell: TableCell,
  root: Root,
  text: TextRenderer,
  list: List,
  listItem: ListItem,
  definition: NullRenderer,
  heading: Heading,
  inlineCode: InlineCode,
  code: CodeBlock,
  html: Html,
  virtualHtml: VirtualHtml,
  parsedHtml: ParsedHtml
};

function TextRenderer(props) {
  return supportsStringRender ? props.children : createElement('span', null, props.children);
}

function Root(props) {
  var useFragment = !props.className;
  var root = useFragment ? React.Fragment || 'div' : 'div';
  return createElement(root, useFragment ? null : props, props.children);
}

function SimpleRenderer(tag, props) {
  return createElement(tag, getCoreProps(props), props.children);
}

function TableCell(props) {
  var style = props.align ? {
    textAlign: props.align
  } : undefined;
  var coreProps = getCoreProps(props);
  return createElement(props.isHeader ? 'th' : 'td', style ? xtend({
    style: style
  }, coreProps) : coreProps, props.children);
}

function Heading(props) {
  return createElement("h".concat(props.level), getCoreProps(props), props.children);
}

function List(props) {
  var attrs = getCoreProps(props);

  if (props.start !== null && props.start !== 1) {
    attrs.start = props.start.toString();
  }

  return createElement(props.ordered ? 'ol' : 'ul', attrs, props.children);
}

function ListItem(props) {
  var checkbox = null;

  if (props.checked !== null) {
    var checked = props.checked;
    checkbox = createElement('input', {
      type: 'checkbox',
      checked: checked,
      readOnly: true
    });
  }

  return createElement('li', getCoreProps(props), checkbox, props.children);
}

function CodeBlock(props) {
  var className = props.language && "language-".concat(props.language);
  var code = createElement('code', className ? {
    className: className
  } : null, props.value);
  return createElement('pre', getCoreProps(props), code);
}

function InlineCode(props) {
  return createElement('code', getCoreProps(props), props.children);
}

function Html(props) {
  if (props.skipHtml) {
    return null;
  }

  var tag = props.isBlock ? 'div' : 'span';

  if (props.escapeHtml) {
    var comp = React.Fragment || tag;
    return createElement(comp, null, props.value);
  }

  var nodeProps = {
    dangerouslySetInnerHTML: {
      __html: props.value
    }
  };
  return createElement(tag, nodeProps);
}

function ParsedHtml(props) {
  return props['data-sourcepos'] ? React.cloneElement(props.element, {
    'data-sourcepos': props['data-sourcepos']
  }) : props.element;
}

function VirtualHtml(props) {
  return createElement(props.tag, getCoreProps(props), props.children);
}

function NullRenderer() {
  return null;
}

function getCoreProps(props) {
  return props['data-sourcepos'] ? {
    'data-sourcepos': props['data-sourcepos']
  } : {};
}

/***/ }),

/***/ "./node_modules/react-markdown/lib/symbols.js":
/*!****************************************************!*\
  !*** ./node_modules/react-markdown/lib/symbols.js ***!
  \****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var HtmlParser = '__RMD_HTML_PARSER__';
exports.HtmlParser = typeof Symbol === 'undefined' ? HtmlParser : Symbol(HtmlParser);

/***/ }),

/***/ "./node_modules/react-markdown/lib/uri-transformer.js":
/*!************************************************************!*\
  !*** ./node_modules/react-markdown/lib/uri-transformer.js ***!
  \************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var protocols = ['http', 'https', 'mailto', 'tel'];

module.exports = function uriTransformer(uri) {
  var url = (uri || '').trim();
  var first = url.charAt(0);

  if (first === '#' || first === '/') {
    return url;
  }

  var colon = url.indexOf(':');

  if (colon === -1) {
    return url;
  }

  var length = protocols.length;
  var index = -1;

  while (++index < length) {
    var protocol = protocols[index];

    if (colon === protocol.length && url.slice(0, protocol.length).toLowerCase() === protocol) {
      return url;
    }
  }

  index = url.indexOf('?');

  if (index !== -1 && colon > index) {
    return url;
  }

  index = url.indexOf('#');

  if (index !== -1 && colon > index) {
    return url;
  } // eslint-disable-next-line no-script-url


  return 'javascript:void(0)';
};

/***/ }),

/***/ "./node_modules/react-markdown/lib/wrap-table-rows.js":
/*!************************************************************!*\
  !*** ./node_modules/react-markdown/lib/wrap-table-rows.js ***!
  \************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var visit = __webpack_require__(/*! unist-util-visit */ "./node_modules/unist-util-visit/index.js");

module.exports = function (node) {
  visit(node, 'table', wrap);
  return node;
};

function wrap(table) {
  var children = table.children;
  table.children = [{
    type: 'tableHead',
    align: table.align,
    children: [children[0]],
    position: children[0].position
  }];

  if (children.length > 1) {
    table.children.push({
      type: 'tableBody',
      align: table.align,
      children: children.slice(1),
      position: {
        start: children[1].position.start,
        end: children[children.length - 1].position.end
      }
    });
  }
}

/***/ }),

/***/ "./node_modules/react-markdown/node_modules/prop-types/checkPropTypes.js":
/*!*******************************************************************************!*\
  !*** ./node_modules/react-markdown/node_modules/prop-types/checkPropTypes.js ***!
  \*******************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
/**
 * Copyright (c) 2013-present, Facebook, Inc.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */



var printWarning = function() {};

if (true) {
  var ReactPropTypesSecret = __webpack_require__(/*! ./lib/ReactPropTypesSecret */ "./node_modules/react-markdown/node_modules/prop-types/lib/ReactPropTypesSecret.js");
  var loggedTypeFailures = {};
  var has = Function.call.bind(Object.prototype.hasOwnProperty);

  printWarning = function(text) {
    var message = 'Warning: ' + text;
    if (typeof console !== 'undefined') {
      console.error(message);
    }
    try {
      // --- Welcome to debugging React ---
      // This error was thrown as a convenience so that you can use this stack
      // to find the callsite that caused this warning to fire.
      throw new Error(message);
    } catch (x) {}
  };
}

/**
 * Assert that the values match with the type specs.
 * Error messages are memorized and will only be shown once.
 *
 * @param {object} typeSpecs Map of name to a ReactPropType
 * @param {object} values Runtime values that need to be type-checked
 * @param {string} location e.g. "prop", "context", "child context"
 * @param {string} componentName Name of the component for error messages.
 * @param {?Function} getStack Returns the component stack.
 * @private
 */
function checkPropTypes(typeSpecs, values, location, componentName, getStack) {
  if (true) {
    for (var typeSpecName in typeSpecs) {
      if (has(typeSpecs, typeSpecName)) {
        var error;
        // Prop type validation may throw. In case they do, we don't want to
        // fail the render phase where it didn't fail before. So we log it.
        // After these have been cleaned up, we'll let them throw.
        try {
          // This is intentionally an invariant that gets caught. It's the same
          // behavior as without this statement except with a better message.
          if (typeof typeSpecs[typeSpecName] !== 'function') {
            var err = Error(
              (componentName || 'React class') + ': ' + location + ' type `' + typeSpecName + '` is invalid; ' +
              'it must be a function, usually from the `prop-types` package, but received `' + typeof typeSpecs[typeSpecName] + '`.'
            );
            err.name = 'Invariant Violation';
            throw err;
          }
          error = typeSpecs[typeSpecName](values, typeSpecName, componentName, location, null, ReactPropTypesSecret);
        } catch (ex) {
          error = ex;
        }
        if (error && !(error instanceof Error)) {
          printWarning(
            (componentName || 'React class') + ': type specification of ' +
            location + ' `' + typeSpecName + '` is invalid; the type checker ' +
            'function must return `null` or an `Error` but returned a ' + typeof error + '. ' +
            'You may have forgotten to pass an argument to the type checker ' +
            'creator (arrayOf, instanceOf, objectOf, oneOf, oneOfType, and ' +
            'shape all require an argument).'
          );
        }
        if (error instanceof Error && !(error.message in loggedTypeFailures)) {
          // Only monitor this failure once because there tends to be a lot of the
          // same error.
          loggedTypeFailures[error.message] = true;

          var stack = getStack ? getStack() : '';

          printWarning(
            'Failed ' + location + ' type: ' + error.message + (stack != null ? stack : '')
          );
        }
      }
    }
  }
}

/**
 * Resets warning cache when testing.
 *
 * @private
 */
checkPropTypes.resetWarningCache = function() {
  if (true) {
    loggedTypeFailures = {};
  }
}

module.exports = checkPropTypes;


/***/ }),

/***/ "./node_modules/react-markdown/node_modules/prop-types/factoryWithTypeCheckers.js":
/*!****************************************************************************************!*\
  !*** ./node_modules/react-markdown/node_modules/prop-types/factoryWithTypeCheckers.js ***!
  \****************************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
/**
 * Copyright (c) 2013-present, Facebook, Inc.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */



var ReactIs = __webpack_require__(/*! react-is */ "./node_modules/react-is/index.js");
var assign = __webpack_require__(/*! object-assign */ "./node_modules/object-assign/index.js");

var ReactPropTypesSecret = __webpack_require__(/*! ./lib/ReactPropTypesSecret */ "./node_modules/react-markdown/node_modules/prop-types/lib/ReactPropTypesSecret.js");
var checkPropTypes = __webpack_require__(/*! ./checkPropTypes */ "./node_modules/react-markdown/node_modules/prop-types/checkPropTypes.js");

var has = Function.call.bind(Object.prototype.hasOwnProperty);
var printWarning = function() {};

if (true) {
  printWarning = function(text) {
    var message = 'Warning: ' + text;
    if (typeof console !== 'undefined') {
      console.error(message);
    }
    try {
      // --- Welcome to debugging React ---
      // This error was thrown as a convenience so that you can use this stack
      // to find the callsite that caused this warning to fire.
      throw new Error(message);
    } catch (x) {}
  };
}

function emptyFunctionThatReturnsNull() {
  return null;
}

module.exports = function(isValidElement, throwOnDirectAccess) {
  /* global Symbol */
  var ITERATOR_SYMBOL = typeof Symbol === 'function' && Symbol.iterator;
  var FAUX_ITERATOR_SYMBOL = '@@iterator'; // Before Symbol spec.

  /**
   * Returns the iterator method function contained on the iterable object.
   *
   * Be sure to invoke the function with the iterable as context:
   *
   *     var iteratorFn = getIteratorFn(myIterable);
   *     if (iteratorFn) {
   *       var iterator = iteratorFn.call(myIterable);
   *       ...
   *     }
   *
   * @param {?object} maybeIterable
   * @return {?function}
   */
  function getIteratorFn(maybeIterable) {
    var iteratorFn = maybeIterable && (ITERATOR_SYMBOL && maybeIterable[ITERATOR_SYMBOL] || maybeIterable[FAUX_ITERATOR_SYMBOL]);
    if (typeof iteratorFn === 'function') {
      return iteratorFn;
    }
  }

  /**
   * Collection of methods that allow declaration and validation of props that are
   * supplied to React components. Example usage:
   *
   *   var Props = require('ReactPropTypes');
   *   var MyArticle = React.createClass({
   *     propTypes: {
   *       // An optional string prop named "description".
   *       description: Props.string,
   *
   *       // A required enum prop named "category".
   *       category: Props.oneOf(['News','Photos']).isRequired,
   *
   *       // A prop named "dialog" that requires an instance of Dialog.
   *       dialog: Props.instanceOf(Dialog).isRequired
   *     },
   *     render: function() { ... }
   *   });
   *
   * A more formal specification of how these methods are used:
   *
   *   type := array|bool|func|object|number|string|oneOf([...])|instanceOf(...)
   *   decl := ReactPropTypes.{type}(.isRequired)?
   *
   * Each and every declaration produces a function with the same signature. This
   * allows the creation of custom validation functions. For example:
   *
   *  var MyLink = React.createClass({
   *    propTypes: {
   *      // An optional string or URI prop named "href".
   *      href: function(props, propName, componentName) {
   *        var propValue = props[propName];
   *        if (propValue != null && typeof propValue !== 'string' &&
   *            !(propValue instanceof URI)) {
   *          return new Error(
   *            'Expected a string or an URI for ' + propName + ' in ' +
   *            componentName
   *          );
   *        }
   *      }
   *    },
   *    render: function() {...}
   *  });
   *
   * @internal
   */

  var ANONYMOUS = '<<anonymous>>';

  // Important!
  // Keep this list in sync with production version in `./factoryWithThrowingShims.js`.
  var ReactPropTypes = {
    array: createPrimitiveTypeChecker('array'),
    bool: createPrimitiveTypeChecker('boolean'),
    func: createPrimitiveTypeChecker('function'),
    number: createPrimitiveTypeChecker('number'),
    object: createPrimitiveTypeChecker('object'),
    string: createPrimitiveTypeChecker('string'),
    symbol: createPrimitiveTypeChecker('symbol'),

    any: createAnyTypeChecker(),
    arrayOf: createArrayOfTypeChecker,
    element: createElementTypeChecker(),
    elementType: createElementTypeTypeChecker(),
    instanceOf: createInstanceTypeChecker,
    node: createNodeChecker(),
    objectOf: createObjectOfTypeChecker,
    oneOf: createEnumTypeChecker,
    oneOfType: createUnionTypeChecker,
    shape: createShapeTypeChecker,
    exact: createStrictShapeTypeChecker,
  };

  /**
   * inlined Object.is polyfill to avoid requiring consumers ship their own
   * https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/is
   */
  /*eslint-disable no-self-compare*/
  function is(x, y) {
    // SameValue algorithm
    if (x === y) {
      // Steps 1-5, 7-10
      // Steps 6.b-6.e: +0 != -0
      return x !== 0 || 1 / x === 1 / y;
    } else {
      // Step 6.a: NaN == NaN
      return x !== x && y !== y;
    }
  }
  /*eslint-enable no-self-compare*/

  /**
   * We use an Error-like object for backward compatibility as people may call
   * PropTypes directly and inspect their output. However, we don't use real
   * Errors anymore. We don't inspect their stack anyway, and creating them
   * is prohibitively expensive if they are created too often, such as what
   * happens in oneOfType() for any type before the one that matched.
   */
  function PropTypeError(message) {
    this.message = message;
    this.stack = '';
  }
  // Make `instanceof Error` still work for returned errors.
  PropTypeError.prototype = Error.prototype;

  function createChainableTypeChecker(validate) {
    if (true) {
      var manualPropTypeCallCache = {};
      var manualPropTypeWarningCount = 0;
    }
    function checkType(isRequired, props, propName, componentName, location, propFullName, secret) {
      componentName = componentName || ANONYMOUS;
      propFullName = propFullName || propName;

      if (secret !== ReactPropTypesSecret) {
        if (throwOnDirectAccess) {
          // New behavior only for users of `prop-types` package
          var err = new Error(
            'Calling PropTypes validators directly is not supported by the `prop-types` package. ' +
            'Use `PropTypes.checkPropTypes()` to call them. ' +
            'Read more at http://fb.me/use-check-prop-types'
          );
          err.name = 'Invariant Violation';
          throw err;
        } else if ( true && typeof console !== 'undefined') {
          // Old behavior for people using React.PropTypes
          var cacheKey = componentName + ':' + propName;
          if (
            !manualPropTypeCallCache[cacheKey] &&
            // Avoid spamming the console because they are often not actionable except for lib authors
            manualPropTypeWarningCount < 3
          ) {
            printWarning(
              'You are manually calling a React.PropTypes validation ' +
              'function for the `' + propFullName + '` prop on `' + componentName  + '`. This is deprecated ' +
              'and will throw in the standalone `prop-types` package. ' +
              'You may be seeing this warning due to a third-party PropTypes ' +
              'library. See https://fb.me/react-warning-dont-call-proptypes ' + 'for details.'
            );
            manualPropTypeCallCache[cacheKey] = true;
            manualPropTypeWarningCount++;
          }
        }
      }
      if (props[propName] == null) {
        if (isRequired) {
          if (props[propName] === null) {
            return new PropTypeError('The ' + location + ' `' + propFullName + '` is marked as required ' + ('in `' + componentName + '`, but its value is `null`.'));
          }
          return new PropTypeError('The ' + location + ' `' + propFullName + '` is marked as required in ' + ('`' + componentName + '`, but its value is `undefined`.'));
        }
        return null;
      } else {
        return validate(props, propName, componentName, location, propFullName);
      }
    }

    var chainedCheckType = checkType.bind(null, false);
    chainedCheckType.isRequired = checkType.bind(null, true);

    return chainedCheckType;
  }

  function createPrimitiveTypeChecker(expectedType) {
    function validate(props, propName, componentName, location, propFullName, secret) {
      var propValue = props[propName];
      var propType = getPropType(propValue);
      if (propType !== expectedType) {
        // `propValue` being instance of, say, date/regexp, pass the 'object'
        // check, but we can offer a more precise error message here rather than
        // 'of type `object`'.
        var preciseType = getPreciseType(propValue);

        return new PropTypeError('Invalid ' + location + ' `' + propFullName + '` of type ' + ('`' + preciseType + '` supplied to `' + componentName + '`, expected ') + ('`' + expectedType + '`.'));
      }
      return null;
    }
    return createChainableTypeChecker(validate);
  }

  function createAnyTypeChecker() {
    return createChainableTypeChecker(emptyFunctionThatReturnsNull);
  }

  function createArrayOfTypeChecker(typeChecker) {
    function validate(props, propName, componentName, location, propFullName) {
      if (typeof typeChecker !== 'function') {
        return new PropTypeError('Property `' + propFullName + '` of component `' + componentName + '` has invalid PropType notation inside arrayOf.');
      }
      var propValue = props[propName];
      if (!Array.isArray(propValue)) {
        var propType = getPropType(propValue);
        return new PropTypeError('Invalid ' + location + ' `' + propFullName + '` of type ' + ('`' + propType + '` supplied to `' + componentName + '`, expected an array.'));
      }
      for (var i = 0; i < propValue.length; i++) {
        var error = typeChecker(propValue, i, componentName, location, propFullName + '[' + i + ']', ReactPropTypesSecret);
        if (error instanceof Error) {
          return error;
        }
      }
      return null;
    }
    return createChainableTypeChecker(validate);
  }

  function createElementTypeChecker() {
    function validate(props, propName, componentName, location, propFullName) {
      var propValue = props[propName];
      if (!isValidElement(propValue)) {
        var propType = getPropType(propValue);
        return new PropTypeError('Invalid ' + location + ' `' + propFullName + '` of type ' + ('`' + propType + '` supplied to `' + componentName + '`, expected a single ReactElement.'));
      }
      return null;
    }
    return createChainableTypeChecker(validate);
  }

  function createElementTypeTypeChecker() {
    function validate(props, propName, componentName, location, propFullName) {
      var propValue = props[propName];
      if (!ReactIs.isValidElementType(propValue)) {
        var propType = getPropType(propValue);
        return new PropTypeError('Invalid ' + location + ' `' + propFullName + '` of type ' + ('`' + propType + '` supplied to `' + componentName + '`, expected a single ReactElement type.'));
      }
      return null;
    }
    return createChainableTypeChecker(validate);
  }

  function createInstanceTypeChecker(expectedClass) {
    function validate(props, propName, componentName, location, propFullName) {
      if (!(props[propName] instanceof expectedClass)) {
        var expectedClassName = expectedClass.name || ANONYMOUS;
        var actualClassName = getClassName(props[propName]);
        return new PropTypeError('Invalid ' + location + ' `' + propFullName + '` of type ' + ('`' + actualClassName + '` supplied to `' + componentName + '`, expected ') + ('instance of `' + expectedClassName + '`.'));
      }
      return null;
    }
    return createChainableTypeChecker(validate);
  }

  function createEnumTypeChecker(expectedValues) {
    if (!Array.isArray(expectedValues)) {
      if (true) {
        if (arguments.length > 1) {
          printWarning(
            'Invalid arguments supplied to oneOf, expected an array, got ' + arguments.length + ' arguments. ' +
            'A common mistake is to write oneOf(x, y, z) instead of oneOf([x, y, z]).'
          );
        } else {
          printWarning('Invalid argument supplied to oneOf, expected an array.');
        }
      }
      return emptyFunctionThatReturnsNull;
    }

    function validate(props, propName, componentName, location, propFullName) {
      var propValue = props[propName];
      for (var i = 0; i < expectedValues.length; i++) {
        if (is(propValue, expectedValues[i])) {
          return null;
        }
      }

      var valuesString = JSON.stringify(expectedValues, function replacer(key, value) {
        var type = getPreciseType(value);
        if (type === 'symbol') {
          return String(value);
        }
        return value;
      });
      return new PropTypeError('Invalid ' + location + ' `' + propFullName + '` of value `' + String(propValue) + '` ' + ('supplied to `' + componentName + '`, expected one of ' + valuesString + '.'));
    }
    return createChainableTypeChecker(validate);
  }

  function createObjectOfTypeChecker(typeChecker) {
    function validate(props, propName, componentName, location, propFullName) {
      if (typeof typeChecker !== 'function') {
        return new PropTypeError('Property `' + propFullName + '` of component `' + componentName + '` has invalid PropType notation inside objectOf.');
      }
      var propValue = props[propName];
      var propType = getPropType(propValue);
      if (propType !== 'object') {
        return new PropTypeError('Invalid ' + location + ' `' + propFullName + '` of type ' + ('`' + propType + '` supplied to `' + componentName + '`, expected an object.'));
      }
      for (var key in propValue) {
        if (has(propValue, key)) {
          var error = typeChecker(propValue, key, componentName, location, propFullName + '.' + key, ReactPropTypesSecret);
          if (error instanceof Error) {
            return error;
          }
        }
      }
      return null;
    }
    return createChainableTypeChecker(validate);
  }

  function createUnionTypeChecker(arrayOfTypeCheckers) {
    if (!Array.isArray(arrayOfTypeCheckers)) {
       true ? printWarning('Invalid argument supplied to oneOfType, expected an instance of array.') : undefined;
      return emptyFunctionThatReturnsNull;
    }

    for (var i = 0; i < arrayOfTypeCheckers.length; i++) {
      var checker = arrayOfTypeCheckers[i];
      if (typeof checker !== 'function') {
        printWarning(
          'Invalid argument supplied to oneOfType. Expected an array of check functions, but ' +
          'received ' + getPostfixForTypeWarning(checker) + ' at index ' + i + '.'
        );
        return emptyFunctionThatReturnsNull;
      }
    }

    function validate(props, propName, componentName, location, propFullName) {
      for (var i = 0; i < arrayOfTypeCheckers.length; i++) {
        var checker = arrayOfTypeCheckers[i];
        if (checker(props, propName, componentName, location, propFullName, ReactPropTypesSecret) == null) {
          return null;
        }
      }

      return new PropTypeError('Invalid ' + location + ' `' + propFullName + '` supplied to ' + ('`' + componentName + '`.'));
    }
    return createChainableTypeChecker(validate);
  }

  function createNodeChecker() {
    function validate(props, propName, componentName, location, propFullName) {
      if (!isNode(props[propName])) {
        return new PropTypeError('Invalid ' + location + ' `' + propFullName + '` supplied to ' + ('`' + componentName + '`, expected a ReactNode.'));
      }
      return null;
    }
    return createChainableTypeChecker(validate);
  }

  function createShapeTypeChecker(shapeTypes) {
    function validate(props, propName, componentName, location, propFullName) {
      var propValue = props[propName];
      var propType = getPropType(propValue);
      if (propType !== 'object') {
        return new PropTypeError('Invalid ' + location + ' `' + propFullName + '` of type `' + propType + '` ' + ('supplied to `' + componentName + '`, expected `object`.'));
      }
      for (var key in shapeTypes) {
        var checker = shapeTypes[key];
        if (!checker) {
          continue;
        }
        var error = checker(propValue, key, componentName, location, propFullName + '.' + key, ReactPropTypesSecret);
        if (error) {
          return error;
        }
      }
      return null;
    }
    return createChainableTypeChecker(validate);
  }

  function createStrictShapeTypeChecker(shapeTypes) {
    function validate(props, propName, componentName, location, propFullName) {
      var propValue = props[propName];
      var propType = getPropType(propValue);
      if (propType !== 'object') {
        return new PropTypeError('Invalid ' + location + ' `' + propFullName + '` of type `' + propType + '` ' + ('supplied to `' + componentName + '`, expected `object`.'));
      }
      // We need to check all keys in case some are required but missing from
      // props.
      var allKeys = assign({}, props[propName], shapeTypes);
      for (var key in allKeys) {
        var checker = shapeTypes[key];
        if (!checker) {
          return new PropTypeError(
            'Invalid ' + location + ' `' + propFullName + '` key `' + key + '` supplied to `' + componentName + '`.' +
            '\nBad object: ' + JSON.stringify(props[propName], null, '  ') +
            '\nValid keys: ' +  JSON.stringify(Object.keys(shapeTypes), null, '  ')
          );
        }
        var error = checker(propValue, key, componentName, location, propFullName + '.' + key, ReactPropTypesSecret);
        if (error) {
          return error;
        }
      }
      return null;
    }

    return createChainableTypeChecker(validate);
  }

  function isNode(propValue) {
    switch (typeof propValue) {
      case 'number':
      case 'string':
      case 'undefined':
        return true;
      case 'boolean':
        return !propValue;
      case 'object':
        if (Array.isArray(propValue)) {
          return propValue.every(isNode);
        }
        if (propValue === null || isValidElement(propValue)) {
          return true;
        }

        var iteratorFn = getIteratorFn(propValue);
        if (iteratorFn) {
          var iterator = iteratorFn.call(propValue);
          var step;
          if (iteratorFn !== propValue.entries) {
            while (!(step = iterator.next()).done) {
              if (!isNode(step.value)) {
                return false;
              }
            }
          } else {
            // Iterator will provide entry [k,v] tuples rather than values.
            while (!(step = iterator.next()).done) {
              var entry = step.value;
              if (entry) {
                if (!isNode(entry[1])) {
                  return false;
                }
              }
            }
          }
        } else {
          return false;
        }

        return true;
      default:
        return false;
    }
  }

  function isSymbol(propType, propValue) {
    // Native Symbol.
    if (propType === 'symbol') {
      return true;
    }

    // falsy value can't be a Symbol
    if (!propValue) {
      return false;
    }

    // 19.4.3.5 Symbol.prototype[@@toStringTag] === 'Symbol'
    if (propValue['@@toStringTag'] === 'Symbol') {
      return true;
    }

    // Fallback for non-spec compliant Symbols which are polyfilled.
    if (typeof Symbol === 'function' && propValue instanceof Symbol) {
      return true;
    }

    return false;
  }

  // Equivalent of `typeof` but with special handling for array and regexp.
  function getPropType(propValue) {
    var propType = typeof propValue;
    if (Array.isArray(propValue)) {
      return 'array';
    }
    if (propValue instanceof RegExp) {
      // Old webkits (at least until Android 4.0) return 'function' rather than
      // 'object' for typeof a RegExp. We'll normalize this here so that /bla/
      // passes PropTypes.object.
      return 'object';
    }
    if (isSymbol(propType, propValue)) {
      return 'symbol';
    }
    return propType;
  }

  // This handles more types than `getPropType`. Only used for error messages.
  // See `createPrimitiveTypeChecker`.
  function getPreciseType(propValue) {
    if (typeof propValue === 'undefined' || propValue === null) {
      return '' + propValue;
    }
    var propType = getPropType(propValue);
    if (propType === 'object') {
      if (propValue instanceof Date) {
        return 'date';
      } else if (propValue instanceof RegExp) {
        return 'regexp';
      }
    }
    return propType;
  }

  // Returns a string that is postfixed to a warning about an invalid type.
  // For example, "undefined" or "of type array"
  function getPostfixForTypeWarning(value) {
    var type = getPreciseType(value);
    switch (type) {
      case 'array':
      case 'object':
        return 'an ' + type;
      case 'boolean':
      case 'date':
      case 'regexp':
        return 'a ' + type;
      default:
        return type;
    }
  }

  // Returns class name of the object, if any.
  function getClassName(propValue) {
    if (!propValue.constructor || !propValue.constructor.name) {
      return ANONYMOUS;
    }
    return propValue.constructor.name;
  }

  ReactPropTypes.checkPropTypes = checkPropTypes;
  ReactPropTypes.resetWarningCache = checkPropTypes.resetWarningCache;
  ReactPropTypes.PropTypes = ReactPropTypes;

  return ReactPropTypes;
};


/***/ }),

/***/ "./node_modules/react-markdown/node_modules/prop-types/index.js":
/*!**********************************************************************!*\
  !*** ./node_modules/react-markdown/node_modules/prop-types/index.js ***!
  \**********************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

/**
 * Copyright (c) 2013-present, Facebook, Inc.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */

if (true) {
  var ReactIs = __webpack_require__(/*! react-is */ "./node_modules/react-is/index.js");

  // By explicitly using `prop-types` you are opting into new development behavior.
  // http://fb.me/prop-types-in-prod
  var throwOnDirectAccess = true;
  module.exports = __webpack_require__(/*! ./factoryWithTypeCheckers */ "./node_modules/react-markdown/node_modules/prop-types/factoryWithTypeCheckers.js")(ReactIs.isElement, throwOnDirectAccess);
} else {}


/***/ }),

/***/ "./node_modules/react-markdown/node_modules/prop-types/lib/ReactPropTypesSecret.js":
/*!*****************************************************************************************!*\
  !*** ./node_modules/react-markdown/node_modules/prop-types/lib/ReactPropTypesSecret.js ***!
  \*****************************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
/**
 * Copyright (c) 2013-present, Facebook, Inc.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */



var ReactPropTypesSecret = 'SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED';

module.exports = ReactPropTypesSecret;


/***/ }),

/***/ "./node_modules/react/index.js":
/*!*******************************************************************************************!*\
  !*** delegated ./node_modules/react/index.js from dll-reference dll_1aef2d0bbc0d334d831c ***!
  \*******************************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

module.exports = (__webpack_require__(/*! dll-reference dll_1aef2d0bbc0d334d831c */ "dll-reference dll_1aef2d0bbc0d334d831c"))("./node_modules/react/index.js");

/***/ }),

/***/ "./node_modules/remark-parse/index.js":
/*!********************************************!*\
  !*** ./node_modules/remark-parse/index.js ***!
  \********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var unherit = __webpack_require__(/*! unherit */ "./node_modules/unherit/index.js");
var xtend = __webpack_require__(/*! xtend */ "./node_modules/xtend/immutable.js");
var Parser = __webpack_require__(/*! ./lib/parser.js */ "./node_modules/remark-parse/lib/parser.js");

module.exports = parse;
parse.Parser = Parser;

function parse(options) {
  var Local = unherit(Parser);
  Local.prototype.options = xtend(Local.prototype.options, this.data('settings'), options);
  this.Parser = Local;
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/block-elements.json":
/*!***********************************************************!*\
  !*** ./node_modules/remark-parse/lib/block-elements.json ***!
  \***********************************************************/
/*! exports provided: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, default */
/***/ (function(module) {

module.exports = ["address","article","aside","base","basefont","blockquote","body","caption","center","col","colgroup","dd","details","dialog","dir","div","dl","dt","fieldset","figcaption","figure","footer","form","frame","frameset","h1","h2","h3","h4","h5","h6","head","header","hgroup","hr","html","iframe","legend","li","link","main","menu","menuitem","meta","nav","noframes","ol","optgroup","option","p","param","pre","section","source","title","summary","table","tbody","td","tfoot","th","thead","title","tr","track","ul"];

/***/ }),

/***/ "./node_modules/remark-parse/lib/decode.js":
/*!*************************************************!*\
  !*** ./node_modules/remark-parse/lib/decode.js ***!
  \*************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var xtend = __webpack_require__(/*! xtend */ "./node_modules/xtend/immutable.js");
var entities = __webpack_require__(/*! parse-entities */ "./node_modules/parse-entities/index.js");

module.exports = factory;

/* Factory to create an entity decoder. */
function factory(ctx) {
  decoder.raw = decodeRaw;

  return decoder;

  /* Normalize `position` to add an `indent`. */
  function normalize(position) {
    var offsets = ctx.offset;
    var line = position.line;
    var result = [];

    while (++line) {
      if (!(line in offsets)) {
        break;
      }

      result.push((offsets[line] || 0) + 1);
    }

    return {
      start: position,
      indent: result
    };
  }

  /* Handle a warning.
   * See https://github.com/wooorm/parse-entities
   * for the warnings. */
  function handleWarning(reason, position, code) {
    if (code === 3) {
      return;
    }

    ctx.file.message(reason, position);
  }

  /* Decode `value` (at `position`) into text-nodes. */
  function decoder(value, position, handler) {
    entities(value, {
      position: normalize(position),
      warning: handleWarning,
      text: handler,
      reference: handler,
      textContext: ctx,
      referenceContext: ctx
    });
  }

  /* Decode `value` (at `position`) into a string. */
  function decodeRaw(value, position, options) {
    return entities(value, xtend(options, {
      position: normalize(position),
      warning: handleWarning
    }));
  }
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/defaults.js":
/*!***************************************************!*\
  !*** ./node_modules/remark-parse/lib/defaults.js ***!
  \***************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = {
  position: true,
  gfm: true,
  commonmark: false,
  footnotes: false,
  pedantic: false,
  blocks: __webpack_require__(/*! ./block-elements.json */ "./node_modules/remark-parse/lib/block-elements.json")
};


/***/ }),

/***/ "./node_modules/remark-parse/lib/locate/break.js":
/*!*******************************************************!*\
  !*** ./node_modules/remark-parse/lib/locate/break.js ***!
  \*******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = locate;

function locate(value, fromIndex) {
  var index = value.indexOf('\n', fromIndex);

  while (index > fromIndex) {
    if (value.charAt(index - 1) !== ' ') {
      break;
    }

    index--;
  }

  return index;
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/locate/code-inline.js":
/*!*************************************************************!*\
  !*** ./node_modules/remark-parse/lib/locate/code-inline.js ***!
  \*************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = locate;

function locate(value, fromIndex) {
  return value.indexOf('`', fromIndex);
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/locate/delete.js":
/*!********************************************************!*\
  !*** ./node_modules/remark-parse/lib/locate/delete.js ***!
  \********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = locate;

function locate(value, fromIndex) {
  return value.indexOf('~~', fromIndex);
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/locate/emphasis.js":
/*!**********************************************************!*\
  !*** ./node_modules/remark-parse/lib/locate/emphasis.js ***!
  \**********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = locate;

function locate(value, fromIndex) {
  var asterisk = value.indexOf('*', fromIndex);
  var underscore = value.indexOf('_', fromIndex);

  if (underscore === -1) {
    return asterisk;
  }

  if (asterisk === -1) {
    return underscore;
  }

  return underscore < asterisk ? underscore : asterisk;
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/locate/escape.js":
/*!********************************************************!*\
  !*** ./node_modules/remark-parse/lib/locate/escape.js ***!
  \********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = locate;

function locate(value, fromIndex) {
  return value.indexOf('\\', fromIndex);
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/locate/link.js":
/*!******************************************************!*\
  !*** ./node_modules/remark-parse/lib/locate/link.js ***!
  \******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = locate;

function locate(value, fromIndex) {
  var link = value.indexOf('[', fromIndex);
  var image = value.indexOf('![', fromIndex);

  if (image === -1) {
    return link;
  }

  /* Link can never be `-1` if an image is found, so we dont need
   * to check for that :) */
  return link < image ? link : image;
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/locate/strong.js":
/*!********************************************************!*\
  !*** ./node_modules/remark-parse/lib/locate/strong.js ***!
  \********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = locate;

function locate(value, fromIndex) {
  var asterisk = value.indexOf('**', fromIndex);
  var underscore = value.indexOf('__', fromIndex);

  if (underscore === -1) {
    return asterisk;
  }

  if (asterisk === -1) {
    return underscore;
  }

  return underscore < asterisk ? underscore : asterisk;
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/locate/tag.js":
/*!*****************************************************!*\
  !*** ./node_modules/remark-parse/lib/locate/tag.js ***!
  \*****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = locate;

function locate(value, fromIndex) {
  return value.indexOf('<', fromIndex);
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/locate/url.js":
/*!*****************************************************!*\
  !*** ./node_modules/remark-parse/lib/locate/url.js ***!
  \*****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = locate;

var PROTOCOLS = ['https://', 'http://', 'mailto:'];

function locate(value, fromIndex) {
  var length = PROTOCOLS.length;
  var index = -1;
  var min = -1;
  var position;

  if (!this.options.gfm) {
    return -1;
  }

  while (++index < length) {
    position = value.indexOf(PROTOCOLS[index], fromIndex);

    if (position !== -1 && (position < min || min === -1)) {
      min = position;
    }
  }

  return min;
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/parse.js":
/*!************************************************!*\
  !*** ./node_modules/remark-parse/lib/parse.js ***!
  \************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var xtend = __webpack_require__(/*! xtend */ "./node_modules/xtend/immutable.js");
var removePosition = __webpack_require__(/*! unist-util-remove-position */ "./node_modules/unist-util-remove-position/index.js");

module.exports = parse;

var C_NEWLINE = '\n';
var EXPRESSION_LINE_BREAKS = /\r\n|\r/g;

/* Parse the bound file. */
function parse() {
  var self = this;
  var value = String(self.file);
  var start = {line: 1, column: 1, offset: 0};
  var content = xtend(start);
  var node;

  /* Clean non-unix newlines: `\r\n` and `\r` are all
   * changed to `\n`.  This should not affect positional
   * information. */
  value = value.replace(EXPRESSION_LINE_BREAKS, C_NEWLINE);

  if (value.charCodeAt(0) === 0xFEFF) {
    value = value.slice(1);

    content.column++;
    content.offset++;
  }

  node = {
    type: 'root',
    children: self.tokenizeBlock(value, content),
    position: {
      start: start,
      end: self.eof || xtend(start)
    }
  };

  if (!self.options.position) {
    removePosition(node, true);
  }

  return node;
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/parser.js":
/*!*************************************************!*\
  !*** ./node_modules/remark-parse/lib/parser.js ***!
  \*************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var xtend = __webpack_require__(/*! xtend */ "./node_modules/xtend/immutable.js");
var toggle = __webpack_require__(/*! state-toggle */ "./node_modules/state-toggle/index.js");
var vfileLocation = __webpack_require__(/*! vfile-location */ "./node_modules/vfile-location/index.js");
var unescape = __webpack_require__(/*! ./unescape */ "./node_modules/remark-parse/lib/unescape.js");
var decode = __webpack_require__(/*! ./decode */ "./node_modules/remark-parse/lib/decode.js");
var tokenizer = __webpack_require__(/*! ./tokenizer */ "./node_modules/remark-parse/lib/tokenizer.js");

module.exports = Parser;

function Parser(doc, file) {
  this.file = file;
  this.offset = {};
  this.options = xtend(this.options);
  this.setOptions({});

  this.inList = false;
  this.inBlock = false;
  this.inLink = false;
  this.atStart = true;

  this.toOffset = vfileLocation(file).toOffset;
  this.unescape = unescape(this, 'escape');
  this.decode = decode(this);
}

var proto = Parser.prototype;

/* Expose core. */
proto.setOptions = __webpack_require__(/*! ./set-options */ "./node_modules/remark-parse/lib/set-options.js");
proto.parse = __webpack_require__(/*! ./parse */ "./node_modules/remark-parse/lib/parse.js");

/* Expose `defaults`. */
proto.options = __webpack_require__(/*! ./defaults */ "./node_modules/remark-parse/lib/defaults.js");

/* Enter and exit helpers. */
proto.exitStart = toggle('atStart', true);
proto.enterList = toggle('inList', false);
proto.enterLink = toggle('inLink', false);
proto.enterBlock = toggle('inBlock', false);

/* Nodes that can interupt a paragraph:
 *
 * ```markdown
 * A paragraph, followed by a thematic break.
 * ___
 * ```
 *
 * In the above example, the thematic break interupts
 * the paragraph. */
proto.interruptParagraph = [
  ['thematicBreak'],
  ['atxHeading'],
  ['fencedCode'],
  ['blockquote'],
  ['html'],
  ['setextHeading', {commonmark: false}],
  ['definition', {commonmark: false}],
  ['footnote', {commonmark: false}]
];

/* Nodes that can interupt a list:
 *
 * ```markdown
 * - One
 * ___
 * ```
 *
 * In the above example, the thematic break interupts
 * the list. */
proto.interruptList = [
  ['atxHeading', {pedantic: false}],
  ['fencedCode', {pedantic: false}],
  ['thematicBreak', {pedantic: false}],
  ['definition', {commonmark: false}],
  ['footnote', {commonmark: false}]
];

/* Nodes that can interupt a blockquote:
 *
 * ```markdown
 * > A paragraph.
 * ___
 * ```
 *
 * In the above example, the thematic break interupts
 * the blockquote. */
proto.interruptBlockquote = [
  ['indentedCode', {commonmark: true}],
  ['fencedCode', {commonmark: true}],
  ['atxHeading', {commonmark: true}],
  ['setextHeading', {commonmark: true}],
  ['thematicBreak', {commonmark: true}],
  ['html', {commonmark: true}],
  ['list', {commonmark: true}],
  ['definition', {commonmark: false}],
  ['footnote', {commonmark: false}]
];

/* Handlers. */
proto.blockTokenizers = {
  newline: __webpack_require__(/*! ./tokenize/newline */ "./node_modules/remark-parse/lib/tokenize/newline.js"),
  indentedCode: __webpack_require__(/*! ./tokenize/code-indented */ "./node_modules/remark-parse/lib/tokenize/code-indented.js"),
  fencedCode: __webpack_require__(/*! ./tokenize/code-fenced */ "./node_modules/remark-parse/lib/tokenize/code-fenced.js"),
  blockquote: __webpack_require__(/*! ./tokenize/blockquote */ "./node_modules/remark-parse/lib/tokenize/blockquote.js"),
  atxHeading: __webpack_require__(/*! ./tokenize/heading-atx */ "./node_modules/remark-parse/lib/tokenize/heading-atx.js"),
  thematicBreak: __webpack_require__(/*! ./tokenize/thematic-break */ "./node_modules/remark-parse/lib/tokenize/thematic-break.js"),
  list: __webpack_require__(/*! ./tokenize/list */ "./node_modules/remark-parse/lib/tokenize/list.js"),
  setextHeading: __webpack_require__(/*! ./tokenize/heading-setext */ "./node_modules/remark-parse/lib/tokenize/heading-setext.js"),
  html: __webpack_require__(/*! ./tokenize/html-block */ "./node_modules/remark-parse/lib/tokenize/html-block.js"),
  footnote: __webpack_require__(/*! ./tokenize/footnote-definition */ "./node_modules/remark-parse/lib/tokenize/footnote-definition.js"),
  definition: __webpack_require__(/*! ./tokenize/definition */ "./node_modules/remark-parse/lib/tokenize/definition.js"),
  table: __webpack_require__(/*! ./tokenize/table */ "./node_modules/remark-parse/lib/tokenize/table.js"),
  paragraph: __webpack_require__(/*! ./tokenize/paragraph */ "./node_modules/remark-parse/lib/tokenize/paragraph.js")
};

proto.inlineTokenizers = {
  escape: __webpack_require__(/*! ./tokenize/escape */ "./node_modules/remark-parse/lib/tokenize/escape.js"),
  autoLink: __webpack_require__(/*! ./tokenize/auto-link */ "./node_modules/remark-parse/lib/tokenize/auto-link.js"),
  url: __webpack_require__(/*! ./tokenize/url */ "./node_modules/remark-parse/lib/tokenize/url.js"),
  html: __webpack_require__(/*! ./tokenize/html-inline */ "./node_modules/remark-parse/lib/tokenize/html-inline.js"),
  link: __webpack_require__(/*! ./tokenize/link */ "./node_modules/remark-parse/lib/tokenize/link.js"),
  reference: __webpack_require__(/*! ./tokenize/reference */ "./node_modules/remark-parse/lib/tokenize/reference.js"),
  strong: __webpack_require__(/*! ./tokenize/strong */ "./node_modules/remark-parse/lib/tokenize/strong.js"),
  emphasis: __webpack_require__(/*! ./tokenize/emphasis */ "./node_modules/remark-parse/lib/tokenize/emphasis.js"),
  deletion: __webpack_require__(/*! ./tokenize/delete */ "./node_modules/remark-parse/lib/tokenize/delete.js"),
  code: __webpack_require__(/*! ./tokenize/code-inline */ "./node_modules/remark-parse/lib/tokenize/code-inline.js"),
  break: __webpack_require__(/*! ./tokenize/break */ "./node_modules/remark-parse/lib/tokenize/break.js"),
  text: __webpack_require__(/*! ./tokenize/text */ "./node_modules/remark-parse/lib/tokenize/text.js")
};

/* Expose precedence. */
proto.blockMethods = keys(proto.blockTokenizers);
proto.inlineMethods = keys(proto.inlineTokenizers);

/* Tokenizers. */
proto.tokenizeBlock = tokenizer('block');
proto.tokenizeInline = tokenizer('inline');
proto.tokenizeFactory = tokenizer;

/* Get all keys in `value`. */
function keys(value) {
  var result = [];
  var key;

  for (key in value) {
    result.push(key);
  }

  return result;
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/set-options.js":
/*!******************************************************!*\
  !*** ./node_modules/remark-parse/lib/set-options.js ***!
  \******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var xtend = __webpack_require__(/*! xtend */ "./node_modules/xtend/immutable.js");
var escapes = __webpack_require__(/*! markdown-escapes */ "./node_modules/markdown-escapes/index.js");
var defaults = __webpack_require__(/*! ./defaults */ "./node_modules/remark-parse/lib/defaults.js");

module.exports = setOptions;

function setOptions(options) {
  var self = this;
  var current = self.options;
  var key;
  var value;

  if (options == null) {
    options = {};
  } else if (typeof options === 'object') {
    options = xtend(options);
  } else {
    throw new Error(
      'Invalid value `' + options + '` ' +
      'for setting `options`'
    );
  }

  for (key in defaults) {
    value = options[key];

    if (value == null) {
      value = current[key];
    }

    if (
      (key !== 'blocks' && typeof value !== 'boolean') ||
      (key === 'blocks' && typeof value !== 'object')
    ) {
      throw new Error('Invalid value `' + value + '` for setting `options.' + key + '`');
    }

    options[key] = value;
  }

  self.options = options;
  self.escape = escapes(options);

  return self;
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/auto-link.js":
/*!*************************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/auto-link.js ***!
  \*************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var whitespace = __webpack_require__(/*! is-whitespace-character */ "./node_modules/is-whitespace-character/index.js");
var decode = __webpack_require__(/*! parse-entities */ "./node_modules/parse-entities/index.js");
var locate = __webpack_require__(/*! ../locate/tag */ "./node_modules/remark-parse/lib/locate/tag.js");

module.exports = autoLink;
autoLink.locator = locate;
autoLink.notInLink = true;

var C_LT = '<';
var C_GT = '>';
var C_AT_SIGN = '@';
var C_SLASH = '/';
var MAILTO = 'mailto:';
var MAILTO_LENGTH = MAILTO.length;

/* Tokenise a link. */
function autoLink(eat, value, silent) {
  var self;
  var subvalue;
  var length;
  var index;
  var queue;
  var character;
  var hasAtCharacter;
  var link;
  var now;
  var content;
  var tokenizers;
  var exit;

  if (value.charAt(0) !== C_LT) {
    return;
  }

  self = this;
  subvalue = '';
  length = value.length;
  index = 0;
  queue = '';
  hasAtCharacter = false;
  link = '';

  index++;
  subvalue = C_LT;

  while (index < length) {
    character = value.charAt(index);

    if (
      whitespace(character) ||
      character === C_GT ||
      character === C_AT_SIGN ||
      (character === ':' && value.charAt(index + 1) === C_SLASH)
    ) {
      break;
    }

    queue += character;
    index++;
  }

  if (!queue) {
    return;
  }

  link += queue;
  queue = '';

  character = value.charAt(index);
  link += character;
  index++;

  if (character === C_AT_SIGN) {
    hasAtCharacter = true;
  } else {
    if (
      character !== ':' ||
      value.charAt(index + 1) !== C_SLASH
    ) {
      return;
    }

    link += C_SLASH;
    index++;
  }

  while (index < length) {
    character = value.charAt(index);

    if (whitespace(character) || character === C_GT) {
      break;
    }

    queue += character;
    index++;
  }

  character = value.charAt(index);

  if (!queue || character !== C_GT) {
    return;
  }

  /* istanbul ignore if - never used (yet) */
  if (silent) {
    return true;
  }

  link += queue;
  content = link;
  subvalue += link + character;
  now = eat.now();
  now.column++;
  now.offset++;

  if (hasAtCharacter) {
    if (link.slice(0, MAILTO_LENGTH).toLowerCase() === MAILTO) {
      content = content.substr(MAILTO_LENGTH);
      now.column += MAILTO_LENGTH;
      now.offset += MAILTO_LENGTH;
    } else {
      link = MAILTO + link;
    }
  }

  /* Temporarily remove all tokenizers except text in autolinks. */
  tokenizers = self.inlineTokenizers;
  self.inlineTokenizers = {text: tokenizers.text};

  exit = self.enterLink();

  content = self.tokenizeInline(content, now);

  self.inlineTokenizers = tokenizers;
  exit();

  return eat(subvalue)({
    type: 'link',
    title: null,
    url: decode(link, {nonTerminated: false}),
    children: content
  });
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/blockquote.js":
/*!**************************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/blockquote.js ***!
  \**************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var trim = __webpack_require__(/*! trim */ "./node_modules/trim/index.js");
var interrupt = __webpack_require__(/*! ../util/interrupt */ "./node_modules/remark-parse/lib/util/interrupt.js");

module.exports = blockquote;

var C_NEWLINE = '\n';
var C_TAB = '\t';
var C_SPACE = ' ';
var C_GT = '>';

/* Tokenise a blockquote. */
function blockquote(eat, value, silent) {
  var self = this;
  var offsets = self.offset;
  var tokenizers = self.blockTokenizers;
  var interruptors = self.interruptBlockquote;
  var now = eat.now();
  var currentLine = now.line;
  var length = value.length;
  var values = [];
  var contents = [];
  var indents = [];
  var add;
  var index = 0;
  var character;
  var rest;
  var nextIndex;
  var content;
  var line;
  var startIndex;
  var prefixed;
  var exit;

  while (index < length) {
    character = value.charAt(index);

    if (character !== C_SPACE && character !== C_TAB) {
      break;
    }

    index++;
  }

  if (value.charAt(index) !== C_GT) {
    return;
  }

  if (silent) {
    return true;
  }

  index = 0;

  while (index < length) {
    nextIndex = value.indexOf(C_NEWLINE, index);
    startIndex = index;
    prefixed = false;

    if (nextIndex === -1) {
      nextIndex = length;
    }

    while (index < length) {
      character = value.charAt(index);

      if (character !== C_SPACE && character !== C_TAB) {
        break;
      }

      index++;
    }

    if (value.charAt(index) === C_GT) {
      index++;
      prefixed = true;

      if (value.charAt(index) === C_SPACE) {
        index++;
      }
    } else {
      index = startIndex;
    }

    content = value.slice(index, nextIndex);

    if (!prefixed && !trim(content)) {
      index = startIndex;
      break;
    }

    if (!prefixed) {
      rest = value.slice(index);

      /* Check if the following code contains a possible
       * block. */
      if (interrupt(interruptors, tokenizers, self, [eat, rest, true])) {
        break;
      }
    }

    line = startIndex === index ? content : value.slice(startIndex, nextIndex);

    indents.push(index - startIndex);
    values.push(line);
    contents.push(content);

    index = nextIndex + 1;
  }

  index = -1;
  length = indents.length;
  add = eat(values.join(C_NEWLINE));

  while (++index < length) {
    offsets[currentLine] = (offsets[currentLine] || 0) + indents[index];
    currentLine++;
  }

  exit = self.enterBlock();
  contents = self.tokenizeBlock(contents.join(C_NEWLINE), now);
  exit();

  return add({
    type: 'blockquote',
    children: contents
  });
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/break.js":
/*!*********************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/break.js ***!
  \*********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var locate = __webpack_require__(/*! ../locate/break */ "./node_modules/remark-parse/lib/locate/break.js");

module.exports = hardBreak;
hardBreak.locator = locate;

var MIN_BREAK_LENGTH = 2;

function hardBreak(eat, value, silent) {
  var length = value.length;
  var index = -1;
  var queue = '';
  var character;

  while (++index < length) {
    character = value.charAt(index);

    if (character === '\n') {
      if (index < MIN_BREAK_LENGTH) {
        return;
      }

      /* istanbul ignore if - never used (yet) */
      if (silent) {
        return true;
      }

      queue += character;

      return eat(queue)({type: 'break'});
    }

    if (character !== ' ') {
      return;
    }

    queue += character;
  }
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/code-fenced.js":
/*!***************************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/code-fenced.js ***!
  \***************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var trim = __webpack_require__(/*! trim-trailing-lines */ "./node_modules/trim-trailing-lines/index.js");

module.exports = fencedCode;

var C_NEWLINE = '\n';
var C_TAB = '\t';
var C_SPACE = ' ';
var C_TILDE = '~';
var C_TICK = '`';

var MIN_FENCE_COUNT = 3;
var CODE_INDENT_COUNT = 4;

function fencedCode(eat, value, silent) {
  var self = this;
  var settings = self.options;
  var length = value.length + 1;
  var index = 0;
  var subvalue = '';
  var fenceCount;
  var marker;
  var character;
  var flag;
  var queue;
  var content;
  var exdentedContent;
  var closing;
  var exdentedClosing;
  var indent;
  var now;

  if (!settings.gfm) {
    return;
  }

  /* Eat initial spacing. */
  while (index < length) {
    character = value.charAt(index);

    if (character !== C_SPACE && character !== C_TAB) {
      break;
    }

    subvalue += character;
    index++;
  }

  indent = index;

  /* Eat the fence. */
  character = value.charAt(index);

  if (character !== C_TILDE && character !== C_TICK) {
    return;
  }

  index++;
  marker = character;
  fenceCount = 1;
  subvalue += character;

  while (index < length) {
    character = value.charAt(index);

    if (character !== marker) {
      break;
    }

    subvalue += character;
    fenceCount++;
    index++;
  }

  if (fenceCount < MIN_FENCE_COUNT) {
    return;
  }

  /* Eat spacing before flag. */
  while (index < length) {
    character = value.charAt(index);

    if (character !== C_SPACE && character !== C_TAB) {
      break;
    }

    subvalue += character;
    index++;
  }

  /* Eat flag. */
  flag = '';
  queue = '';

  while (index < length) {
    character = value.charAt(index);

    if (
      character === C_NEWLINE ||
      character === C_TILDE ||
      character === C_TICK
    ) {
      break;
    }

    if (character === C_SPACE || character === C_TAB) {
      queue += character;
    } else {
      flag += queue + character;
      queue = '';
    }

    index++;
  }

  character = value.charAt(index);

  if (character && character !== C_NEWLINE) {
    return;
  }

  if (silent) {
    return true;
  }

  now = eat.now();
  now.column += subvalue.length;
  now.offset += subvalue.length;

  subvalue += flag;
  flag = self.decode.raw(self.unescape(flag), now);

  if (queue) {
    subvalue += queue;
  }

  queue = '';
  closing = '';
  exdentedClosing = '';
  content = '';
  exdentedContent = '';

  /* Eat content. */
  while (index < length) {
    character = value.charAt(index);
    content += closing;
    exdentedContent += exdentedClosing;
    closing = '';
    exdentedClosing = '';

    if (character !== C_NEWLINE) {
      content += character;
      exdentedClosing += character;
      index++;
      continue;
    }

    /* Add the newline to `subvalue` if its the first
     * character.  Otherwise, add it to the `closing`
     * queue. */
    if (content) {
      closing += character;
      exdentedClosing += character;
    } else {
      subvalue += character;
    }

    queue = '';
    index++;

    while (index < length) {
      character = value.charAt(index);

      if (character !== C_SPACE) {
        break;
      }

      queue += character;
      index++;
    }

    closing += queue;
    exdentedClosing += queue.slice(indent);

    if (queue.length >= CODE_INDENT_COUNT) {
      continue;
    }

    queue = '';

    while (index < length) {
      character = value.charAt(index);

      if (character !== marker) {
        break;
      }

      queue += character;
      index++;
    }

    closing += queue;
    exdentedClosing += queue;

    if (queue.length < fenceCount) {
      continue;
    }

    queue = '';

    while (index < length) {
      character = value.charAt(index);

      if (character !== C_SPACE && character !== C_TAB) {
        break;
      }

      closing += character;
      exdentedClosing += character;
      index++;
    }

    if (!character || character === C_NEWLINE) {
      break;
    }
  }

  subvalue += content + closing;

  return eat(subvalue)({
    type: 'code',
    lang: flag || null,
    value: trim(exdentedContent)
  });
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/code-indented.js":
/*!*****************************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/code-indented.js ***!
  \*****************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var repeat = __webpack_require__(/*! repeat-string */ "./node_modules/repeat-string/index.js");
var trim = __webpack_require__(/*! trim-trailing-lines */ "./node_modules/trim-trailing-lines/index.js");

module.exports = indentedCode;

var C_NEWLINE = '\n';
var C_TAB = '\t';
var C_SPACE = ' ';

var CODE_INDENT_COUNT = 4;
var CODE_INDENT = repeat(C_SPACE, CODE_INDENT_COUNT);

/* Tokenise indented code. */
function indentedCode(eat, value, silent) {
  var index = -1;
  var length = value.length;
  var subvalue = '';
  var content = '';
  var subvalueQueue = '';
  var contentQueue = '';
  var character;
  var blankQueue;
  var indent;

  while (++index < length) {
    character = value.charAt(index);

    if (indent) {
      indent = false;

      subvalue += subvalueQueue;
      content += contentQueue;
      subvalueQueue = '';
      contentQueue = '';

      if (character === C_NEWLINE) {
        subvalueQueue = character;
        contentQueue = character;
      } else {
        subvalue += character;
        content += character;

        while (++index < length) {
          character = value.charAt(index);

          if (!character || character === C_NEWLINE) {
            contentQueue = character;
            subvalueQueue = character;
            break;
          }

          subvalue += character;
          content += character;
        }
      }
    } else if (
      character === C_SPACE &&
      value.charAt(index + 1) === character &&
      value.charAt(index + 2) === character &&
      value.charAt(index + 3) === character
    ) {
      subvalueQueue += CODE_INDENT;
      index += 3;
      indent = true;
    } else if (character === C_TAB) {
      subvalueQueue += character;
      indent = true;
    } else {
      blankQueue = '';

      while (character === C_TAB || character === C_SPACE) {
        blankQueue += character;
        character = value.charAt(++index);
      }

      if (character !== C_NEWLINE) {
        break;
      }

      subvalueQueue += blankQueue + character;
      contentQueue += character;
    }
  }

  if (content) {
    if (silent) {
      return true;
    }

    return eat(subvalue)({
      type: 'code',
      lang: null,
      value: trim(content)
    });
  }
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/code-inline.js":
/*!***************************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/code-inline.js ***!
  \***************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var whitespace = __webpack_require__(/*! is-whitespace-character */ "./node_modules/is-whitespace-character/index.js");
var locate = __webpack_require__(/*! ../locate/code-inline */ "./node_modules/remark-parse/lib/locate/code-inline.js");

module.exports = inlineCode;
inlineCode.locator = locate;

var C_TICK = '`';

/* Tokenise inline code. */
function inlineCode(eat, value, silent) {
  var length = value.length;
  var index = 0;
  var queue = '';
  var tickQueue = '';
  var contentQueue;
  var subqueue;
  var count;
  var openingCount;
  var subvalue;
  var character;
  var found;
  var next;

  while (index < length) {
    if (value.charAt(index) !== C_TICK) {
      break;
    }

    queue += C_TICK;
    index++;
  }

  if (!queue) {
    return;
  }

  subvalue = queue;
  openingCount = index;
  queue = '';
  next = value.charAt(index);
  count = 0;

  while (index < length) {
    character = next;
    next = value.charAt(index + 1);

    if (character === C_TICK) {
      count++;
      tickQueue += character;
    } else {
      count = 0;
      queue += character;
    }

    if (count && next !== C_TICK) {
      if (count === openingCount) {
        subvalue += queue + tickQueue;
        found = true;
        break;
      }

      queue += tickQueue;
      tickQueue = '';
    }

    index++;
  }

  if (!found) {
    if (openingCount % 2 !== 0) {
      return;
    }

    queue = '';
  }

  /* istanbul ignore if - never used (yet) */
  if (silent) {
    return true;
  }

  contentQueue = '';
  subqueue = '';
  length = queue.length;
  index = -1;

  while (++index < length) {
    character = queue.charAt(index);

    if (whitespace(character)) {
      subqueue += character;
      continue;
    }

    if (subqueue) {
      if (contentQueue) {
        contentQueue += subqueue;
      }

      subqueue = '';
    }

    contentQueue += character;
  }

  return eat(subvalue)({
    type: 'inlineCode',
    value: contentQueue
  });
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/definition.js":
/*!**************************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/definition.js ***!
  \**************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var whitespace = __webpack_require__(/*! is-whitespace-character */ "./node_modules/is-whitespace-character/index.js");
var normalize = __webpack_require__(/*! ../util/normalize */ "./node_modules/remark-parse/lib/util/normalize.js");

module.exports = definition;
definition.notInList = true;
definition.notInBlock = true;

var C_DOUBLE_QUOTE = '"';
var C_SINGLE_QUOTE = '\'';
var C_BACKSLASH = '\\';
var C_NEWLINE = '\n';
var C_TAB = '\t';
var C_SPACE = ' ';
var C_BRACKET_OPEN = '[';
var C_BRACKET_CLOSE = ']';
var C_PAREN_OPEN = '(';
var C_PAREN_CLOSE = ')';
var C_COLON = ':';
var C_LT = '<';
var C_GT = '>';

function definition(eat, value, silent) {
  var self = this;
  var commonmark = self.options.commonmark;
  var index = 0;
  var length = value.length;
  var subvalue = '';
  var beforeURL;
  var beforeTitle;
  var queue;
  var character;
  var test;
  var identifier;
  var url;
  var title;

  while (index < length) {
    character = value.charAt(index);

    if (character !== C_SPACE && character !== C_TAB) {
      break;
    }

    subvalue += character;
    index++;
  }

  character = value.charAt(index);

  if (character !== C_BRACKET_OPEN) {
    return;
  }

  index++;
  subvalue += character;
  queue = '';

  while (index < length) {
    character = value.charAt(index);

    if (character === C_BRACKET_CLOSE) {
      break;
    } else if (character === C_BACKSLASH) {
      queue += character;
      index++;
      character = value.charAt(index);
    }

    queue += character;
    index++;
  }

  if (
    !queue ||
    value.charAt(index) !== C_BRACKET_CLOSE ||
    value.charAt(index + 1) !== C_COLON
  ) {
    return;
  }

  identifier = queue;
  subvalue += queue + C_BRACKET_CLOSE + C_COLON;
  index = subvalue.length;
  queue = '';

  while (index < length) {
    character = value.charAt(index);

    if (
      character !== C_TAB &&
      character !== C_SPACE &&
      character !== C_NEWLINE
    ) {
      break;
    }

    subvalue += character;
    index++;
  }

  character = value.charAt(index);
  queue = '';
  beforeURL = subvalue;

  if (character === C_LT) {
    index++;

    while (index < length) {
      character = value.charAt(index);

      if (!isEnclosedURLCharacter(character)) {
        break;
      }

      queue += character;
      index++;
    }

    character = value.charAt(index);

    if (character === isEnclosedURLCharacter.delimiter) {
      subvalue += C_LT + queue + character;
      index++;
    } else {
      if (commonmark) {
        return;
      }

      index -= queue.length + 1;
      queue = '';
    }
  }

  if (!queue) {
    while (index < length) {
      character = value.charAt(index);

      if (!isUnclosedURLCharacter(character)) {
        break;
      }

      queue += character;
      index++;
    }

    subvalue += queue;
  }

  if (!queue) {
    return;
  }

  url = queue;
  queue = '';

  while (index < length) {
    character = value.charAt(index);

    if (
      character !== C_TAB &&
      character !== C_SPACE &&
      character !== C_NEWLINE
    ) {
      break;
    }

    queue += character;
    index++;
  }

  character = value.charAt(index);
  test = null;

  if (character === C_DOUBLE_QUOTE) {
    test = C_DOUBLE_QUOTE;
  } else if (character === C_SINGLE_QUOTE) {
    test = C_SINGLE_QUOTE;
  } else if (character === C_PAREN_OPEN) {
    test = C_PAREN_CLOSE;
  }

  if (!test) {
    queue = '';
    index = subvalue.length;
  } else if (queue) {
    subvalue += queue + character;
    index = subvalue.length;
    queue = '';

    while (index < length) {
      character = value.charAt(index);

      if (character === test) {
        break;
      }

      if (character === C_NEWLINE) {
        index++;
        character = value.charAt(index);

        if (character === C_NEWLINE || character === test) {
          return;
        }

        queue += C_NEWLINE;
      }

      queue += character;
      index++;
    }

    character = value.charAt(index);

    if (character !== test) {
      return;
    }

    beforeTitle = subvalue;
    subvalue += queue + character;
    index++;
    title = queue;
    queue = '';
  } else {
    return;
  }

  while (index < length) {
    character = value.charAt(index);

    if (character !== C_TAB && character !== C_SPACE) {
      break;
    }

    subvalue += character;
    index++;
  }

  character = value.charAt(index);

  if (!character || character === C_NEWLINE) {
    if (silent) {
      return true;
    }

    beforeURL = eat(beforeURL).test().end;
    url = self.decode.raw(self.unescape(url), beforeURL, {nonTerminated: false});

    if (title) {
      beforeTitle = eat(beforeTitle).test().end;
      title = self.decode.raw(self.unescape(title), beforeTitle);
    }

    return eat(subvalue)({
      type: 'definition',
      identifier: normalize(identifier),
      title: title || null,
      url: url
    });
  }
}

/* Check if `character` can be inside an enclosed URI. */
function isEnclosedURLCharacter(character) {
  return character !== C_GT &&
    character !== C_BRACKET_OPEN &&
    character !== C_BRACKET_CLOSE;
}

isEnclosedURLCharacter.delimiter = C_GT;

/* Check if `character` can be inside an unclosed URI. */
function isUnclosedURLCharacter(character) {
  return character !== C_BRACKET_OPEN &&
    character !== C_BRACKET_CLOSE &&
    !whitespace(character);
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/delete.js":
/*!**********************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/delete.js ***!
  \**********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var whitespace = __webpack_require__(/*! is-whitespace-character */ "./node_modules/is-whitespace-character/index.js");
var locate = __webpack_require__(/*! ../locate/delete */ "./node_modules/remark-parse/lib/locate/delete.js");

module.exports = strikethrough;
strikethrough.locator = locate;

var C_TILDE = '~';
var DOUBLE = '~~';

function strikethrough(eat, value, silent) {
  var self = this;
  var character = '';
  var previous = '';
  var preceding = '';
  var subvalue = '';
  var index;
  var length;
  var now;

  if (
    !self.options.gfm ||
    value.charAt(0) !== C_TILDE ||
    value.charAt(1) !== C_TILDE ||
    whitespace(value.charAt(2))
  ) {
    return;
  }

  index = 1;
  length = value.length;
  now = eat.now();
  now.column += 2;
  now.offset += 2;

  while (++index < length) {
    character = value.charAt(index);

    if (
      character === C_TILDE &&
      previous === C_TILDE &&
      (!preceding || !whitespace(preceding))
    ) {
      /* istanbul ignore if - never used (yet) */
      if (silent) {
        return true;
      }

      return eat(DOUBLE + subvalue + DOUBLE)({
        type: 'delete',
        children: self.tokenizeInline(subvalue, now)
      });
    }

    subvalue += previous;
    preceding = previous;
    previous = character;
  }
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/emphasis.js":
/*!************************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/emphasis.js ***!
  \************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var trim = __webpack_require__(/*! trim */ "./node_modules/trim/index.js");
var word = __webpack_require__(/*! is-word-character */ "./node_modules/is-word-character/index.js");
var whitespace = __webpack_require__(/*! is-whitespace-character */ "./node_modules/is-whitespace-character/index.js");
var locate = __webpack_require__(/*! ../locate/emphasis */ "./node_modules/remark-parse/lib/locate/emphasis.js");

module.exports = emphasis;
emphasis.locator = locate;

var C_ASTERISK = '*';
var C_UNDERSCORE = '_';

function emphasis(eat, value, silent) {
  var self = this;
  var index = 0;
  var character = value.charAt(index);
  var now;
  var pedantic;
  var marker;
  var queue;
  var subvalue;
  var length;
  var prev;

  if (character !== C_ASTERISK && character !== C_UNDERSCORE) {
    return;
  }

  pedantic = self.options.pedantic;
  subvalue = character;
  marker = character;
  length = value.length;
  index++;
  queue = '';
  character = '';

  if (pedantic && whitespace(value.charAt(index))) {
    return;
  }

  while (index < length) {
    prev = character;
    character = value.charAt(index);

    if (character === marker && (!pedantic || !whitespace(prev))) {
      character = value.charAt(++index);

      if (character !== marker) {
        if (!trim(queue) || prev === marker) {
          return;
        }

        if (!pedantic && marker === C_UNDERSCORE && word(character)) {
          queue += marker;
          continue;
        }

        /* istanbul ignore if - never used (yet) */
        if (silent) {
          return true;
        }

        now = eat.now();
        now.column++;
        now.offset++;

        return eat(subvalue + queue + marker)({
          type: 'emphasis',
          children: self.tokenizeInline(queue, now)
        });
      }

      queue += marker;
    }

    if (!pedantic && character === '\\') {
      queue += character;
      character = value.charAt(++index);
    }

    queue += character;
    index++;
  }
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/escape.js":
/*!**********************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/escape.js ***!
  \**********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var locate = __webpack_require__(/*! ../locate/escape */ "./node_modules/remark-parse/lib/locate/escape.js");

module.exports = escape;
escape.locator = locate;

function escape(eat, value, silent) {
  var self = this;
  var character;
  var node;

  if (value.charAt(0) === '\\') {
    character = value.charAt(1);

    if (self.escape.indexOf(character) !== -1) {
      /* istanbul ignore if - never used (yet) */
      if (silent) {
        return true;
      }

      if (character === '\n') {
        node = {type: 'break'};
      } else {
        node = {
          type: 'text',
          value: character
        };
      }

      return eat('\\' + character)(node);
    }
  }
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/footnote-definition.js":
/*!***********************************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/footnote-definition.js ***!
  \***********************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var whitespace = __webpack_require__(/*! is-whitespace-character */ "./node_modules/is-whitespace-character/index.js");
var normalize = __webpack_require__(/*! ../util/normalize */ "./node_modules/remark-parse/lib/util/normalize.js");

module.exports = footnoteDefinition;
footnoteDefinition.notInList = true;
footnoteDefinition.notInBlock = true;

var C_BACKSLASH = '\\';
var C_NEWLINE = '\n';
var C_TAB = '\t';
var C_SPACE = ' ';
var C_BRACKET_OPEN = '[';
var C_BRACKET_CLOSE = ']';
var C_CARET = '^';
var C_COLON = ':';

var EXPRESSION_INITIAL_TAB = /^( {4}|\t)?/gm;

function footnoteDefinition(eat, value, silent) {
  var self = this;
  var offsets = self.offset;
  var index;
  var length;
  var subvalue;
  var now;
  var currentLine;
  var content;
  var queue;
  var subqueue;
  var character;
  var identifier;
  var add;
  var exit;

  if (!self.options.footnotes) {
    return;
  }

  index = 0;
  length = value.length;
  subvalue = '';
  now = eat.now();
  currentLine = now.line;

  while (index < length) {
    character = value.charAt(index);

    if (!whitespace(character)) {
      break;
    }

    subvalue += character;
    index++;
  }

  if (
    value.charAt(index) !== C_BRACKET_OPEN ||
    value.charAt(index + 1) !== C_CARET
  ) {
    return;
  }

  subvalue += C_BRACKET_OPEN + C_CARET;
  index = subvalue.length;
  queue = '';

  while (index < length) {
    character = value.charAt(index);

    if (character === C_BRACKET_CLOSE) {
      break;
    } else if (character === C_BACKSLASH) {
      queue += character;
      index++;
      character = value.charAt(index);
    }

    queue += character;
    index++;
  }

  if (
    !queue ||
    value.charAt(index) !== C_BRACKET_CLOSE ||
    value.charAt(index + 1) !== C_COLON
  ) {
    return;
  }

  if (silent) {
    return true;
  }

  identifier = normalize(queue);
  subvalue += queue + C_BRACKET_CLOSE + C_COLON;
  index = subvalue.length;

  while (index < length) {
    character = value.charAt(index);

    if (character !== C_TAB && character !== C_SPACE) {
      break;
    }

    subvalue += character;
    index++;
  }

  now.column += subvalue.length;
  now.offset += subvalue.length;
  queue = '';
  content = '';
  subqueue = '';

  while (index < length) {
    character = value.charAt(index);

    if (character === C_NEWLINE) {
      subqueue = character;
      index++;

      while (index < length) {
        character = value.charAt(index);

        if (character !== C_NEWLINE) {
          break;
        }

        subqueue += character;
        index++;
      }

      queue += subqueue;
      subqueue = '';

      while (index < length) {
        character = value.charAt(index);

        if (character !== C_SPACE) {
          break;
        }

        subqueue += character;
        index++;
      }

      if (subqueue.length === 0) {
        break;
      }

      queue += subqueue;
    }

    if (queue) {
      content += queue;
      queue = '';
    }

    content += character;
    index++;
  }

  subvalue += content;

  content = content.replace(EXPRESSION_INITIAL_TAB, function (line) {
    offsets[currentLine] = (offsets[currentLine] || 0) + line.length;
    currentLine++;

    return '';
  });

  add = eat(subvalue);

  exit = self.enterBlock();
  content = self.tokenizeBlock(content, now);
  exit();

  return add({
    type: 'footnoteDefinition',
    identifier: identifier,
    children: content
  });
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/heading-atx.js":
/*!***************************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/heading-atx.js ***!
  \***************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = atxHeading;

var C_NEWLINE = '\n';
var C_TAB = '\t';
var C_SPACE = ' ';
var C_HASH = '#';

var MAX_ATX_COUNT = 6;

function atxHeading(eat, value, silent) {
  var self = this;
  var settings = self.options;
  var length = value.length + 1;
  var index = -1;
  var now = eat.now();
  var subvalue = '';
  var content = '';
  var character;
  var queue;
  var depth;

  /* Eat initial spacing. */
  while (++index < length) {
    character = value.charAt(index);

    if (character !== C_SPACE && character !== C_TAB) {
      index--;
      break;
    }

    subvalue += character;
  }

  /* Eat hashes. */
  depth = 0;

  while (++index <= length) {
    character = value.charAt(index);

    if (character !== C_HASH) {
      index--;
      break;
    }

    subvalue += character;
    depth++;
  }

  if (depth > MAX_ATX_COUNT) {
    return;
  }

  if (
    !depth ||
    (!settings.pedantic && value.charAt(index + 1) === C_HASH)
  ) {
    return;
  }

  length = value.length + 1;

  /* Eat intermediate white-space. */
  queue = '';

  while (++index < length) {
    character = value.charAt(index);

    if (character !== C_SPACE && character !== C_TAB) {
      index--;
      break;
    }

    queue += character;
  }

  /* Exit when not in pedantic mode without spacing. */
  if (
    !settings.pedantic &&
    queue.length === 0 &&
    character &&
    character !== C_NEWLINE
  ) {
    return;
  }

  if (silent) {
    return true;
  }

  /* Eat content. */
  subvalue += queue;
  queue = '';
  content = '';

  while (++index < length) {
    character = value.charAt(index);

    if (!character || character === C_NEWLINE) {
      break;
    }

    if (
      character !== C_SPACE &&
      character !== C_TAB &&
      character !== C_HASH
    ) {
      content += queue + character;
      queue = '';
      continue;
    }

    while (character === C_SPACE || character === C_TAB) {
      queue += character;
      character = value.charAt(++index);
    }

    while (character === C_HASH) {
      queue += character;
      character = value.charAt(++index);
    }

    while (character === C_SPACE || character === C_TAB) {
      queue += character;
      character = value.charAt(++index);
    }

    index--;
  }

  now.column += subvalue.length;
  now.offset += subvalue.length;
  subvalue += content + queue;

  return eat(subvalue)({
    type: 'heading',
    depth: depth,
    children: self.tokenizeInline(content, now)
  });
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/heading-setext.js":
/*!******************************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/heading-setext.js ***!
  \******************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = setextHeading;

var C_NEWLINE = '\n';
var C_TAB = '\t';
var C_SPACE = ' ';
var C_EQUALS = '=';
var C_DASH = '-';

var MAX_HEADING_INDENT = 3;

/* Map of characters which can be used to mark setext
 * headers, mapping to their corresponding depth. */
var SETEXT_MARKERS = {};

SETEXT_MARKERS[C_EQUALS] = 1;
SETEXT_MARKERS[C_DASH] = 2;

function setextHeading(eat, value, silent) {
  var self = this;
  var now = eat.now();
  var length = value.length;
  var index = -1;
  var subvalue = '';
  var content;
  var queue;
  var character;
  var marker;
  var depth;

  /* Eat initial indentation. */
  while (++index < length) {
    character = value.charAt(index);

    if (character !== C_SPACE || index >= MAX_HEADING_INDENT) {
      index--;
      break;
    }

    subvalue += character;
  }

  /* Eat content. */
  content = '';
  queue = '';

  while (++index < length) {
    character = value.charAt(index);

    if (character === C_NEWLINE) {
      index--;
      break;
    }

    if (character === C_SPACE || character === C_TAB) {
      queue += character;
    } else {
      content += queue + character;
      queue = '';
    }
  }

  now.column += subvalue.length;
  now.offset += subvalue.length;
  subvalue += content + queue;

  /* Ensure the content is followed by a newline and a
   * valid marker. */
  character = value.charAt(++index);
  marker = value.charAt(++index);

  if (character !== C_NEWLINE || !SETEXT_MARKERS[marker]) {
    return;
  }

  subvalue += character;

  /* Eat Setext-line. */
  queue = marker;
  depth = SETEXT_MARKERS[marker];

  while (++index < length) {
    character = value.charAt(index);

    if (character !== marker) {
      if (character !== C_NEWLINE) {
        return;
      }

      index--;
      break;
    }

    queue += character;
  }

  if (silent) {
    return true;
  }

  return eat(subvalue + queue)({
    type: 'heading',
    depth: depth,
    children: self.tokenizeInline(content, now)
  });
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/html-block.js":
/*!**************************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/html-block.js ***!
  \**************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var openCloseTag = __webpack_require__(/*! ../util/html */ "./node_modules/remark-parse/lib/util/html.js").openCloseTag;

module.exports = blockHTML;

var C_TAB = '\t';
var C_SPACE = ' ';
var C_NEWLINE = '\n';
var C_LT = '<';

function blockHTML(eat, value, silent) {
  var self = this;
  var blocks = self.options.blocks;
  var length = value.length;
  var index = 0;
  var next;
  var line;
  var offset;
  var character;
  var count;
  var sequence;
  var subvalue;

  var sequences = [
    [/^<(script|pre|style)(?=(\s|>|$))/i, /<\/(script|pre|style)>/i, true],
    [/^<!--/, /-->/, true],
    [/^<\?/, /\?>/, true],
    [/^<![A-Za-z]/, />/, true],
    [/^<!\[CDATA\[/, /\]\]>/, true],
    [new RegExp('^</?(' + blocks.join('|') + ')(?=(\\s|/?>|$))', 'i'), /^$/, true],
    [new RegExp(openCloseTag.source + '\\s*$'), /^$/, false]
  ];

  /* Eat initial spacing. */
  while (index < length) {
    character = value.charAt(index);

    if (character !== C_TAB && character !== C_SPACE) {
      break;
    }

    index++;
  }

  if (value.charAt(index) !== C_LT) {
    return;
  }

  next = value.indexOf(C_NEWLINE, index + 1);
  next = next === -1 ? length : next;
  line = value.slice(index, next);
  offset = -1;
  count = sequences.length;

  while (++offset < count) {
    if (sequences[offset][0].test(line)) {
      sequence = sequences[offset];
      break;
    }
  }

  if (!sequence) {
    return;
  }

  if (silent) {
    return sequence[2];
  }

  index = next;

  if (!sequence[1].test(line)) {
    while (index < length) {
      next = value.indexOf(C_NEWLINE, index + 1);
      next = next === -1 ? length : next;
      line = value.slice(index + 1, next);

      if (sequence[1].test(line)) {
        if (line) {
          index = next;
        }

        break;
      }

      index = next;
    }
  }

  subvalue = value.slice(0, index);

  return eat(subvalue)({type: 'html', value: subvalue});
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/html-inline.js":
/*!***************************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/html-inline.js ***!
  \***************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var alphabetical = __webpack_require__(/*! is-alphabetical */ "./node_modules/is-alphabetical/index.js");
var locate = __webpack_require__(/*! ../locate/tag */ "./node_modules/remark-parse/lib/locate/tag.js");
var tag = __webpack_require__(/*! ../util/html */ "./node_modules/remark-parse/lib/util/html.js").tag;

module.exports = inlineHTML;
inlineHTML.locator = locate;

var EXPRESSION_HTML_LINK_OPEN = /^<a /i;
var EXPRESSION_HTML_LINK_CLOSE = /^<\/a>/i;

function inlineHTML(eat, value, silent) {
  var self = this;
  var length = value.length;
  var character;
  var subvalue;

  if (value.charAt(0) !== '<' || length < 3) {
    return;
  }

  character = value.charAt(1);

  if (
    !alphabetical(character) &&
    character !== '?' &&
    character !== '!' &&
    character !== '/'
  ) {
    return;
  }

  subvalue = value.match(tag);

  if (!subvalue) {
    return;
  }

  /* istanbul ignore if - not used yet. */
  if (silent) {
    return true;
  }

  subvalue = subvalue[0];

  if (!self.inLink && EXPRESSION_HTML_LINK_OPEN.test(subvalue)) {
    self.inLink = true;
  } else if (self.inLink && EXPRESSION_HTML_LINK_CLOSE.test(subvalue)) {
    self.inLink = false;
  }

  return eat(subvalue)({type: 'html', value: subvalue});
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/link.js":
/*!********************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/link.js ***!
  \********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var whitespace = __webpack_require__(/*! is-whitespace-character */ "./node_modules/is-whitespace-character/index.js");
var locate = __webpack_require__(/*! ../locate/link */ "./node_modules/remark-parse/lib/locate/link.js");

module.exports = link;
link.locator = locate;

var own = {}.hasOwnProperty;

var C_BACKSLASH = '\\';
var C_BRACKET_OPEN = '[';
var C_BRACKET_CLOSE = ']';
var C_PAREN_OPEN = '(';
var C_PAREN_CLOSE = ')';
var C_LT = '<';
var C_GT = '>';
var C_TICK = '`';
var C_DOUBLE_QUOTE = '"';
var C_SINGLE_QUOTE = '\'';

/* Map of characters, which can be used to mark link
 * and image titles. */
var LINK_MARKERS = {};

LINK_MARKERS[C_DOUBLE_QUOTE] = C_DOUBLE_QUOTE;
LINK_MARKERS[C_SINGLE_QUOTE] = C_SINGLE_QUOTE;

/* Map of characters, which can be used to mark link
 * and image titles in commonmark-mode. */
var COMMONMARK_LINK_MARKERS = {};

COMMONMARK_LINK_MARKERS[C_DOUBLE_QUOTE] = C_DOUBLE_QUOTE;
COMMONMARK_LINK_MARKERS[C_SINGLE_QUOTE] = C_SINGLE_QUOTE;
COMMONMARK_LINK_MARKERS[C_PAREN_OPEN] = C_PAREN_CLOSE;

function link(eat, value, silent) {
  var self = this;
  var subvalue = '';
  var index = 0;
  var character = value.charAt(0);
  var pedantic = self.options.pedantic;
  var commonmark = self.options.commonmark;
  var gfm = self.options.gfm;
  var closed;
  var count;
  var opening;
  var beforeURL;
  var beforeTitle;
  var subqueue;
  var hasMarker;
  var markers;
  var isImage;
  var content;
  var marker;
  var length;
  var title;
  var depth;
  var queue;
  var url;
  var now;
  var exit;
  var node;

  /* Detect whether this is an image. */
  if (character === '!') {
    isImage = true;
    subvalue = character;
    character = value.charAt(++index);
  }

  /* Eat the opening. */
  if (character !== C_BRACKET_OPEN) {
    return;
  }

  /* Exit when this is a link and were already inside
   * a link. */
  if (!isImage && self.inLink) {
    return;
  }

  subvalue += character;
  queue = '';
  index++;

  /* Eat the content. */
  length = value.length;
  now = eat.now();
  depth = 0;

  now.column += index;
  now.offset += index;

  while (index < length) {
    character = value.charAt(index);
    subqueue = character;

    if (character === C_TICK) {
      /* Inline-code in link content. */
      count = 1;

      while (value.charAt(index + 1) === C_TICK) {
        subqueue += character;
        index++;
        count++;
      }

      if (!opening) {
        opening = count;
      } else if (count >= opening) {
        opening = 0;
      }
    } else if (character === C_BACKSLASH) {
      /* Allow brackets to be escaped. */
      index++;
      subqueue += value.charAt(index);
    /* In GFM mode, brackets in code still count.
     * In all other modes, they dont.  This empty
     * block prevents the next statements are
     * entered. */
    } else if ((!opening || gfm) && character === C_BRACKET_OPEN) {
      depth++;
    } else if ((!opening || gfm) && character === C_BRACKET_CLOSE) {
      if (depth) {
        depth--;
      } else {
        /* Allow white-space between content and
         * url in GFM mode. */
        if (!pedantic) {
          while (index < length) {
            character = value.charAt(index + 1);

            if (!whitespace(character)) {
              break;
            }

            subqueue += character;
            index++;
          }
        }

        if (value.charAt(index + 1) !== C_PAREN_OPEN) {
          return;
        }

        subqueue += C_PAREN_OPEN;
        closed = true;
        index++;

        break;
      }
    }

    queue += subqueue;
    subqueue = '';
    index++;
  }

  /* Eat the content closing. */
  if (!closed) {
    return;
  }

  content = queue;
  subvalue += queue + subqueue;
  index++;

  /* Eat white-space. */
  while (index < length) {
    character = value.charAt(index);

    if (!whitespace(character)) {
      break;
    }

    subvalue += character;
    index++;
  }

  /* Eat the URL. */
  character = value.charAt(index);
  markers = commonmark ? COMMONMARK_LINK_MARKERS : LINK_MARKERS;
  queue = '';
  beforeURL = subvalue;

  if (character === C_LT) {
    index++;
    beforeURL += C_LT;

    while (index < length) {
      character = value.charAt(index);

      if (character === C_GT) {
        break;
      }

      if (commonmark && character === '\n') {
        return;
      }

      queue += character;
      index++;
    }

    if (value.charAt(index) !== C_GT) {
      return;
    }

    subvalue += C_LT + queue + C_GT;
    url = queue;
    index++;
  } else {
    character = null;
    subqueue = '';

    while (index < length) {
      character = value.charAt(index);

      if (subqueue && own.call(markers, character)) {
        break;
      }

      if (whitespace(character)) {
        if (!pedantic) {
          break;
        }

        subqueue += character;
      } else {
        if (character === C_PAREN_OPEN) {
          depth++;
        } else if (character === C_PAREN_CLOSE) {
          if (depth === 0) {
            break;
          }

          depth--;
        }

        queue += subqueue;
        subqueue = '';

        if (character === C_BACKSLASH) {
          queue += C_BACKSLASH;
          character = value.charAt(++index);
        }

        queue += character;
      }

      index++;
    }

    subvalue += queue;
    url = queue;
    index = subvalue.length;
  }

  /* Eat white-space. */
  queue = '';

  while (index < length) {
    character = value.charAt(index);

    if (!whitespace(character)) {
      break;
    }

    queue += character;
    index++;
  }

  character = value.charAt(index);
  subvalue += queue;

  /* Eat the title. */
  if (queue && own.call(markers, character)) {
    index++;
    subvalue += character;
    queue = '';
    marker = markers[character];
    beforeTitle = subvalue;

    /* In commonmark-mode, things are pretty easy: the
     * marker cannot occur inside the title.
     *
     * Non-commonmark does, however, support nested
     * delimiters. */
    if (commonmark) {
      while (index < length) {
        character = value.charAt(index);

        if (character === marker) {
          break;
        }

        if (character === C_BACKSLASH) {
          queue += C_BACKSLASH;
          character = value.charAt(++index);
        }

        index++;
        queue += character;
      }

      character = value.charAt(index);

      if (character !== marker) {
        return;
      }

      title = queue;
      subvalue += queue + character;
      index++;

      while (index < length) {
        character = value.charAt(index);

        if (!whitespace(character)) {
          break;
        }

        subvalue += character;
        index++;
      }
    } else {
      subqueue = '';

      while (index < length) {
        character = value.charAt(index);

        if (character === marker) {
          if (hasMarker) {
            queue += marker + subqueue;
            subqueue = '';
          }

          hasMarker = true;
        } else if (!hasMarker) {
          queue += character;
        } else if (character === C_PAREN_CLOSE) {
          subvalue += queue + marker + subqueue;
          title = queue;
          break;
        } else if (whitespace(character)) {
          subqueue += character;
        } else {
          queue += marker + subqueue + character;
          subqueue = '';
          hasMarker = false;
        }

        index++;
      }
    }
  }

  if (value.charAt(index) !== C_PAREN_CLOSE) {
    return;
  }

  /* istanbul ignore if - never used (yet) */
  if (silent) {
    return true;
  }

  subvalue += C_PAREN_CLOSE;

  url = self.decode.raw(self.unescape(url), eat(beforeURL).test().end, {nonTerminated: false});

  if (title) {
    beforeTitle = eat(beforeTitle).test().end;
    title = self.decode.raw(self.unescape(title), beforeTitle);
  }

  node = {
    type: isImage ? 'image' : 'link',
    title: title || null,
    url: url
  };

  if (isImage) {
    node.alt = self.decode.raw(self.unescape(content), now) || null;
  } else {
    exit = self.enterLink();
    node.children = self.tokenizeInline(content, now);
    exit();
  }

  return eat(subvalue)(node);
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/list.js":
/*!********************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/list.js ***!
  \********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


/* eslint-disable max-params */

var trim = __webpack_require__(/*! trim */ "./node_modules/trim/index.js");
var repeat = __webpack_require__(/*! repeat-string */ "./node_modules/repeat-string/index.js");
var decimal = __webpack_require__(/*! is-decimal */ "./node_modules/is-decimal/index.js");
var getIndent = __webpack_require__(/*! ../util/get-indentation */ "./node_modules/remark-parse/lib/util/get-indentation.js");
var removeIndent = __webpack_require__(/*! ../util/remove-indentation */ "./node_modules/remark-parse/lib/util/remove-indentation.js");
var interrupt = __webpack_require__(/*! ../util/interrupt */ "./node_modules/remark-parse/lib/util/interrupt.js");

module.exports = list;

var C_ASTERISK = '*';
var C_UNDERSCORE = '_';
var C_PLUS = '+';
var C_DASH = '-';
var C_DOT = '.';
var C_SPACE = ' ';
var C_NEWLINE = '\n';
var C_TAB = '\t';
var C_PAREN_CLOSE = ')';
var C_X_LOWER = 'x';

var TAB_SIZE = 4;
var EXPRESSION_LOOSE_LIST_ITEM = /\n\n(?!\s*$)/;
var EXPRESSION_TASK_ITEM = /^\[([ \t]|x|X)][ \t]/;
var EXPRESSION_BULLET = /^([ \t]*)([*+-]|\d+[.)])( {1,4}(?! )| |\t|$|(?=\n))([^\n]*)/;
var EXPRESSION_PEDANTIC_BULLET = /^([ \t]*)([*+-]|\d+[.)])([ \t]+)/;
var EXPRESSION_INITIAL_INDENT = /^( {1,4}|\t)?/gm;

/* Map of characters which can be used to mark
 * list-items. */
var LIST_UNORDERED_MARKERS = {};

LIST_UNORDERED_MARKERS[C_ASTERISK] = true;
LIST_UNORDERED_MARKERS[C_PLUS] = true;
LIST_UNORDERED_MARKERS[C_DASH] = true;

/* Map of characters which can be used to mark
 * list-items after a digit. */
var LIST_ORDERED_MARKERS = {};

LIST_ORDERED_MARKERS[C_DOT] = true;

/* Map of characters which can be used to mark
 * list-items after a digit. */
var LIST_ORDERED_COMMONMARK_MARKERS = {};

LIST_ORDERED_COMMONMARK_MARKERS[C_DOT] = true;
LIST_ORDERED_COMMONMARK_MARKERS[C_PAREN_CLOSE] = true;

function list(eat, value, silent) {
  var self = this;
  var commonmark = self.options.commonmark;
  var pedantic = self.options.pedantic;
  var tokenizers = self.blockTokenizers;
  var interuptors = self.interruptList;
  var markers;
  var index = 0;
  var length = value.length;
  var start = null;
  var size = 0;
  var queue;
  var ordered;
  var character;
  var marker;
  var nextIndex;
  var startIndex;
  var prefixed;
  var currentMarker;
  var content;
  var line;
  var prevEmpty;
  var empty;
  var items;
  var allLines;
  var emptyLines;
  var item;
  var enterTop;
  var exitBlockquote;
  var isLoose;
  var node;
  var now;
  var end;
  var indented;

  while (index < length) {
    character = value.charAt(index);

    if (character === C_TAB) {
      size += TAB_SIZE - (size % TAB_SIZE);
    } else if (character === C_SPACE) {
      size++;
    } else {
      break;
    }

    index++;
  }

  if (size >= TAB_SIZE) {
    return;
  }

  character = value.charAt(index);

  markers = commonmark ?
    LIST_ORDERED_COMMONMARK_MARKERS :
    LIST_ORDERED_MARKERS;

  if (LIST_UNORDERED_MARKERS[character] === true) {
    marker = character;
    ordered = false;
  } else {
    ordered = true;
    queue = '';

    while (index < length) {
      character = value.charAt(index);

      if (!decimal(character)) {
        break;
      }

      queue += character;
      index++;
    }

    character = value.charAt(index);

    if (!queue || markers[character] !== true) {
      return;
    }

    start = parseInt(queue, 10);
    marker = character;
  }

  character = value.charAt(++index);

  if (character !== C_SPACE && character !== C_TAB) {
    return;
  }

  if (silent) {
    return true;
  }

  index = 0;
  items = [];
  allLines = [];
  emptyLines = [];

  while (index < length) {
    nextIndex = value.indexOf(C_NEWLINE, index);
    startIndex = index;
    prefixed = false;
    indented = false;

    if (nextIndex === -1) {
      nextIndex = length;
    }

    end = index + TAB_SIZE;
    size = 0;

    while (index < length) {
      character = value.charAt(index);

      if (character === C_TAB) {
        size += TAB_SIZE - (size % TAB_SIZE);
      } else if (character === C_SPACE) {
        size++;
      } else {
        break;
      }

      index++;
    }

    if (size >= TAB_SIZE) {
      indented = true;
    }

    if (item && size >= item.indent) {
      indented = true;
    }

    character = value.charAt(index);
    currentMarker = null;

    if (!indented) {
      if (LIST_UNORDERED_MARKERS[character] === true) {
        currentMarker = character;
        index++;
        size++;
      } else {
        queue = '';

        while (index < length) {
          character = value.charAt(index);

          if (!decimal(character)) {
            break;
          }

          queue += character;
          index++;
        }

        character = value.charAt(index);
        index++;

        if (queue && markers[character] === true) {
          currentMarker = character;
          size += queue.length + 1;
        }
      }

      if (currentMarker) {
        character = value.charAt(index);

        if (character === C_TAB) {
          size += TAB_SIZE - (size % TAB_SIZE);
          index++;
        } else if (character === C_SPACE) {
          end = index + TAB_SIZE;

          while (index < end) {
            if (value.charAt(index) !== C_SPACE) {
              break;
            }

            index++;
            size++;
          }

          if (index === end && value.charAt(index) === C_SPACE) {
            index -= TAB_SIZE - 1;
            size -= TAB_SIZE - 1;
          }
        } else if (character !== C_NEWLINE && character !== '') {
          currentMarker = null;
        }
      }
    }

    if (currentMarker) {
      if (!pedantic && marker !== currentMarker) {
        break;
      }

      prefixed = true;
    } else {
      if (!commonmark && !indented && value.charAt(startIndex) === C_SPACE) {
        indented = true;
      } else if (commonmark && item) {
        indented = size >= item.indent || size > TAB_SIZE;
      }

      prefixed = false;
      index = startIndex;
    }

    line = value.slice(startIndex, nextIndex);
    content = startIndex === index ? line : value.slice(index, nextIndex);

    if (
      currentMarker === C_ASTERISK ||
      currentMarker === C_UNDERSCORE ||
      currentMarker === C_DASH
    ) {
      if (tokenizers.thematicBreak.call(self, eat, line, true)) {
        break;
      }
    }

    prevEmpty = empty;
    empty = !trim(content).length;

    if (indented && item) {
      item.value = item.value.concat(emptyLines, line);
      allLines = allLines.concat(emptyLines, line);
      emptyLines = [];
    } else if (prefixed) {
      if (emptyLines.length !== 0) {
        item.value.push('');
        item.trail = emptyLines.concat();
      }

      item = {
        value: [line],
        indent: size,
        trail: []
      };

      items.push(item);
      allLines = allLines.concat(emptyLines, line);
      emptyLines = [];
    } else if (empty) {
      if (prevEmpty) {
        break;
      }

      emptyLines.push(line);
    } else {
      if (prevEmpty) {
        break;
      }

      if (interrupt(interuptors, tokenizers, self, [eat, line, true])) {
        break;
      }

      item.value = item.value.concat(emptyLines, line);
      allLines = allLines.concat(emptyLines, line);
      emptyLines = [];
    }

    index = nextIndex + 1;
  }

  node = eat(allLines.join(C_NEWLINE)).reset({
    type: 'list',
    ordered: ordered,
    start: start,
    loose: null,
    children: []
  });

  enterTop = self.enterList();
  exitBlockquote = self.enterBlock();
  isLoose = false;
  index = -1;
  length = items.length;

  while (++index < length) {
    item = items[index].value.join(C_NEWLINE);
    now = eat.now();

    item = eat(item)(listItem(self, item, now), node);

    if (item.loose) {
      isLoose = true;
    }

    item = items[index].trail.join(C_NEWLINE);

    if (index !== length - 1) {
      item += C_NEWLINE;
    }

    eat(item);
  }

  enterTop();
  exitBlockquote();

  node.loose = isLoose;

  return node;
}

function listItem(ctx, value, position) {
  var offsets = ctx.offset;
  var fn = ctx.options.pedantic ? pedanticListItem : normalListItem;
  var checked = null;
  var task;
  var indent;

  value = fn.apply(null, arguments);

  if (ctx.options.gfm) {
    task = value.match(EXPRESSION_TASK_ITEM);

    if (task) {
      indent = task[0].length;
      checked = task[1].toLowerCase() === C_X_LOWER;
      offsets[position.line] += indent;
      value = value.slice(indent);
    }
  }

  return {
    type: 'listItem',
    loose: EXPRESSION_LOOSE_LIST_ITEM.test(value) ||
      value.charAt(value.length - 1) === C_NEWLINE,
    checked: checked,
    children: ctx.tokenizeBlock(value, position)
  };
}

/* Create a list-item using overly simple mechanics. */
function pedanticListItem(ctx, value, position) {
  var offsets = ctx.offset;
  var line = position.line;

  /* Remove the list-items bullet. */
  value = value.replace(EXPRESSION_PEDANTIC_BULLET, replacer);

  /* The initial line was also matched by the below, so
   * we reset the `line`. */
  line = position.line;

  return value.replace(EXPRESSION_INITIAL_INDENT, replacer);

  /* A simple replacer which removed all matches,
   * and adds their length to `offset`. */
  function replacer($0) {
    offsets[line] = (offsets[line] || 0) + $0.length;
    line++;

    return '';
  }
}

/* Create a list-item using sane mechanics. */
function normalListItem(ctx, value, position) {
  var offsets = ctx.offset;
  var line = position.line;
  var max;
  var bullet;
  var rest;
  var lines;
  var trimmedLines;
  var index;
  var length;

  /* Remove the list-items bullet. */
  value = value.replace(EXPRESSION_BULLET, replacer);

  lines = value.split(C_NEWLINE);

  trimmedLines = removeIndent(value, getIndent(max).indent).split(C_NEWLINE);

  /* We replaced the initial bullet with something
   * else above, which was used to trick
   * `removeIndentation` into removing some more
   * characters when possible.  However, that could
   * result in the initial line to be stripped more
   * than it should be. */
  trimmedLines[0] = rest;

  offsets[line] = (offsets[line] || 0) + bullet.length;
  line++;

  index = 0;
  length = lines.length;

  while (++index < length) {
    offsets[line] = (offsets[line] || 0) +
      lines[index].length - trimmedLines[index].length;
    line++;
  }

  return trimmedLines.join(C_NEWLINE);

  function replacer($0, $1, $2, $3, $4) {
    bullet = $1 + $2 + $3;
    rest = $4;

    /* Make sure that the first nine numbered list items
     * can indent with an extra space.  That is, when
     * the bullet did not receive an extra final space. */
    if (Number($2) < 10 && bullet.length % 2 === 1) {
      $2 = C_SPACE + $2;
    }

    max = $1 + repeat(C_SPACE, $2.length) + $3;

    return max + rest;
  }
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/newline.js":
/*!***********************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/newline.js ***!
  \***********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var whitespace = __webpack_require__(/*! is-whitespace-character */ "./node_modules/is-whitespace-character/index.js");

module.exports = newline;

/* Tokenise newline. */
function newline(eat, value, silent) {
  var character = value.charAt(0);
  var length;
  var subvalue;
  var queue;
  var index;

  if (character !== '\n') {
    return;
  }

  /* istanbul ignore if - never used (yet) */
  if (silent) {
    return true;
  }

  index = 1;
  length = value.length;
  subvalue = character;
  queue = '';

  while (index < length) {
    character = value.charAt(index);

    if (!whitespace(character)) {
      break;
    }

    queue += character;

    if (character === '\n') {
      subvalue += queue;
      queue = '';
    }

    index++;
  }

  eat(subvalue);
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/paragraph.js":
/*!*************************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/paragraph.js ***!
  \*************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var trim = __webpack_require__(/*! trim */ "./node_modules/trim/index.js");
var decimal = __webpack_require__(/*! is-decimal */ "./node_modules/is-decimal/index.js");
var trimTrailingLines = __webpack_require__(/*! trim-trailing-lines */ "./node_modules/trim-trailing-lines/index.js");
var interrupt = __webpack_require__(/*! ../util/interrupt */ "./node_modules/remark-parse/lib/util/interrupt.js");

module.exports = paragraph;

var C_NEWLINE = '\n';
var C_TAB = '\t';
var C_SPACE = ' ';

var TAB_SIZE = 4;

/* Tokenise paragraph. */
function paragraph(eat, value, silent) {
  var self = this;
  var settings = self.options;
  var commonmark = settings.commonmark;
  var gfm = settings.gfm;
  var tokenizers = self.blockTokenizers;
  var interruptors = self.interruptParagraph;
  var index = value.indexOf(C_NEWLINE);
  var length = value.length;
  var position;
  var subvalue;
  var character;
  var size;
  var now;

  while (index < length) {
    /* Eat everything if theres no following newline. */
    if (index === -1) {
      index = length;
      break;
    }

    /* Stop if the next character is NEWLINE. */
    if (value.charAt(index + 1) === C_NEWLINE) {
      break;
    }

    /* In commonmark-mode, following indented lines
     * are part of the paragraph. */
    if (commonmark) {
      size = 0;
      position = index + 1;

      while (position < length) {
        character = value.charAt(position);

        if (character === C_TAB) {
          size = TAB_SIZE;
          break;
        } else if (character === C_SPACE) {
          size++;
        } else {
          break;
        }

        position++;
      }

      if (size >= TAB_SIZE) {
        index = value.indexOf(C_NEWLINE, index + 1);
        continue;
      }
    }

    subvalue = value.slice(index + 1);

    /* Check if the following code contains a possible
     * block. */
    if (interrupt(interruptors, tokenizers, self, [eat, subvalue, true])) {
      break;
    }

    /* Break if the following line starts a list, when
     * already in a list, or when in commonmark, or when
     * in gfm mode and the bullet is *not* numeric. */
    if (
      tokenizers.list.call(self, eat, subvalue, true) &&
      (
        self.inList ||
        commonmark ||
        (gfm && !decimal(trim.left(subvalue).charAt(0)))
      )
    ) {
      break;
    }

    position = index;
    index = value.indexOf(C_NEWLINE, index + 1);

    if (index !== -1 && trim(value.slice(position, index)) === '') {
      index = position;
      break;
    }
  }

  subvalue = value.slice(0, index);

  if (trim(subvalue) === '') {
    eat(subvalue);

    return null;
  }

  /* istanbul ignore if - never used (yet) */
  if (silent) {
    return true;
  }

  now = eat.now();
  subvalue = trimTrailingLines(subvalue);

  return eat(subvalue)({
    type: 'paragraph',
    children: self.tokenizeInline(subvalue, now)
  });
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/reference.js":
/*!*************************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/reference.js ***!
  \*************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var whitespace = __webpack_require__(/*! is-whitespace-character */ "./node_modules/is-whitespace-character/index.js");
var locate = __webpack_require__(/*! ../locate/link */ "./node_modules/remark-parse/lib/locate/link.js");
var normalize = __webpack_require__(/*! ../util/normalize */ "./node_modules/remark-parse/lib/util/normalize.js");

module.exports = reference;
reference.locator = locate;

var T_LINK = 'link';
var T_IMAGE = 'image';
var T_FOOTNOTE = 'footnote';
var REFERENCE_TYPE_SHORTCUT = 'shortcut';
var REFERENCE_TYPE_COLLAPSED = 'collapsed';
var REFERENCE_TYPE_FULL = 'full';
var C_CARET = '^';
var C_BACKSLASH = '\\';
var C_BRACKET_OPEN = '[';
var C_BRACKET_CLOSE = ']';

function reference(eat, value, silent) {
  var self = this;
  var character = value.charAt(0);
  var index = 0;
  var length = value.length;
  var subvalue = '';
  var intro = '';
  var type = T_LINK;
  var referenceType = REFERENCE_TYPE_SHORTCUT;
  var content;
  var identifier;
  var now;
  var node;
  var exit;
  var queue;
  var bracketed;
  var depth;

  /* Check whether were eating an image. */
  if (character === '!') {
    type = T_IMAGE;
    intro = character;
    character = value.charAt(++index);
  }

  if (character !== C_BRACKET_OPEN) {
    return;
  }

  index++;
  intro += character;
  queue = '';

  /* Check whether were eating a footnote. */
  if (self.options.footnotes && value.charAt(index) === C_CARET) {
    /* Exit if `![^` is found, so the `!` will be seen as text after this,
     * and well enter this function again when `[^` is found. */
    if (type === T_IMAGE) {
      return;
    }

    intro += C_CARET;
    index++;
    type = T_FOOTNOTE;
  }

  /* Eat the text. */
  depth = 0;

  while (index < length) {
    character = value.charAt(index);

    if (character === C_BRACKET_OPEN) {
      bracketed = true;
      depth++;
    } else if (character === C_BRACKET_CLOSE) {
      if (!depth) {
        break;
      }

      depth--;
    }

    if (character === C_BACKSLASH) {
      queue += C_BACKSLASH;
      character = value.charAt(++index);
    }

    queue += character;
    index++;
  }

  subvalue = queue;
  content = queue;
  character = value.charAt(index);

  if (character !== C_BRACKET_CLOSE) {
    return;
  }

  index++;
  subvalue += character;
  queue = '';

  while (index < length) {
    character = value.charAt(index);

    if (!whitespace(character)) {
      break;
    }

    queue += character;
    index++;
  }

  character = value.charAt(index);

  /* Inline footnotes cannot have an identifier. */
  if (type !== T_FOOTNOTE && character === C_BRACKET_OPEN) {
    identifier = '';
    queue += character;
    index++;

    while (index < length) {
      character = value.charAt(index);

      if (character === C_BRACKET_OPEN || character === C_BRACKET_CLOSE) {
        break;
      }

      if (character === C_BACKSLASH) {
        identifier += C_BACKSLASH;
        character = value.charAt(++index);
      }

      identifier += character;
      index++;
    }

    character = value.charAt(index);

    if (character === C_BRACKET_CLOSE) {
      referenceType = identifier ? REFERENCE_TYPE_FULL : REFERENCE_TYPE_COLLAPSED;
      queue += identifier + character;
      index++;
    } else {
      identifier = '';
    }

    subvalue += queue;
    queue = '';
  } else {
    if (!content) {
      return;
    }

    identifier = content;
  }

  /* Brackets cannot be inside the identifier. */
  if (referenceType !== REFERENCE_TYPE_FULL && bracketed) {
    return;
  }

  subvalue = intro + subvalue;

  if (type === T_LINK && self.inLink) {
    return null;
  }

  /* istanbul ignore if - never used (yet) */
  if (silent) {
    return true;
  }

  if (type === T_FOOTNOTE && content.indexOf(' ') !== -1) {
    return eat(subvalue)({
      type: 'footnote',
      children: this.tokenizeInline(content, eat.now())
    });
  }

  now = eat.now();
  now.column += intro.length;
  now.offset += intro.length;
  identifier = referenceType === REFERENCE_TYPE_FULL ? identifier : content;

  node = {
    type: type + 'Reference',
    identifier: normalize(identifier)
  };

  if (type === T_LINK || type === T_IMAGE) {
    node.referenceType = referenceType;
  }

  if (type === T_LINK) {
    exit = self.enterLink();
    node.children = self.tokenizeInline(content, now);
    exit();
  } else if (type === T_IMAGE) {
    node.alt = self.decode.raw(self.unescape(content), now) || null;
  }

  return eat(subvalue)(node);
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/strong.js":
/*!**********************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/strong.js ***!
  \**********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var trim = __webpack_require__(/*! trim */ "./node_modules/trim/index.js");
var whitespace = __webpack_require__(/*! is-whitespace-character */ "./node_modules/is-whitespace-character/index.js");
var locate = __webpack_require__(/*! ../locate/strong */ "./node_modules/remark-parse/lib/locate/strong.js");

module.exports = strong;
strong.locator = locate;

var C_ASTERISK = '*';
var C_UNDERSCORE = '_';

function strong(eat, value, silent) {
  var self = this;
  var index = 0;
  var character = value.charAt(index);
  var now;
  var pedantic;
  var marker;
  var queue;
  var subvalue;
  var length;
  var prev;

  if (
    (character !== C_ASTERISK && character !== C_UNDERSCORE) ||
    value.charAt(++index) !== character
  ) {
    return;
  }

  pedantic = self.options.pedantic;
  marker = character;
  subvalue = marker + marker;
  length = value.length;
  index++;
  queue = '';
  character = '';

  if (pedantic && whitespace(value.charAt(index))) {
    return;
  }

  while (index < length) {
    prev = character;
    character = value.charAt(index);

    if (
      character === marker &&
      value.charAt(index + 1) === marker &&
      (!pedantic || !whitespace(prev))
    ) {
      character = value.charAt(index + 2);

      if (character !== marker) {
        if (!trim(queue)) {
          return;
        }

        /* istanbul ignore if - never used (yet) */
        if (silent) {
          return true;
        }

        now = eat.now();
        now.column += 2;
        now.offset += 2;

        return eat(subvalue + queue + subvalue)({
          type: 'strong',
          children: self.tokenizeInline(queue, now)
        });
      }
    }

    if (!pedantic && character === '\\') {
      queue += character;
      character = value.charAt(++index);
    }

    queue += character;
    index++;
  }
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/table.js":
/*!*********************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/table.js ***!
  \*********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var whitespace = __webpack_require__(/*! is-whitespace-character */ "./node_modules/is-whitespace-character/index.js");

module.exports = table;

var C_BACKSLASH = '\\';
var C_TICK = '`';
var C_DASH = '-';
var C_PIPE = '|';
var C_COLON = ':';
var C_SPACE = ' ';
var C_NEWLINE = '\n';
var C_TAB = '\t';

var MIN_TABLE_COLUMNS = 1;
var MIN_TABLE_ROWS = 2;

var TABLE_ALIGN_LEFT = 'left';
var TABLE_ALIGN_CENTER = 'center';
var TABLE_ALIGN_RIGHT = 'right';
var TABLE_ALIGN_NONE = null;

function table(eat, value, silent) {
  var self = this;
  var index;
  var alignments;
  var alignment;
  var subvalue;
  var row;
  var length;
  var lines;
  var queue;
  var character;
  var hasDash;
  var align;
  var cell;
  var preamble;
  var count;
  var opening;
  var now;
  var position;
  var lineCount;
  var line;
  var rows;
  var table;
  var lineIndex;
  var pipeIndex;
  var first;

  /* Exit when not in gfm-mode. */
  if (!self.options.gfm) {
    return;
  }

  /* Get the rows.
   * Detecting tables soon is hard, so there are some
   * checks for performance here, such as the minimum
   * number of rows, and allowed characters in the
   * alignment row. */
  index = 0;
  lineCount = 0;
  length = value.length + 1;
  lines = [];

  while (index < length) {
    lineIndex = value.indexOf(C_NEWLINE, index);
    pipeIndex = value.indexOf(C_PIPE, index + 1);

    if (lineIndex === -1) {
      lineIndex = value.length;
    }

    if (pipeIndex === -1 || pipeIndex > lineIndex) {
      if (lineCount < MIN_TABLE_ROWS) {
        return;
      }

      break;
    }

    lines.push(value.slice(index, lineIndex));
    lineCount++;
    index = lineIndex + 1;
  }

  /* Parse the alignment row. */
  subvalue = lines.join(C_NEWLINE);
  alignments = lines.splice(1, 1)[0] || [];
  index = 0;
  length = alignments.length;
  lineCount--;
  alignment = false;
  align = [];

  while (index < length) {
    character = alignments.charAt(index);

    if (character === C_PIPE) {
      hasDash = null;

      if (alignment === false) {
        if (first === false) {
          return;
        }
      } else {
        align.push(alignment);
        alignment = false;
      }

      first = false;
    } else if (character === C_DASH) {
      hasDash = true;
      alignment = alignment || TABLE_ALIGN_NONE;
    } else if (character === C_COLON) {
      if (alignment === TABLE_ALIGN_LEFT) {
        alignment = TABLE_ALIGN_CENTER;
      } else if (hasDash && alignment === TABLE_ALIGN_NONE) {
        alignment = TABLE_ALIGN_RIGHT;
      } else {
        alignment = TABLE_ALIGN_LEFT;
      }
    } else if (!whitespace(character)) {
      return;
    }

    index++;
  }

  if (alignment !== false) {
    align.push(alignment);
  }

  /* Exit when without enough columns. */
  if (align.length < MIN_TABLE_COLUMNS) {
    return;
  }

  /* istanbul ignore if - never used (yet) */
  if (silent) {
    return true;
  }

  /* Parse the rows. */
  position = -1;
  rows = [];

  table = eat(subvalue).reset({
    type: 'table',
    align: align,
    children: rows
  });

  while (++position < lineCount) {
    line = lines[position];
    row = {type: 'tableRow', children: []};

    /* Eat a newline character when this is not the
     * first row. */
    if (position) {
      eat(C_NEWLINE);
    }

    /* Eat the row. */
    eat(line).reset(row, table);

    length = line.length + 1;
    index = 0;
    queue = '';
    cell = '';
    preamble = true;
    count = null;
    opening = null;

    while (index < length) {
      character = line.charAt(index);

      if (character === C_TAB || character === C_SPACE) {
        if (cell) {
          queue += character;
        } else {
          eat(character);
        }

        index++;
        continue;
      }

      if (character === '' || character === C_PIPE) {
        if (preamble) {
          eat(character);
        } else {
          if (character && opening) {
            queue += character;
            index++;
            continue;
          }

          if ((cell || character) && !preamble) {
            subvalue = cell;

            if (queue.length > 1) {
              if (character) {
                subvalue += queue.slice(0, queue.length - 1);
                queue = queue.charAt(queue.length - 1);
              } else {
                subvalue += queue;
                queue = '';
              }
            }

            now = eat.now();

            eat(subvalue)({
              type: 'tableCell',
              children: self.tokenizeInline(cell, now)
            }, row);
          }

          eat(queue + character);

          queue = '';
          cell = '';
        }
      } else {
        if (queue) {
          cell += queue;
          queue = '';
        }

        cell += character;

        if (character === C_BACKSLASH && index !== length - 2) {
          cell += line.charAt(index + 1);
          index++;
        }

        if (character === C_TICK) {
          count = 1;

          while (line.charAt(index + 1) === character) {
            cell += character;
            index++;
            count++;
          }

          if (!opening) {
            opening = count;
          } else if (count >= opening) {
            opening = 0;
          }
        }
      }

      preamble = false;
      index++;
    }

    /* Eat the alignment row. */
    if (!position) {
      eat(C_NEWLINE + alignments);
    }
  }

  return table;
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/text.js":
/*!********************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/text.js ***!
  \********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = text;

function text(eat, value, silent) {
  var self = this;
  var methods;
  var tokenizers;
  var index;
  var length;
  var subvalue;
  var position;
  var tokenizer;
  var name;
  var min;
  var now;

  /* istanbul ignore if - never used (yet) */
  if (silent) {
    return true;
  }

  methods = self.inlineMethods;
  length = methods.length;
  tokenizers = self.inlineTokenizers;
  index = -1;
  min = value.length;

  while (++index < length) {
    name = methods[index];

    if (name === 'text' || !tokenizers[name]) {
      continue;
    }

    tokenizer = tokenizers[name].locator;

    if (!tokenizer) {
      eat.file.fail('Missing locator: `' + name + '`');
    }

    position = tokenizer.call(self, value, 1);

    if (position !== -1 && position < min) {
      min = position;
    }
  }

  subvalue = value.slice(0, min);
  now = eat.now();

  self.decode(subvalue, now, function (content, position, source) {
    eat(source || content)({
      type: 'text',
      value: content
    });
  });
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/thematic-break.js":
/*!******************************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/thematic-break.js ***!
  \******************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = thematicBreak;

var C_NEWLINE = '\n';
var C_TAB = '\t';
var C_SPACE = ' ';
var C_ASTERISK = '*';
var C_UNDERSCORE = '_';
var C_DASH = '-';

var THEMATIC_BREAK_MARKER_COUNT = 3;

function thematicBreak(eat, value, silent) {
  var index = -1;
  var length = value.length + 1;
  var subvalue = '';
  var character;
  var marker;
  var markerCount;
  var queue;

  while (++index < length) {
    character = value.charAt(index);

    if (character !== C_TAB && character !== C_SPACE) {
      break;
    }

    subvalue += character;
  }

  if (
    character !== C_ASTERISK &&
    character !== C_DASH &&
    character !== C_UNDERSCORE
  ) {
    return;
  }

  marker = character;
  subvalue += character;
  markerCount = 1;
  queue = '';

  while (++index < length) {
    character = value.charAt(index);

    if (character === marker) {
      markerCount++;
      subvalue += queue + marker;
      queue = '';
    } else if (character === C_SPACE) {
      queue += character;
    } else if (
      markerCount >= THEMATIC_BREAK_MARKER_COUNT &&
      (!character || character === C_NEWLINE)
    ) {
      subvalue += queue;

      if (silent) {
        return true;
      }

      return eat(subvalue)({type: 'thematicBreak'});
    } else {
      return;
    }
  }
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenize/url.js":
/*!*******************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenize/url.js ***!
  \*******************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var decode = __webpack_require__(/*! parse-entities */ "./node_modules/parse-entities/index.js");
var whitespace = __webpack_require__(/*! is-whitespace-character */ "./node_modules/is-whitespace-character/index.js");
var locate = __webpack_require__(/*! ../locate/url */ "./node_modules/remark-parse/lib/locate/url.js");

module.exports = url;
url.locator = locate;
url.notInLink = true;

var C_BRACKET_OPEN = '[';
var C_BRACKET_CLOSE = ']';
var C_PAREN_OPEN = '(';
var C_PAREN_CLOSE = ')';
var C_LT = '<';
var C_AT_SIGN = '@';

var HTTP_PROTOCOL = 'http://';
var HTTPS_PROTOCOL = 'https://';
var MAILTO_PROTOCOL = 'mailto:';

var PROTOCOLS = [
  HTTP_PROTOCOL,
  HTTPS_PROTOCOL,
  MAILTO_PROTOCOL
];

var PROTOCOLS_LENGTH = PROTOCOLS.length;

function url(eat, value, silent) {
  var self = this;
  var subvalue;
  var content;
  var character;
  var index;
  var position;
  var protocol;
  var match;
  var length;
  var queue;
  var parenCount;
  var nextCharacter;
  var exit;

  if (!self.options.gfm) {
    return;
  }

  subvalue = '';
  index = -1;
  length = PROTOCOLS_LENGTH;

  while (++index < length) {
    protocol = PROTOCOLS[index];
    match = value.slice(0, protocol.length);

    if (match.toLowerCase() === protocol) {
      subvalue = match;
      break;
    }
  }

  if (!subvalue) {
    return;
  }

  index = subvalue.length;
  length = value.length;
  queue = '';
  parenCount = 0;

  while (index < length) {
    character = value.charAt(index);

    if (whitespace(character) || character === C_LT) {
      break;
    }

    if (
      character === '.' ||
      character === ',' ||
      character === ':' ||
      character === ';' ||
      character === '"' ||
      character === '\'' ||
      character === ')' ||
      character === ']'
    ) {
      nextCharacter = value.charAt(index + 1);

      if (!nextCharacter || whitespace(nextCharacter)) {
        break;
      }
    }

    if (character === C_PAREN_OPEN || character === C_BRACKET_OPEN) {
      parenCount++;
    }

    if (character === C_PAREN_CLOSE || character === C_BRACKET_CLOSE) {
      parenCount--;

      if (parenCount < 0) {
        break;
      }
    }

    queue += character;
    index++;
  }

  if (!queue) {
    return;
  }

  subvalue += queue;
  content = subvalue;

  if (protocol === MAILTO_PROTOCOL) {
    position = queue.indexOf(C_AT_SIGN);

    if (position === -1 || position === length - 1) {
      return;
    }

    content = content.substr(MAILTO_PROTOCOL.length);
  }

  /* istanbul ignore if - never used (yet) */
  if (silent) {
    return true;
  }

  exit = self.enterLink();
  content = self.tokenizeInline(content, eat.now());
  exit();

  return eat(subvalue)({
    type: 'link',
    title: null,
    url: decode(subvalue, {nonTerminated: false}),
    children: content
  });
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/tokenizer.js":
/*!****************************************************!*\
  !*** ./node_modules/remark-parse/lib/tokenizer.js ***!
  \****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = factory;

var MERGEABLE_NODES = {
  text: mergeText,
  blockquote: mergeBlockquote
};

/* Check whether a node is mergeable with adjacent nodes. */
function mergeable(node) {
  var start;
  var end;

  if (node.type !== 'text' || !node.position) {
    return true;
  }

  start = node.position.start;
  end = node.position.end;

  /* Only merge nodes which occupy the same size as their
   * `value`. */
  return start.line !== end.line ||
      end.column - start.column === node.value.length;
}

/* Merge two text nodes: `node` into `prev`. */
function mergeText(prev, node) {
  prev.value += node.value;

  return prev;
}

/* Merge two blockquotes: `node` into `prev`, unless in
 * CommonMark mode. */
function mergeBlockquote(prev, node) {
  if (this.options.commonmark) {
    return node;
  }

  prev.children = prev.children.concat(node.children);

  return prev;
}

/* Construct a tokenizer.  This creates both
 * `tokenizeInline` and `tokenizeBlock`. */
function factory(type) {
  return tokenize;

  /* Tokenizer for a bound `type`. */
  function tokenize(value, location) {
    var self = this;
    var offset = self.offset;
    var tokens = [];
    var methods = self[type + 'Methods'];
    var tokenizers = self[type + 'Tokenizers'];
    var line = location.line;
    var column = location.column;
    var index;
    var length;
    var method;
    var name;
    var matched;
    var valueLength;

    /* Trim white space only lines. */
    if (!value) {
      return tokens;
    }

    /* Expose on `eat`. */
    eat.now = now;
    eat.file = self.file;

    /* Sync initial offset. */
    updatePosition('');

    /* Iterate over `value`, and iterate over all
     * tokenizers.  When one eats something, re-iterate
     * with the remaining value.  If no tokenizer eats,
     * something failed (should not happen) and an
     * exception is thrown. */
    while (value) {
      index = -1;
      length = methods.length;
      matched = false;

      while (++index < length) {
        name = methods[index];
        method = tokenizers[name];

        if (
          method &&
          /* istanbul ignore next */ (!method.onlyAtStart || self.atStart) &&
          (!method.notInList || !self.inList) &&
          (!method.notInBlock || !self.inBlock) &&
          (!method.notInLink || !self.inLink)
        ) {
          valueLength = value.length;

          method.apply(self, [eat, value]);

          matched = valueLength !== value.length;

          if (matched) {
            break;
          }
        }
      }

      /* istanbul ignore if */
      if (!matched) {
        self.file.fail(new Error('Infinite loop'), eat.now());
      }
    }

    self.eof = now();

    return tokens;

    /* Update line, column, and offset based on
     * `value`. */
    function updatePosition(subvalue) {
      var lastIndex = -1;
      var index = subvalue.indexOf('\n');

      while (index !== -1) {
        line++;
        lastIndex = index;
        index = subvalue.indexOf('\n', index + 1);
      }

      if (lastIndex === -1) {
        column += subvalue.length;
      } else {
        column = subvalue.length - lastIndex;
      }

      if (line in offset) {
        if (lastIndex !== -1) {
          column += offset[line];
        } else if (column <= offset[line]) {
          column = offset[line] + 1;
        }
      }
    }

    /* Get offset.  Called before the first character is
     * eaten to retrieve the range's offsets. */
    function getOffset() {
      var indentation = [];
      var pos = line + 1;

      /* Done.  Called when the last character is
       * eaten to retrieve the ranges offsets. */
      return function () {
        var last = line + 1;

        while (pos < last) {
          indentation.push((offset[pos] || 0) + 1);

          pos++;
        }

        return indentation;
      };
    }

    /* Get the current position. */
    function now() {
      var pos = {line: line, column: column};

      pos.offset = self.toOffset(pos);

      return pos;
    }

    /* Store position information for a node. */
    function Position(start) {
      this.start = start;
      this.end = now();
    }

    /* Throw when a value is incorrectly eaten.
     * This shouldnt happen but will throw on new,
     * incorrect rules. */
    function validateEat(subvalue) {
      /* istanbul ignore if */
      if (value.substring(0, subvalue.length) !== subvalue) {
        /* Capture stack-trace. */
        self.file.fail(
          new Error(
            'Incorrectly eaten value: please report this ' +
            'warning on http://git.io/vg5Ft'
          ),
          now()
        );
      }
    }

    /* Mark position and patch `node.position`. */
    function position() {
      var before = now();

      return update;

      /* Add the position to a node. */
      function update(node, indent) {
        var prev = node.position;
        var start = prev ? prev.start : before;
        var combined = [];
        var n = prev && prev.end.line;
        var l = before.line;

        node.position = new Position(start);

        /* If there was already a `position`, this
         * node was merged.  Fixing `start` wasnt
         * hard, but the indent is different.
         * Especially because some information, the
         * indent between `n` and `l` wasnt
         * tracked.  Luckily, that space is
         * (should be?) empty, so we can safely
         * check for it now. */
        if (prev && indent && prev.indent) {
          combined = prev.indent;

          if (n < l) {
            while (++n < l) {
              combined.push((offset[n] || 0) + 1);
            }

            combined.push(before.column);
          }

          indent = combined.concat(indent);
        }

        node.position.indent = indent || [];

        return node;
      }
    }

    /* Add `node` to `parent`s children or to `tokens`.
     * Performs merges where possible. */
    function add(node, parent) {
      var children = parent ? parent.children : tokens;
      var prev = children[children.length - 1];

      if (
        prev &&
        node.type === prev.type &&
        node.type in MERGEABLE_NODES &&
        mergeable(prev) &&
        mergeable(node)
      ) {
        node = MERGEABLE_NODES[node.type].call(self, prev, node);
      }

      if (node !== prev) {
        children.push(node);
      }

      if (self.atStart && tokens.length !== 0) {
        self.exitStart();
      }

      return node;
    }

    /* Remove `subvalue` from `value`.
     * `subvalue` must be at the start of `value`. */
    function eat(subvalue) {
      var indent = getOffset();
      var pos = position();
      var current = now();

      validateEat(subvalue);

      apply.reset = reset;
      reset.test = test;
      apply.test = test;

      value = value.substring(subvalue.length);

      updatePosition(subvalue);

      indent = indent();

      return apply;

      /* Add the given arguments, add `position` to
       * the returned node, and return the node. */
      function apply(node, parent) {
        return pos(add(pos(node), parent), indent);
      }

      /* Functions just like apply, but resets the
       * content:  the line and column are reversed,
       * and the eaten value is re-added.
       * This is useful for nodes with a single
       * type of content, such as lists and tables.
       * See `apply` above for what parameters are
       * expected. */
      function reset() {
        var node = apply.apply(null, arguments);

        line = current.line;
        column = current.column;
        value = subvalue + value;

        return node;
      }

      /* Test the position, after eating, and reverse
       * to a not-eaten state. */
      function test() {
        var result = pos({});

        line = current.line;
        column = current.column;
        value = subvalue + value;

        return result.position;
      }
    }
  }
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/unescape.js":
/*!***************************************************!*\
  !*** ./node_modules/remark-parse/lib/unescape.js ***!
  \***************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = factory;

/* Factory to de-escape a value, based on a list at `key`
 * in `ctx`. */
function factory(ctx, key) {
  return unescape;

  /* De-escape a string using the expression at `key`
   * in `ctx`. */
  function unescape(value) {
    var prev = 0;
    var index = value.indexOf('\\');
    var escape = ctx[key];
    var queue = [];
    var character;

    while (index !== -1) {
      queue.push(value.slice(prev, index));
      prev = index + 1;
      character = value.charAt(prev);

      /* If the following character is not a valid escape,
       * add the slash. */
      if (!character || escape.indexOf(character) === -1) {
        queue.push('\\');
      }

      index = value.indexOf('\\', prev);
    }

    queue.push(value.slice(prev));

    return queue.join('');
  }
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/util/get-indentation.js":
/*!***************************************************************!*\
  !*** ./node_modules/remark-parse/lib/util/get-indentation.js ***!
  \***************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = indentation;

/* Map of characters, and their column length,
 * which can be used as indentation. */
var characters = {' ': 1, '\t': 4};

/* Gets indentation information for a line. */
function indentation(value) {
  var index = 0;
  var indent = 0;
  var character = value.charAt(index);
  var stops = {};
  var size;

  while (character in characters) {
    size = characters[character];

    indent += size;

    if (size > 1) {
      indent = Math.floor(indent / size) * size;
    }

    stops[indent] = index;

    character = value.charAt(++index);
  }

  return {indent: indent, stops: stops};
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/util/html.js":
/*!****************************************************!*\
  !*** ./node_modules/remark-parse/lib/util/html.js ***!
  \****************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var attributeName = '[a-zA-Z_:][a-zA-Z0-9:._-]*';
var unquoted = '[^"\'=<>`\\u0000-\\u0020]+';
var singleQuoted = '\'[^\']*\'';
var doubleQuoted = '"[^"]*"';
var attributeValue = '(?:' + unquoted + '|' + singleQuoted + '|' + doubleQuoted + ')';
var attribute = '(?:\\s+' + attributeName + '(?:\\s*=\\s*' + attributeValue + ')?)';
var openTag = '<[A-Za-z][A-Za-z0-9\\-]*' + attribute + '*\\s*\\/?>';
var closeTag = '<\\/[A-Za-z][A-Za-z0-9\\-]*\\s*>';
var comment = '<!---->|<!--(?:-?[^>-])(?:-?[^-])*-->';
var processing = '<[?].*?[?]>';
var declaration = '<![A-Za-z]+\\s+[^>]*>';
var cdata = '<!\\[CDATA\\[[\\s\\S]*?\\]\\]>';

exports.openCloseTag = new RegExp('^(?:' + openTag + '|' + closeTag + ')');

exports.tag = new RegExp('^(?:' +
  openTag + '|' +
  closeTag + '|' +
  comment + '|' +
  processing + '|' +
  declaration + '|' +
  cdata +
')');


/***/ }),

/***/ "./node_modules/remark-parse/lib/util/interrupt.js":
/*!*********************************************************!*\
  !*** ./node_modules/remark-parse/lib/util/interrupt.js ***!
  \*********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = interrupt;

function interrupt(interruptors, tokenizers, ctx, params) {
  var bools = ['pedantic', 'commonmark'];
  var count = bools.length;
  var length = interruptors.length;
  var index = -1;
  var interruptor;
  var config;
  var fn;
  var offset;
  var bool;
  var ignore;

  while (++index < length) {
    interruptor = interruptors[index];
    config = interruptor[1] || {};
    fn = interruptor[0];
    offset = -1;
    ignore = false;

    while (++offset < count) {
      bool = bools[offset];

      if (config[bool] !== undefined && config[bool] !== ctx.options[bool]) {
        ignore = true;
        break;
      }
    }

    if (ignore) {
      continue;
    }

    if (tokenizers[fn].apply(ctx, params)) {
      return true;
    }
  }

  return false;
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/util/normalize.js":
/*!*********************************************************!*\
  !*** ./node_modules/remark-parse/lib/util/normalize.js ***!
  \*********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var collapseWhiteSpace = __webpack_require__(/*! collapse-white-space */ "./node_modules/collapse-white-space/index.js");

module.exports = normalize;

/* Normalize an identifier.  Collapses multiple white space
 * characters into a single space, and removes casing. */
function normalize(value) {
  return collapseWhiteSpace(value).toLowerCase();
}


/***/ }),

/***/ "./node_modules/remark-parse/lib/util/remove-indentation.js":
/*!******************************************************************!*\
  !*** ./node_modules/remark-parse/lib/util/remove-indentation.js ***!
  \******************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var trim = __webpack_require__(/*! trim */ "./node_modules/trim/index.js");
var repeat = __webpack_require__(/*! repeat-string */ "./node_modules/repeat-string/index.js");
var getIndent = __webpack_require__(/*! ./get-indentation */ "./node_modules/remark-parse/lib/util/get-indentation.js");

module.exports = indentation;

var C_SPACE = ' ';
var C_NEWLINE = '\n';
var C_TAB = '\t';

/* Remove the minimum indent from every line in `value`.
 * Supports both tab, spaced, and mixed indentation (as
 * well as possible). */
function indentation(value, maximum) {
  var values = value.split(C_NEWLINE);
  var position = values.length + 1;
  var minIndent = Infinity;
  var matrix = [];
  var index;
  var indentation;
  var stops;
  var padding;

  values.unshift(repeat(C_SPACE, maximum) + '!');

  while (position--) {
    indentation = getIndent(values[position]);

    matrix[position] = indentation.stops;

    if (trim(values[position]).length === 0) {
      continue;
    }

    if (indentation.indent) {
      if (indentation.indent > 0 && indentation.indent < minIndent) {
        minIndent = indentation.indent;
      }
    } else {
      minIndent = Infinity;

      break;
    }
  }

  if (minIndent !== Infinity) {
    position = values.length;

    while (position--) {
      stops = matrix[position];
      index = minIndent;

      while (index && !(index in stops)) {
        index--;
      }

      if (
        trim(values[position]).length !== 0 &&
        minIndent &&
        index !== minIndent
      ) {
        padding = C_TAB;
      } else {
        padding = '';
      }

      values[position] = padding + values[position].slice(
        index in stops ? stops[index] + 1 : 0
      );
    }
  }

  values.shift();

  return values.join(C_NEWLINE);
}


/***/ }),

/***/ "./node_modules/repeat-string/index.js":
/*!*********************************************!*\
  !*** ./node_modules/repeat-string/index.js ***!
  \*********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
/*!
 * repeat-string <https://github.com/jonschlinkert/repeat-string>
 *
 * Copyright (c) 2014-2015, Jon Schlinkert.
 * Licensed under the MIT License.
 */



/**
 * Results cache
 */

var res = '';
var cache;

/**
 * Expose `repeat`
 */

module.exports = repeat;

/**
 * Repeat the given `string` the specified `number`
 * of times.
 *
 * **Example:**
 *
 * ```js
 * var repeat = require('repeat-string');
 * repeat('A', 5);
 * //=> AAAAA
 * ```
 *
 * @param {String} `string` The string to repeat
 * @param {Number} `number` The number of times to repeat the string
 * @return {String} Repeated string
 * @api public
 */

function repeat(str, num) {
  if (typeof str !== 'string') {
    throw new TypeError('expected a string');
  }

  // cover common, quick use cases
  if (num === 1) return str;
  if (num === 2) return str + str;

  var max = str.length * num;
  if (cache !== str || typeof cache === 'undefined') {
    cache = str;
    res = '';
  } else if (res.length >= max) {
    return res.substr(0, max);
  }

  while (max > res.length && num > 1) {
    if (num & 1) {
      res += str;
    }

    num >>= 1;
    str += str;
  }

  res += str;
  res = res.substr(0, max);
  return res;
}


/***/ }),

/***/ "./node_modules/replace-ext/index.js":
/*!*******************************************!*\
  !*** ./node_modules/replace-ext/index.js ***!
  \*******************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var path = __webpack_require__(/*! path */ "./node_modules/path-browserify/index.js");

function replaceExt(npath, ext) {
  if (typeof npath !== 'string') {
    return npath;
  }

  if (npath.length === 0) {
    return npath;
  }

  var nFileName = path.basename(npath, path.extname(npath)) + ext;
  return path.join(path.dirname(npath), nFileName);
}

module.exports = replaceExt;


/***/ }),

/***/ "./node_modules/state-toggle/index.js":
/*!********************************************!*\
  !*** ./node_modules/state-toggle/index.js ***!
  \********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = factory

/* Construct a state `toggler`: a function which inverses
 * `property` in context based on its current value.
 * The by `toggler` returned function restores that value. */
function factory(key, state, ctx) {
  return enter

  function enter() {
    var context = ctx || this
    var current = context[key]

    context[key] = !state

    return exit

    function exit() {
      context[key] = current
    }
  }
}


/***/ }),

/***/ "./node_modules/trim-trailing-lines/index.js":
/*!***************************************************!*\
  !*** ./node_modules/trim-trailing-lines/index.js ***!
  \***************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = trimTrailingLines

var line = '\n'

/* Remove final newline characters from `value`. */
function trimTrailingLines(value) {
  var val = String(value)
  var index = val.length

  while (val.charAt(--index) === line) {
    /* Empty */
  }

  return val.slice(0, index + 1)
}


/***/ }),

/***/ "./node_modules/trim/index.js":
/*!************************************!*\
  !*** ./node_modules/trim/index.js ***!
  \************************************/
/*! no static exports found */
/***/ (function(module, exports) {


exports = module.exports = trim;

function trim(str){
  return str.replace(/^\s*|\s*$/g, '');
}

exports.left = function(str){
  return str.replace(/^\s*/, '');
};

exports.right = function(str){
  return str.replace(/\s*$/, '');
};


/***/ }),

/***/ "./node_modules/trough/index.js":
/*!**************************************!*\
  !*** ./node_modules/trough/index.js ***!
  \**************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var wrap = __webpack_require__(/*! ./wrap.js */ "./node_modules/trough/wrap.js")

module.exports = trough

trough.wrap = wrap

var slice = [].slice

/* Create new middleware. */
function trough() {
  var fns = []
  var middleware = {}

  middleware.run = run
  middleware.use = use

  return middleware

  /* Run `fns`.  Last argument must be
   * a completion handler. */
  function run() {
    var index = -1
    var input = slice.call(arguments, 0, -1)
    var done = arguments[arguments.length - 1]

    if (typeof done !== 'function') {
      throw new Error('Expected function as last argument, not ' + done)
    }

    next.apply(null, [null].concat(input))

    /* Run the next `fn`, if any. */
    function next(err) {
      var fn = fns[++index]
      var params = slice.call(arguments, 0)
      var values = params.slice(1)
      var length = input.length
      var pos = -1

      if (err) {
        done(err)
        return
      }

      /* Copy non-nully input into values. */
      while (++pos < length) {
        if (values[pos] === null || values[pos] === undefined) {
          values[pos] = input[pos]
        }
      }

      input = values

      /* Next or done. */
      if (fn) {
        wrap(fn, next).apply(null, input)
      } else {
        done.apply(null, [null].concat(input))
      }
    }
  }

  /* Add `fn` to the list. */
  function use(fn) {
    if (typeof fn !== 'function') {
      throw new Error('Expected `fn` to be a function, not ' + fn)
    }

    fns.push(fn)

    return middleware
  }
}


/***/ }),

/***/ "./node_modules/trough/wrap.js":
/*!*************************************!*\
  !*** ./node_modules/trough/wrap.js ***!
  \*************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var slice = [].slice

module.exports = wrap

/* Wrap `fn`.  Can be sync or async; return a promise,
 * receive a completion handler, return new values and
 * errors. */
function wrap(fn, callback) {
  var invoked

  return wrapped

  function wrapped() {
    var params = slice.call(arguments, 0)
    var callback = fn.length > params.length
    var result

    if (callback) {
      params.push(done)
    }

    try {
      result = fn.apply(null, params)
    } catch (err) {
      /* Well, this is quite the pickle.  `fn` received
       * a callback and invoked it (thus continuing the
       * pipeline), but later also threw an error.
       * Were not about to restart the pipeline again,
       * so the only thing left to do is to throw the
       * thing instea. */
      if (callback && invoked) {
        throw err
      }

      return done(err)
    }

    if (!callback) {
      if (result && typeof result.then === 'function') {
        result.then(then, done)
      } else if (result instanceof Error) {
        done(result)
      } else {
        then(result)
      }
    }
  }

  /* Invoke `next`, only once. */
  function done() {
    if (!invoked) {
      invoked = true

      callback.apply(null, arguments)
    }
  }

  /* Invoke `done` with one value.
   * Tracks if an error is passed, too. */
  function then(value) {
    done(null, value)
  }
}


/***/ }),

/***/ "./node_modules/unherit/index.js":
/*!***************************************!*\
  !*** ./node_modules/unherit/index.js ***!
  \***************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var xtend = __webpack_require__(/*! xtend */ "./node_modules/xtend/immutable.js")
var inherits = __webpack_require__(/*! inherits */ "./node_modules/inherits/inherits_browser.js")

module.exports = unherit

/* Create a custom constructor which can be modified
 * without affecting the original class. */
function unherit(Super) {
  var result
  var key
  var value

  inherits(Of, Super)
  inherits(From, Of)

  /* Clone values. */
  result = Of.prototype

  for (key in result) {
    value = result[key]

    if (value && typeof value === 'object') {
      result[key] = 'concat' in value ? value.concat() : xtend(value)
    }
  }

  return Of

  /* Constructor accepting a single argument,
   * which itself is an `arguments` object. */
  function From(parameters) {
    return Super.apply(this, parameters)
  }

  /* Constructor accepting variadic arguments. */
  function Of() {
    if (!(this instanceof Of)) {
      return new From(arguments)
    }

    return Super.apply(this, arguments)
  }
}


/***/ }),

/***/ "./node_modules/unified/index.js":
/*!***************************************!*\
  !*** ./node_modules/unified/index.js ***!
  \***************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


/* Dependencies. */
var extend = __webpack_require__(/*! extend */ "./node_modules/extend/index.js")
var bail = __webpack_require__(/*! bail */ "./node_modules/bail/index.js")
var vfile = __webpack_require__(/*! vfile */ "./node_modules/vfile/index.js")
var trough = __webpack_require__(/*! trough */ "./node_modules/trough/index.js")
var string = __webpack_require__(/*! x-is-string */ "./node_modules/x-is-string/index.js")
var plain = __webpack_require__(/*! is-plain-obj */ "./node_modules/is-plain-obj/index.js")

/* Expose a frozen processor. */
module.exports = unified().freeze()

var slice = [].slice
var own = {}.hasOwnProperty

/* Process pipeline. */
var pipeline = trough()
  .use(pipelineParse)
  .use(pipelineRun)
  .use(pipelineStringify)

function pipelineParse(p, ctx) {
  ctx.tree = p.parse(ctx.file)
}

function pipelineRun(p, ctx, next) {
  p.run(ctx.tree, ctx.file, done)

  function done(err, tree, file) {
    if (err) {
      next(err)
    } else {
      ctx.tree = tree
      ctx.file = file
      next()
    }
  }
}

function pipelineStringify(p, ctx) {
  ctx.file.contents = p.stringify(ctx.tree, ctx.file)
}

/* Function to create the first processor. */
function unified() {
  var attachers = []
  var transformers = trough()
  var namespace = {}
  var frozen = false
  var freezeIndex = -1

  /* Data management. */
  processor.data = data

  /* Lock. */
  processor.freeze = freeze

  /* Plug-ins. */
  processor.attachers = attachers
  processor.use = use

  /* API. */
  processor.parse = parse
  processor.stringify = stringify
  processor.run = run
  processor.runSync = runSync
  processor.process = process
  processor.processSync = processSync

  /* Expose. */
  return processor

  /* Create a new processor based on the processor
   * in the current scope. */
  function processor() {
    var destination = unified()
    var length = attachers.length
    var index = -1

    while (++index < length) {
      destination.use.apply(null, attachers[index])
    }

    destination.data(extend(true, {}, namespace))

    return destination
  }

  /* Freeze: used to signal a processor that has finished
   * configuration.
   *
   * For example, take unified itself.  Its frozen.
   * Plug-ins should not be added to it.  Rather, it should
   * be extended, by invoking it, before modifying it.
   *
   * In essence, always invoke this when exporting a
   * processor. */
  function freeze() {
    var values
    var plugin
    var options
    var transformer

    if (frozen) {
      return processor
    }

    while (++freezeIndex < attachers.length) {
      values = attachers[freezeIndex]
      plugin = values[0]
      options = values[1]
      transformer = null

      if (options === false) {
        continue
      }

      if (options === true) {
        values[1] = undefined
      }

      transformer = plugin.apply(processor, values.slice(1))

      if (typeof transformer === 'function') {
        transformers.use(transformer)
      }
    }

    frozen = true
    freezeIndex = Infinity

    return processor
  }

  /* Data management.
   * Getter / setter for processor-specific informtion. */
  function data(key, value) {
    if (string(key)) {
      /* Set `key`. */
      if (arguments.length === 2) {
        assertUnfrozen('data', frozen)

        namespace[key] = value

        return processor
      }

      /* Get `key`. */
      return (own.call(namespace, key) && namespace[key]) || null
    }

    /* Set space. */
    if (key) {
      assertUnfrozen('data', frozen)
      namespace = key
      return processor
    }

    /* Get space. */
    return namespace
  }

  /* Plug-in management.
   *
   * Pass it:
   * *   an attacher and options,
   * *   a preset,
   * *   a list of presets, attachers, and arguments (list
   *     of attachers and options). */
  function use(value) {
    var settings

    assertUnfrozen('use', frozen)

    if (value === null || value === undefined) {
      /* Empty */
    } else if (typeof value === 'function') {
      addPlugin.apply(null, arguments)
    } else if (typeof value === 'object') {
      if ('length' in value) {
        addList(value)
      } else {
        addPreset(value)
      }
    } else {
      throw new Error('Expected usable value, not `' + value + '`')
    }

    if (settings) {
      namespace.settings = extend(namespace.settings || {}, settings)
    }

    return processor

    function addPreset(result) {
      addList(result.plugins)

      if (result.settings) {
        settings = extend(settings || {}, result.settings)
      }
    }

    function add(value) {
      if (typeof value === 'function') {
        addPlugin(value)
      } else if (typeof value === 'object') {
        if ('length' in value) {
          addPlugin.apply(null, value)
        } else {
          addPreset(value)
        }
      } else {
        throw new Error('Expected usable value, not `' + value + '`')
      }
    }

    function addList(plugins) {
      var length
      var index

      if (plugins === null || plugins === undefined) {
        /* Empty */
      } else if (typeof plugins === 'object' && 'length' in plugins) {
        length = plugins.length
        index = -1

        while (++index < length) {
          add(plugins[index])
        }
      } else {
        throw new Error('Expected a list of plugins, not `' + plugins + '`')
      }
    }

    function addPlugin(plugin, value) {
      var entry = find(plugin)

      if (entry) {
        if (plain(entry[1]) && plain(value)) {
          value = extend(entry[1], value)
        }

        entry[1] = value
      } else {
        attachers.push(slice.call(arguments))
      }
    }
  }

  function find(plugin) {
    var length = attachers.length
    var index = -1
    var entry

    while (++index < length) {
      entry = attachers[index]

      if (entry[0] === plugin) {
        return entry
      }
    }
  }

  /* Parse a file (in string or VFile representation)
   * into a Unist node using the `Parser` on the
   * processor. */
  function parse(doc) {
    var file = vfile(doc)
    var Parser

    freeze()
    Parser = processor.Parser
    assertParser('parse', Parser)

    if (newable(Parser)) {
      return new Parser(String(file), file).parse()
    }

    return Parser(String(file), file) // eslint-disable-line new-cap
  }

  /* Run transforms on a Unist node representation of a file
   * (in string or VFile representation), async. */
  function run(node, file, cb) {
    assertNode(node)
    freeze()

    if (!cb && typeof file === 'function') {
      cb = file
      file = null
    }

    if (!cb) {
      return new Promise(executor)
    }

    executor(null, cb)

    function executor(resolve, reject) {
      transformers.run(node, vfile(file), done)

      function done(err, tree, file) {
        tree = tree || node
        if (err) {
          reject(err)
        } else if (resolve) {
          resolve(tree)
        } else {
          cb(null, tree, file)
        }
      }
    }
  }

  /* Run transforms on a Unist node representation of a file
   * (in string or VFile representation), sync. */
  function runSync(node, file) {
    var complete = false
    var result

    run(node, file, done)

    assertDone('runSync', 'run', complete)

    return result

    function done(err, tree) {
      complete = true
      bail(err)
      result = tree
    }
  }

  /* Stringify a Unist node representation of a file
   * (in string or VFile representation) into a string
   * using the `Compiler` on the processor. */
  function stringify(node, doc) {
    var file = vfile(doc)
    var Compiler

    freeze()
    Compiler = processor.Compiler
    assertCompiler('stringify', Compiler)
    assertNode(node)

    if (newable(Compiler)) {
      return new Compiler(node, file).compile()
    }

    return Compiler(node, file) // eslint-disable-line new-cap
  }

  /* Parse a file (in string or VFile representation)
   * into a Unist node using the `Parser` on the processor,
   * then run transforms on that node, and compile the
   * resulting node using the `Compiler` on the processor,
   * and store that result on the VFile. */
  function process(doc, cb) {
    freeze()
    assertParser('process', processor.Parser)
    assertCompiler('process', processor.Compiler)

    if (!cb) {
      return new Promise(executor)
    }

    executor(null, cb)

    function executor(resolve, reject) {
      var file = vfile(doc)

      pipeline.run(processor, {file: file}, done)

      function done(err) {
        if (err) {
          reject(err)
        } else if (resolve) {
          resolve(file)
        } else {
          cb(null, file)
        }
      }
    }
  }

  /* Process the given document (in string or VFile
   * representation), sync. */
  function processSync(doc) {
    var complete = false
    var file

    freeze()
    assertParser('processSync', processor.Parser)
    assertCompiler('processSync', processor.Compiler)
    file = vfile(doc)

    process(file, done)

    assertDone('processSync', 'process', complete)

    return file

    function done(err) {
      complete = true
      bail(err)
    }
  }
}

/* Check if `func` is a constructor. */
function newable(value) {
  return typeof value === 'function' && keys(value.prototype)
}

/* Check if `value` is an object with keys. */
function keys(value) {
  var key
  for (key in value) {
    return true
  }
  return false
}

/* Assert a parser is available. */
function assertParser(name, Parser) {
  if (typeof Parser !== 'function') {
    throw new Error('Cannot `' + name + '` without `Parser`')
  }
}

/* Assert a compiler is available. */
function assertCompiler(name, Compiler) {
  if (typeof Compiler !== 'function') {
    throw new Error('Cannot `' + name + '` without `Compiler`')
  }
}

/* Assert the processor is not frozen. */
function assertUnfrozen(name, frozen) {
  if (frozen) {
    throw new Error(
      [
        'Cannot invoke `' + name + '` on a frozen processor.\nCreate a new ',
        'processor first, by invoking it: use `processor()` instead of ',
        '`processor`.'
      ].join('')
    )
  }
}

/* Assert `node` is a Unist node. */
function assertNode(node) {
  if (!node || !string(node.type)) {
    throw new Error('Expected node, got `' + node + '`')
  }
}

/* Assert that `complete` is `true`. */
function assertDone(name, asyncName, complete) {
  if (!complete) {
    throw new Error(
      '`' + name + '` finished async. Use `' + asyncName + '` instead'
    )
  }
}


/***/ }),

/***/ "./node_modules/unist-util-is/index.js":
/*!*********************************************!*\
  !*** ./node_modules/unist-util-is/index.js ***!
  \*********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


/* eslint-disable max-params */

/* Expose. */
module.exports = is

/* Assert if `test` passes for `node`.
 * When a `parent` node is known the `index` of node */
function is(test, node, index, parent, context) {
  var hasParent = parent !== null && parent !== undefined
  var hasIndex = index !== null && index !== undefined
  var check = convert(test)

  if (
    hasIndex &&
    (typeof index !== 'number' || index < 0 || index === Infinity)
  ) {
    throw new Error('Expected positive finite index or child node')
  }

  if (hasParent && (!is(null, parent) || !parent.children)) {
    throw new Error('Expected parent node')
  }

  if (!node || !node.type || typeof node.type !== 'string') {
    return false
  }

  if (hasParent !== hasIndex) {
    throw new Error('Expected both parent and index')
  }

  return Boolean(check.call(context, node, index, parent))
}

function convert(test) {
  if (typeof test === 'string') {
    return typeFactory(test)
  }

  if (test === null || test === undefined) {
    return ok
  }

  if (typeof test === 'object') {
    return ('length' in test ? anyFactory : matchesFactory)(test)
  }

  if (typeof test === 'function') {
    return test
  }

  throw new Error('Expected function, string, or object as test')
}

function convertAll(tests) {
  var results = []
  var length = tests.length
  var index = -1

  while (++index < length) {
    results[index] = convert(tests[index])
  }

  return results
}

/* Utility assert each property in `test` is represented
 * in `node`, and each values are strictly equal. */
function matchesFactory(test) {
  return matches

  function matches(node) {
    var key

    for (key in test) {
      if (node[key] !== test[key]) {
        return false
      }
    }

    return true
  }
}

function anyFactory(tests) {
  var checks = convertAll(tests)
  var length = checks.length

  return matches

  function matches() {
    var index = -1

    while (++index < length) {
      if (checks[index].apply(this, arguments)) {
        return true
      }
    }

    return false
  }
}

/* Utility to convert a string into a function which checks
 * a given nodes type for said string. */
function typeFactory(test) {
  return type

  function type(node) {
    return Boolean(node && node.type === test)
  }
}

/* Utility to return true. */
function ok() {
  return true
}


/***/ }),

/***/ "./node_modules/unist-util-remove-position/index.js":
/*!**********************************************************!*\
  !*** ./node_modules/unist-util-remove-position/index.js ***!
  \**********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var visit = __webpack_require__(/*! unist-util-visit */ "./node_modules/unist-util-visit/index.js")

module.exports = removePosition

/* Remove `position`s from `tree`. */
function removePosition(node, force) {
  visit(node, force ? hard : soft)
  return node
}

function hard(node) {
  delete node.position
}

function soft(node) {
  node.position = undefined
}


/***/ }),

/***/ "./node_modules/unist-util-stringify-position/index.js":
/*!*************************************************************!*\
  !*** ./node_modules/unist-util-stringify-position/index.js ***!
  \*************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var own = {}.hasOwnProperty

module.exports = stringify

function stringify(value) {
  /* Nothing. */
  if (!value || typeof value !== 'object') {
    return null
  }

  /* Node. */
  if (own.call(value, 'position') || own.call(value, 'type')) {
    return position(value.position)
  }

  /* Position. */
  if (own.call(value, 'start') || own.call(value, 'end')) {
    return position(value)
  }

  /* Point. */
  if (own.call(value, 'line') || own.call(value, 'column')) {
    return point(value)
  }

  /* ? */
  return null
}

function point(point) {
  if (!point || typeof point !== 'object') {
    point = {}
  }

  return index(point.line) + ':' + index(point.column)
}

function position(pos) {
  if (!pos || typeof pos !== 'object') {
    pos = {}
  }

  return point(pos.start) + '-' + point(pos.end)
}

function index(value) {
  return value && typeof value === 'number' ? value : 1
}


/***/ }),

/***/ "./node_modules/unist-util-visit-parents/index.js":
/*!********************************************************!*\
  !*** ./node_modules/unist-util-visit-parents/index.js ***!
  \********************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


/* Expose. */
module.exports = visitParents

/* Visit. */
function visitParents(tree, type, visitor) {
  var stack = []

  if (typeof type === 'function') {
    visitor = type
    type = null
  }

  one(tree)

  /* Visit a single node. */
  function one(node) {
    var result

    if (!type || node.type === type) {
      result = visitor(node, stack.concat())
    }

    if (node.children && result !== false) {
      return all(node.children, node)
    }

    return result
  }

  /* Visit children in `parent`. */
  function all(children, parent) {
    var length = children.length
    var index = -1
    var child

    stack.push(parent)

    while (++index < length) {
      child = children[index]

      if (child && one(child) === false) {
        return false
      }
    }

    stack.pop()

    return true
  }
}


/***/ }),

/***/ "./node_modules/unist-util-visit/index.js":
/*!************************************************!*\
  !*** ./node_modules/unist-util-visit/index.js ***!
  \************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = visit

var visitParents = __webpack_require__(/*! unist-util-visit-parents */ "./node_modules/unist-util-visit/node_modules/unist-util-visit-parents/index.js")

var CONTINUE = visitParents.CONTINUE
var SKIP = visitParents.SKIP
var EXIT = visitParents.EXIT

visit.CONTINUE = CONTINUE
visit.SKIP = SKIP
visit.EXIT = EXIT

function visit(tree, test, visitor, reverse) {
  if (typeof test === 'function' && typeof visitor !== 'function') {
    reverse = visitor
    visitor = test
    test = null
  }

  visitParents(tree, test, overload, reverse)

  function overload(node, parents) {
    var parent = parents[parents.length - 1]
    var index = parent ? parent.children.indexOf(node) : null
    return visitor(node, index, parent)
  }
}


/***/ }),

/***/ "./node_modules/unist-util-visit/node_modules/unist-util-visit-parents/index.js":
/*!**************************************************************************************!*\
  !*** ./node_modules/unist-util-visit/node_modules/unist-util-visit-parents/index.js ***!
  \**************************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = visitParents

var is = __webpack_require__(/*! unist-util-is */ "./node_modules/unist-util-is/index.js")

var CONTINUE = true
var SKIP = 'skip'
var EXIT = false

visitParents.CONTINUE = CONTINUE
visitParents.SKIP = SKIP
visitParents.EXIT = EXIT

function visitParents(tree, test, visitor, reverse) {
  if (typeof test === 'function' && typeof visitor !== 'function') {
    reverse = visitor
    visitor = test
    test = null
  }

  one(tree, null, [])

  // Visit a single node.
  function one(node, index, parents) {
    var result

    if (!test || is(test, node, index, parents[parents.length - 1] || null)) {
      result = visitor(node, parents)

      if (result === EXIT) {
        return result
      }
    }

    if (node.children && result !== SKIP) {
      return all(node.children, parents.concat(node)) === EXIT ? EXIT : result
    }

    return result
  }

  // Visit children in `parent`.
  function all(children, parents) {
    var min = -1
    var step = reverse ? -1 : 1
    var index = (reverse ? children.length : min) + step
    var child
    var result

    while (index > min && index < children.length) {
      child = children[index]
      result = child && one(child, index, parents)

      if (result === EXIT) {
        return result
      }

      index = typeof result === 'number' ? result : index + step
    }
  }
}


/***/ }),

/***/ "./node_modules/vfile-location/index.js":
/*!**********************************************!*\
  !*** ./node_modules/vfile-location/index.js ***!
  \**********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


module.exports = factory

function factory(file) {
  var contents = indices(String(file))

  return {
    toPosition: offsetToPositionFactory(contents),
    toOffset: positionToOffsetFactory(contents)
  }
}

// Factory to get the line and column-based `position` for `offset` in the bound
// indices.
function offsetToPositionFactory(indices) {
  return offsetToPosition

  // Get the line and column-based `position` for `offset` in the bound indices.
  function offsetToPosition(offset) {
    var index = -1
    var length = indices.length

    if (offset < 0) {
      return {}
    }

    while (++index < length) {
      if (indices[index] > offset) {
        return {
          line: index + 1,
          column: offset - (indices[index - 1] || 0) + 1,
          offset: offset
        }
      }
    }

    return {}
  }
}

// Factory to get the `offset` for a line and column-based `position` in the
// bound indices.
function positionToOffsetFactory(indices) {
  return positionToOffset

  // Get the `offset` for a line and column-based `position` in the bound
  // indices.
  function positionToOffset(position) {
    var line = position && position.line
    var column = position && position.column

    if (!isNaN(line) && !isNaN(column) && line - 1 in indices) {
      return (indices[line - 2] || 0) + column - 1 || 0
    }

    return -1
  }
}

// Get indices of line-breaks in `value`.
function indices(value) {
  var result = []
  var index = value.indexOf('\n')

  while (index !== -1) {
    result.push(index + 1)
    index = value.indexOf('\n', index + 1)
  }

  result.push(value.length + 1)

  return result
}


/***/ }),

/***/ "./node_modules/vfile-message/index.js":
/*!*********************************************!*\
  !*** ./node_modules/vfile-message/index.js ***!
  \*********************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var stringify = __webpack_require__(/*! unist-util-stringify-position */ "./node_modules/unist-util-stringify-position/index.js")

module.exports = VMessage

// Inherit from `Error#`.
function VMessagePrototype() {}
VMessagePrototype.prototype = Error.prototype
VMessage.prototype = new VMessagePrototype()

// Message properties.
var proto = VMessage.prototype

proto.file = ''
proto.name = ''
proto.reason = ''
proto.message = ''
proto.stack = ''
proto.fatal = null
proto.column = null
proto.line = null

// Construct a new VMessage.
//
// Note: We cannot invoke `Error` on the created context, as that adds readonly
// `line` and `column` attributes on Safari 9, thus throwing and failing the
// data.
function VMessage(reason, position, origin) {
  var parts
  var range
  var location

  if (typeof position === 'string') {
    origin = position
    position = null
  }

  parts = parseOrigin(origin)
  range = stringify(position) || '1:1'

  location = {
    start: {line: null, column: null},
    end: {line: null, column: null}
  }

  // Node.
  if (position && position.position) {
    position = position.position
  }

  if (position) {
    // Position.
    if (position.start) {
      location = position
      position = position.start
    } else {
      // Point.
      location.start = position
    }
  }

  if (reason.stack) {
    this.stack = reason.stack
    reason = reason.message
  }

  this.message = reason
  this.name = range
  this.reason = reason
  this.line = position ? position.line : null
  this.column = position ? position.column : null
  this.location = location
  this.source = parts[0]
  this.ruleId = parts[1]
}

function parseOrigin(origin) {
  var result = [null, null]
  var index

  if (typeof origin === 'string') {
    index = origin.indexOf(':')

    if (index === -1) {
      result[1] = origin
    } else {
      result[0] = origin.slice(0, index)
      result[1] = origin.slice(index + 1)
    }
  }

  return result
}


/***/ }),

/***/ "./node_modules/vfile/core.js":
/*!************************************!*\
  !*** ./node_modules/vfile/core.js ***!
  \************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";
/* WEBPACK VAR INJECTION */(function(process) {

var path = __webpack_require__(/*! path */ "./node_modules/path-browserify/index.js");
var replace = __webpack_require__(/*! replace-ext */ "./node_modules/replace-ext/index.js");
var buffer = __webpack_require__(/*! is-buffer */ "./node_modules/is-buffer/index.js");

module.exports = VFile;

var own = {}.hasOwnProperty;
var proto = VFile.prototype;

proto.toString = toString;

/* Order of setting (least specific to most), we need this because
 * otherwise `{stem: 'a', path: '~/b.js'}` would throw, as a path
 * is needed before a stem can be set. */
var order = [
  'history',
  'path',
  'basename',
  'stem',
  'extname',
  'dirname'
];

/* Construct a new file. */
function VFile(options) {
  var prop;
  var index;
  var length;

  if (!options) {
    options = {};
  } else if (typeof options === 'string' || buffer(options)) {
    options = {contents: options};
  } else if ('message' in options && 'messages' in options) {
    return options;
  }

  if (!(this instanceof VFile)) {
    return new VFile(options);
  }

  this.data = {};
  this.messages = [];
  this.history = [];
  this.cwd = process.cwd();

  /* Set path related properties in the correct order. */
  index = -1;
  length = order.length;

  while (++index < length) {
    prop = order[index];

    if (own.call(options, prop)) {
      this[prop] = options[prop];
    }
  }

  /* Set non-path related properties. */
  for (prop in options) {
    if (order.indexOf(prop) === -1) {
      this[prop] = options[prop];
    }
  }
}

/* Access full path (`~/index.min.js`). */
Object.defineProperty(proto, 'path', {
  get: function () {
    return this.history[this.history.length - 1];
  },
  set: function (path) {
    assertNonEmpty(path, 'path');

    if (path !== this.path) {
      this.history.push(path);
    }
  }
});

/* Access parent path (`~`). */
Object.defineProperty(proto, 'dirname', {
  get: function () {
    return typeof this.path === 'string' ? path.dirname(this.path) : undefined;
  },
  set: function (dirname) {
    assertPath(this.path, 'dirname');
    this.path = path.join(dirname || '', this.basename);
  }
});

/* Access basename (`index.min.js`). */
Object.defineProperty(proto, 'basename', {
  get: function () {
    return typeof this.path === 'string' ? path.basename(this.path) : undefined;
  },
  set: function (basename) {
    assertNonEmpty(basename, 'basename');
    assertPart(basename, 'basename');
    this.path = path.join(this.dirname || '', basename);
  }
});

/* Access extname (`.js`). */
Object.defineProperty(proto, 'extname', {
  get: function () {
    return typeof this.path === 'string' ? path.extname(this.path) : undefined;
  },
  set: function (extname) {
    var ext = extname || '';

    assertPart(ext, 'extname');
    assertPath(this.path, 'extname');

    if (ext) {
      if (ext.charAt(0) !== '.') {
        throw new Error('`extname` must start with `.`');
      }

      if (ext.indexOf('.', 1) !== -1) {
        throw new Error('`extname` cannot contain multiple dots');
      }
    }

    this.path = replace(this.path, ext);
  }
});

/* Access stem (`index.min`). */
Object.defineProperty(proto, 'stem', {
  get: function () {
    return typeof this.path === 'string' ? path.basename(this.path, this.extname) : undefined;
  },
  set: function (stem) {
    assertNonEmpty(stem, 'stem');
    assertPart(stem, 'stem');
    this.path = path.join(this.dirname || '', stem + (this.extname || ''));
  }
});

/* Get the value of the file. */
function toString(encoding) {
  var value = this.contents || '';
  return buffer(value) ? value.toString(encoding) : String(value);
}

/* Assert that `part` is not a path (i.e., does
 * not contain `path.sep`). */
function assertPart(part, name) {
  if (part.indexOf(path.sep) !== -1) {
    throw new Error('`' + name + '` cannot be a path: did not expect `' + path.sep + '`');
  }
}

/* Assert that `part` is not empty. */
function assertNonEmpty(part, name) {
  if (!part) {
    throw new Error('`' + name + '` cannot be empty');
  }
}

/* Assert `path` exists. */
function assertPath(path, name) {
  if (!path) {
    throw new Error('Setting `' + name + '` requires `path` to be set too');
  }
}

/* WEBPACK VAR INJECTION */}.call(this, __webpack_require__(/*! ./../process/browser.js */ "./node_modules/process/browser.js")))

/***/ }),

/***/ "./node_modules/vfile/index.js":
/*!*************************************!*\
  !*** ./node_modules/vfile/index.js ***!
  \*************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

"use strict";


var VMessage = __webpack_require__(/*! vfile-message */ "./node_modules/vfile-message/index.js");
var VFile = __webpack_require__(/*! ./core.js */ "./node_modules/vfile/core.js");

module.exports = VFile;

var proto = VFile.prototype;

proto.message = message;
proto.info = info;
proto.fail = fail;

/* Slight backwards compatibility.  Remove in the future. */
proto.warn = message;

/* Create a message with `reason` at `position`.
 * When an error is passed in as `reason`, copies the stack. */
function message(reason, position, origin) {
  var filePath = this.path;
  var message = new VMessage(reason, position, origin);

  if (filePath) {
    message.name = filePath + ':' + message.name;
    message.file = filePath;
  }

  message.fatal = false;

  this.messages.push(message);

  return message;
}

/* Fail. Creates a vmessage, associates it with the file,
 * and throws it. */
function fail() {
  var message = this.message.apply(this, arguments);

  message.fatal = true;

  throw message;
}

/* Info. Creates a vmessage, associates it with the file,
 * and marks the fatality as null. */
function info() {
  var message = this.message.apply(this, arguments);

  message.fatal = null;

  return message;
}


/***/ }),

/***/ "./node_modules/x-is-string/index.js":
/*!*******************************************!*\
  !*** ./node_modules/x-is-string/index.js ***!
  \*******************************************/
/*! no static exports found */
/***/ (function(module, exports) {

var toString = Object.prototype.toString

module.exports = isString

function isString(obj) {
    return toString.call(obj) === "[object String]"
}


/***/ }),

/***/ "./node_modules/xtend/immutable.js":
/*!*****************************************!*\
  !*** ./node_modules/xtend/immutable.js ***!
  \*****************************************/
/*! no static exports found */
/***/ (function(module, exports) {

module.exports = extend

var hasOwnProperty = Object.prototype.hasOwnProperty;

function extend() {
    var target = {}

    for (var i = 0; i < arguments.length; i++) {
        var source = arguments[i]

        for (var key in source) {
            if (hasOwnProperty.call(source, key)) {
                target[key] = source[key]
            }
        }
    }

    return target
}


/***/ }),

/***/ "./pages/experience.js":
/*!*****************************!*\
  !*** ./pages/experience.js ***!
  \*****************************/
/*! exports provided: default */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
__webpack_require__.r(__webpack_exports__);
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_classCallCheck__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/classCallCheck */ "./node_modules/@babel/runtime-corejs2/helpers/esm/classCallCheck.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_createClass__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/createClass */ "./node_modules/@babel/runtime-corejs2/helpers/esm/createClass.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_possibleConstructorReturn__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/possibleConstructorReturn */ "./node_modules/@babel/runtime-corejs2/helpers/esm/possibleConstructorReturn.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_getPrototypeOf__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/getPrototypeOf */ "./node_modules/@babel/runtime-corejs2/helpers/esm/getPrototypeOf.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_inherits__WEBPACK_IMPORTED_MODULE_4__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/inherits */ "./node_modules/@babel/runtime-corejs2/helpers/esm/inherits.js");
/* harmony import */ var react__WEBPACK_IMPORTED_MODULE_5__ = __webpack_require__(/*! react */ "./node_modules/react/index.js");
/* harmony import */ var react__WEBPACK_IMPORTED_MODULE_5___default = /*#__PURE__*/__webpack_require__.n(react__WEBPACK_IMPORTED_MODULE_5__);
/* harmony import */ var react_markdown__WEBPACK_IMPORTED_MODULE_6__ = __webpack_require__(/*! react-markdown */ "./node_modules/react-markdown/lib/react-markdown.js");
/* harmony import */ var react_markdown__WEBPACK_IMPORTED_MODULE_6___default = /*#__PURE__*/__webpack_require__.n(react_markdown__WEBPACK_IMPORTED_MODULE_6__);
/* harmony import */ var _content_output_experience_json__WEBPACK_IMPORTED_MODULE_7__ = __webpack_require__(/*! ../content/output/experience.json */ "./content/output/experience.json");
var _content_output_experience_json__WEBPACK_IMPORTED_MODULE_7___namespace = /*#__PURE__*/__webpack_require__.t(/*! ../content/output/experience.json */ "./content/output/experience.json", 1);









var Experience =
/*#__PURE__*/
function (_React$Component) {
  Object(_babel_runtime_corejs2_helpers_esm_inherits__WEBPACK_IMPORTED_MODULE_4__["default"])(Experience, _React$Component);

  function Experience() {
    Object(_babel_runtime_corejs2_helpers_esm_classCallCheck__WEBPACK_IMPORTED_MODULE_0__["default"])(this, Experience);

    return Object(_babel_runtime_corejs2_helpers_esm_possibleConstructorReturn__WEBPACK_IMPORTED_MODULE_2__["default"])(this, Object(_babel_runtime_corejs2_helpers_esm_getPrototypeOf__WEBPACK_IMPORTED_MODULE_3__["default"])(Experience).apply(this, arguments));
  }

  Object(_babel_runtime_corejs2_helpers_esm_createClass__WEBPACK_IMPORTED_MODULE_1__["default"])(Experience, [{
    key: "render",
    value: function render() {
      return react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        id: "updates",
        className: "ui relaxed divided list"
      }, _content_output_experience_json__WEBPACK_IMPORTED_MODULE_7__.map(function (item, i) {
        var advisors = item.advisors || [];
        return react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "item",
          style: {
            padding: '20px 0'
          }
        }, i == 0 && react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h3", null, "Current Affiliation"), i == 1 && react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h3", null, "Previous Affiliation"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "ui mini image"
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
          src: "/static/images/".concat(item.logo)
        })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "middle aligned content"
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "header"
        }, item.institute.name, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("br", null)), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "description"
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
          href: item.lab.url,
          target: "_blank"
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("b", null, item.lab.name)))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "content"
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("p", {
          style: {
            marginTop: "10px",
            marginBottom: "-5px"
          }
        }, item.role), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "ui list"
        }, advisors.map(function (advisor) {
          return react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
            className: "item"
          }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
            href: advisor.url,
            target: "_blank"
          }, advisor.name));
        }))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "extra",
          style: {
            marginLeft: '0px',
            marginTop: '5px'
          }
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "ui label"
        }, item.period)));
      }));
    }
  }]);

  return Experience;
}(react__WEBPACK_IMPORTED_MODULE_5___default.a.Component);

/* harmony default export */ __webpack_exports__["default"] = (Experience);

/***/ }),

/***/ "./pages/index.js":
/*!************************!*\
  !*** ./pages/index.js ***!
  \************************/
/*! exports provided: default */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
__webpack_require__.r(__webpack_exports__);
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_classCallCheck__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/classCallCheck */ "./node_modules/@babel/runtime-corejs2/helpers/esm/classCallCheck.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_createClass__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/createClass */ "./node_modules/@babel/runtime-corejs2/helpers/esm/createClass.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_possibleConstructorReturn__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/possibleConstructorReturn */ "./node_modules/@babel/runtime-corejs2/helpers/esm/possibleConstructorReturn.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_getPrototypeOf__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/getPrototypeOf */ "./node_modules/@babel/runtime-corejs2/helpers/esm/getPrototypeOf.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_inherits__WEBPACK_IMPORTED_MODULE_4__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/inherits */ "./node_modules/@babel/runtime-corejs2/helpers/esm/inherits.js");
/* harmony import */ var react__WEBPACK_IMPORTED_MODULE_5__ = __webpack_require__(/*! react */ "./node_modules/react/index.js");
/* harmony import */ var react__WEBPACK_IMPORTED_MODULE_5___default = /*#__PURE__*/__webpack_require__.n(react__WEBPACK_IMPORTED_MODULE_5__);
/* harmony import */ var react_markdown__WEBPACK_IMPORTED_MODULE_6__ = __webpack_require__(/*! react-markdown */ "./node_modules/react-markdown/lib/react-markdown.js");
/* harmony import */ var react_markdown__WEBPACK_IMPORTED_MODULE_6___default = /*#__PURE__*/__webpack_require__.n(react_markdown__WEBPACK_IMPORTED_MODULE_6__);
/* harmony import */ var _static_css_style_css__WEBPACK_IMPORTED_MODULE_7__ = __webpack_require__(/*! ../static/css/style.css */ "./static/css/style.css");
/* harmony import */ var _static_css_style_css__WEBPACK_IMPORTED_MODULE_7___default = /*#__PURE__*/__webpack_require__.n(_static_css_style_css__WEBPACK_IMPORTED_MODULE_7__);
/* harmony import */ var _content_output_experience_json__WEBPACK_IMPORTED_MODULE_8__ = __webpack_require__(/*! ../content/output/experience.json */ "./content/output/experience.json");
var _content_output_experience_json__WEBPACK_IMPORTED_MODULE_8___namespace = /*#__PURE__*/__webpack_require__.t(/*! ../content/output/experience.json */ "./content/output/experience.json", 1);
/* harmony import */ var _content_output_fellowship_json__WEBPACK_IMPORTED_MODULE_9__ = __webpack_require__(/*! ../content/output/fellowship.json */ "./content/output/fellowship.json");
var _content_output_fellowship_json__WEBPACK_IMPORTED_MODULE_9___namespace = /*#__PURE__*/__webpack_require__.t(/*! ../content/output/fellowship.json */ "./content/output/fellowship.json", 1);
/* harmony import */ var _content_output_activities_json__WEBPACK_IMPORTED_MODULE_10__ = __webpack_require__(/*! ../content/output/activities.json */ "./content/output/activities.json");
var _content_output_activities_json__WEBPACK_IMPORTED_MODULE_10___namespace = /*#__PURE__*/__webpack_require__.t(/*! ../content/output/activities.json */ "./content/output/activities.json", 1);
/* harmony import */ var _content_output_publications_json__WEBPACK_IMPORTED_MODULE_11__ = __webpack_require__(/*! ../content/output/publications.json */ "./content/output/publications.json");
var _content_output_publications_json__WEBPACK_IMPORTED_MODULE_11___namespace = /*#__PURE__*/__webpack_require__.t(/*! ../content/output/publications.json */ "./content/output/publications.json", 1);
/* harmony import */ var _content_output_posters_json__WEBPACK_IMPORTED_MODULE_12__ = __webpack_require__(/*! ../content/output/posters.json */ "./content/output/posters.json");
var _content_output_posters_json__WEBPACK_IMPORTED_MODULE_12___namespace = /*#__PURE__*/__webpack_require__.t(/*! ../content/output/posters.json */ "./content/output/posters.json", 1);
/* harmony import */ var _content_output_press_json__WEBPACK_IMPORTED_MODULE_13__ = __webpack_require__(/*! ../content/output/press.json */ "./content/output/press.json");
var _content_output_press_json__WEBPACK_IMPORTED_MODULE_13___namespace = /*#__PURE__*/__webpack_require__.t(/*! ../content/output/press.json */ "./content/output/press.json", 1);
/* harmony import */ var _profile__WEBPACK_IMPORTED_MODULE_14__ = __webpack_require__(/*! ./profile */ "./pages/profile.js");
/* harmony import */ var _projects__WEBPACK_IMPORTED_MODULE_15__ = __webpack_require__(/*! ./projects */ "./pages/projects.js");
/* harmony import */ var _posters__WEBPACK_IMPORTED_MODULE_16__ = __webpack_require__(/*! ./posters */ "./pages/posters.js");
/* harmony import */ var _students__WEBPACK_IMPORTED_MODULE_17__ = __webpack_require__(/*! ./students */ "./pages/students.js");
/* harmony import */ var _experience__WEBPACK_IMPORTED_MODULE_18__ = __webpack_require__(/*! ./experience */ "./pages/experience.js");
/* harmony import */ var _timeline__WEBPACK_IMPORTED_MODULE_19__ = __webpack_require__(/*! ./timeline */ "./pages/timeline.js");





















var Index =
/*#__PURE__*/
function (_React$Component) {
  Object(_babel_runtime_corejs2_helpers_esm_inherits__WEBPACK_IMPORTED_MODULE_4__["default"])(Index, _React$Component);

  function Index() {
    Object(_babel_runtime_corejs2_helpers_esm_classCallCheck__WEBPACK_IMPORTED_MODULE_0__["default"])(this, Index);

    return Object(_babel_runtime_corejs2_helpers_esm_possibleConstructorReturn__WEBPACK_IMPORTED_MODULE_2__["default"])(this, Object(_babel_runtime_corejs2_helpers_esm_getPrototypeOf__WEBPACK_IMPORTED_MODULE_3__["default"])(Index).apply(this, arguments));
  }

  Object(_babel_runtime_corejs2_helpers_esm_createClass__WEBPACK_IMPORTED_MODULE_1__["default"])(Index, [{
    key: "componentDidMount",
    value: function componentDidMount() {// $('a').attr('target', 'blank');
    }
  }, {
    key: "render",
    value: function render() {
      return react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", null, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("title", null, "Ryo Suzuki | University of Calgary Assistant Professor"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui stackable grid"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "one wide column"
      }), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "eleven wide column centered"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement(_profile__WEBPACK_IMPORTED_MODULE_14__["default"], null), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("section", {
        className: "container"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        id: "vision"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h2", {
        className: "ui horizontal divider header"
      }, "Mission"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h1", {
        className: "mission"
      }, "Designing ", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("span", {
        className: "highlight"
      }, "Pervasive Dynamic Media"), " With the Power of ", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("span", {
        className: "highlight"
      }, "AR"), " and ", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("span", {
        className: "highlight"
      }, "AI")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("p", {
        className: "description"
      }, "Computers are dynamic media. Much like writing systems, the printing press and painting canvases, they empower and amplify human thought and creativity. This medium, however, is currently confined within the boundaries of flat rectangular screens, restricting our minds and creative potential to what can be done in this small space. Our goal is to redesign this medium to unlock the full potential of human capabilities, by transforming our world itself into a computational medium with the power of AR and AI."), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h2", {
        className: "ui horizontal divider header"
      }, "Principles"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui three stackable cards"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "card"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "content"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h1", {
        className: "highlight"
      }, "Real World"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "header"
      }, "Should Work with Real Objects in the Real World, not Virtual Objects on Screens"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "description"
      }, "Humans have developed tangible and spatial abilities. Media should leverage these innate human capabilities, instead of requiring humans to adapt to media\u2019s capabilities."))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "card"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "content"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h1", {
        className: "highlight"
      }, "Dynamic"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "header"
      }, "Should Leverage Dynamic and Explorable Representations, not Static and Passive Ones"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "description"
      }, "Human thought and creative process is fluid. Media should harness this dynamic and explorable potential, beyond merely watching and consuming static text or passive content."))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "card"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "content"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h1", {
        className: "highlight"
      }, "Pervapsive"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "header"
      }, "Should Work with Every Space, Object, and User, On-the-Fly and On-Demand"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "description"
      }, "Tangible user interfaces exist over decades, but only in research labs. It is time to make them accessible, adaptable, and deployable to everywhere for everyone with the power of AI.")))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h2", {
        className: "ui horizontal divider header"
      }, "Umbrella Themes"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui five stackable cards"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "card"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "content"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        class: "image"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
        src: "/static/images/theme-1.jpg"
      })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h1", null, "Augmented Languages"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "header"
      }, "Reinvent Languages for AR Era"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "description"
      }, "Dynamic pervasive media for reading, writing, presenting, discussing, and communicating ideas"))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "card"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "content"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        class: "image"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
        src: "/static/images/theme-2.jpg"
      })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h1", null, "Explorable Environments"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "header"
      }, "Science Museums for Every Home"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "description"
      }, "Dynamic pervasive media for learning, understanding, exploring, and discovering concepts"))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "card"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "content"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        class: "image"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
        src: "/static/images/theme-3.jpg"
      })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h1", null, "World Canvas"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "header"
      }, "Make the World a Dynamic Canvas"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "description"
      }, "Dynamic pervasive media for creating, sketching, expressing, authoring ideas"))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "card"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "content"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        class: "image"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
        src: "/static/images/theme-4.jpg"
      })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h1", null, "Responsive Environments"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "header"
      }, "Respond Both Visually and Physically"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "description"
      }, "Dynamic pervasive media for touching, crafting, feeling, grasping, and manipulating objects"))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "card"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "content"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        class: "image"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
        src: "/static/images/theme-5.jpg"
      })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h1", null, "Adaptive Reality"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "header"
      }, "Ambient and Context-Aware Intelligence"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "description"
      }, "Dynamic pervasive media for adapting, reminding, visualizing, and assisting behaviors")))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h2", {
        className: "ui horizontal divider header"
      }, "Projects"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui five cards"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "card"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        class: "image"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
        src: "/static/images/project-realitytalk.jpg"
      })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "content"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui tiny label"
      }, "UIST 2022"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "header"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "/realitytalk",
        target: "_blank"
      }, "RealityTalk")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "description"
      }, "Augmented Presentation"))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "card"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        class: "image"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
        src: "/static/images/project-realitysketch.jpg"
      })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "content"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui tiny label"
      }, "UIST 2020"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "header"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "/realitysketch",
        target: "_blank"
      }, "RealitySketch")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "description"
      }, "Visualize Everyday Motion"))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "card"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        class: "image"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
        src: "/static/images/project-sketched-reality.jpg"
      })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "content"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui tiny label"
      }, "UIST 2022"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "header"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "/sketched-reality",
        target: "_blank"
      }, "Sketched Reality")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "description"
      }, "AR Sketch for Robots"))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "card"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        class: "image"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
        src: "/static/images/project-roomshift.jpg"
      })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "content"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui tiny label"
      }, "CHI 2020"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "header"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "/roomshift",
        target: "_blank"
      }, "RoomShift")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "description"
      }, "Reconfigurable Rooms"))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "card"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        class: "image"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
        src: "/static/images/project-teachable-reality.jpg"
      })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "content"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui tiny label"
      }, "CHI 2023"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "header"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "/teachable-reality",
        target: "_blank"
      }, "Teachable Reality")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "description"
      }, "Tangible AR Everywhere"))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "card"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        class: "image"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
        src: "/static/images/project-chameleon-control.jpg"
      })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "content"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui tiny label"
      }, "CHI 2023"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "header"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "/chameleon-control",
        target: "_blank"
      }, "ChameleonControl")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "description"
      }, "Human Teleoperation"))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "card"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        class: "image"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
        src: "/static/images/project-shapebots-3.jpg"
      })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "content"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui tiny label"
      }, "UIST 2019"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "header"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "/shapebots",
        target: "_blank"
      }, "ShapeBots")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "description"
      }, "Swarm Robots as Media"))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "card"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        class: "image"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
        src: "/static/images/project-reactile.jpg"
      })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "content"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui tiny label"
      }, "CHI 2018"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "header"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "/reactile",
        target: "_blank"
      }, "Reactile")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "description"
      }, "Sketch to Program"))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "card"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        class: "image"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
        src: "/static/images/project-hapticbots.jpg"
      })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "content"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui tiny label"
      }, "UIST 2021"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "header"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "/hapticbots",
        target: "_blank"
      }, "HapticBots")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "description"
      }, "On-Demand Haptics"))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "card"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        class: "image"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
        src: "/static/images/project-dynablock-1.jpg"
      })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "content"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui tiny label"
      }, "UIST 2018"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "header"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "/dynablock",
        target: "_blank"
      }, "Dynablock")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "description"
      }, "3D Printer as Media")))))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("section", {
        className: "container"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement(_students__WEBPACK_IMPORTED_MODULE_17__["default"], null)), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("section", {
        className: "container"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement(_projects__WEBPACK_IMPORTED_MODULE_15__["default"], null)), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("section", {
        className: "container"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        id: "dissertation"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h1", null, "PhD Dissertation"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "project ui vertical segment stackable grid",
        "data-id": ""
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "six wide column"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "/phd-thesis"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
        className: "ui rounded images",
        src: "/static/images/collective.jpg"
      }))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ten wide column"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "phd-thesis"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h1", {
        className: "ui header",
        style: {
          marginBottom: '10px'
        }
      }, "Collective Shape-changing Interfaces"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h2", {
        style: {
          margin: '5px 0'
        }
      }, "Dynamic Shape Construction and Transformation with Collective Elements")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("p", null, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("br", null), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("strong", null, "Ryo Suzuki"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("br", null), "PhD Dissertation (University of Colorado Boulder)", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("br", null)))))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("section", {
        className: "container"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        id: "posters"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h1", null, "Selected Posters and Demos"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui vertical segment"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui bulleted list"
      }, _content_output_posters_json__WEBPACK_IMPORTED_MODULE_12__.map(function (item) {
        return react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "project ui vertical segment stackable grid",
          "data-id": ""
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "six wide column"
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
          href: "/phd-thesis"
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
          className: "ui rounded images",
          src: "/static/posters/".concat(item.id, ".jpg")
        }))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "ten wide column"
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h1", {
          className: "ui header",
          style: {
            marginBottom: '10px'
          }
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("span", null, item.name), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("span", {
          className: "ui big label"
        }, item.series), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("span", {
          className: "ui teal large label",
          style: {
            display: ['morphio'].includes(item.id) ? 'inline-block' : 'none'
          }
        }, "Best Paper Award"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("span", {
          className: "ui teal large label",
          style: {
            display: ['realitysketch'].includes(item.id) ? 'inline-block' : 'none'
          }
        }, "Honorable Mention Award")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "item",
          target: "_blank",
          style: {
            lineHeight: '1.8rem'
          }
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
          href: '/publications/' + item.pdf
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("b", null, item.title)), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("br", null), item.author, ".", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("br", null), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
          href: '/publications/' + item.pdf,
          target: "_blank",
          style: {
            marginRight: '5px',
            display: item.pdf ? 'inline' : 'none'
          }
        }, "[PDF]"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
          href: '/publications/' + item.poster,
          target: "_blank",
          style: {
            marginRight: '5px',
            display: item.poster ? 'inline' : 'none'
          }
        }, "[Poster]"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
          href: '/publications/' + item.slide,
          target: "_blank",
          style: {
            marginRight: '5px',
            display: item.slide ? 'inline' : 'none'
          }
        }, "[Slide]"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
          href: "http://doi.acm.org/".concat(item.doi),
          target: "_blank",
          style: {
            marginRight: '5px',
            display: item.doi ? 'inline' : 'none'
          }
        }, "[DOI]"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
          href: item.video,
          target: "_blank",
          style: {
            marginRight: '5px',
            display: item.video ? 'inline' : 'none'
          }
        }, "[Video]"))));
        /*
        return (
          <div className="item" target="_blank" style={{ lineHeight: '1.8rem' }}>
            <b>[{ item.series }]</b><br/>
            <a href={ '/publications/' + item.pdf }><b>{ item.title }</b></a>
            <br />
            { item.author }, <i>{ item.booktitle } ({item.series })</i>. { item.publisher }, { item.address }, { item.pages }.
            <br/>
            <a href={ '/publications/' + item.pdf } target="_blank" style={{ marginRight: '5px', display: item.pdf ? 'inline' : 'none' }}>[PDF]</a>
            <a href={ '/publications/' + item.poster } target="_blank" style={{ marginRight: '5px', display: item.poster ? 'inline' : 'none' }}>[Poster]</a>
            <a href={ '/publications/' + item.slide } target="_blank" style={{ marginRight: '5px', display: item.slide ? 'inline' : 'none' }}>[Slide]</a>
            <a href={ item.url } target="_blank" style={{ marginRight: '5px', display: item.url ? 'inline' : 'none' }}>[DOI]</a>
          </div>
        )
        */
      }))))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("section", {
        className: "container"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        id: "press"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h1", null, "Press Coverage"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui vertical segment"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui bulleted list"
      }, _content_output_press_json__WEBPACK_IMPORTED_MODULE_13__.map(function (item) {
        return react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "item"
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
          href: item.url,
          target: "_blank"
        }, "[", item.date, "] ", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("b", null, item.media), " ", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("i", null, item.title)));
      }))))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("section", {
        className: "container"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        id: "activities"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h1", null, "Professional Activities"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui vertical segment"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement(react_markdown__WEBPACK_IMPORTED_MODULE_6___default.a, {
        source: _content_output_activities_json__WEBPACK_IMPORTED_MODULE_10__.bodyContent
      }))))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        id: "side",
        className: "three wide column centered",
        style: {
          marginTop: '50px'
        }
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement(_experience__WEBPACK_IMPORTED_MODULE_18__["default"], null), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement(_timeline__WEBPACK_IMPORTED_MODULE_19__["default"], null), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("br", null), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        className: "twitter-timeline",
        height: "1500px",
        href: "https://twitter.com/ryosuzk?ref_src=twsrc%5Etfw"
      }, "Tweets by @ryosuzk"), " ", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("script", {
        async: true,
        src: "https://platform.twitter.com/widgets.js",
        charset: "utf-8"
      })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "one wide column"
      })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui stackable grid"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "sixteen wide column centered"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("p", {
        style: {
          textAlign: 'center'
        }
      }))));
    }
  }]);

  return Index;
}(react__WEBPACK_IMPORTED_MODULE_5___default.a.Component);

/* harmony default export */ __webpack_exports__["default"] = (Index);

/***/ }),

/***/ "./pages/posters.js":
/*!**************************!*\
  !*** ./pages/posters.js ***!
  \**************************/
/*! exports provided: default */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
__webpack_require__.r(__webpack_exports__);
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_classCallCheck__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/classCallCheck */ "./node_modules/@babel/runtime-corejs2/helpers/esm/classCallCheck.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_createClass__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/createClass */ "./node_modules/@babel/runtime-corejs2/helpers/esm/createClass.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_possibleConstructorReturn__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/possibleConstructorReturn */ "./node_modules/@babel/runtime-corejs2/helpers/esm/possibleConstructorReturn.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_getPrototypeOf__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/getPrototypeOf */ "./node_modules/@babel/runtime-corejs2/helpers/esm/getPrototypeOf.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_inherits__WEBPACK_IMPORTED_MODULE_4__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/inherits */ "./node_modules/@babel/runtime-corejs2/helpers/esm/inherits.js");
/* harmony import */ var react__WEBPACK_IMPORTED_MODULE_5__ = __webpack_require__(/*! react */ "./node_modules/react/index.js");
/* harmony import */ var react__WEBPACK_IMPORTED_MODULE_5___default = /*#__PURE__*/__webpack_require__.n(react__WEBPACK_IMPORTED_MODULE_5__);
/* harmony import */ var react_markdown__WEBPACK_IMPORTED_MODULE_6__ = __webpack_require__(/*! react-markdown */ "./node_modules/react-markdown/lib/react-markdown.js");
/* harmony import */ var react_markdown__WEBPACK_IMPORTED_MODULE_6___default = /*#__PURE__*/__webpack_require__.n(react_markdown__WEBPACK_IMPORTED_MODULE_6__);
/* harmony import */ var _content_output_summary_json__WEBPACK_IMPORTED_MODULE_7__ = __webpack_require__(/*! ../content/output/summary.json */ "./content/output/summary.json");
var _content_output_summary_json__WEBPACK_IMPORTED_MODULE_7___namespace = /*#__PURE__*/__webpack_require__.t(/*! ../content/output/summary.json */ "./content/output/summary.json", 1);








var ids = ['ultrabots'];
var posters = [];

for (var _i = 0, _ids = ids; _i < _ids.length; _i++) {
  var id = _ids[_i];

  var poster = __webpack_require__("./content/output/posters sync recursive ^\\.\\/.*\\.json$")("./".concat(id, ".json"));

  posters.push(poster);
}

var Posters =
/*#__PURE__*/
function (_React$Component) {
  Object(_babel_runtime_corejs2_helpers_esm_inherits__WEBPACK_IMPORTED_MODULE_4__["default"])(Posters, _React$Component);

  function Posters() {
    Object(_babel_runtime_corejs2_helpers_esm_classCallCheck__WEBPACK_IMPORTED_MODULE_0__["default"])(this, Posters);

    return Object(_babel_runtime_corejs2_helpers_esm_possibleConstructorReturn__WEBPACK_IMPORTED_MODULE_2__["default"])(this, Object(_babel_runtime_corejs2_helpers_esm_getPrototypeOf__WEBPACK_IMPORTED_MODULE_3__["default"])(Posters).apply(this, arguments));
  }

  Object(_babel_runtime_corejs2_helpers_esm_createClass__WEBPACK_IMPORTED_MODULE_1__["default"])(Posters, [{
    key: "componentDidMount",
    value: function componentDidMount() {}
  }, {
    key: "render",
    value: function render() {
      return react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        id: "posters"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h1", null, "Selected Posters and Demos"), posters.map(function (poster) {
        return react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "project ui vertical segment stackable grid",
          "data-id": poster.id
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "six wide column"
        }, poster.id));
      }));
    }
  }]);

  return Posters;
}(react__WEBPACK_IMPORTED_MODULE_5___default.a.Component);

/* harmony default export */ __webpack_exports__["default"] = (Posters);

/***/ }),

/***/ "./pages/profile.js":
/*!**************************!*\
  !*** ./pages/profile.js ***!
  \**************************/
/*! exports provided: default */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
__webpack_require__.r(__webpack_exports__);
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_classCallCheck__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/classCallCheck */ "./node_modules/@babel/runtime-corejs2/helpers/esm/classCallCheck.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_createClass__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/createClass */ "./node_modules/@babel/runtime-corejs2/helpers/esm/createClass.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_possibleConstructorReturn__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/possibleConstructorReturn */ "./node_modules/@babel/runtime-corejs2/helpers/esm/possibleConstructorReturn.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_getPrototypeOf__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/getPrototypeOf */ "./node_modules/@babel/runtime-corejs2/helpers/esm/getPrototypeOf.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_inherits__WEBPACK_IMPORTED_MODULE_4__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/inherits */ "./node_modules/@babel/runtime-corejs2/helpers/esm/inherits.js");
/* harmony import */ var react__WEBPACK_IMPORTED_MODULE_5__ = __webpack_require__(/*! react */ "./node_modules/react/index.js");
/* harmony import */ var react__WEBPACK_IMPORTED_MODULE_5___default = /*#__PURE__*/__webpack_require__.n(react__WEBPACK_IMPORTED_MODULE_5__);







var Profile =
/*#__PURE__*/
function (_React$Component) {
  Object(_babel_runtime_corejs2_helpers_esm_inherits__WEBPACK_IMPORTED_MODULE_4__["default"])(Profile, _React$Component);

  function Profile() {
    Object(_babel_runtime_corejs2_helpers_esm_classCallCheck__WEBPACK_IMPORTED_MODULE_0__["default"])(this, Profile);

    return Object(_babel_runtime_corejs2_helpers_esm_possibleConstructorReturn__WEBPACK_IMPORTED_MODULE_2__["default"])(this, Object(_babel_runtime_corejs2_helpers_esm_getPrototypeOf__WEBPACK_IMPORTED_MODULE_3__["default"])(Profile).apply(this, arguments));
  }

  Object(_babel_runtime_corejs2_helpers_esm_createClass__WEBPACK_IMPORTED_MODULE_1__["default"])(Profile, [{
    key: "render",
    value: function render() {
      return react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("header", {
        className: "ui stackable grid"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui sixteen wide column"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h1", {
        className: "ui huge header"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
        style: {
          maxWidth: '62px',
          marginRight: '15px'
        },
        src: "/static/images/profile.png",
        className: "ui circular image"
      }), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "content"
      }, "Ryo Suzuki", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "sub header",
        style: {
          fontSize: '1.5rem'
        }
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("b", null, "University of Calgary"), ", Assistant Professor in Computer Science"))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("video", {
        id: "top-video",
        poster: "/static/posters/top.png",
        preload: "metadata",
        autoPlay: true,
        loop: true,
        muted: true,
        playsInline: true,
        "webkit-playsinline": ""
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("source", {
        src: "/static/video/top.mp4",
        type: "video/mp4"
      })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        id: "profile",
        style: {
          fontSize: '1.3rem'
        }
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("p", null, "I am an Assistant Professor at the ", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "https://ucalgary.ca",
        target: "_blank"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("b", null, "University of Calgary")), " in the ", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "https://science.ucalgary.ca/computer-science",
        target: "_blank"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("b", null, "Department of Computer Science")), ". I am part of ", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "https://ilab.cpsc.ucalgary.ca/",
        target: "_blank"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("b", null, "HCI Group")), " at UCalgary."), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        class: "ui segment",
        style: {
          borderColor: '#191970'
        }
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("p", {
        style: {
          marginBottom: '-5px',
          color: '#191970'
        }
      }, "I am actively looking for prospective ", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("b", null, "PhD students, PostDocs, or Student Interns"), ", who are interested in joining or collaborating with us, across disciplines (e.g., Industrial Design, Mechanical Engineering, Material Science, Electrical Engineering, Architectural Design, and Computer Science). If you are interested, please feel free to send me an email. We could start from some possible collaborative projects. :)")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("p", null, "My research interest lies on the intersection at ", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("b", null, "Human-Computer Interaction (HCI)"), ". I direct ", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("b", null, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "https://programmable-reality-lab.github.io/",
        target: "_blank"
      }, "Programmable Reality Lab")), " at the University of Calgary, where we try to blend digital and physical worlds by exploring novel interactions for ", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("b", null, "AR/VR, Tangible UI, and AI"), "."), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("p", null, "Previously, I was a PhD student at the ", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "https://www.colorado.edu/cs/",
        target: "_blank"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("b", null, "University of Colorado Boulder")), ", advised by ", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "http://leithinger.com/",
        target: "_blank"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("b", null, "Daniel Leithinger")), " and ", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "http://mdgross.net/",
        target: "_blank"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("b", null, "Mark D. Gross")), " in ", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "https://www.colorado.edu/atlas/thing-lab",
        target: "_blank"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("b", null, "THING Lab")), " and ", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "http://hcc.colorado.edu/",
        target: "_blank"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("b", null, "Human-Computer Interaction Group")))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui horizontal list",
        style: {
          marginTop: '20px'
        }
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "item"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "https://scholar.google.com/citations?user=klWjaQIAAAAJ",
        target: "_blank",
        style: {
          fontSize: '1.2em'
        }
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("i", {
        className: "fas fa-graduation-cap fa-fw"
      }), "Google Scholar")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "item"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "/cv.pdf",
        target: "_blank",
        style: {
          fontSize: '1.2em'
        }
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("i", {
        className: "far fa-file fa-fw"
      }), "CV")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "item"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "mailto:ryo.suzuki@ucalgary.ca",
        target: "_blank",
        style: {
          fontSize: '1.2em'
        }
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("i", {
        className: "far fa-envelope fa-fw"
      }), "Email")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "item"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "https://www.facebook.com/ryosuzk",
        target: "_blank",
        style: {
          fontSize: '1.2em'
        }
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("i", {
        className: "fab fa-facebook-square fa-fw"
      }), "ryosuzk")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "item"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "https://twitter.com/ryosuzk",
        target: "_blank",
        style: {
          fontSize: '1.2em'
        }
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("i", {
        className: "fab fa-twitter fa-fw"
      }), "ryosuzk")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "item"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "https://twitter.com/HCI_Comics",
        target: "_blank",
        style: {
          fontSize: '1.2em'
        }
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("i", {
        className: "fab fa-twitter fa-fw"
      }), "HCI_Comics (ja)")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "item"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "https://github.com/ryosuzuki",
        target: "_blank",
        style: {
          fontSize: '1.2em'
        }
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("i", {
        className: "fab fa-github-alt fa-fw"
      }), "ryosuzuki")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "item"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: "https://www.linkedin.com/in/ryosuzuki/",
        target: "_blank",
        style: {
          fontSize: '1.2em'
        }
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("i", {
        className: "fab fa-linkedin-in fa-fw"
      }), "ryosuzuki")))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "one wide column"
      }));
    }
  }]);

  return Profile;
}(react__WEBPACK_IMPORTED_MODULE_5___default.a.Component);

/* harmony default export */ __webpack_exports__["default"] = (Profile);

/***/ }),

/***/ "./pages/projects.js":
/*!***************************!*\
  !*** ./pages/projects.js ***!
  \***************************/
/*! exports provided: default */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
__webpack_require__.r(__webpack_exports__);
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_classCallCheck__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/classCallCheck */ "./node_modules/@babel/runtime-corejs2/helpers/esm/classCallCheck.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_createClass__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/createClass */ "./node_modules/@babel/runtime-corejs2/helpers/esm/createClass.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_possibleConstructorReturn__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/possibleConstructorReturn */ "./node_modules/@babel/runtime-corejs2/helpers/esm/possibleConstructorReturn.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_getPrototypeOf__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/getPrototypeOf */ "./node_modules/@babel/runtime-corejs2/helpers/esm/getPrototypeOf.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_inherits__WEBPACK_IMPORTED_MODULE_4__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/inherits */ "./node_modules/@babel/runtime-corejs2/helpers/esm/inherits.js");
/* harmony import */ var react__WEBPACK_IMPORTED_MODULE_5__ = __webpack_require__(/*! react */ "./node_modules/react/index.js");
/* harmony import */ var react__WEBPACK_IMPORTED_MODULE_5___default = /*#__PURE__*/__webpack_require__.n(react__WEBPACK_IMPORTED_MODULE_5__);
/* harmony import */ var react_markdown__WEBPACK_IMPORTED_MODULE_6__ = __webpack_require__(/*! react-markdown */ "./node_modules/react-markdown/lib/react-markdown.js");
/* harmony import */ var react_markdown__WEBPACK_IMPORTED_MODULE_6___default = /*#__PURE__*/__webpack_require__.n(react_markdown__WEBPACK_IMPORTED_MODULE_6__);
/* harmony import */ var _content_output_summary_json__WEBPACK_IMPORTED_MODULE_7__ = __webpack_require__(/*! ../content/output/summary.json */ "./content/output/summary.json");
var _content_output_summary_json__WEBPACK_IMPORTED_MODULE_7___namespace = /*#__PURE__*/__webpack_require__.t(/*! ../content/output/summary.json */ "./content/output/summary.json", 1);








var ids = ['teachable-reality', 'chameleon-control', 'sketched-reality', 'realitytalk', 'mixels', 'selective-self-assembly', 'ar-and-robotics', 'expandable-robots', 'electro-voxel', 'hapticbots', 'realitysketch', 'pufferbot', 'roomshift', 'lift-tiles', 'shapebots', 'morphio', 'dynablock', 'tabby', 'reactile', 'pep', 'flux-marker', 'trace-diff', 'mixed-initiative', 'refazer', 'atelier'];
var projects = [];

for (var _i = 0, _ids = ids; _i < _ids.length; _i++) {
  var id = _ids[_i];

  var project = __webpack_require__("./content/output/projects sync recursive ^\\.\\/.*\\.json$")("./".concat(id, ".json"));

  projects.push(project);
}

var Projects =
/*#__PURE__*/
function (_React$Component) {
  Object(_babel_runtime_corejs2_helpers_esm_inherits__WEBPACK_IMPORTED_MODULE_4__["default"])(Projects, _React$Component);

  function Projects() {
    Object(_babel_runtime_corejs2_helpers_esm_classCallCheck__WEBPACK_IMPORTED_MODULE_0__["default"])(this, Projects);

    return Object(_babel_runtime_corejs2_helpers_esm_possibleConstructorReturn__WEBPACK_IMPORTED_MODULE_2__["default"])(this, Object(_babel_runtime_corejs2_helpers_esm_getPrototypeOf__WEBPACK_IMPORTED_MODULE_3__["default"])(Projects).apply(this, arguments));
  }

  Object(_babel_runtime_corejs2_helpers_esm_createClass__WEBPACK_IMPORTED_MODULE_1__["default"])(Projects, [{
    key: "componentDidMount",
    value: function componentDidMount() {}
  }, {
    key: "render",
    value: function render() {
      return react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        id: "projects"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h1", null, "Full Papers"), projects.map(function (project) {
        var link = "/".concat(project.id);

        if (project.external) {
          link = project.external;
        }

        return react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "project ui vertical segment stackable grid",
          "data-id": project.id
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "six wide column"
        }, project.image && react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
          href: link,
          target: "_blank",
          className: "cover-image-container"
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
          className: "ui rounded images cover-image",
          src: "/static/images/".concat(project.image)
        })), !project.image && react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("video", {
          poster: "/static/posters/".concat(project.id, ".jpg"),
          autoplay: "",
          loop: "loop",
          muted: "true",
          playsinline: "",
          width: "100%",
          onclick: "this.play()",
          onmouseover: "this.play()"
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("source", {
          src: "/static/video/".concat(project.id, ".mp4"),
          type: "video/mp4"
        }))), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "ten wide column"
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
          href: link,
          target: "_blank"
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h1", {
          className: "ui header",
          style: {
            marginBottom: '10px'
          }
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("span", null, project.name), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("span", {
          className: "ui big label"
        }, project.conference.name), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("span", {
          className: "ui teal large label",
          style: {
            display: ['morphio'].includes(project.id) ? 'inline-block' : 'none'
          }
        }, "Best Paper Award"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("span", {
          className: "ui teal large label",
          style: {
            display: ['realitysketch'].includes(project.id) ? 'inline-block' : 'none'
          }
        }, "Honorable Mention Award")), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h2", {
          style: {
            margin: '5px 0'
          }
        }, project.description)), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("p", null, project.authors.map(function (author) {
          return author.includes('Ryo Suzuki') ? react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("strong", null, author) : react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("span", null, author);
        }).reduce(function (prev, curr) {
          return [prev, ', ', curr];
        }), " \xA0 ", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("span", {
          style: {
            color: 'gray'
          }
        }, project.note))));
      }));
    }
  }]);

  return Projects;
}(react__WEBPACK_IMPORTED_MODULE_5___default.a.Component);

/* harmony default export */ __webpack_exports__["default"] = (Projects);

/***/ }),

/***/ "./pages/students.js":
/*!***************************!*\
  !*** ./pages/students.js ***!
  \***************************/
/*! exports provided: default */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
__webpack_require__.r(__webpack_exports__);
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_classCallCheck__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/classCallCheck */ "./node_modules/@babel/runtime-corejs2/helpers/esm/classCallCheck.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_createClass__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/createClass */ "./node_modules/@babel/runtime-corejs2/helpers/esm/createClass.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_possibleConstructorReturn__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/possibleConstructorReturn */ "./node_modules/@babel/runtime-corejs2/helpers/esm/possibleConstructorReturn.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_getPrototypeOf__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/getPrototypeOf */ "./node_modules/@babel/runtime-corejs2/helpers/esm/getPrototypeOf.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_inherits__WEBPACK_IMPORTED_MODULE_4__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/inherits */ "./node_modules/@babel/runtime-corejs2/helpers/esm/inherits.js");
/* harmony import */ var react__WEBPACK_IMPORTED_MODULE_5__ = __webpack_require__(/*! react */ "./node_modules/react/index.js");
/* harmony import */ var react__WEBPACK_IMPORTED_MODULE_5___default = /*#__PURE__*/__webpack_require__.n(react__WEBPACK_IMPORTED_MODULE_5__);
/* harmony import */ var react_markdown__WEBPACK_IMPORTED_MODULE_6__ = __webpack_require__(/*! react-markdown */ "./node_modules/react-markdown/lib/react-markdown.js");
/* harmony import */ var react_markdown__WEBPACK_IMPORTED_MODULE_6___default = /*#__PURE__*/__webpack_require__.n(react_markdown__WEBPACK_IMPORTED_MODULE_6__);
/* harmony import */ var _content_output_ilab_summary_json__WEBPACK_IMPORTED_MODULE_7__ = __webpack_require__(/*! ../content/output/ilab-summary.json */ "./content/output/ilab-summary.json");
var _content_output_ilab_summary_json__WEBPACK_IMPORTED_MODULE_7___namespace = /*#__PURE__*/__webpack_require__.t(/*! ../content/output/ilab-summary.json */ "./content/output/ilab-summary.json", 1);








var names = ['shivesh-jadon', 'neil-chulpongsatorn', 'marcus-friedel', 'adnan-karim', 'aditya-gunturu', 'mehrad-faridan', 'jian-liao', 'zhijie-xia', 'nishan-soni', 'bheesha-kumari', 'jarin-thundathil', 'kevin-van', 'melissa-hoang', 'abhinav-pillai', 'keiichi-ihara', 'mille-skovhus-lunding', 'rasmus-lunding', 'ryota-gomi', 'kyzyl-monteiro', 'hiroki-kaimoto', 'ritik-vatsal', 'shrivatsa-mishra', 'tian-xia', 'christopher-rodriguez', 'harrison-chen', 'kaynen-mitchell', 'manjot-khangura', 'manuel-rodriguez'];
var students = [];

for (var _i = 0, _names = names; _i < _names.length; _i++) {
  var name = _names[_i];
  var student = _content_output_ilab_summary_json__WEBPACK_IMPORTED_MODULE_7__.fileMap["content/output/people/".concat(name, ".json")];
  student.id = name;
  student.photo = "https://raw.githubusercontent.com/ucalgary-ilab/ilab-website/master/static/images/people/".concat(name, ".jpg");
  students.push(student);
}

var Students =
/*#__PURE__*/
function (_React$Component) {
  Object(_babel_runtime_corejs2_helpers_esm_inherits__WEBPACK_IMPORTED_MODULE_4__["default"])(Students, _React$Component);

  function Students() {
    Object(_babel_runtime_corejs2_helpers_esm_classCallCheck__WEBPACK_IMPORTED_MODULE_0__["default"])(this, Students);

    return Object(_babel_runtime_corejs2_helpers_esm_possibleConstructorReturn__WEBPACK_IMPORTED_MODULE_2__["default"])(this, Object(_babel_runtime_corejs2_helpers_esm_getPrototypeOf__WEBPACK_IMPORTED_MODULE_3__["default"])(Students).apply(this, arguments));
  }

  Object(_babel_runtime_corejs2_helpers_esm_createClass__WEBPACK_IMPORTED_MODULE_1__["default"])(Students, [{
    key: "componentDidMount",
    value: function componentDidMount() {}
  }, {
    key: "renderLink",
    value: function renderLink(student, key) {
      if (!student[key]) {
        return '';
      }

      var title = student[key].split('/')[3];
      var href;
      var icon;

      switch (key) {
        case 'url':
          icon = 'fas fa-link fa-fw';
          break;

        case 'scholar':
          icon = 'fas fa-graduation-cap fa-fw';
          break;

        case 'twitter':
          icon = 'fab fa-twitter fa-fw';
          break;

        case 'facebook':
          icon = 'fab fa-facebook-square fa-fw';
          break;

        case 'github':
          icon = 'fab fa-github-alt fa-fw';
          break;

        case 'cv':
          icon = 'far fa-file fa-fw';
          break;

        case 'email':
          icon = 'far fa-envelope fa-fw';
          break;

        case 'linkedin':
          icon = 'fab fa-linkedin-in fa-fw';
          break;
      }

      return react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "item"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        href: student[key],
        target: "_blank",
        style: {
          fontSize: '1.2em'
        }
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("i", {
        className: icon
      })));
    }
  }, {
    key: "getTitle",
    value: function getTitle(student) {
      var type = student.type;

      if (type === 'alumni') {
        type = student.past;
      }

      switch (type) {
        case 'postdoc':
          student.title = 'Postdoc';
          break;

        case 'phd':
          student.title = 'PhD';
          break;

        case 'master':
          student.title = 'Master';
          break;

        case 'undergrad':
          student.title = 'Undergrad';
          break;

        case 'visiting':
          student.title = 'Visiting';
          break;
      }

      return student;
    }
  }, {
    key: "renderStudent",
    value: function renderStudent(student) {
      student = this.getTitle(student);
      return react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("a", {
        className: "card",
        href: "https://ilab.ucalgary.ca/people/".concat(student.id),
        target: "_blank"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "center aligned"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "image profile-container"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("img", {
        className: "ui rounded profile image",
        src: student.photo
      })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "header"
      }, student.name, " ", react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("br", null), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("span", {
        className: "meta"
      }, student.title))));
    }
  }, {
    key: "render",
    value: function render() {
      var _this = this;

      return react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        id: "students"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h1", null, "Students"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h3", null, "Current Students"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui ten cards"
      }, students.filter(function (student) {
        return student.type !== 'alumni';
      }).map(function (student) {
        return _this.renderStudent(student);
      })), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h3", null, "Alumni"), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        className: "ui ten cards"
      }, students.filter(function (student) {
        return student.type === 'alumni';
      }).map(function (student) {
        return _this.renderStudent(student);
      })));
    }
  }]);

  return Students;
}(react__WEBPACK_IMPORTED_MODULE_5___default.a.Component);

/* harmony default export */ __webpack_exports__["default"] = (Students);

/***/ }),

/***/ "./pages/timeline.js":
/*!***************************!*\
  !*** ./pages/timeline.js ***!
  \***************************/
/*! exports provided: default */
/***/ (function(module, __webpack_exports__, __webpack_require__) {

"use strict";
__webpack_require__.r(__webpack_exports__);
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_classCallCheck__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/classCallCheck */ "./node_modules/@babel/runtime-corejs2/helpers/esm/classCallCheck.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_createClass__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/createClass */ "./node_modules/@babel/runtime-corejs2/helpers/esm/createClass.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_possibleConstructorReturn__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/possibleConstructorReturn */ "./node_modules/@babel/runtime-corejs2/helpers/esm/possibleConstructorReturn.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_getPrototypeOf__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/getPrototypeOf */ "./node_modules/@babel/runtime-corejs2/helpers/esm/getPrototypeOf.js");
/* harmony import */ var _babel_runtime_corejs2_helpers_esm_inherits__WEBPACK_IMPORTED_MODULE_4__ = __webpack_require__(/*! @babel/runtime-corejs2/helpers/esm/inherits */ "./node_modules/@babel/runtime-corejs2/helpers/esm/inherits.js");
/* harmony import */ var react__WEBPACK_IMPORTED_MODULE_5__ = __webpack_require__(/*! react */ "./node_modules/react/index.js");
/* harmony import */ var react__WEBPACK_IMPORTED_MODULE_5___default = /*#__PURE__*/__webpack_require__.n(react__WEBPACK_IMPORTED_MODULE_5__);
/* harmony import */ var react_markdown__WEBPACK_IMPORTED_MODULE_6__ = __webpack_require__(/*! react-markdown */ "./node_modules/react-markdown/lib/react-markdown.js");
/* harmony import */ var react_markdown__WEBPACK_IMPORTED_MODULE_6___default = /*#__PURE__*/__webpack_require__.n(react_markdown__WEBPACK_IMPORTED_MODULE_6__);
/* harmony import */ var _content_output_news_json__WEBPACK_IMPORTED_MODULE_7__ = __webpack_require__(/*! ../content/output/news.json */ "./content/output/news.json");
var _content_output_news_json__WEBPACK_IMPORTED_MODULE_7___namespace = /*#__PURE__*/__webpack_require__.t(/*! ../content/output/news.json */ "./content/output/news.json", 1);









var Timeline =
/*#__PURE__*/
function (_React$Component) {
  Object(_babel_runtime_corejs2_helpers_esm_inherits__WEBPACK_IMPORTED_MODULE_4__["default"])(Timeline, _React$Component);

  function Timeline() {
    Object(_babel_runtime_corejs2_helpers_esm_classCallCheck__WEBPACK_IMPORTED_MODULE_0__["default"])(this, Timeline);

    return Object(_babel_runtime_corejs2_helpers_esm_possibleConstructorReturn__WEBPACK_IMPORTED_MODULE_2__["default"])(this, Object(_babel_runtime_corejs2_helpers_esm_getPrototypeOf__WEBPACK_IMPORTED_MODULE_3__["default"])(Timeline).apply(this, arguments));
  }

  Object(_babel_runtime_corejs2_helpers_esm_createClass__WEBPACK_IMPORTED_MODULE_1__["default"])(Timeline, [{
    key: "render",
    value: function render() {
      return react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
        id: "updates",
        className: "ui relaxed divided list"
      }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("h3", null, "Recent Updates"), _content_output_news_json__WEBPACK_IMPORTED_MODULE_7__.map(function (item) {
        return react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "item"
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "header"
        }, item.date), react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement("div", {
          className: "content"
        }, react__WEBPACK_IMPORTED_MODULE_5___default.a.createElement(react_markdown__WEBPACK_IMPORTED_MODULE_6___default.a, {
          source: item.text
        })));
      }));
    }
  }]);

  return Timeline;
}(react__WEBPACK_IMPORTED_MODULE_5___default.a.Component);

/* harmony default export */ __webpack_exports__["default"] = (Timeline);
/*
<div class="ui relaxed divided list">
  <div class="item">
    <i class="large github middle aligned icon"></i>
    <div class="content">
      <a class="header">Semantic-Org/Semantic-UI</a>
      <div class="description">Updated 10 mins ago</div>
    </div>
  </div>
  <div class="item">
    <i class="large github middle aligned icon"></i>
    <div class="content">
      <a class="header">Semantic-Org/Semantic-UI-Docs</a>
      <div class="description">Updated 22 mins ago</div>
    </div>
  </div>
  <div class="item">
    <i class="large github middle aligned icon"></i>
    <div class="content">
      <a class="header">Semantic-Org/Semantic-UI-Meteor</a>
      <div class="description">Updated 34 mins ago</div>
    </div>
  </div>
</div>
*/

/***/ }),

/***/ 0:
/*!*******************************************************************************************************************************************!*\
  !*** multi next-client-pages-loader?page=%2F&absolutePagePath=%2FUsers%2Fryosuzuki%2FDocuments%2Fryosuzuki%2Fhomepage%2Fpages%2Findex.js ***!
  \*******************************************************************************************************************************************/
/*! no static exports found */
/***/ (function(module, exports, __webpack_require__) {

module.exports = __webpack_require__(/*! next-client-pages-loader?page=%2F&absolutePagePath=%2FUsers%2Fryosuzuki%2FDocuments%2Fryosuzuki%2Fhomepage%2Fpages%2Findex.js! */"./node_modules/next/dist/build/webpack/loaders/next-client-pages-loader.js?page=%2F&absolutePagePath=%2FUsers%2Fryosuzuki%2FDocuments%2Fryosuzuki%2Fhomepage%2Fpages%2Findex.js!./");


/***/ }),

/***/ "dll-reference dll_1aef2d0bbc0d334d831c":
/*!*******************************************!*\
  !*** external "dll_1aef2d0bbc0d334d831c" ***!
  \*******************************************/
/*! no static exports found */
/***/ (function(module, exports) {

module.exports = dll_1aef2d0bbc0d334d831c;

/***/ })

},[[0,"static/runtime/webpack.js","styles"]]]);
//# sourceMappingURL=index.js.map