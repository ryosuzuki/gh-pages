(window.webpackJsonp=window.webpackJsonp||[]).push([["b11c"],{"+iuc":function(t,e,o){o("wgeU"),o("FlQf"),o("bBy9"),o("B9jh"),o("dL40"),o("xvv9"),o("V+O7"),t.exports=o("WEpk").Set},"/+P4":function(t,e,o){var i=o("Bhuq"),a=o("TRZx");function n(e){return t.exports=n=a?i:function(t){return t.__proto__||i(t)},n(e)}t.exports=n},"/HRN":function(t,e){t.exports=function(t,e){if(!(t instanceof e))throw new TypeError("Cannot call a class as a function")}},"0Bsm":function(t,e,o){"use strict";var i=o("KI45"),a=i(o("UXZV")),n=i(o("/HRN")),s=i(o("WaGi")),r=i(o("ZDA2")),c=i(o("/+P4")),l=i(o("N9n2")),u=function(t){return t&&t.__esModule?t:{default:t}};Object.defineProperty(e,"__esModule",{value:!0});var h=u(o("q1tI")),p=u(o("17x9"));e.default=function(t){var e=function(e){function o(){return(0,n.default)(this,o),(0,r.default)(this,(0,c.default)(o).apply(this,arguments))}return(0,l.default)(o,e),(0,s.default)(o,[{key:"render",value:function(){return h.default.createElement(t,(0,a.default)({router:this.context.router},this.props))}}]),o}(h.default.Component);return e.contextTypes={router:p.default.object},e.getInitialProps=t.getInitialProps,e}},"0tVQ":function(t,e,o){o("FlQf"),o("VJsP"),t.exports=o("WEpk").Array.from},"16Al":function(t,e,o){"use strict";var i=o("WbBG");function a(){}t.exports=function(){function t(t,e,o,a,n,s){if(s!==i){var r=new Error("Calling PropTypes validators directly is not supported by the `prop-types` package. Use PropTypes.checkPropTypes() to call them. Read more at http://fb.me/use-check-prop-types");throw r.name="Invariant Violation",r}}function e(){return t}t.isRequired=t;var o={array:t,bool:t,func:t,number:t,object:t,string:t,symbol:t,any:t,arrayOf:e,element:t,instanceOf:e,node:t,objectOf:e,oneOf:e,oneOfType:e,shape:e,exact:e};return o.checkPropTypes=a,o.PropTypes=o,o}},"17x9":function(t,e,o){t.exports=o("16Al")()},"3RXq":function(t){t.exports={id:"lift-tiles",name:"LiftTiles",description:"Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces",title:"LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces",authors:["Ryo Suzuki","Ryosuke Nakayama","Dan Liu","Yasuaki Kakehi","Mark D. Gross","Daniel Leithinger"],year:2020,booktitle:"In Proceedings of the 14th ACM International Conference on Tangible, Embedded and Embodied Interaction (TEI '20)",publisher:"ACM, New York, NY, USA",pages:"143–151",doi:"https://doi.org/10.1145/3374920.3374941",conference:{name:"TEI 2020",fullname:"The ACM International Conference on Tangible, Embedded and Embodied Interaction (TEI 2020)",url:"https://tei.acm.org/2020/"},video:"https://www.youtube.com/watch?v=0LHeTkOMR84",embed:"https://www.youtube.com/embed/0LHeTkOMR84",pdf:"tei-2020-lift-tiles.pdf",slide:"tei-2020-lift-tiles-slide.pdf",poster:"uist-2019-lift-tiles-poster.pdf","acm-dl":"https://dl.acm.org/doi/10.1145/3374920.3374941",pageCount:9,slideCount:37,related:{title:"LiftTiles: Modular and Reconfigurable Room-scale Shape Displays through Retractable Inflatable Actuators",authors:["Ryo Suzuki","Ryosuke Nakayama","Dan Liu","Yasuaki Kakehi","Mark D. Gross","Daniel Leithinger"],year:2019,booktitle:"In Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST '19)",publisher:"ACM, New York, NY, USA",pages:"1-3",doi:"https://doi.org/10.1145/3332167.3357105",url:"http://uist.acm.org/uist2019",pdf:"uist-2019-lift-tiles.pdf",suffix:"adjunct",pageCount:3},bodyContent:'# Abstract\n\nLarge-scale shape-changing interfaces have great potential, but creating such systems requires substantial time, cost, space, and efforts, which hinders the research community to explore inter- actions beyond the scale of human hands. We introduce modular inflatable actuators as building blocks for prototyping room-scale shape-changing interfaces. Each actuator can change its height from 15cm to 150cm, actuated and controlled by air pressure. Each unit is low-cost (8 USD), lightweight (10 kg), compact (15 cm), and robust (e.g., can support 10 kg weight) making it well-suited for prototyping room-scale shape transformations. Moreover, our mod- ular and reconfigurable design allows researchers and designers to quickly construct different geometries and to explore various appli- cations. This paper contributes to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/top.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/top.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-1-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-1-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-1-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-1-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-1-3.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-1-3.jpg" /></a>\n  </div>\n</div>\n\n\x3c!--\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-2-1.png" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-2-2.png" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-2-2.png" /></a>\n  </div>\n</div>\n --\x3e\n\n\n# LiftTiles: Room-scale Modular Inflatable Actuators\n\nThis paper introduces LiftTiles, modular inflatable actuators for prototyping room-scale shape-changing interfaces. Each inflatable actuator has a large footprint (e.g., 30 cm x 30 cm) and enables large-scale shape transformation. The ac- tuator is fabricated from a flexible plastic tube and constant force springs. It extends when inflated and retracts by the force of its spring when deflated. By controlling the internal air volume, the actuator can change its height from 15 cm to 150 cm. We designed each module as low cost (e.g., 8 USD), lightweight (e.g., 1.8kg), and robust (e.g., with- stand more than 10 kg weight), so that it is suitable for rapid prototyping of room-sized interfaces. Our design utilizes constant force springs to provide greater scalability, simplified fabrication, and stronger retraction force, all essential for large-scale shape-change.\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/unit.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/unit.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-3-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-3-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-3-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-3-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-3-3.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-3-3.jpg" /></a>\n  </div>\n</div>\n\nOur inflatable actuator leverages an extendable structure similar to a party horn. Each inflatable actuator consists of a flexible plastic tube and two constant force springs, which are rolled at their resting positions (Figure 2). When pumping air into the tube, the actuator extends as the internal air pressure increases and the end of the tube unwinds. When releasing air through a release valve, the inflatable tube retracts due to the force of the embedded spring returning to its resting position.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/unit-animation.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/unit-animation.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-4-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-4-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-4-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-4-2.jpg" /></a>\n  </div>\n</div>\n\nThe goal of our system is to provide an accessible prototyping building block for room-scale shape-changing interfaces. To achieve this goal, we set the following three design require- ments:\n\n- **Fast**: LIftTiles provide a means to quickly mock up a room-size shape-changing interface\n\n- **Extendable**:  range (from 15cm to 150cm),\n\n- **Low-cost** (8 USD per tile),\n\n- **Light**: (10 kg per tile),\n\n- **Compact**: (Each tile compresses to 15 cm)\n\n- **Modular**: (can reconfigure the arrangement)\n\n- **Strong**: (each tile supports up 10 kg)\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-5-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-5-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-5-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-5-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-5-3.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-5-3.jpg" /></a>\n  </div>\n</div>\n\n\x3c!--\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/cad.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/cad.mp4" type="video/mp4"></source>\n</video>\n --\x3e\n\n# Modular Design\n\nEach actuator is modular and can connect with the air supply of neighboring actuators. Each solenoid air intake valve is connected to a T-fitting. Adjacent actua- tors are pneumatically connected with a silicon tube between the T-fittings (Figure 6). This way, an array of actuators is connected to a shared pressurized supply line.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/modular.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/modular.mp4" type="video/mp4"></source>\n</video>\n\n</br>\n\nAlso, due to the relatively compact size and small weight of the modules, various ar- chitectural surfaces can be considered for installation, such as placed on the floor or installed sideways from a wall. The actuators can be arranged in a grid, a line, or as individ- ual, loosely arranged units. Figure 10 depicts a few example configurations, which can also be reconfigured by the person installing them or the end user. An individual actuator can be picked up and moved, which may let the user handle it like a traditional piece of furniture, but could also open up new possibilities for interaction, such as actuators with a motorized base that reconfigures the arrangement according to the current use case.\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/tile-animation.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/tile-animation.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-6-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-6-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-6-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-6-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-6-3.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-6-3.jpg" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-7-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-7-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-7-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-7-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-7-3.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-7-3.jpg" /></a>\n  </div>\n</div>\n\n\n# Prototyping Applications\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-10.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-10.jpg" /></a>\n  </div>\n</div>\n\nTo gauge how well LiftTiles is suited to our goal of supporting rapid prototyping and user testing, we built several differ- ent example applications.\n\n- **Adaptive Floor Furniture**: The first application is a recon- figurable floor that creates adaptive furniture, inspired by an adaptive dynamic table (e.g., TRANSFORM [45]) and dy- namic physical affordances (e.g., inFORM [8]). We created a shape-changing floor by arranging 25 modules in a 5 x 5 grid. Each unit is individually actuated to provide a chair, a table, and a bed (Figure 1).\n\n- **Landscape Haptics for VR**: While hand-sized shape displays have rendered haptic sensations for VR [34]), we propose to render furniture-size props, walls, and game elements that users can feel through walking and touching while experi- encing immersive VR scenes. Similar to the applications pro- posed by HapticTurk [3], TurkDeck [4], and TilePop [44], In this application, the haptics increases the feeling of presence in games by providing room-scale dynamic haptic feedback. Also, this can aid the design process by rendering full-scale object mock-ups of large objects like a car.\n\n- **Shape-changing Wall**: By leveraging the block’s compact form factor, we also prototyped a horizontal shape-changing wall to adapt to user needs and display information. The wall becomes a space separator to make a temporary meet- ing space, while it can also act as public signage by displaying an arrow shape to guide a passer-by.\n\n- **Deployable Pop-Up Structure**: As the actuators are compact compared to traditional furniture and walls, they can be deployed in temporary use cases such as festivals, trade shows, concerts, and disaster relief. In these applications, the actuators are shipped in a compact box and laid out in an empty room, similar to tiles. After connecting them to a compressor and control computer, the operator selects a use case and the blocks render a layout of chairs, beds, tables, and partitions.\n\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-8-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-8-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-8-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-8-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-8-3.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-8-3.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-8-4.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-8-4.jpg" /></a>\n  </div>\n</div>\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/wall-animation.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/wall-animation.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-2-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-2-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-9-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-9-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-9-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-9-2.jpg" /></a>\n  </div>\n</div>\n\n\n# Conclusion\n\nThis paper introduces constructive building blocks for proto- typing room-scale shape-changing interfaces. To this end, we designed a modular inflatable actuator that is highly extend- able (from 15cm to 150cm), low-cost (8 USD), lightweight (10 kg), compact (15 cm), and strong (e.g., can support 10 kg weight) making it well-suited for prototyping room-scale shape transformations. Moreover, our modular and recon- figurable design allows researchers and designers to quickly construct different geometries and to investigate various ap- plications. In this paper, we contributed to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block. We plan to evaluate the potential of our system by inviting designers to use our blocks in their own designs and see whether and/or how our building blocks enhances their ideation process.',bodyHtml:"<h1>Abstract</h1>\n<p>Large-scale shape-changing interfaces have great potential, but creating such systems requires substantial time, cost, space, and efforts, which hinders the research community to explore inter- actions beyond the scale of human hands. We introduce modular inflatable actuators as building blocks for prototyping room-scale shape-changing interfaces. Each actuator can change its height from 15cm to 150cm, actuated and controlled by air pressure. Each unit is low-cost (8 USD), lightweight (10 kg), compact (15 cm), and robust (e.g., can support 10 kg weight) making it well-suited for prototyping room-scale shape transformations. Moreover, our mod- ular and reconfigurable design allows researchers and designers to quickly construct different geometries and to explore various appli- cations. This paper contributes to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/lift-tiles/video/top.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/lift-tiles/video/top.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-1-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-1-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-1-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-1-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-1-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-1-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;!--\n&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-2-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-2-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-2-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-2-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n--&gt;</p>\n<h1>LiftTiles: Room-scale Modular Inflatable Actuators</h1>\n<p>This paper introduces LiftTiles, modular inflatable actuators for prototyping room-scale shape-changing interfaces. Each inflatable actuator has a large footprint (e.g., 30 cm x 30 cm) and enables large-scale shape transformation. The ac- tuator is fabricated from a flexible plastic tube and constant force springs. It extends when inflated and retracts by the force of its spring when deflated. By controlling the internal air volume, the actuator can change its height from 15 cm to 150 cm. We designed each module as low cost (e.g., 8 USD), lightweight (e.g., 1.8kg), and robust (e.g., with- stand more than 10 kg weight), so that it is suitable for rapid prototyping of room-sized interfaces. Our design utilizes constant force springs to provide greater scalability, simplified fabrication, and stronger retraction force, all essential for large-scale shape-change.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/lift-tiles/video/unit.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/lift-tiles/video/unit.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-3-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-3-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-3-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-3-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-3-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-3-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>Our inflatable actuator leverages an extendable structure similar to a party horn. Each inflatable actuator consists of a flexible plastic tube and two constant force springs, which are rolled at their resting positions (Figure 2). When pumping air into the tube, the actuator extends as the internal air pressure increases and the end of the tube unwinds. When releasing air through a release valve, the inflatable tube retracts due to the force of the embedded spring returning to its resting position.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/lift-tiles/video/unit-animation.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/lift-tiles/video/unit-animation.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-4-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-4-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-4-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-4-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>The goal of our system is to provide an accessible prototyping building block for room-scale shape-changing interfaces. To achieve this goal, we set the following three design require- ments:</p>\n<ul>\n<li>\n<p><strong>Fast</strong>: LIftTiles provide a means to quickly mock up a room-size shape-changing interface</p>\n</li>\n<li>\n<p><strong>Extendable</strong>:  range (from 15cm to 150cm),</p>\n</li>\n<li>\n<p><strong>Low-cost</strong> (8 USD per tile),</p>\n</li>\n<li>\n<p><strong>Light</strong>: (10 kg per tile),</p>\n</li>\n<li>\n<p><strong>Compact</strong>: (Each tile compresses to 15 cm)</p>\n</li>\n<li>\n<p><strong>Modular</strong>: (can reconfigure the arrangement)</p>\n</li>\n<li>\n<p><strong>Strong</strong>: (each tile supports up 10 kg)</p>\n</li>\n</ul>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-5-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-5-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-5-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-5-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-5-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-5-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;!--\n&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/lift-tiles/video/cad.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/lift-tiles/video/cad.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;\n--&gt;</p>\n<h1>Modular Design</h1>\n<p>Each actuator is modular and can connect with the air supply of neighboring actuators. Each solenoid air intake valve is connected to a T-fitting. Adjacent actua- tors are pneumatically connected with a silicon tube between the T-fittings (Figure 6). This way, an array of actuators is connected to a shared pressurized supply line.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/lift-tiles/video/modular.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/lift-tiles/video/modular.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;/br&gt;</p>\n<p>Also, due to the relatively compact size and small weight of the modules, various ar- chitectural surfaces can be considered for installation, such as placed on the floor or installed sideways from a wall. The actuators can be arranged in a grid, a line, or as individ- ual, loosely arranged units. Figure 10 depicts a few example configurations, which can also be reconfigured by the person installing them or the end user. An individual actuator can be picked up and moved, which may let the user handle it like a traditional piece of furniture, but could also open up new possibilities for interaction, such as actuators with a motorized base that reconfigures the arrangement according to the current use case.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/lift-tiles/video/tile-animation.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/lift-tiles/video/tile-animation.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-6-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-6-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-6-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-6-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-6-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-6-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-7-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-7-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-7-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-7-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-7-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-7-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>Prototyping Applications</h1>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-10.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-10.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>To gauge how well LiftTiles is suited to our goal of supporting rapid prototyping and user testing, we built several differ- ent example applications.</p>\n<ul>\n<li>\n<p><strong>Adaptive Floor Furniture</strong>: The first application is a recon- figurable floor that creates adaptive furniture, inspired by an adaptive dynamic table (e.g., TRANSFORM [45]) and dy- namic physical affordances (e.g., inFORM [8]). We created a shape-changing floor by arranging 25 modules in a 5 x 5 grid. Each unit is individually actuated to provide a chair, a table, and a bed (Figure 1).</p>\n</li>\n<li>\n<p><strong>Landscape Haptics for VR</strong>: While hand-sized shape displays have rendered haptic sensations for VR [34]), we propose to render furniture-size props, walls, and game elements that users can feel through walking and touching while experi- encing immersive VR scenes. Similar to the applications pro- posed by HapticTurk [3], TurkDeck [4], and TilePop [44], In this application, the haptics increases the feeling of presence in games by providing room-scale dynamic haptic feedback. Also, this can aid the design process by rendering full-scale object mock-ups of large objects like a car.</p>\n</li>\n<li>\n<p><strong>Shape-changing Wall</strong>: By leveraging the block’s compact form factor, we also prototyped a horizontal shape-changing wall to adapt to user needs and display information. The wall becomes a space separator to make a temporary meet- ing space, while it can also act as public signage by displaying an arrow shape to guide a passer-by.</p>\n</li>\n<li>\n<p><strong>Deployable Pop-Up Structure</strong>: As the actuators are compact compared to traditional furniture and walls, they can be deployed in temporary use cases such as festivals, trade shows, concerts, and disaster relief. In these applications, the actuators are shipped in a compact box and laid out in an empty room, similar to tiles. After connecting them to a compressor and control computer, the operator selects a use case and the blocks render a layout of chairs, beds, tables, and partitions.</p>\n</li>\n</ul>\n<p>&lt;div class=&quot;figures ui four column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-8-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-8-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-8-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-8-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-8-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-8-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-8-4.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-8-4.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/lift-tiles/video/wall-animation.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/lift-tiles/video/wall-animation.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui four column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-2-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-2-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-2-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-2-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-9-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-9-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/lift-tiles/figure-9-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/lift-tiles/figure-9-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>Conclusion</h1>\n<p>This paper introduces constructive building blocks for proto- typing room-scale shape-changing interfaces. To this end, we designed a modular inflatable actuator that is highly extend- able (from 15cm to 150cm), low-cost (8 USD), lightweight (10 kg), compact (15 cm), and strong (e.g., can support 10 kg weight) making it well-suited for prototyping room-scale shape transformations. Moreover, our modular and recon- figurable design allows researchers and designers to quickly construct different geometries and to investigate various ap- plications. In this paper, we contributed to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block. We plan to evaluate the potential of our system by inviting designers to use our blocks in their own designs and see whether and/or how our building blocks enhances their ideation process.</p>\n",dir:"content/output/projects",base:"lift-tiles.json",ext:".json",sourceBase:"lift-tiles.md",sourceExt:".md"}},"4JlD":function(t,e,o){"use strict";var i=function(t){switch(typeof t){case"string":return t;case"boolean":return t?"true":"false";case"number":return isFinite(t)?t:"";default:return""}};t.exports=function(t,e,o,r){return e=e||"&",o=o||"=",null===t&&(t=void 0),"object"==typeof t?n(s(t),function(s){var r=encodeURIComponent(i(s))+o;return a(t[s])?n(t[s],function(t){return r+encodeURIComponent(i(t))}).join(e):r+encodeURIComponent(i(t[s]))}).join(e):r?encodeURIComponent(i(r))+o+encodeURIComponent(i(t)):""};var a=Array.isArray||function(t){return"[object Array]"===Object.prototype.toString.call(t)};function n(t,e){if(t.map)return t.map(e);for(var o=[],i=0;i<t.length;i++)o.push(e(t[i],i));return o}var s=Object.keys||function(t){var e=[];for(var o in t)Object.prototype.hasOwnProperty.call(t,o)&&e.push(o);return e}},"4Vye":function(t,e,o){"use strict";var i=function(t){if(t&&t.__esModule)return t;var e={};if(null!=t)for(var o in t)Object.hasOwnProperty.call(t,o)&&(e[o]=t[o]);return e.default=t,e};Object.defineProperty(e,"__esModule",{value:!0});var a=i(o("q1tI"));e.RequestContext=a.createContext(null)},"5wCx":function(t){t.exports={id:"chameleon-control",name:"ChameleonControl",description:"Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms",title:"ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms",authors:["Mehrad Faridan","Bheesha Kumari","Ryo Suzuki"],year:2023,booktitle:"In Proceedings of the ACM CHI Conference on Human Factors in Computing Systems (CHI '23)",publisher:"ACM, New York, NY, USA",doi:"https://doi.org/10.1145/3544548.3581449",conference:{name:"CHI 2023",fullname:"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2023)",url:"https://chi2023.acm.org/"},pdf:"chi-2023-chameleon-control.pdf",video:"https://www.youtube.com/watch?v=VOe3fETd3sk",embed:"https://www.youtube.com/embed/VOe3fETd3sk",arxiv:"https://arxiv.org/abs/2302.11053","acm-dl":"https://dl.acm.org/doi/10.1145/3544548.3581449",pageCount:13,slideCount:0,bodyContent:"# Abstract\n\nWe present ChameleonControl, a real-human teleoperation system for scalable remote instruction in hands-on classrooms. In contrast to existing video or AR/VR-based remote hands-on education, ChameleonControl uses a real human as a surrogate of a remote instructor. Building on existing human-based telepresence approaches, we contribute a novel method to teleoperate a human surrogate through synchronized mixed reality hand gestural navigation and verbal communication. By overlaying the remote instructor's virtual hands in the local user's MR view, the remote instructor can guide and control the local user as if they were physically present. This allows the local user/surrogate to synchronize their hand movements and gestures with the remote instructor, effectively teleoperating a real human. We deploy and evaluate our system in classrooms of physiotherapy training, as well as other application domains such as mechanical assembly, sign language and cooking lessons. The study results confirm that our approach can increase engagement and the sense of co-presence, showing potential for the future of remote hands-on classrooms.",bodyHtml:"<h1>Abstract</h1>\n<p>We present ChameleonControl, a real-human teleoperation system for scalable remote instruction in hands-on classrooms. In contrast to existing video or AR/VR-based remote hands-on education, ChameleonControl uses a real human as a surrogate of a remote instructor. Building on existing human-based telepresence approaches, we contribute a novel method to teleoperate a human surrogate through synchronized mixed reality hand gestural navigation and verbal communication. By overlaying the remote instructor's virtual hands in the local user's MR view, the remote instructor can guide and control the local user as if they were physically present. This allows the local user/surrogate to synchronize their hand movements and gestures with the remote instructor, effectively teleoperating a real human. We deploy and evaluate our system in classrooms of physiotherapy training, as well as other application domains such as mechanical assembly, sign language and cooking lessons. The study results confirm that our approach can increase engagement and the sense of co-presence, showing potential for the future of remote hands-on classrooms.</p>\n",dir:"content/output/projects",base:"chameleon-control.json",ext:".json",sourceBase:"chameleon-control.md",sourceExt:".md"}},"6T/A":function(t){t.exports={}},"7kuZ":function(t){t.exports={id:"hapticbots",name:"HapticBots",description:"Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots",title:"HapticBots: Distributed Encountered-type Haptics for VR with Multiple Shape-changing Mobile Robots",authors:["Ryo Suzuki","Eyal Ofek","Mike Sinclair","Daniel Leithinger","Mar Gonzalez-Franco"],year:2021,booktitle:"In Proceedings of the 34th Annual ACM Symposium on User Interface Software and Technology (UIST '21)",publisher:"ACM, New York, NY, USA",pages:"1-13",doi:"https://doi.org/10.1145/3472749.3474821",conference:{name:"UIST 2021",fullname:"The ACM Symposium on User Interface Software and Technology (UIST 2021)",url:"http://uist.acm.org/uist2021"},pdf:"uist-2021-hapticbots.pdf",slide:"uist-2021-hapticbots-slide.pdf","acm-dl":"https://doi.org/10.1145/3472749.3474821",video:"https://www.youtube.com/watch?v=s5NVJMYhfjk","short-video":"https://www.youtube.com/watch?v=HTiZgOESJyQ",embed:"https://www.youtube.com/embed/s5NVJMYhfjk",github:"https://github.com/ryosuzuki/hapticbots",arxiv:"https://arxiv.org/abs/2108.10829",pageCount:13,slideCount:21,bodyContent:'# Abstract\n\nHapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic approaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability---these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in- time touch-points on the user’s hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various ap- plications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.\n\n<video poster="/static/projects/hapticbots/video-poster/top.jpg" preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/hapticbots/video/top.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-1.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-1.jpg" /></a>\n  </div>\n</div>\n\n\n# Introduction\n\nEfective haptic feedback promises to enrich Virtual Reality (VR) experiences in many application domains [17], but supporting general-purpose haptic feedback is still a difcult challenge. A common approach to providing haptic feedback for VR is to use a hand-held or wearable device. However, these wearable hand- grounded devices are inherently limited in their ability to render a world grounded force, such as surfaces that can be touched or pushed with the user’s hand.\n\nTo fll this gap, **encountered-type haptics** are introduced as an alternative approach. In contrast to hand-held or wearable devices, the encountered-type haptics provide haptic sensations through actuated physical environments by dynamically moving physical objects or transforming the physical shape when the user encounters the virtual object.\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-2-2.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-2-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-2-4.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-2-4.jpg" /></a>\n  </div>\n</div>\n\nDiferent approaches have been developed for encountered-type haptics: from grounded robotic arms (e.g., Snake Charmer, VR- Robot) to shape displays (e.g., shapeShift, Feelex, inForce). However, the current approaches still face a number of challenges and limitations. For example, shape displays often require large, heavy, and mechanically complex devices, reducing reliability and deployability of the system for use outside research labs. Also, the resolution fdelity and the display’s size are still limited, making it difcult to render smooth and continuous surfaces across a large interaction area. Alternately, robotic arms can bring a small piece of a surface to meet the user hand on demand, but the speed at which humans move challenges the ability to cover just in time large interaction spaces with a single device. Scaling the number of robotic arms is also a challenge as complex 3D path planning is required to avoid unnecessary collision with both the user and the other arms.\n\n\n# Distributed Encountered-type Haptics\n\n\n<video poster="/static/projects/hapticbots/video-poster/surface.jpg" preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/hapticbots/video/surface.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-3-1.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-3-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-3-2.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-3-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-3-3.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-3-3.jpg" /></a>\n  </div>\n</div>\n\n\n\nThis paper introduces a novel encountered-type haptics approach, which we call **distributed encountered-type haptics**. Distributed encountered-type haptics employ multiple shape-changing mobile robots to simulate a consistent physical object that the user can encounter through hands or fngers. By synchronously controlling multiple robots, these robots can approximate diferent objects and surfaces distributed in a large interaction area.\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-4-1.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-4-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-4-2.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-4-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-4-3.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-4-3.jpg" /></a>\n  </div>\n</div>\n\n\n\nOur proposed approach enables deployable, scalable, and general- purpose encountered-type haptics for VR, providing a number of advantages compared to the existing approaches, including shape displays, robotic arms, and non-transformable mobile robots.\n\n1. **Deployable**: Each mobile robot is light and compact, making the system portable and easy to deploy.\n\n2. **Scalable**: Since each robot is simple and modular, it can scale to increase the number of touch-points and covered area. Moreover, the use of multiple robots can reduce the average distance that a robot needs to travel, which reduces the robots’ speed requirements.\n\n3. **General-purpose**: Finally, the shape-changing capability of each robot can signifcantly increase the expressive- ness of haptic rendering by transforming itself to closely match with the virtual object on-demand and in real-time. This allows for greater fexibility needed for general-purpose applications.\n\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-6.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-6.jpg" /></a>\n  </div>\n</div>\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-5-1.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-5-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-5-2.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-5-2.jpg" /></a>\n  </div>\n</div>\n\n\n\n# HapticBots\n\nTo demonstrate this idea, we built HapticBots, an open source 1 tabletop shape- changing mobile robots that are specifcally designed for distributed encountered-type haptics. HapticBots consists of of-the-shelf mo- bile robots (Sony TOIO), and custom height-changing mechanisms to haptically render general large surfaces with varying normal directions (-60 to 60 degrees). It can cover a large space (55 cm × 55 cm) above the table (a dynamic range of 24 cm elevation) at high speed (24 cm/sec and 2.8 cm/sec for horizontal and vertical speed, respectively). Each robot is compact (4.7 × 4.7 × 8 cm, 135 g) and its tracking system consists of an expandable, pattern-printed paper mat; thus, it is portable and deployable.\n\nOur HapticBots’ hardware design is inspired by ShapeBots, but as far as we know, our system is the frst exploration of using multiple tabletop shape-changing robots for VR haptics. Apply- ing to VR haptics introduces a set of challenging requirements, which led to a new distributed haptics system design as well as to new hardware for each of the robots: 1) Efcient path planning integrated with real-time hand tracking: The system coordinates the movements of all robots with the user’s hand. We track and anticipate potential touch points at a high frame rate (60 FPS) and guide the robots to encounter the user’s hands in a just in time fashion. 2) Precise height and tilt control: In contrast to ShapeBots’ open-loop system, HapticBots enables more precise height and tilt control with embedded encoders and closed-loop control system to render surfaces with varying normal angles. 3) Actuator robust- ness: We vastly improved actuator force by around 70x (21.8N vs. 0.3N holding force of ShapeBots) to provide meaningful force feedback. In addition to these technical contributions, we developed various VR applications to demonstrate the new possibilities for encoun- tered haptics, including remote collaboration, medical training, 3D modeling, and entertainment.\n\n\n<video poster="/static/projects/hapticbots/video-poster/robot.jpg" preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/hapticbots/video/robot.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-7-2.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-7-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-7-1.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-7-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-7-4.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-7-4.jpg" /></a>\n  </div>\n</div>\n\n\nOur HapticBots linear actuator is designed to achieve all of these requirements with reasonable force capabilities. Figure 5 illustrates the mechanical design of a linear actuator. In our design, the two retractable metal tapes on motorized reels occupy a small footprint but extend and hold their shape while resisting modest loads in certain directions. Our reel-based linear actuator uses compact DC motors (Pololu 1000:1 Micro Metal Gearmotor HP 6V, Product No. 2373). This motor has a cross-section of 1.0 × 1.2 cm and its length is 2.9 cm. The no-load speed of the geared motor is 31 rpm, which extends the metal tape at 2.8 cm/sec. The motor’s maximum stall torque is 12 kg·cm. We accommodate two motors placed side by side to minimize the overall footprint size.\n\nFor the reel, we use an of-the-shelf metal tape measure reel (Crescent Lufkin CS8506 1/2 x 6 inch metal tape measure). The material choice of this reel is one of the key design considerations as it determines the vertical load-bearing capability. On the other hand, a strong material makes it more difcult for this small DC motor to successfully rotate and rotate the reel. After the test of eight diferent tape measures devices with various materials, stifnesses, thicknesses, and widths, we determined the Crescent Lufkin CS8506 tape measure to work most reliably in our setting. The tape has 0.15 mm thickness and is 1.2 cm (1/2 inch) width wide, and slightly curved to avoid buckling. We cut this tape measure to 36 cm and drilled a 3 mm hole at the end to fx it to the shaft with an M3 screw.\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-11-1.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-11-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-11-2.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-11-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-11-3.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-11-3.jpg" /></a>\n  </div>\n</div>\n\n<video poster="/static/projects/hapticbots/video-poster/surface-3.jpg" preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/hapticbots/video/surface-3.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-12-1.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-12-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-12-2.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-12-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-12-3.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-12-3.jpg" /></a>\n  </div>\n</div>\n\nOur system consists of multiple coordinated height-changing robots and the associated VR software. Each robot is made of 1) custom-built shape-changing mechanisms with reel-based actuators, and 2) an off-the-shelf mobile robot (Sony Toio) that can move on a mat printed with a pattern for position tracking. For the VR system, we used Oculus Quest HMD and its hand tracking capability for interaction. The software system synchronizes virtual scenes with physical environment (e.g., each robot’s position, orientation, and height), so that the robots can provide a haptic sensation in a timely manner. This section describes the design and implementation of the both hardware and software systems, then provides technical evaluation of HapticBots prototype.\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-9-1.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-9-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-9-2.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-9-2.jpg" /></a>\n  </div>\n</div>\n\nFor the virtual reality environment and gestural tracking, we use an Oculus Quest HMD with its built- in hand tracking mechanism. The main computer (MacBook Pro 16 inch, Core i9, 32GB RAM) runs as the Node.js server. The server controls and communicates with the linear actu- ators and the Unity application on an Oculus Quest through Wi-Fi (with UDP and Websocket protocol, respectively) and Toio robots through Bluetooth. The host computer communicates with seven Toio robots through Bluetooth V4.2 (Bluetooth Low Energy). The HapticBots computer operates at 60 FPS for tracking and control.\n\nWe use the Unity game engine to render a virtual environment. As each robot moves along the planar surface, it constantly changes its height and orien- tation to best ft the virtual surface above it. To obtain the target height and surface normal of the robot, the system uses a vertical ray casting to measure the height of the virtual contact point given its position on the desk.\n\n\n<video poster="/static/projects/hapticbots/video-poster/control.jpg" preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/hapticbots/video/control.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-8-1.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-8-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-8-2.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-8-2.jpg" /></a>\n  </div>\n</div>\n\n\n\n# Applications\n\nVR is an accessible way to create realistic training setups to improve skills or prepare for complex situations before they happen in real life. With its fast encounter-type approach, users of HapticBots can train their muscle memory to learn where diferent physical elements such as the interface of a fight cockpit are located (Figure 2). HapticBots can simulate continuous surfaces, and the robots can follow the user’s fngers as they move and even elevate them during palpation diagnostics. These features could be relevant for medical education and surgery training.\n\n\n<video poster="/static/projects/hapticbots/video-poster/applications.jpg" preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/hapticbots/video/applications.mp4" type="video/mp4"></source>\n</video>\n\n\n\n### Design and 3D Modeling\nIn addition to its continuous shape rendering capabilities, the design of HapticBots being based on dual actuators makes the system robust to lateral bending and provides the ability to control diferent tilts to render topography of a terrain surface. This enables activities like map and city exploration or terrain simulation, which can be necessary for architectural design or virtual scene/object modeling\n\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-14-1.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-14-1.jpg" /></a>\n  </div>\n</div>\n\n### Remote Collaboration\nTangible interfaces can enrich remote collaboration through shared synchronized physical objects [7]. Using two connected HapticBots setups, we can reproduce remote physical objects, or introduce shared virtual objects. Figure 17 shows an example of a chess game application where the user moves the chess fgures phys- ically through robots. As a user is replacing an opponent piece from the board, she can feel the robots at the correct place on the board. This interaction could extend to multiple end points to create shared, distributed multi-user spaces.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-14-2.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-14-2.jpg" /></a>\n  </div>\n</div>\n\n### Gaming and Entertainment\nWorld-building games like Minecraft often rely on players con- structing terrains and objects. However the lack of haptics distracts from the immersive experience. HapticBots can augment the game experience during construction or game play in these VR games. Apart from the previously mentioned interactions to grab, push, and encounter, multiple robots can act in coordinated ways to simulate larger objects. They can also provide proxy objects that interact with additional props and game controllers, such as an axe in Minecraft\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/hapticbots/figure-14-3.jpg" data-lightbox="lightbox"><img src="/static/projects/hapticbots/figure-14-3.jpg" /></a>\n  </div>\n</div>',bodyHtml:"<h1>Abstract</h1>\n<p>HapticBots introduces a novel encountered-type haptic approach for Virtual Reality (VR) based on multiple tabletop-size shape-changing robots. These robots move on a tabletop and change their height and orientation to haptically render various surfaces and objects on-demand. Compared to previous encountered-type haptic approaches like shape displays or robotic arms, our proposed approach has an advantage in deployability, scalability, and generalizability---these robots can be easily deployed due to their compact form factor. They can support multiple concurrent touch points in a large area thanks to the distributed nature of the robots. We propose and evaluate a novel set of interactions enabled by these robots which include: 1) rendering haptics for VR objects by providing just-in- time touch-points on the user’s hand, 2) simulating continuous surfaces with the concurrent height and position change, and 3) enabling the user to pick up and move VR objects through graspable proxy objects. Finally, we demonstrate HapticBots with various ap- plications, including remote collaboration, education and training, design and 3D modeling, and gaming and entertainment.</p>\n<p>&lt;video poster=&quot;/static/projects/hapticbots/video-poster/top.jpg&quot; preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/hapticbots/video/top.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>Introduction</h1>\n<p>Efective haptic feedback promises to enrich Virtual Reality (VR) experiences in many application domains [17], but supporting general-purpose haptic feedback is still a difcult challenge. A common approach to providing haptic feedback for VR is to use a hand-held or wearable device. However, these wearable hand- grounded devices are inherently limited in their ability to render a world grounded force, such as surfaces that can be touched or pushed with the user’s hand.</p>\n<p>To fll this gap, <strong>encountered-type haptics</strong> are introduced as an alternative approach. In contrast to hand-held or wearable devices, the encountered-type haptics provide haptic sensations through actuated physical environments by dynamically moving physical objects or transforming the physical shape when the user encounters the virtual object.</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-2-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-2-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-2-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-2-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-2-4.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-2-4.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>Diferent approaches have been developed for encountered-type haptics: from grounded robotic arms (e.g., Snake Charmer, VR- Robot) to shape displays (e.g., shapeShift, Feelex, inForce). However, the current approaches still face a number of challenges and limitations. For example, shape displays often require large, heavy, and mechanically complex devices, reducing reliability and deployability of the system for use outside research labs. Also, the resolution fdelity and the display’s size are still limited, making it difcult to render smooth and continuous surfaces across a large interaction area. Alternately, robotic arms can bring a small piece of a surface to meet the user hand on demand, but the speed at which humans move challenges the ability to cover just in time large interaction spaces with a single device. Scaling the number of robotic arms is also a challenge as complex 3D path planning is required to avoid unnecessary collision with both the user and the other arms.</p>\n<h1>Distributed Encountered-type Haptics</h1>\n<p>&lt;video poster=&quot;/static/projects/hapticbots/video-poster/surface.jpg&quot; preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/hapticbots/video/surface.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-3-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-3-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-3-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-3-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-3-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-3-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>This paper introduces a novel encountered-type haptics approach, which we call <strong>distributed encountered-type haptics</strong>. Distributed encountered-type haptics employ multiple shape-changing mobile robots to simulate a consistent physical object that the user can encounter through hands or fngers. By synchronously controlling multiple robots, these robots can approximate diferent objects and surfaces distributed in a large interaction area.</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-4-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-4-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-4-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-4-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-4-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-4-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>Our proposed approach enables deployable, scalable, and general- purpose encountered-type haptics for VR, providing a number of advantages compared to the existing approaches, including shape displays, robotic arms, and non-transformable mobile robots.</p>\n<ol>\n<li>\n<p><strong>Deployable</strong>: Each mobile robot is light and compact, making the system portable and easy to deploy.</p>\n</li>\n<li>\n<p><strong>Scalable</strong>: Since each robot is simple and modular, it can scale to increase the number of touch-points and covered area. Moreover, the use of multiple robots can reduce the average distance that a robot needs to travel, which reduces the robots’ speed requirements.</p>\n</li>\n<li>\n<p><strong>General-purpose</strong>: Finally, the shape-changing capability of each robot can signifcantly increase the expressive- ness of haptic rendering by transforming itself to closely match with the virtual object on-demand and in real-time. This allows for greater fexibility needed for general-purpose applications.</p>\n</li>\n</ol>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-6.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-6.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-5-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-5-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-5-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-5-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>HapticBots</h1>\n<p>To demonstrate this idea, we built HapticBots, an open source 1 tabletop shape- changing mobile robots that are specifcally designed for distributed encountered-type haptics. HapticBots consists of of-the-shelf mo- bile robots (Sony TOIO), and custom height-changing mechanisms to haptically render general large surfaces with varying normal directions (-60 to 60 degrees). It can cover a large space (55 cm × 55 cm) above the table (a dynamic range of 24 cm elevation) at high speed (24 cm/sec and 2.8 cm/sec for horizontal and vertical speed, respectively). Each robot is compact (4.7 × 4.7 × 8 cm, 135 g) and its tracking system consists of an expandable, pattern-printed paper mat; thus, it is portable and deployable.</p>\n<p>Our HapticBots’ hardware design is inspired by ShapeBots, but as far as we know, our system is the frst exploration of using multiple tabletop shape-changing robots for VR haptics. Apply- ing to VR haptics introduces a set of challenging requirements, which led to a new distributed haptics system design as well as to new hardware for each of the robots: 1) Efcient path planning integrated with real-time hand tracking: The system coordinates the movements of all robots with the user’s hand. We track and anticipate potential touch points at a high frame rate (60 FPS) and guide the robots to encounter the user’s hands in a just in time fashion. 2) Precise height and tilt control: In contrast to ShapeBots’ open-loop system, HapticBots enables more precise height and tilt control with embedded encoders and closed-loop control system to render surfaces with varying normal angles. 3) Actuator robust- ness: We vastly improved actuator force by around 70x (21.8N vs. 0.3N holding force of ShapeBots) to provide meaningful force feedback. In addition to these technical contributions, we developed various VR applications to demonstrate the new possibilities for encoun- tered haptics, including remote collaboration, medical training, 3D modeling, and entertainment.</p>\n<p>&lt;video poster=&quot;/static/projects/hapticbots/video-poster/robot.jpg&quot; preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/hapticbots/video/robot.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-7-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-7-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-7-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-7-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-7-4.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-7-4.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>Our HapticBots linear actuator is designed to achieve all of these requirements with reasonable force capabilities. Figure 5 illustrates the mechanical design of a linear actuator. In our design, the two retractable metal tapes on motorized reels occupy a small footprint but extend and hold their shape while resisting modest loads in certain directions. Our reel-based linear actuator uses compact DC motors (Pololu 1000:1 Micro Metal Gearmotor HP 6V, Product No. 2373). This motor has a cross-section of 1.0 × 1.2 cm and its length is 2.9 cm. The no-load speed of the geared motor is 31 rpm, which extends the metal tape at 2.8 cm/sec. The motor’s maximum stall torque is 12 kg·cm. We accommodate two motors placed side by side to minimize the overall footprint size.</p>\n<p>For the reel, we use an of-the-shelf metal tape measure reel (Crescent Lufkin CS8506 1/2 x 6 inch metal tape measure). The material choice of this reel is one of the key design considerations as it determines the vertical load-bearing capability. On the other hand, a strong material makes it more difcult for this small DC motor to successfully rotate and rotate the reel. After the test of eight diferent tape measures devices with various materials, stifnesses, thicknesses, and widths, we determined the Crescent Lufkin CS8506 tape measure to work most reliably in our setting. The tape has 0.15 mm thickness and is 1.2 cm (1/2 inch) width wide, and slightly curved to avoid buckling. We cut this tape measure to 36 cm and drilled a 3 mm hole at the end to fx it to the shaft with an M3 screw.</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-11-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-11-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-11-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-11-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-11-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-11-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;video poster=&quot;/static/projects/hapticbots/video-poster/surface-3.jpg&quot; preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/hapticbots/video/surface-3.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-12-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-12-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-12-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-12-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-12-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-12-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>Our system consists of multiple coordinated height-changing robots and the associated VR software. Each robot is made of 1) custom-built shape-changing mechanisms with reel-based actuators, and 2) an off-the-shelf mobile robot (Sony Toio) that can move on a mat printed with a pattern for position tracking. For the VR system, we used Oculus Quest HMD and its hand tracking capability for interaction. The software system synchronizes virtual scenes with physical environment (e.g., each robot’s position, orientation, and height), so that the robots can provide a haptic sensation in a timely manner. This section describes the design and implementation of the both hardware and software systems, then provides technical evaluation of HapticBots prototype.</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-9-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-9-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-9-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-9-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>For the virtual reality environment and gestural tracking, we use an Oculus Quest HMD with its built- in hand tracking mechanism. The main computer (MacBook Pro 16 inch, Core i9, 32GB RAM) runs as the Node.js server. The server controls and communicates with the linear actu- ators and the Unity application on an Oculus Quest through Wi-Fi (with UDP and Websocket protocol, respectively) and Toio robots through Bluetooth. The host computer communicates with seven Toio robots through Bluetooth V4.2 (Bluetooth Low Energy). The HapticBots computer operates at 60 FPS for tracking and control.</p>\n<p>We use the Unity game engine to render a virtual environment. As each robot moves along the planar surface, it constantly changes its height and orien- tation to best ft the virtual surface above it. To obtain the target height and surface normal of the robot, the system uses a vertical ray casting to measure the height of the virtual contact point given its position on the desk.</p>\n<p>&lt;video poster=&quot;/static/projects/hapticbots/video-poster/control.jpg&quot; preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/hapticbots/video/control.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-8-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-8-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-8-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-8-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>Applications</h1>\n<p>VR is an accessible way to create realistic training setups to improve skills or prepare for complex situations before they happen in real life. With its fast encounter-type approach, users of HapticBots can train their muscle memory to learn where diferent physical elements such as the interface of a fight cockpit are located (Figure 2). HapticBots can simulate continuous surfaces, and the robots can follow the user’s fngers as they move and even elevate them during palpation diagnostics. These features could be relevant for medical education and surgery training.</p>\n<p>&lt;video poster=&quot;/static/projects/hapticbots/video-poster/applications.jpg&quot; preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/hapticbots/video/applications.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<h3>Design and 3D Modeling</h3>\n<p>In addition to its continuous shape rendering capabilities, the design of HapticBots being based on dual actuators makes the system robust to lateral bending and provides the ability to control diferent tilts to render topography of a terrain surface. This enables activities like map and city exploration or terrain simulation, which can be necessary for architectural design or virtual scene/object modeling</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-14-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-14-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h3>Remote Collaboration</h3>\n<p>Tangible interfaces can enrich remote collaboration through shared synchronized physical objects [7]. Using two connected HapticBots setups, we can reproduce remote physical objects, or introduce shared virtual objects. Figure 17 shows an example of a chess game application where the user moves the chess fgures phys- ically through robots. As a user is replacing an opponent piece from the board, she can feel the robots at the correct place on the board. This interaction could extend to multiple end points to create shared, distributed multi-user spaces.</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-14-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-14-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h3>Gaming and Entertainment</h3>\n<p>World-building games like Minecraft often rely on players con- structing terrains and objects. However the lack of haptics distracts from the immersive experience. HapticBots can augment the game experience during construction or game play in these VR games. Apart from the previously mentioned interactions to grab, push, and encounter, multiple robots can act in coordinated ways to simulate larger objects. They can also provide proxy objects that interact with additional props and game controllers, such as an axe in Minecraft</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/hapticbots/figure-14-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/hapticbots/figure-14-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n",dir:"content/output/projects",base:"hapticbots.json",ext:".json",sourceBase:"hapticbots.md",sourceExt:".md"}},"8gHz":function(t,e,o){var i=o("5K7Z"),a=o("eaoh"),n=o("UWiX")("species");t.exports=function(t,e){var o,s=i(t).constructor;return void 0===s||null==(o=i(s)[n])?e:a(o)}},"8iia":function(t,e,o){var i=o("QMMT"),a=o("RRc/");t.exports=function(t){return function(){if(i(this)!=t)throw TypeError(t+"#toJSON isn't generic");return a(this)}}},"8v4N":function(t){t.exports={id:"realitycanvas",name:"RealityCanvas",description:"Augmented Reality Sketching for Embedded and Responsive Scribble Animation Effects",title:"RealityCanvas: Augmented Reality Sketching for Embedded and Responsive Scribble Animation Effects",authors:["Zhijie Xia","Kyzyl Monteiro","Kevin Van","Ryo Suzuki"],year:2023,booktitle:"In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (UIST '23)",publisher:"ACM, New York, NY, USA",conference:{name:"UIST 2023",fullname:"The ACM Symposium on User Interface Software and Technology (UIST 2023)",url:"http://uist.acm.org/uist2023"},pdf:"uist-2023-realitycanvas.pdf",bodyContent:"# Abstract\n\nWe introduce RealityCanvas, a mobile AR sketching tool that can easily augment real-world physical motion with responsive hand-drawn animation. Recent research in AR sketching tools has enabled users to not only embed static drawings into the real world but also dynamically animate them with physical motion. However, existing tools often lack the flexibility and expressiveness of possible animations, as they primarily support simple line-based geometry. To address this limitation, we explore both expressive and improvisational AR sketched animation by introducing a set of responsive scribble animation techniques that can be directly embedded through sketching interactions: 1) object binding, 2) flip-book animation, 3) action trigger, 4) particle effects, 5) motion trajectory, and 6) contour highlight. These six animation effects were derived from the analysis of 172 existing video-edited scribble animations. We showcase these techniques through various applications, such as video creation, augmented education, storytelling, and AR prototyping. The results of our user study and expert interviews confirm that our tool can lower the barrier to creating AR-based sketched animation, while allowing creative, expressive, and improvisational AR sketching experiences.",bodyHtml:"<h1>Abstract</h1>\n<p>We introduce RealityCanvas, a mobile AR sketching tool that can easily augment real-world physical motion with responsive hand-drawn animation. Recent research in AR sketching tools has enabled users to not only embed static drawings into the real world but also dynamically animate them with physical motion. However, existing tools often lack the flexibility and expressiveness of possible animations, as they primarily support simple line-based geometry. To address this limitation, we explore both expressive and improvisational AR sketched animation by introducing a set of responsive scribble animation techniques that can be directly embedded through sketching interactions: 1) object binding, 2) flip-book animation, 3) action trigger, 4) particle effects, 5) motion trajectory, and 6) contour highlight. These six animation effects were derived from the analysis of 172 existing video-edited scribble animations. We showcase these techniques through various applications, such as video creation, augmented education, storytelling, and AR prototyping. The results of our user study and expert interviews confirm that our tool can lower the barrier to creating AR-based sketched animation, while allowing creative, expressive, and improvisational AR sketching experiences.</p>\n",dir:"content/output/projects",base:"realitycanvas.json",ext:".json",sourceBase:"realitycanvas.md",sourceExt:".md"}},"9BDd":function(t,e,o){o("GvbO"),t.exports=o("WEpk").Array.isArray},"9EOK":function(t,e,o){"use strict";var i=function(t){if(t&&t.__esModule)return t;var e={};if(null!=t)for(var o in t)Object.hasOwnProperty.call(t,o)&&(e[o]=t[o]);return e.default=t,e};Object.defineProperty(e,"__esModule",{value:!0});var a=i(o("q1tI"));e.RouterContext=a.createContext(null)},"9Jkg":function(t,e,o){t.exports=o("oh+g")},"9gtZ":function(t){t.exports={id:"roomshift",name:"RoomShift",description:"Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots",title:"RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots",authors:["Ryo Suzuki","Hooman Hedayati","Clement Zheng","James Bohn","Daniel Szafir","Ellen Yi-Luen Do","Mark D Gross","Daniel Leithinger"],year:2020,booktitle:"In Proceedings of the ACM CHI Conference on Human Factors in Computing Systems (CHI '20)",publisher:"ACM, New York, NY, USA",pages:"Paper 396, 11 pages",doi:"https://doi.org/10.1145/3313831.3376523",conference:{name:"CHI 2020",fullname:"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2020)",url:"https://chi2020.acm.org/"},video:"https://www.youtube.com/watch?v=6_PAirnlDnk",embed:"https://www.youtube.com/embed/6_PAirnlDnk","short-video":"https://www.youtube.com/watch?v=4OWU60gTOFE",pdf:"chi-2020-roomshift.pdf",arxiv:"https://arxiv.org/abs/2008.08695","acm-dl":"https://dl.acm.org/doi/10.1145/3313831.3376523",pageCount:11,slideCount:0,bodyContent:'# Abstract\n\nThis paper presents RoomShift, a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/roomshift/video/top.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-1-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-1-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-1-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-1-2.jpg" /></a>\n  </div>\n</div>\n\n# Introduction\n\nThere is a clear need to provide haptic sensations in virtual environments. Recent advances in display and tracking technologies promise immersive experience in virtual reality, but objects seen in VR such as walls and furniture are only visual: the user cannot touch, feel, sit on, or place objects on them. This limits the sense of full immersion in the virtual world. To overcome these limitations, various haptic interfaces have been explored. In the previous work, most haptic interfaces focus on finger-tip haptic feedback with actuated controllers or on-body haptic sensations with wearable devices. In contrast, encountered-type haptic feedback with a dynamic environment promises to increase the immersion of virtual experiences, which are difficult to achieve using an only handheld or wearable haptic devices. Through a dynamic haptic environment, users can touch and interact with the whole virtual scene with their bodies --- they can walk, sit on, and lean against objects in the VR environment. Existing approaches for actuated environments, however, are often limited in speed of transformation (e.g., slow transformation with inflatables) and the range of supported interactions (e.g., only walking).\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/roomshift/video/wall.mp4" type="video/mp4"></source>\n</video>\n\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-9-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-9-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-9-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-9-2.jpg" /></a>\n  </div>\n</div>\n\nThis paper introduces RoomShift, a room-scale dynamic haptic environment for virtual reality. RoomShift provides haptic sensations by reconfiguring physical environments using a small swarm of robot assistants. Inspired by shelf-moving robots that are used in robotic warehouses, we developed a swarm of shape-changing robots that can move a range of existing furniture. Each robot has a mechanical lift that extends from 30 cm to 100 cm to pick up, carry, and place objects such as chairs, tables, and walls. This way, users can touch, sit, place, and lean against objects in the virtual environment.\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-2-1.png" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-2-2.png" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-2-2.png" /></a>\n  </div>\n</div>\n\n\n# RoomShift: Furniture-moving Robots\n\nRoomShift consists of a small swarm of shape-changing robots; each robot uses a Roomba as a mobile base. On this base is mounted a custom mechanical scissor lift made of two linear actuators and a metal drying rack. As the mechanical lift is compact in its closed state, the robot can move under a table or chair with 30 cm clearance, and extend the scissor lift to pick it up.\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/roomshift/video/carry.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-3-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-3-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-3-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-3-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-3-3.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-3-3.jpg" /></a>\n  </div>\n</div>\n\nRoomShift comprises nine shape-changing swarm robots based on the Roomba Create 2. For the mechanical lift structure, we repurposed an off-the-shelf expandable laundry rack (Room Essentials Compact Drying Rack) and attached two linear actuators (Homend DC12V 8 inch Stroke Linear Actuator, which extends from 32 cm to 52 cm) at the base of the rack. The linear actuators are fixed to the endpoints of the scissor structure with 8 mm steel rods, so that when the actuator contracts, the mounted scissor structure extends vertically (from 30 cm to 100 cm). The scissor structure moves at a speed of 1.3 cm / sec. To mount the scissor structure, we fixed a 6mm acrylic bottom plate (35 cm x 35 cm) and four omni-directional casters (Dorhea Ball Transfer Bearing Unit) to relieve the Roomba of most of the weight that the robot carries. Each robot moves at 20 cm / sec. Figure 3 illustrates the mechanical design of each RoomShift robot.\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-4-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-4-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-4-2.png" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-4-2.png" /></a>\n  </div>\n</div>\n\nOne advantage of our approach is that the robot need not support the weight of the user. Once the robot places the furniture, it serves as a static object. Thus, when a user sits on or puts weight on it, all of the weight goes to the furniture, instead of the robot, which significantly reduces the possibility of a mechanical breakdown.\nAlthough the maximum load for the Roomba is 9 kg, the corner-mounted casters distribute and carry heavier loads. Thus, our robots can lift and carry heavier objects than an unmodified Roomba. The maximum weight the robot can lift and carry is 22 kg. When we put a heavier object than 23 kg, we observed the scissor structure started to break. The strength of the scissor structure suffices to lift lightweight chairs and tables, such as the IKEA honeycomb furniture used in our prototypes. The weight of the furniture we have tested (depicted in Figure) ranges from 3.5 to 11.2 kg. For heavier objects, multiple robots can also coordinate to lift a piece together if there is sufficient space under the furniture. Also, with a more robust scissor structure, we can carry heavier objects, as we observed the Roomba base itself (with the corner-mounted casters) can carry up to 30 kg load.\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-5-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-5-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-5-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-5-2.jpg" /></a>\n  </div>\n</div>\n\nThis approach also increases flexibility because different types of furniture can be actuated with the height-adjustable scissor lift. For example, Figure illustrates various static props that the RoomShift robot can actuate. These objects include furniture such as a desk, a long table, different chairs, and a side table. Note that due to the robot’s minimum collapsed size, objects must have at least 30 cm clearance below them, and enough horizontal space to fit the robot. A designer can also create custom props for specific applications, for instance, the styrofoam wall mounted to a side table seen in the Figure.\n\n\n# Tracking and Control\n\nTo accurately control the RoomShift robots, we require precise motion tracking that can cover the play area in which a user walks. We use an optical tracking system with 20 IR cameras (Qualisys Miqus 5) that can track objects in a 10 m × 10 m space. The system tracks six degrees of freedom (DOF) position of the objects\nwith retro-reflective spherical markers at 60 FPS frame rate.\nTo track the robots as well as physical props, we attached five 30 mm spherical retroreflective markers. For the robot, we attached markers to a pair of parallel bars, so that the markers’ relative positions remain constant regardless of the height of the scissor lift. We can also estimate the height of the scissor structure by measuring the orientation of the marker pattern (the pink plane surface depicted in the Figure).\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/roomshift/video/tracking.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-7-3.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-7-3.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-7-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-7-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-7-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-7-2.jpg" /></a>\n  </div>\n</div>\n\nTo control the robots’ movements, we use a simple path planning algorithm. The input is 1) the current positions of the robots, 2) the positions of obstacles (e.g., furniture, other robots, and users), and 3) the target locations. The algorithm outputs the goal of each robot at the next time step. The system continuously updates the path and drives them to their target locations. The main server continuously tracks the robot positions, calculates their wheel speeds, and sends commands at 30 Hz over WiFi.\n\nTo pick up and place these, the robot follows a predefined sequence, approaching the object from an angle where it will not collide with the object’s legs. To avoid the collision with the legs of furniture, each object has a user-defined entry and exit point (Figure 8). We also register the height of target furniture before the system starts (e.g., 70 cm for Table_A, 40 cm for Chair_B), so that it can extend the scissor lift to certain target height. We could also put a simple sensor on top of the scissor structure to make it a closed-loop system.\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-6-1.png" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-6-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-6-2.png" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-6-2.png" /></a>\n  </div>\n</div>\n\nThe main computer runs a Node.js server and the Qualisys tracking software. The 6DOF tracking data that the Qualisys tracking system captures is streamed to the Node.js server through the WebSocket protocol. Based on the tracking data, a web browser client renders the VR scene with A-Frame. The user experiences the VR scene using an Oculus Go head mounted display and its built-in VR browser. We synchronize the desktop computer and the Oculus Go browser with real-time communication through WebSocket. When the virtual scene changes, the system moves the robots to dynamically reconfigure the physical scene. First, the system computes the types of props and each target position based on the relative position from the user.\n\n\n# Interactions and Applications\n\nIn this paper, we specifically focus on architectural application scenarios, such as rendering physical room interiors for virtual real estate tours and collaborative architectural design, two increasingly common application areas for VR. Virtual real estate tours reduce the time and cost compared to on-site viewings, but currently lack the bodily experience of being able to touch surfaces and sit down. In architectural design, VR aids the communication between architects and clients, where proposed designs can be experienced, discussed and modified before building them. We are motivated by how RoomShift can enable people with various physical abilities to experience, test and co-design these environments with their bodies. Most of the elements in these applications can be covered with a finite set of furniture and props (e.g., chairs, desks, and walls). We discuss some of the basic interactions to support these applications.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-8.png" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-8.png" /></a>\n  </div>\n</div>\n\nTo support these scenarios, we propose four types of basic interactions RoomShift can support, with the spectrum between embodied interactions and controller-based interactions, as illustrated in the Figure.\n\n1. Experiencing Architectural Spaces: Walking and Touching\n\n2. Architectural Co-Design: Physically Moving Furniture\n\n3. Navigating Large Spaces: Teleporting in VR\n\n4. Virtual Scene Editing: Virtually Moving Furniture\n\nEmbodied interactions refer to interaction with virtual scenes through physical movements and manipulation. The user can implicitly interact with the system by walking around or explicitly interact with the virtual scene by physically moving furniture. On the other hand, the user can also interact with the virtual scene with controller-based gestural interactions. An example is when the user relocates a distant piece of furniture or remove the wall in the room. The user can also virtually teleport their location to navigate through space.\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-10-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-10-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-10-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-10-2.jpg" /></a>\n  </div>\n</div>\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-11-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-11-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-11-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-11-2.jpg" /></a>\n  </div>\n</div>\n\nFor example, the most basic interaction is to render an architectural space that the user can walk around in and touch.  As the user walks around the space, the robots move the props to maintain the illusion of a larger number of objects.\nIn addition, the system can mimic larger objects with a single moving robot. For example, when the user is interacting with a large table, either new physical table segments can be added or a single robot can continually move the current table according to the user’s position to simulate touching a larger one.\n\nAlso, RoomShift supports teleportation by reconfiguring the room layout to match the new view location. When the user teleports to a new location in the VR scene, the system calculates the positions of the virtual objects relative to the new location and moves the furniture and robots in and out of the play area to enable a fast scene reconfiguration and to avoid collisions with the user and each other.',bodyHtml:"<h1>Abstract</h1>\n<p>This paper presents RoomShift, a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/roomshift/video/top.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-1-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-1-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-1-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-1-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>Introduction</h1>\n<p>There is a clear need to provide haptic sensations in virtual environments. Recent advances in display and tracking technologies promise immersive experience in virtual reality, but objects seen in VR such as walls and furniture are only visual: the user cannot touch, feel, sit on, or place objects on them. This limits the sense of full immersion in the virtual world. To overcome these limitations, various haptic interfaces have been explored. In the previous work, most haptic interfaces focus on finger-tip haptic feedback with actuated controllers or on-body haptic sensations with wearable devices. In contrast, encountered-type haptic feedback with a dynamic environment promises to increase the immersion of virtual experiences, which are difficult to achieve using an only handheld or wearable haptic devices. Through a dynamic haptic environment, users can touch and interact with the whole virtual scene with their bodies --- they can walk, sit on, and lean against objects in the VR environment. Existing approaches for actuated environments, however, are often limited in speed of transformation (e.g., slow transformation with inflatables) and the range of supported interactions (e.g., only walking).</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/roomshift/video/wall.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-9-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-9-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-9-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-9-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>This paper introduces RoomShift, a room-scale dynamic haptic environment for virtual reality. RoomShift provides haptic sensations by reconfiguring physical environments using a small swarm of robot assistants. Inspired by shelf-moving robots that are used in robotic warehouses, we developed a swarm of shape-changing robots that can move a range of existing furniture. Each robot has a mechanical lift that extends from 30 cm to 100 cm to pick up, carry, and place objects such as chairs, tables, and walls. This way, users can touch, sit, place, and lean against objects in the virtual environment.</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-2-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-2-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-2-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-2-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>RoomShift: Furniture-moving Robots</h1>\n<p>RoomShift consists of a small swarm of shape-changing robots; each robot uses a Roomba as a mobile base. On this base is mounted a custom mechanical scissor lift made of two linear actuators and a metal drying rack. As the mechanical lift is compact in its closed state, the robot can move under a table or chair with 30 cm clearance, and extend the scissor lift to pick it up.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/roomshift/video/carry.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-3-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-3-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-3-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-3-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-3-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-3-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>RoomShift comprises nine shape-changing swarm robots based on the Roomba Create 2. For the mechanical lift structure, we repurposed an off-the-shelf expandable laundry rack (Room Essentials Compact Drying Rack) and attached two linear actuators (Homend DC12V 8 inch Stroke Linear Actuator, which extends from 32 cm to 52 cm) at the base of the rack. The linear actuators are fixed to the endpoints of the scissor structure with 8 mm steel rods, so that when the actuator contracts, the mounted scissor structure extends vertically (from 30 cm to 100 cm). The scissor structure moves at a speed of 1.3 cm / sec. To mount the scissor structure, we fixed a 6mm acrylic bottom plate (35 cm x 35 cm) and four omni-directional casters (Dorhea Ball Transfer Bearing Unit) to relieve the Roomba of most of the weight that the robot carries. Each robot moves at 20 cm / sec. Figure 3 illustrates the mechanical design of each RoomShift robot.</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-4-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-4-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-4-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-4-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>One advantage of our approach is that the robot need not support the weight of the user. Once the robot places the furniture, it serves as a static object. Thus, when a user sits on or puts weight on it, all of the weight goes to the furniture, instead of the robot, which significantly reduces the possibility of a mechanical breakdown.\nAlthough the maximum load for the Roomba is 9 kg, the corner-mounted casters distribute and carry heavier loads. Thus, our robots can lift and carry heavier objects than an unmodified Roomba. The maximum weight the robot can lift and carry is 22 kg. When we put a heavier object than 23 kg, we observed the scissor structure started to break. The strength of the scissor structure suffices to lift lightweight chairs and tables, such as the IKEA honeycomb furniture used in our prototypes. The weight of the furniture we have tested (depicted in Figure) ranges from 3.5 to 11.2 kg. For heavier objects, multiple robots can also coordinate to lift a piece together if there is sufficient space under the furniture. Also, with a more robust scissor structure, we can carry heavier objects, as we observed the Roomba base itself (with the corner-mounted casters) can carry up to 30 kg load.</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-5-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-5-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-5-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-5-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>This approach also increases flexibility because different types of furniture can be actuated with the height-adjustable scissor lift. For example, Figure illustrates various static props that the RoomShift robot can actuate. These objects include furniture such as a desk, a long table, different chairs, and a side table. Note that due to the robot’s minimum collapsed size, objects must have at least 30 cm clearance below them, and enough horizontal space to fit the robot. A designer can also create custom props for specific applications, for instance, the styrofoam wall mounted to a side table seen in the Figure.</p>\n<h1>Tracking and Control</h1>\n<p>To accurately control the RoomShift robots, we require precise motion tracking that can cover the play area in which a user walks. We use an optical tracking system with 20 IR cameras (Qualisys Miqus 5) that can track objects in a 10 m × 10 m space. The system tracks six degrees of freedom (DOF) position of the objects\nwith retro-reflective spherical markers at 60 FPS frame rate.\nTo track the robots as well as physical props, we attached five 30 mm spherical retroreflective markers. For the robot, we attached markers to a pair of parallel bars, so that the markers’ relative positions remain constant regardless of the height of the scissor lift. We can also estimate the height of the scissor structure by measuring the orientation of the marker pattern (the pink plane surface depicted in the Figure).</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/roomshift/video/tracking.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-7-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-7-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-7-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-7-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-7-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-7-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>To control the robots’ movements, we use a simple path planning algorithm. The input is 1) the current positions of the robots, 2) the positions of obstacles (e.g., furniture, other robots, and users), and 3) the target locations. The algorithm outputs the goal of each robot at the next time step. The system continuously updates the path and drives them to their target locations. The main server continuously tracks the robot positions, calculates their wheel speeds, and sends commands at 30 Hz over WiFi.</p>\n<p>To pick up and place these, the robot follows a predefined sequence, approaching the object from an angle where it will not collide with the object’s legs. To avoid the collision with the legs of furniture, each object has a user-defined entry and exit point (Figure 8). We also register the height of target furniture before the system starts (e.g., 70 cm for Table_A, 40 cm for Chair_B), so that it can extend the scissor lift to certain target height. We could also put a simple sensor on top of the scissor structure to make it a closed-loop system.</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-6-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-6-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-6-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-6-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>The main computer runs a Node.js server and the Qualisys tracking software. The 6DOF tracking data that the Qualisys tracking system captures is streamed to the Node.js server through the WebSocket protocol. Based on the tracking data, a web browser client renders the VR scene with A-Frame. The user experiences the VR scene using an Oculus Go head mounted display and its built-in VR browser. We synchronize the desktop computer and the Oculus Go browser with real-time communication through WebSocket. When the virtual scene changes, the system moves the robots to dynamically reconfigure the physical scene. First, the system computes the types of props and each target position based on the relative position from the user.</p>\n<h1>Interactions and Applications</h1>\n<p>In this paper, we specifically focus on architectural application scenarios, such as rendering physical room interiors for virtual real estate tours and collaborative architectural design, two increasingly common application areas for VR. Virtual real estate tours reduce the time and cost compared to on-site viewings, but currently lack the bodily experience of being able to touch surfaces and sit down. In architectural design, VR aids the communication between architects and clients, where proposed designs can be experienced, discussed and modified before building them. We are motivated by how RoomShift can enable people with various physical abilities to experience, test and co-design these environments with their bodies. Most of the elements in these applications can be covered with a finite set of furniture and props (e.g., chairs, desks, and walls). We discuss some of the basic interactions to support these applications.</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-8.png&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-8.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>To support these scenarios, we propose four types of basic interactions RoomShift can support, with the spectrum between embodied interactions and controller-based interactions, as illustrated in the Figure.</p>\n<ol>\n<li>\n<p>Experiencing Architectural Spaces: Walking and Touching</p>\n</li>\n<li>\n<p>Architectural Co-Design: Physically Moving Furniture</p>\n</li>\n<li>\n<p>Navigating Large Spaces: Teleporting in VR</p>\n</li>\n<li>\n<p>Virtual Scene Editing: Virtually Moving Furniture</p>\n</li>\n</ol>\n<p>Embodied interactions refer to interaction with virtual scenes through physical movements and manipulation. The user can implicitly interact with the system by walking around or explicitly interact with the virtual scene by physically moving furniture. On the other hand, the user can also interact with the virtual scene with controller-based gestural interactions. An example is when the user relocates a distant piece of furniture or remove the wall in the room. The user can also virtually teleport their location to navigate through space.</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-10-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-10-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-10-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-10-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-11-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-11-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/roomshift/figure-11-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;\n&lt;img src=&quot;/static/projects/roomshift/figure-11-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>For example, the most basic interaction is to render an architectural space that the user can walk around in and touch.  As the user walks around the space, the robots move the props to maintain the illusion of a larger number of objects.\nIn addition, the system can mimic larger objects with a single moving robot. For example, when the user is interacting with a large table, either new physical table segments can be added or a single robot can continually move the current table according to the user’s position to simulate touching a larger one.</p>\n<p>Also, RoomShift supports teleportation by reconfiguring the room layout to match the new view location. When the user teleports to a new location in the VR scene, the system calculates the positions of the virtual objects relative to the new location and moves the furniture and robots in and out of the play area to enable a fast scene reconfiguration and to avoid collisions with the user and each other.</p>\n",dir:"content/output/projects",base:"roomshift.json",ext:".json",sourceBase:"roomshift.md",sourceExt:".md"}},B9jh:function(t,e,o){"use strict";var i=o("Wu5q"),a=o("n3ko");t.exports=o("raTm")("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return i.def(a(this,"Set"),t=0===t?0:t,t)}},i)},BIYM:function(t){t.exports={id:"physica",name:"Physica",description:"Interactive Tangible Physics Simulation based on Tabletop Mobile Robots Towards Explorable Physics Education",title:"Physica: Interactive Tangible Physics Simulation based on Tabletop Mobile Robots Towards Explorable Physics Education",authors:["Jiatong Li","Ryo Suzuki","Ken Nakagaki"],year:2023,booktitle:"In Proceedings of the 2023 ACM Designing Interactive Systems Conference (DIS '23)",publisher:"ACM, New York, NY, USA",doi:"https://doi.org/10.1145/3526113.3545626",conference:{name:"DIS 2023",fullname:"The ACM Designing Interactive Systems Conference (DIS 2023)",url:"https://dis.acm.org/2023/"},pdf:"dis-2023-physica.pdf",video:"https://www.youtube.com/watch?v=7DKpq52282g",embed:"https://www.youtube.com/embed/7DKpq52282g","acm-dl":"https://dl.acm.org/doi/10.1145/3563657.3596037",pageCount:15,slideCount:0,bodyContent:"# Abstract\n\nIn this paper, we introduce Physica, a tangible physics simulation system and approach based on tabletop mobile robots. In Physica, each tabletop robot can physically represent distinct simulated objects that are controlled through an underlying physics simulation, such as gravitational force, molecular movement, and spring force. It aims to bring the benefits of tangible and haptic interaction into explorable physics learning, which was traditionally only available on screen-based interfaces. The system utilizes off-the-shelf mobile robots (Sony Toio) and an open-source physics simulation tool (Teilchen). Built on top of them, we implement the interaction software pipeline that consists of 1) an event detector to reflect tangible interaction by users, and 2) target speed control to minimize the gap between the robot motion and simulated moving objects. To present the potential for physics education, we demonstrate various application scenarios that illustrate different forms of learning using Physica. In our user study, we investigate the effect and the potential of our approach through a perception study and interviews with physics educators.",bodyHtml:"<h1>Abstract</h1>\n<p>In this paper, we introduce Physica, a tangible physics simulation system and approach based on tabletop mobile robots. In Physica, each tabletop robot can physically represent distinct simulated objects that are controlled through an underlying physics simulation, such as gravitational force, molecular movement, and spring force. It aims to bring the benefits of tangible and haptic interaction into explorable physics learning, which was traditionally only available on screen-based interfaces. The system utilizes off-the-shelf mobile robots (Sony Toio) and an open-source physics simulation tool (Teilchen). Built on top of them, we implement the interaction software pipeline that consists of 1) an event detector to reflect tangible interaction by users, and 2) target speed control to minimize the gap between the robot motion and simulated moving objects. To present the potential for physics education, we demonstrate various application scenarios that illustrate different forms of learning using Physica. In our user study, we investigate the effect and the potential of our approach through a perception study and interviews with physics educators.</p>\n",dir:"content/output/projects",base:"physica.json",ext:".json",sourceBase:"physica.md",sourceExt:".md"}},BTJg:function(t){t.exports={id:"expandable-robots",name:"HRI for Expandable Robots",description:"Designing Expandable-Structure Robots for Human-Robot Interaction",title:"Designing Expandable-Structure Robots for Human-Robot Interaction",authors:["Hooman Hedayati","Ryo Suzuki","Wyatt Rees","Daniel Leithinger","Daniel Szafir"],year:2022,booktitle:"Frontiers in Robotics and AI",publisher:"ACM, New York, NY, USA",doi:"https://doi.org/10.3389/frobt.2022.719639",conference:{name:"Frontiers 2022",fullname:"Frontiers in Robotics and AI (Frontiers 2022)",url:"https://www.frontiersin.org/journals/robotics-and-ai"},bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"expandable-robots.json",ext:".json",sourceBase:"expandable-robots.md",sourceExt:".md"}},Bauz:function(t){t.exports={id:"selective-self-assembly",name:"Selective Self-Assembly",description:"Selective Self-Assembly using Re-Programmable Magnetic Pixels",title:"Selective Self-Assembly using Re-Programmable Magnetic Pixels",authors:["Martin Nisser","Yashaswini Makaram","Faraz Faruqi","Ryo Suzuki","Stefanie Mueller"],year:2022,booktitle:"In Proceedings of 2022 IEEE/RSJ In- ternational Conference on Intelligent Robots and Systems (IROS '22)",publisher:"ACM, New York, NY, USA",conference:{name:"IROS 2022",fullname:"In Proceedings of 2022 IEEE/RSJ In- ternational Conference on Intelligent Robots and Systems (IROS 2022)",url:"https://iros2022.org/"},external:"https://hcie.csail.mit.edu/research/selective/selective.html",bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"selective-self-assembly.json",ext:".json",sourceBase:"selective-self-assembly.md",sourceExt:".md"}},Bu4q:function(t,e,o){"use strict";var i=o("KI45"),a=i(o("ln6h")),n=(i(o("pLtp")),i(o("O40h")));Object.defineProperty(e,"__esModule",{value:!0});var s=o("CxY0");function r(){var t=window.location,e=t.protocol,o=t.hostname,i=t.port;return"".concat(e,"//").concat(o).concat(i?":"+i:"")}function c(t){return"string"==typeof t?t:t.displayName||t.name||"Unknown"}function l(t){return t.finished||t.headersSent}function u(){return(u=(0,n.default)(a.default.mark(function t(e,o){var i,n;return a.default.wrap(function(t){for(;;)switch(t.prev=t.next){case 0:t.next=4;break;case 4:if(e.getInitialProps){t.next=6;break}return t.abrupt("return",{});case 6:return t.next=8,e.getInitialProps(o);case 8:if(i=t.sent,!o.res||!l(o.res)){t.next=11;break}return t.abrupt("return",i);case 11:if(i){t.next=14;break}throw n='"'.concat(c(e),'.getInitialProps()" should resolve to an object. But found "').concat(i,'" instead.'),new Error(n);case 14:return t.abrupt("return",i);case 15:case"end":return t.stop()}},t)}))).apply(this,arguments)}e.execOnce=function(t){var e=this,o=!1;return function(){if(!o){o=!0;for(var i=arguments.length,a=new Array(i),n=0;n<i;n++)a[n]=arguments[n];t.apply(e,a)}}},e.getLocationOrigin=r,e.getURL=function(){var t=window.location.href,e=r();return t.substring(e.length)},e.getDisplayName=c,e.isResSent=l,e.loadGetInitialProps=function(t,e){return u.apply(this,arguments)},e.urlObjectKeys=["auth","hash","host","hostname","href","path","pathname","port","protocol","query","search","slashes"],e.formatWithValidation=function(t,e){return s.format(t,e)}},C2SN:function(t,e,o){var i=o("93I4"),a=o("kAMH"),n=o("UWiX")("species");t.exports=function(t){var e;return a(t)&&("function"!=typeof(e=t.constructor)||e!==Array&&!a(e.prototype)||(e=void 0),i(e)&&null===(e=e[n])&&(e=void 0)),void 0===e?Array:e}},CTYI:function(t){t.exports={id:"flux-marker",name:"FluxMarker",description:"Enhancing Tactile Graphics with Dynamic Tactile Markers for Blind People",title:"FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers",authors:["Ryo Suzuki","Abigale Stangl","Mark D. Gross","Tom Yeh"],year:2017,booktitle:"In Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '17)",publisher:"ACM, New York, NY, USA",pages:"190-199",doi:"https://doi.org/10.1145/3132525.3132548",conference:{name:"ASSETS 2017",fullname:"The International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS 2017)",url:"https://assets17.sigaccess.org/"},pdf:"assets-2017-fluxmarker.pdf",video:"https://www.youtube.com/watch?v=VbwIZ9V6i_g",embed:"https://www.youtube.com/embed/VbwIZ9V6i_g",slide:"assets-2017-fluxmarker-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3132548",arxiv:"https://arxiv.org/abs/1708.03783",pageCount:10,slideCount:53,bodyContent:"# Abstract\n\nFor people with visual impairments, tactile graphics are an impor- tant means to learn and explore information. However, raised line tactile graphics created with traditional materials such as emboss- ing are static. While available refreshable displays can dynamically change the content, they are still too expensive for many users, and are limited in size. These factors limit wide-spread adoption and the representation of large graphics or data sets. In this paper, we present FluxMaker, an inexpensive scalable system that renders dy- namic information on top of static tactile graphics with movable tactile markers. These dynamic tactile markers can be easily re- configured and used to annotate static raised line tactile graphics, including maps, graphs, and diagrams. We developed a hardware prototype that actuates magnetic tactile markers driven by low-cost and scalable electromagnetic coil arrays, which can be fabricated with standard printed circuit board manufacturing. We evaluate our prototype with six participants with visual impairments and found positive results across four application areas: location finding or navigating on tactile maps, data analysis, and physicalization, fea- ture identification for tactile graphics, and drawing support. The user study confirms advantages in application domains such as ed- ucation and data exploration.",bodyHtml:"<h1>Abstract</h1>\n<p>For people with visual impairments, tactile graphics are an impor- tant means to learn and explore information. However, raised line tactile graphics created with traditional materials such as emboss- ing are static. While available refreshable displays can dynamically change the content, they are still too expensive for many users, and are limited in size. These factors limit wide-spread adoption and the representation of large graphics or data sets. In this paper, we present FluxMaker, an inexpensive scalable system that renders dy- namic information on top of static tactile graphics with movable tactile markers. These dynamic tactile markers can be easily re- configured and used to annotate static raised line tactile graphics, including maps, graphs, and diagrams. We developed a hardware prototype that actuates magnetic tactile markers driven by low-cost and scalable electromagnetic coil arrays, which can be fabricated with standard printed circuit board manufacturing. We evaluate our prototype with six participants with visual impairments and found positive results across four application areas: location finding or navigating on tactile maps, data analysis, and physicalization, fea- ture identification for tactile graphics, and drawing support. The user study confirms advantages in application domains such as ed- ucation and data exploration.</p>\n",dir:"content/output/projects",base:"flux-marker.json",ext:".json",sourceBase:"flux-marker.md",sourceExt:".md"}},"CiV/":function(t){t.exports={id:"teachable-reality",name:"Teachable Reality",description:"Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching",title:"Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching",authors:["Kyzyl Monteiro","Ritik Vatsal","Neil Chulpongsatorn","Aman Parnami","Ryo Suzuki"],year:2023,booktitle:"In Proceedings of the ACM CHI Conference on Human Factors in Computing Systems (CHI '23)",publisher:"ACM, New York, NY, USA",doi:"https://doi.org/10.1145/3544548.3581449",conference:{name:"CHI 2023",fullname:"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2023)",url:"https://chi2023.acm.org/"},pdf:"chi-2023-teachable-reality.pdf",video:"https://www.youtube.com/watch?v=JssiyfrhIJw",embed:"https://www.youtube.com/embed/JssiyfrhIJw",arxiv:"https://arxiv.org/abs/2302.11046","acm-dl":"https://dl.acm.org/doi/10.1145/3544548.3581449",pageCount:15,slideCount:0,bodyContent:"# Abstract\n\nThis paper introduces Teachable Reality, an augmented reality (AR) prototyping tool for creating interactive tangible AR applications with arbitrary everyday objects. Teachable Reality leverages vision-based interactive machine teaching (e.g., Teachable Machine), which captures real-world interactions for AR prototyping. It identifies the user-defined tangible and gestural interactions using an on-demand computer vision model. Based on this, the user can easily create functional AR prototypes without programming, enabled by a trigger-action authoring interface. Therefore, our approach allows the flexibility, customizability, and generalizability of tangible AR applications that can address the limitation of current marker-based approaches. We explore the design space and demonstrate various AR prototypes, which include tangible and deformable interfaces, context-aware assistants, and body-driven AR applications. The results of our user study and expert interviews confirm that our approach can lower the barrier to creating functional AR prototypes while also allowing flexible and general-purpose prototyping experiences.",bodyHtml:"<h1>Abstract</h1>\n<p>This paper introduces Teachable Reality, an augmented reality (AR) prototyping tool for creating interactive tangible AR applications with arbitrary everyday objects. Teachable Reality leverages vision-based interactive machine teaching (e.g., Teachable Machine), which captures real-world interactions for AR prototyping. It identifies the user-defined tangible and gestural interactions using an on-demand computer vision model. Based on this, the user can easily create functional AR prototypes without programming, enabled by a trigger-action authoring interface. Therefore, our approach allows the flexibility, customizability, and generalizability of tangible AR applications that can address the limitation of current marker-based approaches. We explore the design space and demonstrate various AR prototypes, which include tangible and deformable interfaces, context-aware assistants, and body-driven AR applications. The results of our user study and expert interviews confirm that our approach can lower the barrier to creating functional AR prototypes while also allowing flexible and general-purpose prototyping experiences.</p>\n",dir:"content/output/projects",base:"teachable-reality.json",ext:".json",sourceBase:"teachable-reality.md",sourceExt:".md"}},CxY0:function(t,e,o){"use strict";var i=o("nYho"),a=o("Nehr");function n(){this.protocol=null,this.slashes=null,this.auth=null,this.host=null,this.port=null,this.hostname=null,this.hash=null,this.search=null,this.query=null,this.pathname=null,this.path=null,this.href=null}e.parse=y,e.resolve=function(t,e){return y(t,!1,!0).resolve(e)},e.resolveObject=function(t,e){return t?y(t,!1,!0).resolveObject(e):e},e.format=function(t){a.isString(t)&&(t=y(t));return t instanceof n?t.format():n.prototype.format.call(t)},e.Url=n;var s=/^([a-z0-9.+-]+:)/i,r=/:[0-9]*$/,c=/^(\/\/?(?!\/)[^\?\s]*)(\?[^\s]*)?$/,l=["{","}","|","\\","^","`"].concat(["<",">",'"',"`"," ","\r","\n","\t"]),u=["'"].concat(l),h=["%","/","?",";","#"].concat(u),p=["/","?","#"],g=/^[+a-z0-9A-Z_-]{0,63}$/,d=/^([+a-z0-9A-Z_-]{0,63})(.*)$/,m={javascript:!0,"javascript:":!0},f={javascript:!0,"javascript:":!0},b={http:!0,https:!0,ftp:!0,gopher:!0,file:!0,"http:":!0,"https:":!0,"ftp:":!0,"gopher:":!0,"file:":!0},v=o("s4NR");function y(t,e,o){if(t&&a.isObject(t)&&t instanceof n)return t;var i=new n;return i.parse(t,e,o),i}n.prototype.parse=function(t,e,o){if(!a.isString(t))throw new TypeError("Parameter 'url' must be a string, not "+typeof t);var n=t.indexOf("?"),r=-1!==n&&n<t.indexOf("#")?"?":"#",l=t.split(r);l[0]=l[0].replace(/\\/g,"/");var y=t=l.join(r);if(y=y.trim(),!o&&1===t.split("#").length){var q=c.exec(y);if(q)return this.path=y,this.href=y,this.pathname=q[1],q[2]?(this.search=q[2],this.query=e?v.parse(this.search.substr(1)):this.search.substr(1)):e&&(this.search="",this.query={}),this}var w=s.exec(y);if(w){var j=(w=w[0]).toLowerCase();this.protocol=j,y=y.substr(w.length)}if(o||w||y.match(/^\/\/[^@\/]+@[^@\/]+/)){var x="//"===y.substr(0,2);!x||w&&f[w]||(y=y.substr(2),this.slashes=!0)}if(!f[w]&&(x||w&&!b[w])){for(var k,T,I=-1,S=0;S<p.length;S++){-1!==(A=y.indexOf(p[S]))&&(-1===I||A<I)&&(I=A)}-1!==(T=-1===I?y.lastIndexOf("@"):y.lastIndexOf("@",I))&&(k=y.slice(0,T),y=y.slice(T+1),this.auth=decodeURIComponent(k)),I=-1;for(S=0;S<h.length;S++){var A;-1!==(A=y.indexOf(h[S]))&&(-1===I||A<I)&&(I=A)}-1===I&&(I=y.length),this.host=y.slice(0,I),y=y.slice(I),this.parseHost(),this.hostname=this.hostname||"";var R="["===this.hostname[0]&&"]"===this.hostname[this.hostname.length-1];if(!R)for(var C=this.hostname.split(/\./),E=(S=0,C.length);S<E;S++){var P=C[S];if(P&&!P.match(g)){for(var D="",z=0,M=P.length;z<M;z++)P.charCodeAt(z)>127?D+="x":D+=P[z];if(!D.match(g)){var F=C.slice(0,S),B=C.slice(S+1),O=P.match(d);O&&(F.push(O[1]),B.unshift(O[2])),B.length&&(y="/"+B.join(".")+y),this.hostname=F.join(".");break}}}this.hostname.length>255?this.hostname="":this.hostname=this.hostname.toLowerCase(),R||(this.hostname=i.toASCII(this.hostname));var W=this.port?":"+this.port:"",H=this.hostname||"";this.host=H+W,this.href+=this.host,R&&(this.hostname=this.hostname.substr(1,this.hostname.length-2),"/"!==y[0]&&(y="/"+y))}if(!m[j])for(S=0,E=u.length;S<E;S++){var U=u[S];if(-1!==y.indexOf(U)){var N=encodeURIComponent(U);N===U&&(N=escape(U)),y=y.split(U).join(N)}}var _=y.indexOf("#");-1!==_&&(this.hash=y.substr(_),y=y.slice(0,_));var V=y.indexOf("?");if(-1!==V?(this.search=y.substr(V),this.query=y.substr(V+1),e&&(this.query=v.parse(this.query)),y=y.slice(0,V)):e&&(this.search="",this.query={}),y&&(this.pathname=y),b[j]&&this.hostname&&!this.pathname&&(this.pathname="/"),this.pathname||this.search){W=this.pathname||"";var L=this.search||"";this.path=W+L}return this.href=this.format(),this},n.prototype.format=function(){var t=this.auth||"";t&&(t=(t=encodeURIComponent(t)).replace(/%3A/i,":"),t+="@");var e=this.protocol||"",o=this.pathname||"",i=this.hash||"",n=!1,s="";this.host?n=t+this.host:this.hostname&&(n=t+(-1===this.hostname.indexOf(":")?this.hostname:"["+this.hostname+"]"),this.port&&(n+=":"+this.port)),this.query&&a.isObject(this.query)&&Object.keys(this.query).length&&(s=v.stringify(this.query));var r=this.search||s&&"?"+s||"";return e&&":"!==e.substr(-1)&&(e+=":"),this.slashes||(!e||b[e])&&!1!==n?(n="//"+(n||""),o&&"/"!==o.charAt(0)&&(o="/"+o)):n||(n=""),i&&"#"!==i.charAt(0)&&(i="#"+i),r&&"?"!==r.charAt(0)&&(r="?"+r),e+n+(o=o.replace(/[?#]/g,function(t){return encodeURIComponent(t)}))+(r=r.replace("#","%23"))+i},n.prototype.resolve=function(t){return this.resolveObject(y(t,!1,!0)).format()},n.prototype.resolveObject=function(t){if(a.isString(t)){var e=new n;e.parse(t,!1,!0),t=e}for(var o=new n,i=Object.keys(this),s=0;s<i.length;s++){var r=i[s];o[r]=this[r]}if(o.hash=t.hash,""===t.href)return o.href=o.format(),o;if(t.slashes&&!t.protocol){for(var c=Object.keys(t),l=0;l<c.length;l++){var u=c[l];"protocol"!==u&&(o[u]=t[u])}return b[o.protocol]&&o.hostname&&!o.pathname&&(o.path=o.pathname="/"),o.href=o.format(),o}if(t.protocol&&t.protocol!==o.protocol){if(!b[t.protocol]){for(var h=Object.keys(t),p=0;p<h.length;p++){var g=h[p];o[g]=t[g]}return o.href=o.format(),o}if(o.protocol=t.protocol,t.host||f[t.protocol])o.pathname=t.pathname;else{for(var d=(t.pathname||"").split("/");d.length&&!(t.host=d.shift()););t.host||(t.host=""),t.hostname||(t.hostname=""),""!==d[0]&&d.unshift(""),d.length<2&&d.unshift(""),o.pathname=d.join("/")}if(o.search=t.search,o.query=t.query,o.host=t.host||"",o.auth=t.auth,o.hostname=t.hostname||t.host,o.port=t.port,o.pathname||o.search){var m=o.pathname||"",v=o.search||"";o.path=m+v}return o.slashes=o.slashes||t.slashes,o.href=o.format(),o}var y=o.pathname&&"/"===o.pathname.charAt(0),q=t.host||t.pathname&&"/"===t.pathname.charAt(0),w=q||y||o.host&&t.pathname,j=w,x=o.pathname&&o.pathname.split("/")||[],k=(d=t.pathname&&t.pathname.split("/")||[],o.protocol&&!b[o.protocol]);if(k&&(o.hostname="",o.port=null,o.host&&(""===x[0]?x[0]=o.host:x.unshift(o.host)),o.host="",t.protocol&&(t.hostname=null,t.port=null,t.host&&(""===d[0]?d[0]=t.host:d.unshift(t.host)),t.host=null),w=w&&(""===d[0]||""===x[0])),q)o.host=t.host||""===t.host?t.host:o.host,o.hostname=t.hostname||""===t.hostname?t.hostname:o.hostname,o.search=t.search,o.query=t.query,x=d;else if(d.length)x||(x=[]),x.pop(),x=x.concat(d),o.search=t.search,o.query=t.query;else if(!a.isNullOrUndefined(t.search)){if(k)o.hostname=o.host=x.shift(),(R=!!(o.host&&o.host.indexOf("@")>0)&&o.host.split("@"))&&(o.auth=R.shift(),o.host=o.hostname=R.shift());return o.search=t.search,o.query=t.query,a.isNull(o.pathname)&&a.isNull(o.search)||(o.path=(o.pathname?o.pathname:"")+(o.search?o.search:"")),o.href=o.format(),o}if(!x.length)return o.pathname=null,o.search?o.path="/"+o.search:o.path=null,o.href=o.format(),o;for(var T=x.slice(-1)[0],I=(o.host||t.host||x.length>1)&&("."===T||".."===T)||""===T,S=0,A=x.length;A>=0;A--)"."===(T=x[A])?x.splice(A,1):".."===T?(x.splice(A,1),S++):S&&(x.splice(A,1),S--);if(!w&&!j)for(;S--;S)x.unshift("..");!w||""===x[0]||x[0]&&"/"===x[0].charAt(0)||x.unshift(""),I&&"/"!==x.join("/").substr(-1)&&x.push("");var R,C=""===x[0]||x[0]&&"/"===x[0].charAt(0);k&&(o.hostname=o.host=C?"":x.length?x.shift():"",(R=!!(o.host&&o.host.indexOf("@")>0)&&o.host.split("@"))&&(o.auth=R.shift(),o.host=o.hostname=R.shift()));return(w=w||o.host&&x.length)&&!C&&x.unshift(""),x.length?o.pathname=x.join("/"):(o.pathname=null,o.path=null),a.isNull(o.pathname)&&a.isNull(o.search)||(o.path=(o.pathname?o.pathname:"")+(o.search?o.search:"")),o.auth=t.auth||o.auth,o.slashes=o.slashes||t.slashes,o.href=o.format(),o},n.prototype.parseHost=function(){var t=this.host,e=r.exec(t);e&&(":"!==(e=e[0])&&(this.port=e.substr(1)),t=t.substr(0,t.length-e.length)),t&&(this.hostname=t)}},EXMj:function(t,e){t.exports=function(t,e,o,i){if(!(t instanceof e)||void 0!==i&&i in t)throw TypeError(o+": incorrect invocation!");return t}},GbvX:function(t){t.exports={id:"dynablock",name:"Dynablock",description:"Dynamic 3D Printing for Instant and Reconstructable Shape Formation",title:"Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation",authors:["Ryo Suzuki","Junichi Yamaoka","Daniel Leithinger","Tom Yeh","Mark D. Gross","Yoshihiro Kawahara","Yasuaki Kakehi"],year:2018,booktitle:"In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology (UIST '18)",publisher:"ACM, New York, NY, USA",pages:"99-111",doi:"https://doi.org/10.1145/3242587.3242659",conference:{name:"UIST 2018",fullname:"The ACM Symposium on User Interface Software and Technology (UIST 2018)",url:"http://uist.acm.org/uist2018"},pdf:"uist-2018-dynablock.pdf",video:"https://www.youtube.com/watch?v=7nPlr3O9xu8",embed:"https://www.youtube.com/embed/7nPlr3O9xu8","short-video":"https://www.youtube.com/watch?v=92eGI-gYYc4",slide:"uist-2018-dynablock-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3242659",talk:"https://www.youtube.com/watch?v=R3FRUtOIiCQ",poster:"uist-2018-dynablock-poster.pdf",pageCount:12,slideCount:53,bodyContent:'\x3c!--\nLinks:\n[**[PDF](http://ryosuzuki.org/publications/uist-2018-dynablock.pdf)**]\n[**[ACM DL](https://dl.acm.org/citation.cfm?id=3242659)**]\n[**[Video](https://www.youtube.com/watch?v=7nPlr3O9xu8)**]\n[**[Slide](http://ryosuzuki.org/publications/uist-2018-dynablock-slide.pdf)**]\n[**[Talk](https://www.youtube.com/watch?v=R3FRUtOIiCQ)**]\n --\x3e\n\n# Abstract\n\nThis paper introduces Dynamic 3D Printing, a fast and re- constructable shape formation system. Dynamic 3D Printing assembles an arbitrary three-dimensional shape from a large number of small physical elements. It can also disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbi- trary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and imple- mentation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/top.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/top.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-1-1.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-1-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-1-2.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-1-2.jpg" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-2.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-3.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-3.jpg" /></a>\n  </div>\n</div>\n\n# Dynamic 3D Printing\n\nWhat if 3D printers could form a physical object in seconds? What if the object, once it is no longer needed, could quickly and easily be disassembled and reconstructed as a new object? Today’s 3D printers take hours to print objects, and output a single static object. However, we envision a future in which 3D printing could instantly create objects from reusable and reconstructable materials.\n\nThis paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.\n\nWe define Dynamic 3D Printing as a class of systems that have the following properties:\n\n- Immediate: The system can form a physical shape in sec- onds.\n\n- Reconstructable: Rendered shapes can be disassembled and reconstructed by hand or with the system, and the blocks are reusable.\n\n- Arbitrary Shapes: It can create arbitrary three dimensional shapes.\n\n- Graspable: The output shapes and structure are graspable and solid.\n\n# Parallel Assembler\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-3-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-3-1.png" /></a>\n  </div>\n</div>\n\nDynamic 3D printing deploys a large number of small dis- crete material elements, which are assembled to form arbitrary shaped macro-scale objects. Individual elements are passive, which requires an external actuator to perform the assembly. As illustrated in the above Figure, the assembler consists of an N x N grid of motorized pins and linear actuators. The elements, which are the same size as the pins, are stacked on top of the pins (Figure 3 A). When stacked, the elements are connected in vertical direction, while discon- nected with nearby elements in horizontal direction. Similar to existing pin-based shape displays, the assembler can incrementally generate 2.5D shapes by individually moving pins to push elements to the surface.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/mechanism.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/mechanism.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-4-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-4-1.png" /></a>\n  </div>\n  <p class="column">\n    This paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.\n  </p>\n</div>\n\n\n# Implementation\n\nThe assembler consists of a 24 x 16 array of motor-driven pins. Each pin moves up and down, driven by a small DC motor (TTMotors TGPP06-D700) and a 3D printed lead screw (2 mm pitch, 4 starts, 120 mm in length). TGPP06-D700 is 6 mm in diameter and 29 mm in length and can rotate 47 rpm with 1:700 gear ratio. The 2 mm 4 starts lead screw can travel 12 mm per second without load, and each motor consumes approximately 60 mA. The pins are 3D printed with a nut at the bottom to travel along the lead screw. Each pin is 120 mm long and has a 7mm square cross section with a 5 mm diameter hole from top to bottom, and an N45 disk magnet (φ 3mm x 2.4 mm thickness) is attached at the top. Guide grids at the top prevent pins from rotating and ensure that pins travel vertically. The 24 x 16 guide grids have 7.5 mm square holes with 10.16 mm pitch and are cut from a 5 mm acrylic plate. We fabricated the pins, the lead screws, and blocks with an inkjet 3D printer (Keyence Agilista 3200) with water soluble support material. In total, we fabricated 384 (= 24 x 16) pins and lead screws, and 3,072 (= 24 x 16 x 8 layers) blocks. To create the magnetic blocks, we embedded spherical magnets in each block by hand and inserted disk magnets using a bench vice.\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-3.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-3.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-2.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-3.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-3.jpg" /></a>\n  </div>\n</div>\n\n\n# Future Vision\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/claytronics.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/claytronics.mp4" type="video/mp4"></source>\n</video>\n[Video Credit: Carnegie Mellon University, Claytronics Vision]\n\n<br/>\n\nWith these capabilities, a 3D printer would become an inter- active medium, rather than merely a fabrication device. For example, such a 3D printer could be used in a Virtual Real- ity or Augmented Reality application to dynamically form a tangible object or controller to provide haptic feedback and engage users physically. For children, it could dynamically form a physical educational manipulative, such as a molec- ular or architectural model, to learn and explore topics, for example in a science museum. Designers could use it to ren- der a physical product to present to clients and interactively change the product’s design through direct manipulation. In this vision, Dynamic 3D printing is an environment in which the user thinks, designs, explores, and communicates through dynamic and interactive physical representation.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-8-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-8-1.png" /></a>\n  </div>\n</div>\n\nDynamic 3D printing would enable a new design workflow for digital fabrication. One notable advantage of dynamic 3D printing is the capability of connecting and disconnecting building blocks through direct manipulation. The user can also define variables or abstract attributes for parametric design through direct and gestural interaction. By leveraging this capability, the user could interactively design and fabri- cate in a physical space, similar to the man-machine dialogue proposed by Frazer et al. and later tangible CAD interfaces.',bodyHtml:'<p>&lt;!--\nLinks:\n[<strong><a href="http://ryosuzuki.org/publications/uist-2018-dynablock.pdf">PDF</a></strong>]\n[<strong><a href="https://dl.acm.org/citation.cfm?id=3242659">ACM DL</a></strong>]\n[<strong><a href="https://www.youtube.com/watch?v=7nPlr3O9xu8">Video</a></strong>]\n[<strong><a href="http://ryosuzuki.org/publications/uist-2018-dynablock-slide.pdf">Slide</a></strong>]\n[<strong><a href="https://www.youtube.com/watch?v=R3FRUtOIiCQ">Talk</a></strong>]\n--&gt;</p>\n<h1>Abstract</h1>\n<p>This paper introduces Dynamic 3D Printing, a fast and re- constructable shape formation system. Dynamic 3D Printing assembles an arbitrary three-dimensional shape from a large number of small physical elements. It can also disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbi- trary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and imple- mentation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/dynablock/webm/top.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/dynablock/video/top.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/dynablock/figure-1-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/dynablock/figure-1-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/dynablock/figure-1-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/dynablock/figure-1-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/dynablock/figure-2-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/dynablock/figure-2-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/dynablock/figure-2-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/dynablock/figure-2-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/dynablock/figure-2-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/dynablock/figure-2-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>Dynamic 3D Printing</h1>\n<p>What if 3D printers could form a physical object in seconds? What if the object, once it is no longer needed, could quickly and easily be disassembled and reconstructed as a new object? Today’s 3D printers take hours to print objects, and output a single static object. However, we envision a future in which 3D printing could instantly create objects from reusable and reconstructable materials.</p>\n<p>This paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.</p>\n<p>We define Dynamic 3D Printing as a class of systems that have the following properties:</p>\n<ul>\n<li>\n<p>Immediate: The system can form a physical shape in sec- onds.</p>\n</li>\n<li>\n<p>Reconstructable: Rendered shapes can be disassembled and reconstructed by hand or with the system, and the blocks are reusable.</p>\n</li>\n<li>\n<p>Arbitrary Shapes: It can create arbitrary three dimensional shapes.</p>\n</li>\n<li>\n<p>Graspable: The output shapes and structure are graspable and solid.</p>\n</li>\n</ul>\n<h1>Parallel Assembler</h1>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/dynablock/figure-3-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/dynablock/figure-3-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>Dynamic 3D printing deploys a large number of small dis- crete material elements, which are assembled to form arbitrary shaped macro-scale objects. Individual elements are passive, which requires an external actuator to perform the assembly. As illustrated in the above Figure, the assembler consists of an N x N grid of motorized pins and linear actuators. The elements, which are the same size as the pins, are stacked on top of the pins (Figure 3 A). When stacked, the elements are connected in vertical direction, while discon- nected with nearby elements in horizontal direction. Similar to existing pin-based shape displays, the assembler can incrementally generate 2.5D shapes by individually moving pins to push elements to the surface.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/dynablock/webm/mechanism.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/dynablock/video/mechanism.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/dynablock/figure-4-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/dynablock/figure-4-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;p class=&quot;column&quot;&gt;\nThis paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.\n&lt;/p&gt;\n&lt;/div&gt;</p>\n<h1>Implementation</h1>\n<p>The assembler consists of a 24 x 16 array of motor-driven pins. Each pin moves up and down, driven by a small DC motor (TTMotors TGPP06-D700) and a 3D printed lead screw (2 mm pitch, 4 starts, 120 mm in length). TGPP06-D700 is 6 mm in diameter and 29 mm in length and can rotate 47 rpm with 1:700 gear ratio. The 2 mm 4 starts lead screw can travel 12 mm per second without load, and each motor consumes approximately 60 mA. The pins are 3D printed with a nut at the bottom to travel along the lead screw. Each pin is 120 mm long and has a 7mm square cross section with a 5 mm diameter hole from top to bottom, and an N45 disk magnet (φ 3mm x 2.4 mm thickness) is attached at the top. Guide grids at the top prevent pins from rotating and ensure that pins travel vertically. The 24 x 16 guide grids have 7.5 mm square holes with 10.16 mm pitch and are cut from a 5 mm acrylic plate. We fabricated the pins, the lead screws, and blocks with an inkjet 3D printer (Keyence Agilista 3200) with water soluble support material. In total, we fabricated 384 (= 24 x 16) pins and lead screws, and 3,072 (= 24 x 16 x 8 layers) blocks. To create the magnetic blocks, we embedded spherical magnets in each block by hand and inserted disk magnets using a bench vice.</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/dynablock/figure-5-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/dynablock/figure-5-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/dynablock/figure-5-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/dynablock/figure-5-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/dynablock/figure-5-3.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/dynablock/figure-5-3.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/dynablock/figure-7-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/dynablock/figure-7-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/dynablock/figure-7-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/dynablock/figure-7-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/dynablock/figure-7-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/dynablock/figure-7-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>Future Vision</h1>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/dynablock/webm/claytronics.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/dynablock/video/claytronics.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;\n[Video Credit: Carnegie Mellon University, Claytronics Vision]</p>\n<p>&lt;br/&gt;</p>\n<p>With these capabilities, a 3D printer would become an inter- active medium, rather than merely a fabrication device. For example, such a 3D printer could be used in a Virtual Real- ity or Augmented Reality application to dynamically form a tangible object or controller to provide haptic feedback and engage users physically. For children, it could dynamically form a physical educational manipulative, such as a molec- ular or architectural model, to learn and explore topics, for example in a science museum. Designers could use it to ren- der a physical product to present to clients and interactively change the product’s design through direct manipulation. In this vision, Dynamic 3D printing is an environment in which the user thinks, designs, explores, and communicates through dynamic and interactive physical representation.</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/dynablock/figure-8-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/dynablock/figure-8-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>Dynamic 3D printing would enable a new design workflow for digital fabrication. One notable advantage of dynamic 3D printing is the capability of connecting and disconnecting building blocks through direct manipulation. The user can also define variables or abstract attributes for parametric design through direct and gestural interaction. By leveraging this capability, the user could interactively design and fabri- cate in a physical space, similar to the man-machine dialogue proposed by Frazer et al. and later tangible CAD interfaces.</p>\n',dir:"content/output/projects",base:"dynablock.json",ext:".json",sourceBase:"dynablock.md",sourceExt:".md"}},GvbO:function(t,e,o){var i=o("Y7ZC");i(i.S,"Array",{isArray:o("kAMH")})},IMSK:function(t){t.exports={id:"phd-thesis",name:"Collective Shape-changing Interfaces",description:"Dynamic Shape Construction and Transformation with Collective Elements",title:"Dynamic Shape Construction and Transformation with Collective Elements",authors:["Ryo Suzuki"],year:2020,booktitle:"PhD Dissertation",publisher:"University of Colorado Boulder",pages:"1-289",conference:{name:"PhD Dissertation",fullname:"PhD Dissertation"},pdf:"phd-dissertation.pdf",slide:"phd-defense.pdf",talk:"https://www.youtube.com/watch?v=FHmp7BIhXJI",pageCount:250,slideCount:198,related:{title:"Collective Shape-changing Interfaces",authors:["Ryo Suzuki"],year:2019,booktitle:"In Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST '19 Doctoral Consortium)",publisher:"ACM, New York, NY, USA",pages:"154–157",doi:"https://doi.org/10.1145/3332167.3356877",pdf:"uist-2019-collective.pdf",suffix:"dc",pageCount:4},bodyContent:'# Abstract\nThis thesis explores dynamic and collective shape construction as a new way to **physicalize** digital information for interactive physical displays --- i.e., **shape-changing displays enabled by a swarm of collective elements**. Through physical form of digital objects, the user can directly touch, grasp, and manipulate digital information through rich tangible and embodied interactions, but at the same time, such physical objects can dynamically change their shape for an interactive computer display and interface through collective shape construction and transformation with a swarm of elements. The goal of this thesis is to envision and illustrate how such an interface might support human activities by transforming physical forms at various sizes, from millimeter to meter scale.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-1.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-1.jpg" /></a>\n  </div>\n</div>\n\nTo achieve this goal, this thesis introduces **collective shape-changing interface**, a new type of shape-changing interfaces constructed by discrete, collective, physical elements. This proposed approach promises to address the current limitations of shape-changing interfaces --- wherein a swarm of modular elements enables us to decompose the large, monolithic shape-changing objects into a set of simple, distributed elements. At the same time, their swarm behaviors enable us to make an unbounded shape transformation for expressive representation. This thesis contributes to the first exploration of this new class of shape-changing interfaces and proposes two approaches: active and passive shape construction. In active shape construction, collective elements can dynamically move and reconfigure themselves to construct a three-dimensional shape. Passive shape construction instead leverages external actuation to assemble and transform collective passive objects for dynamic shape creation. I explore and demonstrate how active and passive collective shape construction can be used as a future of computer interfaces, by developing various prototypes built on top of novel hardware and software platforms. Given these investigations, I discuss the design implications and possible research directions towards the future of collective shape-changing user interfaces.\n\n\n# Introduction\n\nWhat if computer displays can represent information not only *graphically* but also *physically*? What if such physical forms of information could be as malleable and programmable as the pixels on a computer screen? If so, it could be used as a dynamic physical medium to interact with the digital world.\n\nIvan Sutherland, a founder of virtual and augmented reality, once envisioned that the future of computer displays would be *"a room within which the computer can control the existence of matter"*\nThis radical vision has inspired many researchers over the decades, as such technologies could open up a new\nparadigm of human-machine interfaces.\n\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-2.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-2.jpg" /></a>\n  </div>\n</div>\n\nHowever, we are still far from this exciting future. Today\'s computer interfaces mostly focus on *screen-based* interaction, where the screens serve as a ``looking window\'\' of the digital world --- the user can see digital information through the glass, but a barrier between what is inside (digital world) and what is outside (physical world) confines how we interact with the digital world. Current technologies do not allow us to directly touch, feel, grasp, and manipulate digital objects, in the same way that humans have done with physical objects for hundreds of thousands of years.\n\n\n# Thesis Statement\n\nThe goal of this thesis is to bring Sutherland\'s vision closer to reality by developing a new form of interactive and dynamic physical displays, and to illustrate how such an interface might support human activities by transforming physical forms and environments at various scales.\n\nAs a step toward this vision, this thesis explores dynamic and collective shape construction as a new way to **physicalize** digital information for interactive physical displays --- i.e., **shape-changing displays** enabled by a swarm of collective elements.\nCollective elements refer to discrete physical objects that can construct a physical, three-dimensional shape.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-3.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-3.jpg" /></a>\n  </div>\n</div>\n\nEach individual element can dynamically change its shape, position, and other physical properties through internal or external actuation, as to collectively construct and transform the overall physical shape.\nThis enables a new way of representing digital information.\nSuch physical shapes allow the user not only to see information, but to touch, feel, grasp, construct, and manipulate it, in the same way that interact with physical objects.\nAt the same time, these physical objects must also embody dynamic computation. Collective elements can dynamically and programmatically reconfigure themselves, as if they are rendered in an interactive computer display and interface.\n\nIn contrast to shape changes made of monolithic materials, constructing shapes out of discrete elements enables rich expressiveness in representing information.\nLike pixels on a screen, they make shapes by collectively transforming the overall structure.\nAdditionally, their components can be simple and interchangeable, thus allowing for scale.\nThese elements can also interact with existing environments, and they make everyday objects and environments more dynamic, adaptive, and interactive by collectively actuating and reconfiguring them in a programmable fashion.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-4.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-4.jpg" /></a>\n  </div>\n</div>\n\nMaking shapes out of discrete collective elements is not a new idea.\nThere is a long history of modular self-reconfigurable robots and swarm robotics.\nThese areas of research have explored the idea of collective and general-purpose shape transformation for robotic applications, such as space exploration, rescue, and navigation.\nHowever, there are many critical challenges when we apply this approach for **interactive interfaces**.\n\nFor example, the speed of transformation needs to be much faster than for robotic applications, as the interactive system must change and respond to the user in real-time (e.g., in seconds, not minutes or hours). Another consideration is scalability.\nTo display meaningful information, it may require a relatively large number of elements, which often introduces implementation problems.\nFinally, unlike autonomous systems, interactive systems must consider the interaction between humans and objects --- there remains work to be done in understanding how we might interact with such collective elements and how these interfaces could support everyday human activities.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-5.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-5.jpg" /></a>\n  </div>\n</div>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-7.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-7.jpg" /></a>\n  </div>\n</div>\n\nThis thesis addresses these questions by investigating how collective shape construction and transformation can be used for *interactive computer interfaces*.\nTo this end, this thesis introduces **collective shape-changing interfaces**, a new class of shape-changing interfaces constructed by a swarm of discrete physical elements.\nThe main contribution of this thesis is the first exploration of this new class of shape-changing interfaces in the following four domains:\n\n1. **Shape representation**: <br/>\nexplore what types of shape representations are possible to display information\n\n\n2. **Reconfiguration methods**: <br/>\nexplore how both active and passive elements can be used to construct a shape for interactive interfaces\n\n\n3. **Interaction**: <br/>\nexplore how the user can interact with many collective elements through direct physical manipulation,\n\n\n4. **Applications**: <br/>\nexplore, illustrate, and demonstrate what kind of applications are achievable for human-computer interaction.\n\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-1.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-1.jpg" /></a>\n  </div>\n</div>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-9.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-9.jpg" /></a>\n  </div>\n</div>\n\n\nThis new class of shape-changing interfaces promises to address some of the limitations of the current shape-changing interfaces. For example, a swarm of modular elements enables us to decompose the large, monolithic shape-changing objects into a set of simple, distributed elements. This significantly contributes to the deployability of the system in everyday environments. In addition, the swarm behaviors enable us to make unbounded shape transformations for expressive representation.\nThrough my explorations of various proof-of-concept prototypes, I demonstrate how we can push the boundary of the current shape-changing interfaces by leveraging the collective behaviors of both active and passive elements.\nI also demonstrate how these dynamic shapes can support a range of application scenarios, such as interactive displays, adaptive environments, dynamic data physicalization, and accessibility support for people with visual impairments.\nFinally, I discuss the challenges and opportunities for using this approach towards the future of dynamic physical media.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-6.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-6.jpg" /></a>\n  </div>\n</div>\n\n\n# Thesis Contributions\nThis thesis makes contributions to the field of Human-Computer Interaction in the following areas:\n\n1. A design space exploration of dynamic shape construction with collective elements\n\n\n2. A new taxonomy and investigation of active and passive shape construction and transformation with collective elements\n\n\n3. A novel technique for creating a dynamic shape with active shape-transformable swarm robots (e.g., ShapeBots, LiftTiles)\n\n\n4. A novel technique for constructing 3D shapes with an assembly of passive magnetically connectable blocks (e.g., Dynablock)\n\n\n5. A novel technique for actuating passive magnetic markers with scalable electro-magnetic actuation (e.g., FluxMarker, Reactile)\n\n\n6. A novel technique for actuating existing objects to reconfigure spatial layouts (e.g., RoomShift)\n\n\n7. A novel interaction technique for programming the dynamic shape construction on a 2D surface with direct physical manipulation (e.g., Reactile)\n\n\n<br/>\n\n# Future Vision\n\n<div class="video-container" style="display: block">\n  <iframe\n    class="embed"\n    width="100%"\n    height="315"\n    src="https://www.youtube.com/embed/_huMQVBAJsY"\n    frameborder="0"\n    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"\n    allowFullScreen={true}\n    mozAllowFullScreen={true}\n    msAllowFullScreen={true}\n    oAllowFullScreen={true}\n    webkitAllowFullScreen={true}\n  ></iframe>\n</div>\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-1.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-2.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-3.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-3.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-4.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-4.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-5.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-5.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-6.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-6.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-7.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-7.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-8.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-8.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-9.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-9.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-10.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-10.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-11.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-11.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-12.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-12.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-13.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-13.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-14.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-14.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-15.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-15.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-16.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-16.jpg" /></a>\n  </div>\n</div>\n\n# PhD Dissertation Defense\n\n<div class="video-container" style="display: block">\n  <iframe\n    class="embed"\n    width="100%"\n    height="315"\n    src="https://www.youtube.com/embed/FHmp7BIhXJI"\n    frameborder="0"\n    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"\n    allowFullScreen={true}\n    mozAllowFullScreen={true}\n    msAllowFullScreen={true}\n    oAllowFullScreen={true}\n    webkitAllowFullScreen={true}\n  ></iframe>\n</div>\n\n<br/>\n\nThesis Committee:\n\n- **Daniel Leithinger** (CU Boulder, chair)\n\n\n- **Mark Gross** (CU Boulder)\n\n\n- **Tom Yeh** (CU Boulder)\n\n\n- **Hiroshi Ishii** (MIT Media Lab)\n\n\n- **Takeo Igarashi** (The University of Tokyo)\n\n<br/>\n\nDefense Date: May 13th, 2020',bodyHtml:"<h1>Abstract</h1>\n<p>This thesis explores dynamic and collective shape construction as a new way to <strong>physicalize</strong> digital information for interactive physical displays --- i.e., <strong>shape-changing displays enabled by a swarm of collective elements</strong>. Through physical form of digital objects, the user can directly touch, grasp, and manipulate digital information through rich tangible and embodied interactions, but at the same time, such physical objects can dynamically change their shape for an interactive computer display and interface through collective shape construction and transformation with a swarm of elements. The goal of this thesis is to envision and illustrate how such an interface might support human activities by transforming physical forms at various sizes, from millimeter to meter scale.</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>To achieve this goal, this thesis introduces <strong>collective shape-changing interface</strong>, a new type of shape-changing interfaces constructed by discrete, collective, physical elements. This proposed approach promises to address the current limitations of shape-changing interfaces --- wherein a swarm of modular elements enables us to decompose the large, monolithic shape-changing objects into a set of simple, distributed elements. At the same time, their swarm behaviors enable us to make an unbounded shape transformation for expressive representation. This thesis contributes to the first exploration of this new class of shape-changing interfaces and proposes two approaches: active and passive shape construction. In active shape construction, collective elements can dynamically move and reconfigure themselves to construct a three-dimensional shape. Passive shape construction instead leverages external actuation to assemble and transform collective passive objects for dynamic shape creation. I explore and demonstrate how active and passive collective shape construction can be used as a future of computer interfaces, by developing various prototypes built on top of novel hardware and software platforms. Given these investigations, I discuss the design implications and possible research directions towards the future of collective shape-changing user interfaces.</p>\n<h1>Introduction</h1>\n<p>What if computer displays can represent information not only <em>graphically</em> but also <em>physically</em>? What if such physical forms of information could be as malleable and programmable as the pixels on a computer screen? If so, it could be used as a dynamic physical medium to interact with the digital world.</p>\n<p>Ivan Sutherland, a founder of virtual and augmented reality, once envisioned that the future of computer displays would be <em>&quot;a room within which the computer can control the existence of matter&quot;</em>\nThis radical vision has inspired many researchers over the decades, as such technologies could open up a new\nparadigm of human-machine interfaces.</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>However, we are still far from this exciting future. Today's computer interfaces mostly focus on <em>screen-based</em> interaction, where the screens serve as a ``looking window'' of the digital world --- the user can see digital information through the glass, but a barrier between what is inside (digital world) and what is outside (physical world) confines how we interact with the digital world. Current technologies do not allow us to directly touch, feel, grasp, and manipulate digital objects, in the same way that humans have done with physical objects for hundreds of thousands of years.</p>\n<h1>Thesis Statement</h1>\n<p>The goal of this thesis is to bring Sutherland's vision closer to reality by developing a new form of interactive and dynamic physical displays, and to illustrate how such an interface might support human activities by transforming physical forms and environments at various scales.</p>\n<p>As a step toward this vision, this thesis explores dynamic and collective shape construction as a new way to <strong>physicalize</strong> digital information for interactive physical displays --- i.e., <strong>shape-changing displays</strong> enabled by a swarm of collective elements.\nCollective elements refer to discrete physical objects that can construct a physical, three-dimensional shape.</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>Each individual element can dynamically change its shape, position, and other physical properties through internal or external actuation, as to collectively construct and transform the overall physical shape.\nThis enables a new way of representing digital information.\nSuch physical shapes allow the user not only to see information, but to touch, feel, grasp, construct, and manipulate it, in the same way that interact with physical objects.\nAt the same time, these physical objects must also embody dynamic computation. Collective elements can dynamically and programmatically reconfigure themselves, as if they are rendered in an interactive computer display and interface.</p>\n<p>In contrast to shape changes made of monolithic materials, constructing shapes out of discrete elements enables rich expressiveness in representing information.\nLike pixels on a screen, they make shapes by collectively transforming the overall structure.\nAdditionally, their components can be simple and interchangeable, thus allowing for scale.\nThese elements can also interact with existing environments, and they make everyday objects and environments more dynamic, adaptive, and interactive by collectively actuating and reconfiguring them in a programmable fashion.</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-4.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-4.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>Making shapes out of discrete collective elements is not a new idea.\nThere is a long history of modular self-reconfigurable robots and swarm robotics.\nThese areas of research have explored the idea of collective and general-purpose shape transformation for robotic applications, such as space exploration, rescue, and navigation.\nHowever, there are many critical challenges when we apply this approach for <strong>interactive interfaces</strong>.</p>\n<p>For example, the speed of transformation needs to be much faster than for robotic applications, as the interactive system must change and respond to the user in real-time (e.g., in seconds, not minutes or hours). Another consideration is scalability.\nTo display meaningful information, it may require a relatively large number of elements, which often introduces implementation problems.\nFinally, unlike autonomous systems, interactive systems must consider the interaction between humans and objects --- there remains work to be done in understanding how we might interact with such collective elements and how these interfaces could support everyday human activities.</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-5.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-5.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-7.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-7.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>This thesis addresses these questions by investigating how collective shape construction and transformation can be used for <em>interactive computer interfaces</em>.\nTo this end, this thesis introduces <strong>collective shape-changing interfaces</strong>, a new class of shape-changing interfaces constructed by a swarm of discrete physical elements.\nThe main contribution of this thesis is the first exploration of this new class of shape-changing interfaces in the following four domains:</p>\n<ol>\n<li>\n<p><strong>Shape representation</strong>: &lt;br/&gt;\nexplore what types of shape representations are possible to display information</p>\n</li>\n<li>\n<p><strong>Reconfiguration methods</strong>: &lt;br/&gt;\nexplore how both active and passive elements can be used to construct a shape for interactive interfaces</p>\n</li>\n<li>\n<p><strong>Interaction</strong>: &lt;br/&gt;\nexplore how the user can interact with many collective elements through direct physical manipulation,</p>\n</li>\n<li>\n<p><strong>Applications</strong>: &lt;br/&gt;\nexplore, illustrate, and demonstrate what kind of applications are achievable for human-computer interaction.</p>\n</li>\n</ol>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-2-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-2-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-9.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-9.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>This new class of shape-changing interfaces promises to address some of the limitations of the current shape-changing interfaces. For example, a swarm of modular elements enables us to decompose the large, monolithic shape-changing objects into a set of simple, distributed elements. This significantly contributes to the deployability of the system in everyday environments. In addition, the swarm behaviors enable us to make unbounded shape transformations for expressive representation.\nThrough my explorations of various proof-of-concept prototypes, I demonstrate how we can push the boundary of the current shape-changing interfaces by leveraging the collective behaviors of both active and passive elements.\nI also demonstrate how these dynamic shapes can support a range of application scenarios, such as interactive displays, adaptive environments, dynamic data physicalization, and accessibility support for people with visual impairments.\nFinally, I discuss the challenges and opportunities for using this approach towards the future of dynamic physical media.</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-6.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-6.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>Thesis Contributions</h1>\n<p>This thesis makes contributions to the field of Human-Computer Interaction in the following areas:</p>\n<ol>\n<li>\n<p>A design space exploration of dynamic shape construction with collective elements</p>\n</li>\n<li>\n<p>A new taxonomy and investigation of active and passive shape construction and transformation with collective elements</p>\n</li>\n<li>\n<p>A novel technique for creating a dynamic shape with active shape-transformable swarm robots (e.g., ShapeBots, LiftTiles)</p>\n</li>\n<li>\n<p>A novel technique for constructing 3D shapes with an assembly of passive magnetically connectable blocks (e.g., Dynablock)</p>\n</li>\n<li>\n<p>A novel technique for actuating passive magnetic markers with scalable electro-magnetic actuation (e.g., FluxMarker, Reactile)</p>\n</li>\n<li>\n<p>A novel technique for actuating existing objects to reconfigure spatial layouts (e.g., RoomShift)</p>\n</li>\n<li>\n<p>A novel interaction technique for programming the dynamic shape construction on a 2D surface with direct physical manipulation (e.g., Reactile)</p>\n</li>\n</ol>\n<p>&lt;br/&gt;</p>\n<h1>Future Vision</h1>\n<p>&lt;div class=&quot;video-container&quot; style=&quot;display: block&quot;&gt;\n&lt;iframe\nclass=&quot;embed&quot;\nwidth=&quot;100%&quot;\nheight=&quot;315&quot;\nsrc=&quot;https://www.youtube.com/embed/_huMQVBAJsY&quot;\nframeborder=&quot;0&quot;\nallow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot;\nallowFullScreen={true}\nmozAllowFullScreen={true}\nmsAllowFullScreen={true}\noAllowFullScreen={true}\nwebkitAllowFullScreen={true}</p>\n<blockquote>\n<p>&lt;/iframe&gt;\n&lt;/div&gt;</p>\n</blockquote>\n<p>&lt;div class=&quot;figures ui four column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-10-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-10-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-10-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-10-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-10-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-10-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-10-4.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-10-4.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-10-5.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-10-5.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-10-6.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-10-6.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-10-7.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-10-7.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-10-8.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-10-8.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-10-9.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-10-9.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-10-10.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-10-10.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-10-11.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-10-11.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-10-12.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-10-12.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-10-13.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-10-13.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-10-14.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-10-14.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-10-15.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-10-15.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/phd-thesis/figure-10-16.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/phd-thesis/figure-10-16.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>PhD Dissertation Defense</h1>\n<p>&lt;div class=&quot;video-container&quot; style=&quot;display: block&quot;&gt;\n&lt;iframe\nclass=&quot;embed&quot;\nwidth=&quot;100%&quot;\nheight=&quot;315&quot;\nsrc=&quot;https://www.youtube.com/embed/FHmp7BIhXJI&quot;\nframeborder=&quot;0&quot;\nallow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot;\nallowFullScreen={true}\nmozAllowFullScreen={true}\nmsAllowFullScreen={true}\noAllowFullScreen={true}\nwebkitAllowFullScreen={true}</p>\n<blockquote>\n<p>&lt;/iframe&gt;\n&lt;/div&gt;</p>\n</blockquote>\n<p>&lt;br/&gt;</p>\n<p>Thesis Committee:</p>\n<ul>\n<li>\n<p><strong>Daniel Leithinger</strong> (CU Boulder, chair)</p>\n</li>\n<li>\n<p><strong>Mark Gross</strong> (CU Boulder)</p>\n</li>\n<li>\n<p><strong>Tom Yeh</strong> (CU Boulder)</p>\n</li>\n<li>\n<p><strong>Hiroshi Ishii</strong> (MIT Media Lab)</p>\n</li>\n<li>\n<p><strong>Takeo Igarashi</strong> (The University of Tokyo)</p>\n</li>\n</ul>\n<p>&lt;br/&gt;</p>\n<p>Defense Date: May 13th, 2020</p>\n",dir:"content/output/projects",base:"phd-thesis.json",ext:".json",sourceBase:"phd-thesis.md",sourceExt:".md"}},IP1Z:function(t,e,o){"use strict";var i=o("2faE"),a=o("rr1i");t.exports=function(t,e,o){e in t?i.f(t,e,a(0,o)):t[e]=o}},"JMW+":function(t,e,o){"use strict";var i,a,n,s,r=o("uOPS"),c=o("5T2Y"),l=o("2GTP"),u=o("QMMT"),h=o("Y7ZC"),p=o("93I4"),g=o("eaoh"),d=o("EXMj"),m=o("oioR"),f=o("8gHz"),b=o("QXhf").set,v=o("q6LJ")(),y=o("ZW5q"),q=o("RDmV"),w=o("vBP9"),j=o("zXhZ"),x=c.TypeError,k=c.process,T=k&&k.versions,I=T&&T.v8||"",S=c.Promise,A="process"==u(k),R=function(){},C=a=y.f,E=!!function(){try{var t=S.resolve(1),e=(t.constructor={})[o("UWiX")("species")]=function(t){t(R,R)};return(A||"function"==typeof PromiseRejectionEvent)&&t.then(R)instanceof e&&0!==I.indexOf("6.6")&&-1===w.indexOf("Chrome/66")}catch(i){}}(),P=function(t){var e;return!(!p(t)||"function"!=typeof(e=t.then))&&e},D=function(t,e){if(!t._n){t._n=!0;var o=t._c;v(function(){for(var i=t._v,a=1==t._s,n=0,s=function(e){var o,n,s,r=a?e.ok:e.fail,c=e.resolve,l=e.reject,u=e.domain;try{r?(a||(2==t._h&&F(t),t._h=1),!0===r?o=i:(u&&u.enter(),o=r(i),u&&(u.exit(),s=!0)),o===e.promise?l(x("Promise-chain cycle")):(n=P(o))?n.call(o,c,l):c(o)):l(i)}catch(h){u&&!s&&u.exit(),l(h)}};o.length>n;)s(o[n++]);t._c=[],t._n=!1,e&&!t._h&&z(t)})}},z=function(t){b.call(c,function(){var e,o,i,a=t._v,n=M(t);if(n&&(e=q(function(){A?k.emit("unhandledRejection",a,t):(o=c.onunhandledrejection)?o({promise:t,reason:a}):(i=c.console)&&i.error&&i.error("Unhandled promise rejection",a)}),t._h=A||M(t)?2:1),t._a=void 0,n&&e.e)throw e.v})},M=function(t){return 1!==t._h&&0===(t._a||t._c).length},F=function(t){b.call(c,function(){var e;A?k.emit("rejectionHandled",t):(e=c.onrejectionhandled)&&e({promise:t,reason:t._v})})},B=function(t){var e=this;e._d||(e._d=!0,(e=e._w||e)._v=t,e._s=2,e._a||(e._a=e._c.slice()),D(e,!0))},O=function(t){var e,o=this;if(!o._d){o._d=!0,o=o._w||o;try{if(o===t)throw x("Promise can't be resolved itself");(e=P(t))?v(function(){var i={_w:o,_d:!1};try{e.call(t,l(O,i,1),l(B,i,1))}catch(a){B.call(i,a)}}):(o._v=t,o._s=1,D(o,!1))}catch(i){B.call({_w:o,_d:!1},i)}}};E||(S=function(t){d(this,S,"Promise","_h"),g(t),i.call(this);try{t(l(O,this,1),l(B,this,1))}catch(e){B.call(this,e)}},(i=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1}).prototype=o("XJU/")(S.prototype,{then:function(t,e){var o=C(f(this,S));return o.ok="function"!=typeof t||t,o.fail="function"==typeof e&&e,o.domain=A?k.domain:void 0,this._c.push(o),this._a&&this._a.push(o),this._s&&D(this,!1),o.promise},catch:function(t){return this.then(void 0,t)}}),n=function(){var t=new i;this.promise=t,this.resolve=l(O,t,1),this.reject=l(B,t,1)},y.f=C=function(t){return t===S||t===s?new n(t):a(t)}),h(h.G+h.W+h.F*!E,{Promise:S}),o("RfKB")(S,"Promise"),o("TJWN")("Promise"),s=o("WEpk").Promise,h(h.S+h.F*!E,"Promise",{reject:function(t){var e=C(this);return(0,e.reject)(t),e.promise}}),h(h.S+h.F*(r||!E),"Promise",{resolve:function(t){return j(r&&this===s?S:this,t)}}),h(h.S+h.F*!(E&&o("TuGD")(function(t){S.all(t).catch(R)})),"Promise",{all:function(t){var e=this,o=C(e),i=o.resolve,a=o.reject,n=q(function(){var o=[],n=0,s=1;m(t,!1,function(t){var r=n++,c=!1;o.push(void 0),s++,e.resolve(t).then(function(t){c||(c=!0,o[r]=t,--s||i(o))},a)}),--s||i(o)});return n.e&&a(n.v),o.promise},race:function(t){var e=this,o=C(e),i=o.reject,a=q(function(){m(t,!1,function(t){e.resolve(t).then(o.resolve,i)})});return a.e&&i(a.v),o.promise}})},JQMT:function(t,e,o){"use strict";var i=o("KI45"),a=i(o("ln6h")),n=i(o("O40h")),s=i(o("doui")),r=i(o("eVuF")),c=i(o("UXZV")),l=i(o("ttDY")),u=i(o("0iUn")),h=i(o("sLSF")),p=function(t){return t&&t.__esModule?t:{default:t}};Object.defineProperty(e,"__esModule",{value:!0});var g=o("CxY0"),d=p(o("kiME")),m=o("Bu4q");function f(t){return t.replace(/\/$/,"")||"/"}var b=function(){function t(e,o,i,a){var n=this,s=a.initialProps,r=a.pageLoader,h=a.App,p=a.Component,g=a.err;(0,u.default)(this,t),this.onPopState=function(t){if(t.state){if((!t.state.options||!t.state.options.fromExternal)&&(!n._bps||n._bps(t.state))){var e=t.state,o=e.url,i=e.as,a=e.options;0,n.replace(o,i,a)}}else{var s=n.pathname,r=n.query;n.changeState("replaceState",m.formatWithValidation({pathname:s,query:r}),m.getURL())}},this.route=f(e),this.components={},"/_error"!==e&&(this.components[this.route]={Component:p,props:s,err:g}),this.components["/_app"]={Component:h},this.events=t.events,this.pageLoader=r,this.pathname=e,this.query=o,this.asPath=i,this.subscriptions=new l.default,this.componentLoadCancel=null,"undefined"!=typeof window&&(this.changeState("replaceState",m.formatWithValidation({pathname:e,query:o}),i),window.addEventListener("popstate",this.onPopState),window.addEventListener("unload",function(){if(history.state){var t=history.state,e=t.url,o=t.as,i=t.options;n.changeState("replaceState",e,o,(0,c.default)({},i,{fromExternal:!0}))}}))}return(0,h.default)(t,[{key:"update",value:function(t,e){var o=this.components[t];if(!o)throw new Error("Cannot update unavailable route: ".concat(t));var i=(0,c.default)({},o,{Component:e});this.components[t]=i,"/_app"!==t?t===this.route&&this.notify(i):this.notify(this.components[this.route])}},{key:"reload",value:function(e){var o=this;return new r.default(function(i,a){if(delete o.components[e],o.pageLoader.clearCache(e),e!==o.route)return i();var n=o.pathname,s=o.query,r=window.location.href,c=window.location.pathname+window.location.search+window.location.hash;t.events.emit("routeChangeStart",r),o.getRouteInfo(e,n,s,c).then(function(e){var n=e.error;return n&&n.cancelled?i():(o.notify(e),n?(t.events.emit("routeChangeError",n,r),a(n)):void t.events.emit("routeChangeComplete",r))})})}},{key:"back",value:function(){window.history.back()}},{key:"push",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:t,o=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};return this.change("pushState",t,e,o)}},{key:"replace",value:function(t){var e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:t,o=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};return this.change("replaceState",t,e,o)}},{key:"change",value:function(e,o,i,a){var n=this;return new r.default(function(s,r){var l="object"==typeof o?m.formatWithValidation(o):o,u="object"==typeof i?m.formatWithValidation(i):i;if(__NEXT_DATA__.nextExport&&(u=t._rewriteUrlForNextExport(u)),n.abortComponentLoad(u),n.onlyAHashChange(u))return t.events.emit("hashChangeStart",u),n.changeState(e,l,u),n.scrollToHash(u),t.events.emit("hashChangeComplete",u),!0;var h=g.parse(l,!0),p=h.pathname,d=h.query;n.urlIsNew(u)||(e="replaceState");var b=f(p),v=a.shallow,y=void 0!==v&&v;t.events.emit("routeChangeStart",u),n.getRouteInfo(b,p,d,u,y).then(function(o){var i=o.error;if(i&&i.cancelled)return s(!1);t.events.emit("beforeHistoryChange",u),n.changeState(e,l,u,a);var r=window.location.hash.substring(1);if(n.set(b,p,d,u,(0,c.default)({},o,{hash:r})),i)throw t.events.emit("routeChangeError",i,u),i;return t.events.emit("routeChangeComplete",u),s(!0)},r)})}},{key:"changeState",value:function(t,e,o){var i=arguments.length>3&&void 0!==arguments[3]?arguments[3]:{};"pushState"===t&&m.getURL()===o||window.history[t]({url:e,as:o,options:i},null,o)}},{key:"getRouteInfo",value:function(t,e,o,i){var a=this,n=arguments.length>4&&void 0!==arguments[4]&&arguments[4],s=this.components[t];return n&&s&&this.route===t?r.default.resolve(s):new r.default(function(e,o){if(s)return e(s);a.fetchComponent(t).then(function(t){return e({Component:t})},o)}).then(function(n){var s=n.Component;return new r.default(function(r,c){var l={pathname:e,query:o,asPath:i};a.getInitialProps(s,l).then(function(e){n.props=e,a.components[t]=n,r(n)},c)})}).catch(function(t){return new r.default(function(n){return"PAGE_LOAD_ERROR"===t.code?(window.location.href=i,t.cancelled=!0,n({error:t})):t.cancelled?n({error:t}):void n(a.fetchComponent("/_error").then(function(i){var n={Component:i,err:t},s={err:t,pathname:e,query:o};return new r.default(function(e){a.getInitialProps(i,s).then(function(o){n.props=o,n.error=t,e(n)},function(o){console.error("Error in error page `getInitialProps`: ",o),n.error=t,n.props={},e(n)})})}))})})}},{key:"set",value:function(t,e,o,i,a){this.route=t,this.pathname=e,this.query=o,this.asPath=i,this.notify(a)}},{key:"beforePopState",value:function(t){this._bps=t}},{key:"onlyAHashChange",value:function(t){if(!this.asPath)return!1;var e=this.asPath.split("#"),o=(0,s.default)(e,2),i=o[0],a=o[1],n=t.split("#"),r=(0,s.default)(n,2),c=r[0],l=r[1];return!(!l||i!==c||a!==l)||i===c&&a!==l}},{key:"scrollToHash",value:function(t){var e=t.split("#"),o=(0,s.default)(e,2)[1];if(""!==o){var i=document.getElementById(o);if(i)i.scrollIntoView();else{var a=document.getElementsByName(o)[0];a&&a.scrollIntoView()}}else window.scrollTo(0,0)}},{key:"urlIsNew",value:function(t){return this.asPath!==t}},{key:"prefetch",value:function(t){var e=this;return new r.default(function(o,i){var a=f(g.parse(t).pathname);e.pageLoader.prefetch(a).then(o,i)})}},{key:"fetchComponent",value:function(){var t=(0,n.default)(a.default.mark(function t(e){var o,i,n,s;return a.default.wrap(function(t){for(;;)switch(t.prev=t.next){case 0:return o=!1,i=this.componentLoadCancel=function(){o=!0},t.next=4,this.pageLoader.loadPage(e);case 4:if(n=t.sent,!o){t.next=9;break}throw(s=new Error('Abort fetching component for route: "'.concat(e,'"'))).cancelled=!0,s;case 9:return i===this.componentLoadCancel&&(this.componentLoadCancel=null),t.abrupt("return",n);case 11:case"end":return t.stop()}},t,this)}));return function(e){return t.apply(this,arguments)}}()},{key:"getInitialProps",value:function(){var t=(0,n.default)(a.default.mark(function t(e,o){var i,n,s,r,c;return a.default.wrap(function(t){for(;;)switch(t.prev=t.next){case 0:return i=!1,n=function(){i=!0},this.componentLoadCancel=n,s=this.components["/_app"].Component,t.next=6,m.loadGetInitialProps(s,{Component:e,router:this,ctx:o});case 6:if(r=t.sent,n===this.componentLoadCancel&&(this.componentLoadCancel=null),!i){t.next=12;break}throw(c=new Error("Loading initial props cancelled")).cancelled=!0,c;case 12:return t.abrupt("return",r);case 13:case"end":return t.stop()}},t,this)}));return function(e,o){return t.apply(this,arguments)}}()},{key:"abortComponentLoad",value:function(e){this.componentLoadCancel&&(t.events.emit("routeChangeError",new Error("Route Cancelled"),e),this.componentLoadCancel(),this.componentLoadCancel=null)}},{key:"notify",value:function(t){var e=this.components["/_app"].Component;this.subscriptions.forEach(function(o){return o((0,c.default)({},t,{App:e}))})}},{key:"subscribe",value:function(t){var e=this;return this.subscriptions.add(t),function(){return e.subscriptions.delete(t)}}}],[{key:"_rewriteUrlForNextExport",value:function(t){var e=t.split("#"),o=(0,s.default)(e,2),i=o[0],a=o[1],n=i.split("?"),r=(0,s.default)(n,2),c=r[0],l=r[1];return c=c.replace(/\/$/,""),/\.[^/]+\/?$/.test(c)||(c+="/"),l&&(c+="?"+l),a&&(c+="#"+a),c}}]),t}();b.events=d.default(),e.default=b},Jg5j:function(t){t.exports={id:"trace-diff",name:"TraceDiff",description:"Debugging Unexpected Code Behavior Using Trace Divergences",title:"TraceDiff: Debugging Unexpected Code Behavior Using Trace Divergences",authors:["Ryo Suzuki","Gustavo Soares","Andrew Head","Elena Glassman","Ruan Reis","Melina Mongiovi","Loris D’Antoni","Bjöern Hartmann"],year:2017,booktitle:"In Proceedings of 2017 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC '17)",publisher:"IEEE Press, Piscataway, NJ, USA",pages:"107-115",doi:"https://doi.org/10.1109/VLHCC.2017.8103457",conference:{name:"VL/HCC 2017",fullname:"IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC 2017)",url:"https://sites.google.com/site/vlhcc2017/"},pdf:"vlhcc-2017-tracediff.pdf",slide:"vlhcc-2017-tracediff-slide.pdf",github:"https://github.com/ryosuzuki/trace-diff",demo:"https://ryosuzuki.github.io/trace-diff/",ieee:"http://ieeexplore.ieee.org/document/8103457/",arxiv:"https://arxiv.org/abs/1708.03786a",related:{title:"Exploring the Design Space of Automatically Synthesized Hints for Introductory Programming Assignments",authors:["Ryo Suzuki","Gustavo Soares","Elena Glassman","Andrew Head","Loris D'Antoni","Björn Hartmann"],year:2017,booktitle:"In Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA '17)",publisher:"ACM, New York, NY, USA",pages:"2951-2958",doi:"https://doi.org/10.1145/3027063.3053187",pdf:"chi-2017-lbw.pdf",suffix:"lbw",pageCount:6},pageCount:9,slideCount:77,bodyContent:'<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/trace-diff/top.mp4" type="video/mp4"></source>\n</video>\n\n# Abstract\n\nRecent advances in program synthesis offer means to automatically debug student submissions and generate personalized feedback in massive programming classrooms. When automatically generating feedback for programming assignments, a key challenge is designing pedagogically useful hints that are as effective as the manual feedback given by teachers. Through an analysis of teachers’ hint-giving practices in 132 online Q&A posts, we establish three design guidelines that an effective feedback design should follow. Based on these guidelines, we develop a feedback system that leverages both program synthesis and visualization techniques. Our system compares the dynamic code execution of both incorrect and fixed code and highlights how the error leads to a difference in behavior and where the incorrect code trace diverges from the expected solution. Results from our study suggest that our system enables students to detect and fix bugs that are not caught by students using another existing visual debugging tool.',bodyHtml:"<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/trace-diff/top.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<h1>Abstract</h1>\n<p>Recent advances in program synthesis offer means to automatically debug student submissions and generate personalized feedback in massive programming classrooms. When automatically generating feedback for programming assignments, a key challenge is designing pedagogically useful hints that are as effective as the manual feedback given by teachers. Through an analysis of teachers’ hint-giving practices in 132 online Q&amp;A posts, we establish three design guidelines that an effective feedback design should follow. Based on these guidelines, we develop a feedback system that leverages both program synthesis and visualization techniques. Our system compares the dynamic code execution of both incorrect and fixed code and highlights how the error leads to a difference in behavior and where the incorrect code trace diverges from the expected solution. Results from our study suggest that our system enables students to detect and fix bugs that are not caught by students using another existing visual debugging tool.</p>\n",dir:"content/output/projects",base:"trace-diff.json",ext:".json",sourceBase:"trace-diff.md",sourceExt:".md"}},K47E:function(t,e){t.exports=function(t){if(void 0===t)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return t}},KI45:function(t,e){t.exports=function(t){return t&&t.__esModule?t:{default:t}}},KZOl:function(t){t.exports={id:"ar-and-robotics",name:"Augmented Reality and Robotics",description:"A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces",title:"Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces",authors:["Ryo Suzuki","Adnan Karim","Tian Xia","Hooman Hedayati","Nicolai Marquardt"],year:2022,conference:{name:"CHI 2022",fullname:"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2022)",url:"https://chi2022.acm.org/"},external:"https://ilab.ucalgary.ca/ar-and-robotics/",bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"ar-and-robotics.json",ext:".json",sourceBase:"ar-and-robotics.md",sourceExt:".md"}},MCSJ:function(t,e){t.exports=function(t,e,o){var i=void 0===o;switch(e.length){case 0:return i?t():t.call(o);case 1:return i?t(e[0]):t.call(o,e[0]);case 2:return i?t(e[0],e[1]):t.call(o,e[0],e[1]);case 3:return i?t(e[0],e[1],e[2]):t.call(o,e[0],e[1],e[2]);case 4:return i?t(e[0],e[1],e[2],e[3]):t.call(o,e[0],e[1],e[2],e[3])}return t.apply(o,e)}},Mqbl:function(t,e,o){var i=o("JB68"),a=o("w6GO");o("zn7N")("keys",function(){return function(t){return a(i(t))}})},N9n2:function(t,e,o){var i=o("SqZg"),a=o("vjea");t.exports=function(t,e){if("function"!=typeof e&&null!==e)throw new TypeError("Super expression must either be null or a function");t.prototype=i(e&&e.prototype,{constructor:{value:t,writable:!0,configurable:!0}}),e&&a(t,e)}},Nehr:function(t,e,o){"use strict";t.exports={isString:function(t){return"string"==typeof t},isObject:function(t){return"object"==typeof t&&null!==t},isNull:function(t){return null===t},isNullOrUndefined:function(t){return null==t}}},NwJ3:function(t,e,o){var i=o("SBuE"),a=o("UWiX")("iterator"),n=Array.prototype;t.exports=function(t){return void 0!==t&&(i.Array===t||n[a]===t)}},O40h:function(t,e,o){"use strict";o.r(e),o.d(e,"default",function(){return s});var i=o("eVuF"),a=o.n(i);function n(t,e,o,i,n,s,r){try{var c=t[s](r),l=c.value}catch(u){return void o(u)}c.done?e(l):a.a.resolve(l).then(i,n)}function s(t){return function(){var e=this,o=arguments;return new a.a(function(i,a){var s=t.apply(e,o);function r(t){n(s,i,a,r,c,"next",t)}function c(t){n(s,i,a,r,c,"throw",t)}r(void 0)})}}},OsUb:function(t){t.exports={id:"realitytalk",name:"RealityTalk",description:"Real-time Speech-driven Augmented Presentation for AR Live Storytelling",title:"RealityTalk: Real-time Speech-driven Augmented Presentation for AR Live Storytelling",authors:["Jian Liao","Adnan Karim","Shivesh Jadon","Rubaiat Habib Kazi","Ryo Suzuki"],year:2022,booktitle:"In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology (UIST '22)",publisher:"ACM, New York, NY, USA",doi:"https://doi.org/10.1145/3526113.3545702",conference:{name:"UIST 2022",fullname:"The ACM Symposium on User Interface Software and Technology (UIST 2022)",url:"http://uist.acm.org/uist2022"},pdf:"uist-2022-realitytalk.pdf",video:"https://www.youtube.com/watch?v=vfIMeICV-7c",embed:"https://www.youtube.com/embed/vfIMeICV-7c",arxiv:"https://arxiv.org/abs/2208.06350","acm-dl":"https://dl.acm.org/doi/10.1145/3526113.3545702",pageCount:12,slideCount:0,bodyContent:"# Abstract\n\nWe present RealityTalk, a system that augments real-time live presentations with speech-driven interactive virtual elements. Augmented presentations leverage embedded visuals and animation for engaging and expressive storytelling. However, existing tools for live presentations often lack interactivity and improvisation, while creating such effects in video editing tools require significant time and expertise. RealityTalk enables users to create live augmented presentations with real-time speech-driven interactions. The user can interactively prompt, move, and manipulate graphical elements through real-time speech and supporting modalities. Based on our analysis of 177 existing video-edited augmented presentations ([https://ilab.ucalgary.ca/realitytalk/](https://ilab.ucalgary.ca/realitytalk/)), we propose a novel set of interaction techniques and then incorporated them into RealityTalk. We evaluate our tool from a presenter's perspective to demonstrate the effectiveness of our system.",bodyHtml:'<h1>Abstract</h1>\n<p>We present RealityTalk, a system that augments real-time live presentations with speech-driven interactive virtual elements. Augmented presentations leverage embedded visuals and animation for engaging and expressive storytelling. However, existing tools for live presentations often lack interactivity and improvisation, while creating such effects in video editing tools require significant time and expertise. RealityTalk enables users to create live augmented presentations with real-time speech-driven interactions. The user can interactively prompt, move, and manipulate graphical elements through real-time speech and supporting modalities. Based on our analysis of 177 existing video-edited augmented presentations (<a href="https://ilab.ucalgary.ca/realitytalk/">https://ilab.ucalgary.ca/realitytalk/</a>), we propose a novel set of interaction techniques and then incorporated them into RealityTalk. We evaluate our tool from a presenter\'s perspective to demonstrate the effectiveness of our system.</p>\n',dir:"content/output/projects",base:"realitytalk.json",ext:".json",sourceBase:"realitytalk.md",sourceExt:".md"}},PBE1:function(t,e,o){"use strict";var i=o("Y7ZC"),a=o("WEpk"),n=o("5T2Y"),s=o("8gHz"),r=o("zXhZ");i(i.P+i.R,"Promise",{finally:function(t){var e=s(this,a.Promise||n.Promise),o="function"==typeof t;return this.then(o?function(o){return r(e,t()).then(function(){return o})}:t,o?function(o){return r(e,t()).then(function(){throw o})}:t)}})},PSd4:function(t){t.exports={id:"mixed-initiative",name:"Mixed-Initiative Code Feedback",description:"Writing Reusable Code Feedback at Scale with Mixed-Initiative Program Synthesis",title:"Writing Reusable Code Feedback at Scale with Mixed-Initiative Program Synthesis",authors:["Andrew Head","Elena Glassman","Gustavo Soares","Ryo Suzuki","Lucas Figueredo","Loris D’Antoni","Björn Hartmann"],note:"(the first three authors equally contributed)",year:2017,booktitle:"In Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale (L@S '17)",publisher:"ACM, New York, NY, USA",pages:"89-98",doi:"https://doi.org/10.1145/3051457.3051467",conference:{name:"L@S 2017",fullname:"The ACM Conference on Learning at Scale (L@S 2017)",url:"http://learningatscale.acm.org/las2017"},pdf:"las-2017-mixed.pdf",slide:"las-2017-mixed-slide.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=3051467",pageCount:10,slideCount:62,bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"mixed-initiative.json",ext:".json",sourceBase:"mixed-initiative.md",sourceExt:".md"}},"Q/yX":function(t,e,o){"use strict";var i=o("Y7ZC"),a=o("ZW5q"),n=o("RDmV");i(i.S,"Promise",{try:function(t){var e=a.f(this),o=n(t);return(o.e?e.reject:e.resolve)(o.v),e.promise}})},QXhf:function(t,e,o){var i,a,n,s=o("2GTP"),r=o("MCSJ"),c=o("MvwC"),l=o("Hsns"),u=o("5T2Y"),h=u.process,p=u.setImmediate,g=u.clearImmediate,d=u.MessageChannel,m=u.Dispatch,f=0,b={},v=function(){var t=+this;if(b.hasOwnProperty(t)){var e=b[t];delete b[t],e()}},y=function(t){v.call(t.data)};p&&g||(p=function(t){for(var e=[],o=1;arguments.length>o;)e.push(arguments[o++]);return b[++f]=function(){r("function"==typeof t?t:Function(t),e)},i(f),f},g=function(t){delete b[t]},"process"==o("a0xu")(h)?i=function(t){h.nextTick(s(v,t,1))}:m&&m.now?i=function(t){m.now(s(v,t,1))}:d?(n=(a=new d).port2,a.port1.onmessage=y,i=s(n.postMessage,n,1)):u.addEventListener&&"function"==typeof postMessage&&!u.importScripts?(i=function(t){u.postMessage(t+"","*")},u.addEventListener("message",y,!1)):i="onreadystatechange"in l("script")?function(t){c.appendChild(l("script")).onreadystatechange=function(){c.removeChild(this),v.call(t)}}:function(t){setTimeout(s(v,t,1),0)}),t.exports={set:p,clear:g}},Qq35:function(t){t.exports={id:"augmented-math",name:"Augmented Math",description:"Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks",title:"Augmented Math: Authoring AR-Based Explorable Explanations by Augmenting Static Math Textbooks",authors:["Neil Chulpongsatorn","Mille Skovhus Lunding","Nishan Soni","Ryo Suzuki"],year:2023,booktitle:"In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (UIST '23)",publisher:"ACM, New York, NY, USA",conference:{name:"UIST 2023",fullname:"The ACM Symposium on User Interface Software and Technology (UIST 2023)",url:"http://uist.acm.org/uist2023"},pdf:"uist-2023-augmented-math.pdf",bodyContent:"# Abstract\n\nWe introduce Augmented Math, a machine learning-based approach to authoring AR explorable explanations by augmenting static math textbooks without programming. To augment a static document, our system first extracts mathematical formulas and figures from a given document using optical character recognition (OCR) and computer vision. By binding and manipulating these extracted contents, the user can see the interactive animation overlaid onto the document through mobile AR interfaces. This empowers non-technical users, such as teachers or students, to transform existing math textbooks and handouts into on-demand and personalized explorable explanations. To design our system, we first analyzed existing explorable math explanations to identify common design strategies. Based on the findings, we developed a set of augmentation techniques that can be automatically generated based on the extracted content, which are 1) dynamic values, 2) interactive figures, 3) relationship highlights, 4) concrete examples, and 5) step-by-step hints. To evaluate our system, we conduct two user studies: preliminary user testing and expert interviews. The study results confirm that our system allows more engaging experiences for learning math concepts.",bodyHtml:"<h1>Abstract</h1>\n<p>We introduce Augmented Math, a machine learning-based approach to authoring AR explorable explanations by augmenting static math textbooks without programming. To augment a static document, our system first extracts mathematical formulas and figures from a given document using optical character recognition (OCR) and computer vision. By binding and manipulating these extracted contents, the user can see the interactive animation overlaid onto the document through mobile AR interfaces. This empowers non-technical users, such as teachers or students, to transform existing math textbooks and handouts into on-demand and personalized explorable explanations. To design our system, we first analyzed existing explorable math explanations to identify common design strategies. Based on the findings, we developed a set of augmentation techniques that can be automatically generated based on the extracted content, which are 1) dynamic values, 2) interactive figures, 3) relationship highlights, 4) concrete examples, and 5) step-by-step hints. To evaluate our system, we conduct two user studies: preliminary user testing and expert interviews. The study results confirm that our system allows more engaging experiences for learning math concepts.</p>\n",dir:"content/output/projects",base:"augmented-math.json",ext:".json",sourceBase:"augmented-math.md",sourceExt:".md"}},RDmV:function(t,e){t.exports=function(t){try{return{e:!1,v:t()}}catch(e){return{e:!0,v:e}}}},"RRc/":function(t,e,o){var i=o("oioR");t.exports=function(t,e){var o=[];return i(t,!1,o.push,o,e),o}},Rp86:function(t,e,o){o("bBy9"),o("FlQf"),t.exports=o("fXsU")},TJWN:function(t,e,o){"use strict";var i=o("5T2Y"),a=o("WEpk"),n=o("2faE"),s=o("jmDH"),r=o("UWiX")("species");t.exports=function(t){var e="function"==typeof a[t]?a[t]:i[t];s&&e&&!e[r]&&n.f(e,r,{configurable:!0,get:function(){return this}})}},TuGD:function(t,e,o){var i=o("UWiX")("iterator"),a=!1;try{var n=[7][i]();n.return=function(){a=!0},Array.from(n,function(){throw 2})}catch(s){}t.exports=function(t,e){if(!e&&!a)return!1;var o=!1;try{var n=[7],r=n[i]();r.next=function(){return{done:o=!0}},n[i]=function(){return r},t(n)}catch(s){}return o}},UXZV:function(t,e,o){t.exports=o("UbbE")},UbbE:function(t,e,o){o("o8NH"),t.exports=o("WEpk").Object.assign},"V+O7":function(t,e,o){o("aPfg")("Set")},V7Et:function(t,e,o){var i=o("2GTP"),a=o("M1xp"),n=o("JB68"),s=o("tEej"),r=o("v6xn");t.exports=function(t,e){var o=1==t,c=2==t,l=3==t,u=4==t,h=6==t,p=5==t||h,g=e||r;return function(e,r,d){for(var m,f,b=n(e),v=a(b),y=i(r,d,3),q=s(v.length),w=0,j=o?g(e,q):c?g(e,0):void 0;q>w;w++)if((p||w in v)&&(f=y(m=v[w],w,b),t))if(o)j[w]=f;else if(f)switch(t){case 3:return!0;case 5:return m;case 6:return w;case 2:j.push(m)}else if(u)return!1;return h?-1:l||u?u:j}}},VECh:function(t){t.exports={id:"holobots",name:"HoloBots",description:"Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality",title:"HoloBots: Augmenting Holographic Telepresence with Mobile Robots for Tangible Remote Collaboration in Mixed Reality",authors:["Keiichi Ihara","Mehrad Faridan","Ayumi Ichikawa","Ikkaku Kawaguchi","Ryo Suzuki"],year:2023,booktitle:"In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (UIST '23)",publisher:"ACM, New York, NY, USA",conference:{name:"UIST 2023",fullname:"The ACM Symposium on User Interface Software and Technology (UIST 2023)",url:"http://uist.acm.org/uist2023"},pdf:"uist-2023-holobots.pdf",bodyContent:"# Abstract\n\nThis paper introduces HoloBots, a mixed reality interface that augments holographic telepresence with a synchronized actuated tangible user interface. Beyond existing mixed reality telepresence, HoloBots lets remote users not only be visually and spatially present, but also enables them to physically engage with the local users and their environment, allowing them to touch, grasp, manipulate, and interact with remote physical objects as if they were co-located in the same shared space. We achieve this by synchronizing holographic user motion (Hololens 2 and Azure Kinect) with tabletop mobile robots (Sony Toio), which enables scalable, deployable, and generalizable tangible remote collaboration. HoloBots allows various interactions, such as object actuation, virtual hand physicalization, world-in-miniature exploration, shared tangible interfaces, embodied guidance, and haptic communication. We evaluate our system by comparing it with hologram-only and robot-only conditions. Both quantitative and qualitative results with 12 participants confirm that our system significantly enhances the level of co-presence and shared experience of mixed reality remote collaboration, compared to the other conditions.",bodyHtml:"<h1>Abstract</h1>\n<p>This paper introduces HoloBots, a mixed reality interface that augments holographic telepresence with a synchronized actuated tangible user interface. Beyond existing mixed reality telepresence, HoloBots lets remote users not only be visually and spatially present, but also enables them to physically engage with the local users and their environment, allowing them to touch, grasp, manipulate, and interact with remote physical objects as if they were co-located in the same shared space. We achieve this by synchronizing holographic user motion (Hololens 2 and Azure Kinect) with tabletop mobile robots (Sony Toio), which enables scalable, deployable, and generalizable tangible remote collaboration. HoloBots allows various interactions, such as object actuation, virtual hand physicalization, world-in-miniature exploration, shared tangible interfaces, embodied guidance, and haptic communication. We evaluate our system by comparing it with hologram-only and robot-only conditions. Both quantitative and qualitative results with 12 participants confirm that our system significantly enhances the level of co-presence and shared experience of mixed reality remote collaboration, compared to the other conditions.</p>\n",dir:"content/output/projects",base:"holobots.json",ext:".json",sourceBase:"holobots.md",sourceExt:".md"}},VJsP:function(t,e,o){"use strict";var i=o("2GTP"),a=o("Y7ZC"),n=o("JB68"),s=o("sNwI"),r=o("NwJ3"),c=o("tEej"),l=o("IP1Z"),u=o("fNZA");a(a.S+a.F*!o("TuGD")(function(t){Array.from(t)}),"Array",{from:function(t){var e,o,a,h,p=n(t),g="function"==typeof this?this:Array,d=arguments.length,m=d>1?arguments[1]:void 0,f=void 0!==m,b=0,v=u(p);if(f&&(m=i(m,d>2?arguments[2]:void 0,2)),null==v||g==Array&&r(v))for(o=new g(e=c(p.length));e>b;b++)l(o,b,f?m(p[b],b):p[b]);else for(h=v.call(p),o=new g;!(a=h.next()).done;b++)l(o,b,f?s(h,m,[a.value,b],!0):a.value);return o.length=b,o}})},VKFn:function(t,e,o){o("bBy9"),o("FlQf"),t.exports=o("ldVq")},"W/HP":function(t){t.exports={id:"pep",name:"PEP",description:"3D Printed Electronic Papercrafts - An Integrated Approach for 3D Sculpting Paper-based Electronic Devices",title:"PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-based Electronic Devices",authors:["Hyunjoo Oh","Tung D. Ta","Ryo Suzuki","Mark D. Gross","Yoshihiro Kawahara","Lining Yao"],year:2018,booktitle:"In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18)",publisher:"ACM, New York, NY, USA",pages:"Paper 441, 12 pages",doi:"https://doi.org/10.1145/3173574.3174015",conference:{name:"CHI 2018",fullname:"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2018)",url:"https://chi2018.acm.org/"},pdf:"chi-2018-pep.pdf",video:"https://vimeo.com/252080903",embed:"https://www.youtube.com/embed/DTd863suDN0","short-video":"https://www.youtube.com/watch?v=DTd863suDN0","acm-dl":"https://dl.acm.org/citation.cfm?id=3174015",pageCount:12,slideCount:0,bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"pep.json",ext:".json",sourceBase:"pep.md",sourceExt:".md"}},WaGi:function(t,e,o){var i=o("hfKm");function a(t,e){for(var o=0;o<e.length;o++){var a=e[o];a.enumerable=a.enumerable||!1,a.configurable=!0,"value"in a&&(a.writable=!0),i(t,a.key,a)}}t.exports=function(t,e,o){return e&&a(t.prototype,e),o&&a(t,o),t}},WbBG:function(t,e,o){"use strict";t.exports="SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED"},Wu5q:function(t,e,o){"use strict";var i=o("2faE").f,a=o("oVml"),n=o("XJU/"),s=o("2GTP"),r=o("EXMj"),c=o("oioR"),l=o("MPFp"),u=o("UO39"),h=o("TJWN"),p=o("jmDH"),g=o("6/1s").fastKey,d=o("n3ko"),m=p?"_s":"size",f=function(t,e){var o,i=g(e);if("F"!==i)return t._i[i];for(o=t._f;o;o=o.n)if(o.k==e)return o};t.exports={getConstructor:function(t,e,o,l){var u=t(function(t,i){r(t,u,e,"_i"),t._t=e,t._i=a(null),t._f=void 0,t._l=void 0,t[m]=0,null!=i&&c(i,o,t[l],t)});return n(u.prototype,{clear:function(){for(var t=d(this,e),o=t._i,i=t._f;i;i=i.n)i.r=!0,i.p&&(i.p=i.p.n=void 0),delete o[i.i];t._f=t._l=void 0,t[m]=0},delete:function(t){var o=d(this,e),i=f(o,t);if(i){var a=i.n,n=i.p;delete o._i[i.i],i.r=!0,n&&(n.n=a),a&&(a.p=n),o._f==i&&(o._f=a),o._l==i&&(o._l=n),o[m]--}return!!i},forEach:function(t){d(this,e);for(var o,i=s(t,arguments.length>1?arguments[1]:void 0,3);o=o?o.n:this._f;)for(i(o.v,o.k,this);o&&o.r;)o=o.p},has:function(t){return!!f(d(this,e),t)}}),p&&i(u.prototype,"size",{get:function(){return d(this,e)[m]}}),u},def:function(t,e,o){var i,a,n=f(t,e);return n?n.v=o:(t._l=n={i:a=g(e,!0),k:e,v:o,p:i=t._l,n:void 0,r:!1},t._f||(t._f=n),i&&(i.n=n),t[m]++,"F"!==a&&(t._i[a]=n)),t},getEntry:f,setStrong:function(t,e,o){l(t,e,function(t,o){this._t=d(t,e),this._k=o,this._l=void 0},function(){for(var t=this._k,e=this._l;e&&e.r;)e=e.p;return this._t&&(this._l=e=e?e.n:this._t._f)?u(0,"keys"==t?e.k:"values"==t?e.v:[e.k,e.v]):(this._t=void 0,u(1))},o?"entries":"values",!o,!0),h(e)}}},"X0/d":function(t){t.exports={id:"morphio",name:"MorphIO",description:"Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction",title:"MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction",authors:["Ryosuke Nakayama*","Ryo Suzuki*","Satoshi Nakamaru","Ryuma Niiyama","Yoshihiro Kawahara","Yasuaki Kakehi"],note:"(the first two authors equally contributed)",year:2018,booktitle:"In Proceedings of the 2019 on Designing Interactive Systems Conference (DIS '19)",publisher:"ACM, New York, NY, USA",pages:"975-986",doi:"https://doi.org/10.1145/3322276.3322337",conference:{name:"DIS 2019",fullname:"The ACM conference on Designing Interactive Systems (DIS 2019) - Best Paper Award",url:"https://dis2019.com/"},pdf:"dis-2019-morphio.pdf",slide:"dis-2019-morphio-slide.pdf",video:"https://www.youtube.com/watch?v=ZkCcazfFD-M",embed:"https://www.youtube.com/embed/ZkCcazfFD-M","acm-dl":"https://dl.acm.org/citation.cfm?id=3322337",pageCount:12,slideCount:52,bodyContent:'# Abstract\n\nWe introduce **MorphIO, entirely soft sensing and actuation modules** for programming by demonstration of soft robots and shape-changing interfaces. MorphIO’s hardware consists of a **soft pneumatic actuator containing a conductive sponge sensor**. This allows both input and output of three-dimensional deformation of a soft material. Leveraging this capability, MorphIO enables a user to **record and later playback physical motion** of programmable shape-changing materials. In addition, the modular design of MorphIO’s unit allows the user to construct various shapes and topologies through magnetic connection. We demonstrate several application scenarios, including tangible character animation, locomotion experiment of a soft robot, and prototyping tools for animated soft objects. Our user study with six participants confirms the benefits of MorphIO, as compared to the existing programming paradigm.\n\n\n<video poster="/static/projects/morphio/video-poster/top.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/top.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/top.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-3.png" /></a>\n  </div>\n</div>\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-5.png" /></a>\n  </div>\n</div>\n\n# Introduction\n\n**Programmable soft materials** have a great potential for many application domains, such as soft robotics, material interfaces, accessibility, and haptic interfaces.\n**However, programming of such materials is hard.**\nThe dominant programming paradigm of soft robots and material interfaces is largely confined within a digital screen, leaving little room for users to interactively explore physical motion through tangible interaction. In such a workflow—compiling code on a digital screen then trans- ferring it into the physical object—users need to repeatedly switch between the digital and physical worlds. This leaves a large gulf of execution in their programming experiences.\nThus, the traditional programming paradigm significantly limits the user’s ability to experiment with the design of expressive motion. Moreover, due to this barrier, such an opportunity is largely limited to highly skilled programmers and researchers who are proficient in hardware programming.\n\n\n# MorphIO\n\nThis paper introduces **MorphIO, entirely soft sensing and actuation modules** for programming by demonstration of soft robots and shape-changing interfaces.\nMorphIO’s hardware consists of a soft pneumatic actuator containing a conductive sponge sensor. This allows for integrated and entirely soft shape-changing modules that can both sense and actuate a variety of three-dimensional deformations. Leveraging this capability, MorphIO enables the user to program behaviors by **recording and later playing back physical motions** through tangible interaction. In addition, the modular design of MorphIO’s unit allows the user to construct various shapes and topologies through magnetic connection, then **synthesize multiple recorded motions to achieve more complex behaviors**, such as bending, gripping, and walking.\n\n\n<video poster="/static/projects/morphio/video-poster/module.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/module.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/module.mp4" type="video/mp4"></source>\n</video>\n\n<br/>\n\n<video poster="/static/projects/morphio/video-poster/bear.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/bear.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/bear.mp4" type="video/mp4"></source>\n</video>\n\n\n# System Overview\n\nThe programming workflow with MorphIO is the following:\n\n- **Step 1:** A user starts manipulating the MorphIO unit.\n\n- **Step 2:** The demonstrated motion is detected and recorded through internal sensors, and the recorded sensor values are stored in the software.\n\n- **Step 3:** Once the user clicks play in the graphical user interface, the pneumatic pump starts supplying air.\n\n- **Step 4:** By controlling the air flow through switching on and off the solenoid valves, the system can control the behavior of the pneumatic actuator as it plays back the recorded motion.\n\nThe MorphIO system consists of the following components: A sensor and actuation unit, a sensing and actuation control unit, a microcontroller, software to control these units, and a visual interface for users to control behaviors. Figure illustrates the overview architecture of MorphIO.\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-6-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-6-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-6-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-6-2.png" /></a>\n  </div>\n</div>\n\n\n# Entirely Soft Sensing and Actuation Modules\n\nOur main contribution is a design and fabrication method for **a conductive sponge sensor** that can be embedded into an air chamber in the pneumatic actuator. The conductive sponge sensor leverages the porous structure to **sense the three-dimensional deformation by measuring the internal resistance value**; when contracted, the resistance value be- tween the top and bottom surfaces drops, and when extended, it increases. In contrast to existing sensing techniques, an elastic sponge allows for a higher degree of freedom in sensing capability (e.g., stretching, bending, and compression) without sacrificing the softness of the interface.\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-2-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-2-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-2-2.png" /></a>\n  </div>\n</div>\n\n\n<video poster="/static/projects/morphio/video-poster/mechanism.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/mechanism.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/mechanism.mp4" type="video/mp4"></source>\n</video>\n\n\n<br />\n\nMoreover, our **modular design** and **graphical interface** allows for easy experiments involving multiple units. For example, the system can visualize multiple recorded sensor values, so that the user can see, customize, and synthesize recorded motion to construct more complex behaviors. These hardware and software designs were informed by our formative study, wherein we interviewed five experienced researchers from the robotics and HCI communities.\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-3-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-3-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-3-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-3-2.png" /></a>\n  </div>\n</div>\n\n<video poster="/static/projects/morphio/video-poster/unit-x2.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/unit-x2.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/unit-x2.mp4" type="video/mp4"></source>\n</video>\n\n<video poster="/static/projects/morphio/video-poster/unit-x3.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/unit-x3.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/unit-x3.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-3.png" /></a>\n  </div>\n</div>\n\n\n\n\n# Fabrication Process\n\nThe fabrication process follows three steps: 1) Fabricate an elastic sponge, 2) impregnate into conductive ink, and 3) attach electrodes and wires.\nTo fabricate an elastic sponge, we first prepare 6.0 g of elastomer prepolymer solution and 29.1 g of sodium-chloride, then mix them together by using a planetary centrifugal mixer. The mixed solution is injected into a 3D printed cylindrical mold (16mm diameter, 40mm height). Then we dry the material with an oven at 100 C degrees for one hour. Once dried, we immerse the sponge in water, so that the sodium chloride can melt, leaving a porous structure within the elastomer sponge.\n\n<video poster="/static/projects/morphio/video-poster/fabrication.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/fabrication.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/fabrication.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-4.png" /></a>\n  </div>\n</div>\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-5.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-6.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-6.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-7.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-7.png" /></a>\n  </div>\n</div>\n\n\n\n# Applications\n\nWe demonstrate several possible applications scenarios with MorphIO. 1) Tangible character animation, 2) Animating existing soft objects, 3) Remote manipulation of soft grippers, 4) Locomotion experimentation with soft robots.\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-4.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-3.png" /></a>\n  </div>\n</div>\n\n<video poster="/static/projects/morphio/video-poster/locomotion.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/locomotion.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/locomotion.mp4" type="video/mp4"></source>\n</video>\n\n\n# Evaluation\n\nWe conducted a user evaluation study to understand the bene- fits and limitations of MorphIO. In this study, we focused on answering the following research questions:\n\n- **RQ1:** Does MorphIO save time and reduce the number of iterations to program the target behavior, compared to the existing approach?\n\n- **RQ2:** Does MorphIO increase the expressiveness of the physical motion?\n\nTo answer these questions, we conducted a controlled experiment where we compared MorphIO (left) with the current programming approach. We chose Arduino IDE (right) as a base condition for the comparison, as this is the most common programming approach identified through our formative study. We provide three basic tasks to construct a program. For each task, the participants were asked to program three differ- ent emotions—happiness, anger, and sadness—of an animated character. We chose these emotions based on Ekman’s basic emotions for communication.\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-3.png" /></a>\n  </div>\n</div>\n\nThe average time of task completion time of MorphIO was 2m 19s, compared to 5m 21s with the control condition. The average number of iterations of MorphIO was 4.4 times, compared to 6.4 with the control condition, which confirms that MorphIO is significantly efficient in terms of task completion time and the number of iterations. When asked about the achievement of the expressions using a 9-point Likert scale, the average score with MorphIO was 6.5, compared to 6.3 with the control condition. We did not find differences between the two conditions. Thus, we conclude the result of our study as follows: **RQ1: Yes, RQ2: No**.\n\nBased on our post interviews, we discuss the benefits and limitations of our approach: **1) tangible interactions are suitable for sculpting rough motion**, **2) programming allows for fine-tuning more precise adjustments**. Thus, for future research, systems might allow users to quickly make a rough motion, which can automatically be converted into digital parameters so that the user can also precisely control and adjust the motion. The same human- computer cooperation approach can be applied to other design domains: For example, when designing an object, the user can quickly make rough shapes with clay, while letting a machine finish the details. We believe this insight can lead the HCI community to further explore design approaches wherein users and machines cooperate for enhanced interaction design.\n\n\n# Future Vision\nWe believe this approach’s potential for lowering the barrier and opening new opportunities for a larger community to begin designing, prototyping, and exploring soft material motion—not by coding on a screen, but by sculpting behaviors in the physical world.\nWe envision the future where people can interactively explore various behaviors through tangible interactions, **just like sculpting behaviors with clay**.\n\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-14.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-14.png" /></a>\n  </div>\n</div>',bodyHtml:"<h1>Abstract</h1>\n<p>We introduce <strong>MorphIO, entirely soft sensing and actuation modules</strong> for programming by demonstration of soft robots and shape-changing interfaces. MorphIO’s hardware consists of a <strong>soft pneumatic actuator containing a conductive sponge sensor</strong>. This allows both input and output of three-dimensional deformation of a soft material. Leveraging this capability, MorphIO enables a user to <strong>record and later playback physical motion</strong> of programmable shape-changing materials. In addition, the modular design of MorphIO’s unit allows the user to construct various shapes and topologies through magnetic connection. We demonstrate several application scenarios, including tangible character animation, locomotion experiment of a soft robot, and prototyping tools for animated soft objects. Our user study with six participants confirms the benefits of MorphIO, as compared to the existing programming paradigm.</p>\n<p>&lt;video poster=&quot;/static/projects/morphio/video-poster/top.png&quot; preload=&quot;metadata&quot; autoplay loop muted playsinline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/morphio/webm/top.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/morphio/video/top.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-1-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-1-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-1-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-1-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-1-3.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-1-3.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-1-4.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-1-4.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-1-5.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-1-5.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>Introduction</h1>\n<p><strong>Programmable soft materials</strong> have a great potential for many application domains, such as soft robotics, material interfaces, accessibility, and haptic interfaces.\n<strong>However, programming of such materials is hard.</strong>\nThe dominant programming paradigm of soft robots and material interfaces is largely confined within a digital screen, leaving little room for users to interactively explore physical motion through tangible interaction. In such a workflow—compiling code on a digital screen then trans- ferring it into the physical object—users need to repeatedly switch between the digital and physical worlds. This leaves a large gulf of execution in their programming experiences.\nThus, the traditional programming paradigm significantly limits the user’s ability to experiment with the design of expressive motion. Moreover, due to this barrier, such an opportunity is largely limited to highly skilled programmers and researchers who are proficient in hardware programming.</p>\n<h1>MorphIO</h1>\n<p>This paper introduces <strong>MorphIO, entirely soft sensing and actuation modules</strong> for programming by demonstration of soft robots and shape-changing interfaces.\nMorphIO’s hardware consists of a soft pneumatic actuator containing a conductive sponge sensor. This allows for integrated and entirely soft shape-changing modules that can both sense and actuate a variety of three-dimensional deformations. Leveraging this capability, MorphIO enables the user to program behaviors by <strong>recording and later playing back physical motions</strong> through tangible interaction. In addition, the modular design of MorphIO’s unit allows the user to construct various shapes and topologies through magnetic connection, then <strong>synthesize multiple recorded motions to achieve more complex behaviors</strong>, such as bending, gripping, and walking.</p>\n<p>&lt;video poster=&quot;/static/projects/morphio/video-poster/module.png&quot; preload=&quot;metadata&quot; autoplay loop muted playsinline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/morphio/webm/module.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/morphio/video/module.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;br/&gt;</p>\n<p>&lt;video poster=&quot;/static/projects/morphio/video-poster/bear.png&quot; preload=&quot;metadata&quot; autoplay loop muted playsinline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/morphio/webm/bear.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/morphio/video/bear.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<h1>System Overview</h1>\n<p>The programming workflow with MorphIO is the following:</p>\n<ul>\n<li>\n<p><strong>Step 1:</strong> A user starts manipulating the MorphIO unit.</p>\n</li>\n<li>\n<p><strong>Step 2:</strong> The demonstrated motion is detected and recorded through internal sensors, and the recorded sensor values are stored in the software.</p>\n</li>\n<li>\n<p><strong>Step 3:</strong> Once the user clicks play in the graphical user interface, the pneumatic pump starts supplying air.</p>\n</li>\n<li>\n<p><strong>Step 4:</strong> By controlling the air flow through switching on and off the solenoid valves, the system can control the behavior of the pneumatic actuator as it plays back the recorded motion.</p>\n</li>\n</ul>\n<p>The MorphIO system consists of the following components: A sensor and actuation unit, a sensing and actuation control unit, a microcontroller, software to control these units, and a visual interface for users to control behaviors. Figure illustrates the overview architecture of MorphIO.</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-6-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-6-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-6-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-6-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>Entirely Soft Sensing and Actuation Modules</h1>\n<p>Our main contribution is a design and fabrication method for <strong>a conductive sponge sensor</strong> that can be embedded into an air chamber in the pneumatic actuator. The conductive sponge sensor leverages the porous structure to <strong>sense the three-dimensional deformation by measuring the internal resistance value</strong>; when contracted, the resistance value be- tween the top and bottom surfaces drops, and when extended, it increases. In contrast to existing sensing techniques, an elastic sponge allows for a higher degree of freedom in sensing capability (e.g., stretching, bending, and compression) without sacrificing the softness of the interface.</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-2-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-2-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-2-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-2-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;video poster=&quot;/static/projects/morphio/video-poster/mechanism.png&quot; preload=&quot;metadata&quot; autoplay loop muted playsinline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/morphio/webm/mechanism.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/morphio/video/mechanism.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;br /&gt;</p>\n<p>Moreover, our <strong>modular design</strong> and <strong>graphical interface</strong> allows for easy experiments involving multiple units. For example, the system can visualize multiple recorded sensor values, so that the user can see, customize, and synthesize recorded motion to construct more complex behaviors. These hardware and software designs were informed by our formative study, wherein we interviewed five experienced researchers from the robotics and HCI communities.</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-3-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-3-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-3-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-3-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;video poster=&quot;/static/projects/morphio/video-poster/unit-x2.png&quot; preload=&quot;metadata&quot; autoplay loop muted playsinline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/morphio/webm/unit-x2.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/morphio/video/unit-x2.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;video poster=&quot;/static/projects/morphio/video-poster/unit-x3.png&quot; preload=&quot;metadata&quot; autoplay loop muted playsinline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/morphio/webm/unit-x3.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/morphio/video/unit-x3.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-4-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-4-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-4-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-4-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-4-3.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-4-3.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>Fabrication Process</h1>\n<p>The fabrication process follows three steps: 1) Fabricate an elastic sponge, 2) impregnate into conductive ink, and 3) attach electrodes and wires.\nTo fabricate an elastic sponge, we first prepare 6.0 g of elastomer prepolymer solution and 29.1 g of sodium-chloride, then mix them together by using a planetary centrifugal mixer. The mixed solution is injected into a 3D printed cylindrical mold (16mm diameter, 40mm height). Then we dry the material with an oven at 100 C degrees for one hour. Once dried, we immerse the sponge in water, so that the sodium chloride can melt, leaving a porous structure within the elastomer sponge.</p>\n<p>&lt;video poster=&quot;/static/projects/morphio/video-poster/fabrication.png&quot; preload=&quot;metadata&quot; autoplay loop muted playsinline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/morphio/webm/fabrication.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/morphio/video/fabrication.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui four column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-5-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-5-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-5-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-5-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-5-3.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-5-3.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-5-4.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-5-4.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-5-5.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-5-5.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-5-6.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-5-6.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-5-7.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-5-7.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>Applications</h1>\n<p>We demonstrate several possible applications scenarios with MorphIO. 1) Tangible character animation, 2) Animating existing soft objects, 3) Remote manipulation of soft grippers, 4) Locomotion experimentation with soft robots.</p>\n<p>&lt;div class=&quot;figures ui four column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-10-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-10-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-10-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-10-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-10-3.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-10-3.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-10-4.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-10-4.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-11-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-11-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-11-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-11-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-11-3.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-11-3.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;video poster=&quot;/static/projects/morphio/video-poster/locomotion.png&quot; preload=&quot;metadata&quot; autoplay loop muted playsinline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/morphio/webm/locomotion.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/morphio/video/locomotion.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<h1>Evaluation</h1>\n<p>We conducted a user evaluation study to understand the bene- fits and limitations of MorphIO. In this study, we focused on answering the following research questions:</p>\n<ul>\n<li>\n<p><strong>RQ1:</strong> Does MorphIO save time and reduce the number of iterations to program the target behavior, compared to the existing approach?</p>\n</li>\n<li>\n<p><strong>RQ2:</strong> Does MorphIO increase the expressiveness of the physical motion?</p>\n</li>\n</ul>\n<p>To answer these questions, we conducted a controlled experiment where we compared MorphIO (left) with the current programming approach. We chose Arduino IDE (right) as a base condition for the comparison, as this is the most common programming approach identified through our formative study. We provide three basic tasks to construct a program. For each task, the participants were asked to program three differ- ent emotions—happiness, anger, and sadness—of an animated character. We chose these emotions based on Ekman’s basic emotions for communication.</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-12-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-12-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-12-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-12-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-12-3.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-12-3.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>The average time of task completion time of MorphIO was 2m 19s, compared to 5m 21s with the control condition. The average number of iterations of MorphIO was 4.4 times, compared to 6.4 with the control condition, which confirms that MorphIO is significantly efficient in terms of task completion time and the number of iterations. When asked about the achievement of the expressions using a 9-point Likert scale, the average score with MorphIO was 6.5, compared to 6.3 with the control condition. We did not find differences between the two conditions. Thus, we conclude the result of our study as follows: <strong>RQ1: Yes, RQ2: No</strong>.</p>\n<p>Based on our post interviews, we discuss the benefits and limitations of our approach: <strong>1) tangible interactions are suitable for sculpting rough motion</strong>, <strong>2) programming allows for fine-tuning more precise adjustments</strong>. Thus, for future research, systems might allow users to quickly make a rough motion, which can automatically be converted into digital parameters so that the user can also precisely control and adjust the motion. The same human- computer cooperation approach can be applied to other design domains: For example, when designing an object, the user can quickly make rough shapes with clay, while letting a machine finish the details. We believe this insight can lead the HCI community to further explore design approaches wherein users and machines cooperate for enhanced interaction design.</p>\n<h1>Future Vision</h1>\n<p>We believe this approach’s potential for lowering the barrier and opening new opportunities for a larger community to begin designing, prototyping, and exploring soft material motion—not by coding on a screen, but by sculpting behaviors in the physical world.\nWe envision the future where people can interactively explore various behaviors through tangible interactions, <strong>just like sculpting behaviors with clay</strong>.</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/morphio/figure-14.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/morphio/figure-14.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n",dir:"content/output/projects",base:"morphio.json",ext:".json",sourceBase:"morphio.md",sourceExt:".md"}},"XJU/":function(t,e,o){var i=o("NegM");t.exports=function(t,e,o){for(var a in e)o&&t[a]?t[a]=e[a]:i(t,a,e[a]);return t}},XXOK:function(t,e,o){t.exports=o("Rp86")},YFqc:function(t,e,o){t.exports=o("cTJO")},YuTi:function(t,e){t.exports=function(t){return t.webpackPolyfill||(t.deprecate=function(){},t.paths=[],t.children||(t.children=[]),Object.defineProperty(t,"loaded",{enumerable:!0,get:function(){return t.l}}),Object.defineProperty(t,"id",{enumerable:!0,get:function(){return t.i}}),t.webpackPolyfill=1),t}},ZDA2:function(t,e,o){var i=o("iZP3"),a=o("K47E");t.exports=function(t,e){return!e||"object"!==i(e)&&"function"!=typeof e?a(t):e}},ZW5q:function(t,e,o){"use strict";var i=o("eaoh");function a(t){var e,o;this.promise=new t(function(t,i){if(void 0!==e||void 0!==o)throw TypeError("Bad Promise constructor");e=t,o=i}),this.resolve=i(e),this.reject=i(o)}t.exports.f=function(t){return new a(t)}},aPfg:function(t,e,o){"use strict";var i=o("Y7ZC"),a=o("eaoh"),n=o("2GTP"),s=o("oioR");t.exports=function(t){i(i.S,t,{from:function(t){var e,o,i,r,c=arguments[1];return a(this),(e=void 0!==c)&&a(c),null==t?new this:(o=[],e?(i=0,r=n(c,arguments[2],2),s(t,!1,function(t){o.push(r(t,i++))})):s(t,!1,o.push,o),new this(o))}})}},aW7e:function(t,e,o){o("wgeU"),o("FlQf"),o("bBy9"),o("JMW+"),o("PBE1"),o("Q/yX"),t.exports=o("WEpk").Promise},b3CU:function(t,e,o){var i=o("pbKT"),a=o("vjea");function n(e,o,s){return!function(){if("undefined"==typeof Reflect||!i)return!1;if(i.sham)return!1;if("function"==typeof Proxy)return!0;try{return Date.prototype.toString.call(i(Date,[],function(){})),!0}catch(t){return!1}}()?t.exports=n=function(t,e,o){var i=[null];i.push.apply(i,e);var n=new(Function.bind.apply(t,i));return o&&a(n,o.prototype),n}:t.exports=n=i,n.apply(null,arguments)}t.exports=n},b6cx:function(t,e,o){(window.__NEXT_P=window.__NEXT_P||[]).push(["/project",function(){var t=o("mI7R");return{page:t.default||t}}])},cHUd:function(t,e,o){"use strict";var i=o("Y7ZC");t.exports=function(t){i(i.S,t,{of:function(){for(var t=arguments.length,e=new Array(t);t--;)e[t]=arguments[t];return new this(e)}})}},cTJO:function(t,e,o){"use strict";var i=o("KI45"),a=i(o("9Jkg")),n=i(o("/HRN")),s=i(o("WaGi")),r=i(o("ZDA2")),c=i(o("/+P4")),l=i(o("N9n2")),u=function(t){if(t&&t.__esModule)return t;var e={};if(null!=t)for(var o in t)Object.hasOwnProperty.call(t,o)&&(e[o]=t[o]);return e.default=t,e},h=function(t){return t&&t.__esModule?t:{default:t}};Object.defineProperty(e,"__esModule",{value:!0});var p=o("CxY0"),g=u(o("q1tI")),d=(h(o("17x9")),u(o("nOHt"))),m=o("Bu4q");function f(t){return t&&"object"==typeof t?m.formatWithValidation(t):t}var b=function(t){function e(){var t,o,i,a,s;return(0,n.default)(this,e),(t=(0,r.default)(this,(0,c.default)(e).apply(this,arguments))).formatUrls=(o=function(t,e){return{href:f(t),as:f(e)}},i=null,a=null,s=null,function(t,e){if(t===i&&e===a)return s;var n=o(t,e);return i=t,a=e,s=n,n}),t.linkClicked=function(e){var o=e.currentTarget,i=o.nodeName,a=o.target;if("A"!==i||!(a&&"_self"!==a||e.metaKey||e.ctrlKey||e.shiftKey||e.nativeEvent&&2===e.nativeEvent.which)){var n=t.formatUrls(t.props.href,t.props.as),s=n.href,r=n.as;if(function(t){var e=p.parse(t,!1,!0),o=p.parse(m.getLocationOrigin(),!1,!0);return!e.host||e.protocol===o.protocol&&e.host===o.host}(s)){var c=window.location.pathname;s=p.resolve(c,s),r=r?p.resolve(c,r):s,e.preventDefault();var l=t.props.scroll;null==l&&(l=r.indexOf("#")<0),d.default[t.props.replace?"replace":"push"](s,r,{shallow:t.props.shallow}).then(function(t){t&&l&&(window.scrollTo(0,0),document.body.focus())}).catch(function(e){t.props.onError&&t.props.onError(e)})}}},t}return(0,l.default)(e,t),(0,s.default)(e,[{key:"componentDidMount",value:function(){this.prefetch()}},{key:"componentDidUpdate",value:function(t){(0,a.default)(this.props.href)!==(0,a.default)(t.href)&&this.prefetch()}},{key:"prefetch",value:function(){if(this.props.prefetch&&"undefined"!=typeof window){var t=window.location.pathname,e=this.formatUrls(this.props.href,this.props.as).href,o=p.resolve(t,e);d.default.prefetch(o)}}},{key:"render",value:function(){var t=this,e=this.props.children,o=this.formatUrls(this.props.href,this.props.as),i=o.href,a=o.as;"string"==typeof e&&(e=g.default.createElement("a",null,e));var n=g.Children.only(e),s={onClick:function(e){n.props&&"function"==typeof n.props.onClick&&n.props.onClick(e),e.defaultPrevented||t.linkClicked(e)}};return!this.props.passHref&&("a"!==n.type||"href"in n.props)||(s.href=a||i),s.href&&"undefined"!=typeof __NEXT_DATA__&&__NEXT_DATA__.nextExport&&(s.href=d.Router._rewriteUrlForNextExport(s.href)),g.default.cloneElement(n,s)}}]),e}(g.Component);e.default=b},czwh:function(t,e,o){var i=o("Y7ZC"),a=o("oVml"),n=o("eaoh"),s=o("5K7Z"),r=o("93I4"),c=o("KUxP"),l=o("wYmx"),u=(o("5T2Y").Reflect||{}).construct,h=c(function(){function t(){}return!(u(function(){},[],t)instanceof t)}),p=!c(function(){u(function(){})});i(i.S+i.F*(h||p),"Reflect",{construct:function(t,e){n(t),s(e);var o=arguments.length<3?t:n(arguments[2]);if(p&&!h)return u(t,e,o);if(t==o){switch(e.length){case 0:return new t;case 1:return new t(e[0]);case 2:return new t(e[0],e[1]);case 3:return new t(e[0],e[1],e[2]);case 4:return new t(e[0],e[1],e[2],e[3])}var i=[null];return i.push.apply(i,e),new(l.apply(t,i))}var c=o.prototype,g=a(r(c)?c:Object.prototype),d=Function.apply.call(t,g,e);return r(d)?d:g}})},d04V:function(t,e,o){t.exports=o("0tVQ")},dL40:function(t,e,o){var i=o("Y7ZC");i(i.P+i.R,"Set",{toJSON:o("8iia")("Set")})},dfwq:function(t,e,o){"use strict";o.r(e);var i=o("p0XB"),a=o.n(i);var n=o("d04V"),s=o.n(n),r=o("yLu3"),c=o.n(r);function l(t){return function(t){if(a()(t)){for(var e=0,o=new Array(t.length);e<t.length;e++)o[e]=t[e];return o}}(t)||function(t){if(c()(Object(t))||"[object Arguments]"===Object.prototype.toString.call(t))return s()(t)}(t)||function(){throw new TypeError("Invalid attempt to spread non-iterable instance")}()}o.d(e,"default",function(){return l})},doui:function(t,e,o){"use strict";o.r(e);var i=o("p0XB"),a=o.n(i);var n=o("XXOK"),s=o.n(n);function r(t,e){return function(t){if(a()(t))return t}(t)||function(t,e){var o=[],i=!0,a=!1,n=void 0;try{for(var r,c=s()(t);!(i=(r=c.next()).done)&&(o.push(r.value),!e||o.length!==e);i=!0);}catch(l){a=!0,n=l}finally{try{i||null==c.return||c.return()}finally{if(a)throw n}}return o}(t,e)||function(){throw new TypeError("Invalid attempt to destructure non-iterable instance")}()}o.d(e,"default",function(){return r})},e3jU:function(t){t.exports=[{date:"2022-02",tag:"electro-voxel",media:"Engadget",title:"Scientists create cube robots that can shapeshift in space",url:"https://www.engadget.com/shapeshifting-robots-space-exploration-mit-csail-electrovoxels-140026431.html"},{date:"2022-02",tag:"electro-voxel",media:"TechXplore",title:"Robotic cubes: Self-reconfiguring ElectroVoxels use embedded electromagnets to test applications for space exploration",url:"https://techxplore.com/news/2022-02-robotic-cubes-self-reconfiguring-electrovoxels-embedded.html"},{date:"2022-02",tag:"electro-voxel",media:"TechEBlog",title:"MIT Researchers Develop Shape-Shifting ElectroVoxel Robots for Space Exploration",url:"https://www.techeblog.com/mit-electrovoxel-shape-shifting-robot-space/"},{date:"2022-02",tag:"electro-voxel",media:"IEEE Spectrum",title:"Video Friday: Your weekly selection of awesome robot videos",url:"https://spectrum.ieee.org/video-friday-lunar-rover"},{date:"2022-02",tag:"electro-voxel",media:"Arduino Blog",title:"ElectroVoxel robots reconfigure themselves using magnets",url:"https://blog.arduino.cc/2022/02/08/electrovoxel-robots-reconfigure-themselves-using-magnets/"},{date:"2022-02",tag:"electro-voxel",media:"Hackster.io",title:"These Magnetic Robots Assemble Like Voltron",url:"https://www.hackster.io/news/these-magnetic-robots-assemble-like-voltron-37e4b1f4ec5d"},{date:"2021-11",tag:"snap",media:"UCalgary News",title:"“Touchable spoken words” bring the fantastic to life",url:"https://science.ucalgary.ca/news/touchable-spoken-words-bring-fantastic-life"},{date:"2021-07",tag:"roomshift",media:"IEEE Computer Graphics and Applications",title:"Cover Story of “Real Virtual Reality” (vol. 41)",url:"https://www.computer.org/csdl/magazine/cg/2021/04/09487526/1vg3mB6LNJe"},{date:"2021-03",tag:"realitysketch",media:"IT Media News",title:"Evolution of “AR Drawing”? RealitySketch, a sketching technology that works with objects in reality",url:"https://www.itmedia.co.jp/news/articles/2103/18/news027.html"},{date:"2020-12",tag:"realitysketch",media:"TechXplore",title:"RealitySketch: An AR interface to create responsive sketches",url:"https://techxplore.com/news/2020-12-realitysketch-ar-interface-responsive.html"},{date:"2020-10",tag:"pufferbot",media:"ACM TechNews",title:"Pufferfish-inspired robot could improve drone safety",url:"https://cacm.acm.org/news/248245-pufferfish-inspired-robot-could-improve-drone-safety/fulltext"},{date:"2020-10",tag:"pufferbot",media:"Interesting Engineering",title:"Pufferfish Mimicking Drones to Improve Aerial Safety",url:"https://interestingengineering.com/pufferfish-mimicking-drones-to-improve-aerial-safety"},{date:"2020-10",tag:"pufferbot",media:"New Atlas",title:"Drone draws on the pufferfish to protect itself and others",url:"https://newatlas.com/drones/pufferbot-drone"},{date:"2020-10",tag:"roomshift",media:"Techable",title:"University of Colorado researchers unveil ’RoomShift’ to move props in VR space in real life",url:"https://techable.jp/archives/139024"},{date:"2020-10",tag:"roomshift",media:"Hackster.io",title:"Putting the Reality in Virtual Reality",url:"https://www.hackster.io/news/putting-the-reality-in-virtual-reality-8c21db67e605"},{date:"2020-09",tag:"pufferbot",media:"Hackster.io",title:"PufferBot Is an Aerial Robot That Can Change Shape In-Flight",url:"https://www.hackster.io/news/pufferbot-is-an-aerial-robot-that-can-change-shape-in-flight-0be9c211ce07"},{date:"2020-09",tag:"roomshift",media:"TechXplore",title:"RoomShift: A room-scale haptic and dynamic environment for VR applications",url:"https://techxplore.com/news/2020-09-roomshift-room-scale-haptic-dynamic-environment.html"},{date:"2020-09",tag:"pufferbot",media:"Engineering 360",title:"Team builds drone inspired by the pufferfish",url:"https://insights.globalspec.com/article/15041/team-builds-drone-inspired-by-the-pufferfish"},{date:"2020-09",tag:"roomshift",media:"TechXplore",title:"PufferBot: A flying robot with an expandable body",url:"https://techxplore.com/news/2020-09-pufferbot-robot-body.html"},{date:"2020-09",tag:"roomshift",media:"Yahoo News Japan",title:"The University of Colorado Announced ”RoomShift” where Robot Rearranges Furniture to Create Virtual Spaces in a Realistic Way",url:"https://www.itmedia.co.jp/news/articles/2009/07/news097.html"},{date:"2020-09",tag:"roomshift",media:"IT Media News",title:"RoomShift: Reconfigurable Environments for Virtual Reality",url:"https://www.itmedia.co.jp/news/articles/2009/07/news097.html"},{date:"2020-02",tag:"pufferbot",media:"IT Media News",title:"Giant whistle module expands the room with the University of Colorado and other ”LiftTiles” developments",url:"https://www.itmedia.co.jp/news/articles/2002/26/news029.html"},{date:"2020-01",tag:"lift-tiles",media:"Arduino Blog",title:"Prototype room-scale, shape-changing interfaces with LiftTiles",url:"https://blog.arduino.cc/2020/01/27/prototype-room-scale-shape-changing-interfaces-with-lifttiles/"},{date:"2020-01",tag:"lift-tiles",media:"TechXplore",title:"LiftTiles: Actuator-based Building Blocks for Shape-changing Interfaces",url:"https://techxplore.com/news/2020-01-lifttiles-actuator-based-blocks-shape-changing-interfaces.html"},{date:"2020-01",tag:"shapebots",media:"ITMedia News",title:"A Swarm of Self-transforming Robots to Assist People",url:"https://www.itmedia.co.jp/news/articles/2001/15/news032.html"},{date:"2019-10",tag:"lift-tiles",media:"Hackster.io",title:"LiftTiles Turn Walls and Floors Into Reconfigurable Structures on Demand",url:"https://www.hackster.io/news/lifttiles-turn-walls-and-floors-into-reconfigurable-structures-on-demand-4a226d58bc74"},{date:"2019-10",tag:"lift-tiles",media:"Element 14",title:"Engineers Develop LiftTiles, a Scale Shape-changing Interface",url:"https://www.element14.com/community/community/applications/industrial-automation-space/blog/2019/10/25/engineers-develop-lifttiles-a-scale-shape-changing-interface?CMP=SOM-PRG-TWITTER-BLOG-CATWELL-LIFT-TILES-COMM"},{date:"2019-11",tag:"shapebots",media:"Bouncy",title:"Swarm Robots that can Change Shape to Visualize Data",url:"https://bouncy.news/53532?fbclid=IwAR0jyfBKo8LJ3aiUidDfZUsQqJ5-oSMxRuiZyJju0g_F6A_hi1tOeboPM4E"},{date:"2019-10",tag:"shapebots",media:"Hackster.io",title:"Swarming Robots Can Change Their Configuration to Handle Different Tasks",url:"https://www.hackster.io/news/shapebots-swarming-robots-can-change-their-configuration-to-handle-different-tasks-59a5ae926e1d"},{date:"2019-09",tag:"shapebots",media:"TechXplore",title:"ShapeBots: A Swarm of Shape-shifting Robots that Visually Display Data",url:"https://techxplore.com/news/2019-09-shapebots-swarm-shape-shifting-robots-visually.html"},{date:"2019-09",tag:"shapebots",media:"Hackaday",title:"Tiny Robots that Grow Taller and Wider",url:"https://hackaday.com/2019/10/04/tiny-robots-that-grow-taller-and-wider/"},{date:"2019-09",media:"Robotic Gizmo",title:"ShapeBots: Shape-changing Swarm Robots",url:"https://www.roboticgizmos.com/shapebots/"},{date:"2019-09",tag:"shapebots",media:"Gadgetify",title:"ShapeBots: Shape Changing Swarm Robots",url:"http://www.gadgetify.com/shapebots/"},{date:"2018-10",tag:"dynablock",media:"3DPrint.com",title:"Dynablock: 3D Prints That Assemble and Disassemble in Seconds",url:"https://3dprint.com/227781/3d-prints-that-assemble-in-seconds/"},{date:"2018-10",tag:"dynablock",media:"Hackster.io",title:"The Dynamic 3D Printing That Assembles and Disassembles Objects in Seconds",url:"https://www.hackster.io/news/dynablock-the-dynamic-3d-printing-that-assembles-and-disassembles-objects-in-seconds-a7f7d4bf6cad"},{date:"2018-10",tag:"dynablock",media:"Arduino Blog",title:"Create Shapes Over and Over with the Dynablock 3D Printer",url:"https://blog.arduino.cc/2018/10/22/create-shapes-over-and-over-with-the-dynablock-3d-printer/"},{date:"2018-10",tag:"dynablock",media:"3DRuck.com",title:"Dynablock: Dynamic 3D Printer Creates Objects in Seconds",url:"https://3druck.com/forschung/dynablock-dynamischer-3d-drucker-erstellt-objekte-in-sekunden-2776738/"},{date:"2018-10",tag:"dynablock",media:"World Business Satellite",title:"Repeatable 3D Printer",url:"https://txbiz.tv-tokyo.co.jp/wbs/trend_tamago/post_168589/"},{date:"2018-10",tag:"dynablock",media:"Nikkei Newspaper",title:"Modeling 3D Objects with Magnet-Embedded Blocks",url:"https://active.nikkeibp.co.jp/atclact/active/17/071100318/101600559/"},{date:"2016-06",tag:"atelier",media:"Wired",title:"It’s Not Just Robots: Skilled Jobs Are Going to Meatware",url:"https://www.wired.com/2016/06/its-not-just-robots-skilled-jobs-are-going-to-meatware/"}]},eVuF:function(t,e,o){t.exports=o("aW7e")},"eW+b":function(t){t.exports={id:"sketched-reality",name:"Sketched Reality",description:"Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI",title:"Sketched Reality: Sketching Bi-Directional Interactions Between Virtual and Physical Worlds with AR and Actuated Tangible UI",authors:["Hiroki Kaimoto","Kyzyl Monteiro","Mehrad Faridan","Jiatong Li","Samin Farajian","Yasuaki Kakehi","Ken Nakagaki","Ryo Suzuki"],year:2022,booktitle:"In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology (UIST '22)",publisher:"ACM, New York, NY, USA",doi:"https://doi.org/10.1145/3526113.3545626",conference:{name:"UIST 2022",fullname:"The ACM Symposium on User Interface Software and Technology (UIST 2022)",url:"http://uist.acm.org/uist2022"},pdf:"uist-2022-sketched-reality.pdf",video:"https://www.youtube.com/watch?v=xy-IeVgoEpY",embed:"https://www.youtube.com/embed/xy-IeVgoEpY",arxiv:"https://arxiv.org/abs/2208.06341","acm-dl":"https://dl.acm.org/doi/10.1145/3526113.3545626",pageCount:12,slideCount:0,bodyContent:'# Abstract\n\nThis paper introduces Sketched Reality, an approach that com- bines AR sketching and actuated tangible user interfaces (TUI) for bi-directional sketching interaction. Bi-directional sketching enables virtual sketches and physical objects to "affect" each other through physical actuation and digital computation. In the existing AR sketching, the relationship between virtual and physical worlds is only one-directional --- while physical interaction can affect virtual sketches, virtual sketches have no return effect on the physical objects or environment. In contrast, bi-directional sketching interaction allows the seamless coupling between sketches and actuated TUIs. In this paper, we employ tabletop-size small robots (Sony Toio) and an iPad-based AR sketching tool to demonstrate the concept. In our system, virtual sketches drawn and simulated on an iPad (e.g., lines, walls, pendulums, and springs) can move, actuate, collide, and constrain physical Toio robots, as if virtual sketches and the physical objects exist in the same space through seamless coupling between AR and robot motion. This paper contributes a set of novel interactions and a design space of bi-directional AR sketching. We demonstrate a series of potential applications, such as tangible physics education, explorable mechanism, tangible gaming for children, and in-situ robot programming via sketching.',bodyHtml:"<h1>Abstract</h1>\n<p>This paper introduces Sketched Reality, an approach that com- bines AR sketching and actuated tangible user interfaces (TUI) for bi-directional sketching interaction. Bi-directional sketching enables virtual sketches and physical objects to &quot;affect&quot; each other through physical actuation and digital computation. In the existing AR sketching, the relationship between virtual and physical worlds is only one-directional --- while physical interaction can affect virtual sketches, virtual sketches have no return effect on the physical objects or environment. In contrast, bi-directional sketching interaction allows the seamless coupling between sketches and actuated TUIs. In this paper, we employ tabletop-size small robots (Sony Toio) and an iPad-based AR sketching tool to demonstrate the concept. In our system, virtual sketches drawn and simulated on an iPad (e.g., lines, walls, pendulums, and springs) can move, actuate, collide, and constrain physical Toio robots, as if virtual sketches and the physical objects exist in the same space through seamless coupling between AR and robot motion. This paper contributes a set of novel interactions and a design space of bi-directional AR sketching. We demonstrate a series of potential applications, such as tangible physics education, explorable mechanism, tangible gaming for children, and in-situ robot programming via sketching.</p>\n",dir:"content/output/projects",base:"sketched-reality.json",ext:".json",sourceBase:"sketched-reality.md",sourceExt:".md"}},ejaO:function(t){t.exports={id:"tabby",name:"Tabby",description:"Explorable Design for 3D Printing Textures",title:"Tabby: Explorable Design for 3D Printing Textures",authors:["Ryo Suzuki","Koji Yatani","Mark D. Gross","Tom Yeh"],year:2018,booktitle:"The Pacific Conference on Computer Graphics and Applications (Pacific Graphics 2018)",publisher:"The Eurographics Association",pages:"1-4",doi:"https://doi.org/10.2312/pg.20181273",conference:{name:"Pacific Graphics 2018",fullname:"The Pacific Conference on Computer Graphics and Applications (Pacific Graphics 2018)",url:"http://sweb.cityu.edu.hk/pg2018/"},pdf:"pg-2018-tabby.pdf",video:"https://www.youtube.com/watch?v=rRgw8lH74CA",embed:"https://www.youtube.com/embed/rRgw8lH74CA","acm-dl":"https://diglib.eg.org/handle/10.2312/pg20181273",slide:"pg-2018-tabby-slide.pdf",pageCount:4,slideCount:40,bodyContent:'# Abstract\nThis paper presents **Tabby, an interactive and explorable design tool for 3D printing textures**. Tabby allows texture design with direct manipulation in the following workflow: 1) select a target surface, 2) sketch and manipulate a texture with 2D drawings, and then 3) generate 3D printing textures onto an arbitrary curved surface. To enable efficient texture creation, Tabby leverages an auto-completion approach which automates the tedious, repetitive process of applying texture, while allowing flexible customization. Our user evaluation study with seven participants confirms that Tabby can effectively support the design exploration of different patterns for both novice and experienced users.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/tabby/top.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-4.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-6.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-6.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-7.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-7.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-8.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-8.png" /></a>\n  </div>\n</div>',bodyHtml:"<h1>Abstract</h1>\n<p>This paper presents <strong>Tabby, an interactive and explorable design tool for 3D printing textures</strong>. Tabby allows texture design with direct manipulation in the following workflow: 1) select a target surface, 2) sketch and manipulate a texture with 2D drawings, and then 3) generate 3D printing textures onto an arbitrary curved surface. To enable efficient texture creation, Tabby leverages an auto-completion approach which automates the tedious, repetitive process of applying texture, while allowing flexible customization. Our user evaluation study with seven participants confirms that Tabby can effectively support the design exploration of different patterns for both novice and experienced users.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/tabby/top.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui four column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/tabby/figure-1-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/tabby/figure-1-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/tabby/figure-1-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/tabby/figure-1-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/tabby/figure-1-3.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/tabby/figure-1-3.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/tabby/figure-1-4.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/tabby/figure-1-4.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui four column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/tabby/figure-1-5.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/tabby/figure-1-5.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/tabby/figure-1-6.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/tabby/figure-1-6.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/tabby/figure-1-7.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/tabby/figure-1-7.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/tabby/figure-1-8.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/tabby/figure-1-8.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n",dir:"content/output/projects",base:"tabby.json",ext:".json",sourceBase:"tabby.md",sourceExt:".md"}},fXsU:function(t,e,o){var i=o("5K7Z"),a=o("fNZA");t.exports=o("WEpk").getIterator=function(t){var e=a(t);if("function"!=typeof e)throw TypeError(t+" is not iterable!");return i(e.call(t))}},"iBc/":function(t){t.exports={id:"realitysketch",name:"RealitySketch",description:"Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching",title:"RealitySketch: Embedding Responsive Graphics and Visualizations in AR through Dynamic Sketching",authors:["Ryo Suzuki","Rubaiat Habib Kazi","Li-Yi Wei","Stephen DiVerdi","Wilmot Li","Daniel Leithinger"],year:2020,booktitle:"In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology (UIST '20)",publisher:"ACM, New York, NY, USA",pages:"1-16",doi:"https://doi.org/10.1145/3379337.3415892",conference:{name:"UIST 2020",fullname:"The ACM Symposium on User Interface Software and Technology (UIST 2020) - Honorable Mention Award",url:"https://uist.acm.org/uist2020/"},pdf:"uist-2020-realitysketch.pdf",video:"https://www.youtube.com/watch?v=L0p-BNU9rXU",embed:"https://www.youtube.com/embed/L0p-BNU9rXU",arxiv:"https://arxiv.org/abs/2008.08688","acm-dl":"https://dl.acm.org/doi/10.1145/3379337.3415892",pageCount:16,slideCount:0,bodyContent:'# Abstract\n\nWe present RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, floating in mid-air without responding to the real world. This paper introduces a new way to embed *dynamic* and *responsive* graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-defined programs and configurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/realitysketch/video/top.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-1-2.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-1-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-1-3.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-1-3.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-1-4.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-1-4.jpg" /></a>\n  </div>\n</div>\n\n\n# Introduction\n\nOver the last decades, interactive and dynamic sketching has been one of the central themes in human-computer interaction (HCI) research. Since Sutherland first demonstrated the power of interactive sketching for computer-aided design, HCI researchers have explored ways to sketch dynamic contents that can interactively respond to user input, thus enabling us to think and communicate through a dynamic visual medium. The applications of such tools are vast, including mathematics and physics education, animated art creation, and interactive data visualization. Through these research outcomes, we have now developed a rich vocabulary of dynamic sketching techniques to fluidly create interactive, animated content in real-time.\n\nWith the advent of augmented and mixed reality interfaces, we now have a unique opportunity to expand such practices beyond screen-based interactions towards reality-based interactions. In fact, there is an increasing number of tools that provide sketching interfaces for augmented reality, from commercial products like Just a Line, Vuforia Chalk, and DoodleLens to research projects like SymbiosisSketch and Mobi3DSketch. These tools allow users to sketch digital elements and embed them in the real world. However, a key limitation is that the sketched content is *static* --- it does not respond, change, and animate based on user actions or real-world phenomena.\n\n\nWhat if, instead, these sketched elements could *dynamically respond* when the real world changes? For example, imagine a line sketched onto a physical pendulum that moves as the pendulum swings. This capability would allow us to directly manipulate the sketch through tangible interactions or capture and visualize the pendulum motion to understand the underlying phenomenon.\n\nAs a first step toward this goal, we present **RealitySketch, an end-user sketching tool to support real-time creation and sharing of embedded interactive graphics and visualization in AR**. To create graphics, the user sketches on a phone or tablet screen, which embeds interactive visualizations into a scene. The sketched elements can be bound to physical objects such that they respond dynamically as the real-world changes.\n\n# RealitySketch: How it works\n\n## Design Space and Basic Setup\n\nThe goal of this paper is to provide a way to embed dynamic and responsive graphics through dynamic sketching. To better understand the scope, we first define the terminology:\n\n1. **Embedded**: graphics are presented and *spatially integrated within the real-world*\n\n\n2. **Responsive**: *graphics change and animate* based on the user’s interactions\n\n\nThe important aspect of embedded and responsive graphics is that graphics must interact with the real-world. Here, the “real- world” means either the environment itself, a physical phenomenon, or a user’s tangible and gestural interactions.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/realitysketch/video/sketch.mp4" type="video/mp4"></source>\n</video>\n\n<br/>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-3-4.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-3-4.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-3-1.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-3-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-3-2.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-3-2.jpg" /></a>\n  </div>\n</div>\n\nTo enable that, RealitySketch leverages mobile augmented reality (ARKit) to embed sketches in the real world. The user sketches with a finger or a pen on a touchscreen, where the sketched elements are overlaid onto a camera view of the real world.\n\nOur approach proposes the following four-step workflow:\n\n1. **Object tracking**: the user specifies a visual entity (e.g., a physical object, a skeletal joint) to track in the real-world scene\n\n\n2. **Parameterization**: the user parameterizes tracked entities by drawing lines or arcs that define specific variables of interest\n\n\n3. **Parameter binding**: the user binds these variables to the graphical properties of sketched elements (e.g., length, angle, etc.) to define their dynamic behavior as the real-world variables change\n\n\n4. **Visualization**: the user can also interact, analyze, and visualize real-world movements through several visualization effects.\n\nWe contextualize our approach in the larger design space of dynamic graphics authoring approaches.\nThe main contribution of this paper is a set of interaction techniques that enable these steps without *pre-defined programs* and configurations --- rather, the system lets the user perform in real-time and improvisational ways.\n\n\n\n## Step 1: Track an Object\n\nFor embedded and responsive graphics, the graphical elements need to be tightly coupled with physical objects and environments. Thus, capturing and tracking an object is vital to make the graphics dynamically change and move.\nTo specify an object, the user enters the selection mode and then taps an object on the screen. Once selected, our system highlights the selected object with a white contour line and starts tracking the object in the 3D scene.\n\nIn our current implementation, the system tracks objects based on color tracking implemented with OpenCV. When the user taps an object on the screen, the algorithm gets the HSV value at the x and y position. Then the system captures similar colors at each frame based on a certain upper and lower threshold range. This color tracking was fast enough for most of our applications (e.g., 20-30 FPS with iPad Pro 11 inch 2018).\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/realitysketch/video/tracking.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-4-1.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-4-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-4-2.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-4-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-4-3.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-4-3.jpg" /></a>\n  </div>\n</div>\n\n\n## Step 2: Sketch to Parameterize\n\nNext, the user parameterizes the real world to define and capture the dynamic value of interest. In this paper, we specifically focus on parameterization that can be done through simple sketching interactions using line segments.\n\nFirst, when entering the sketching mode, the user can start drawing a line segment onto the scene. All the sketched lines are projected onto a 2D surface within the 3D scene. The system takes the two end-points to create the line segment. This creates a variable that defines the distance between two points on the surface. To create a dynamic line segment, the user draws a line whose endpoint is attached to the selected object. As one end of this dynamic line segment is bound to the selected object, if the user moves the object in the real world, the line segment and its parameter (e.g., distance value) will change accordingly. The system visually renders the line segment values using labels.\n\nRealitySketch employs simple heuristics to determine the nature (e.g., static vs. dynamic, distance vs angle, free move vs constraints, etc) of the line segment. If the start or end point of the line segment is close to an existing tracked object, then the system binds the end point to the tracked object. Thus, for example, if the user draws a line between two tracked objects, then both ends attach to an object. In that case, the line segment captures the distance between those two objects.\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/realitysketch/video/sketch.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-5-1.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-5-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-5-2.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-5-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-5-3.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-5-3.jpg" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-6-1.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-6-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-6-2.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-6-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-6-3.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-6-3.jpg" /></a>\n  </div>\n</div>\n\n## Step 3: Bind Parameters to Make Them Responsive\n\nTo make the existing line segments responsive, the user can bind variables between two elements. The parameter of a static line segment can be bound to another variable. For example, suppose the user has a variable named angle for a dynamic line segment. When the user taps the label of another angle of the static line segment, the system shows a popup to let the user enter the variable name. If the entered variable name matches an existing name, the angle of the static line segment will be dynamically bound to the existing parameter\n\n\nSimilarly, the user can define a constraint by typing a function of an existing variable. For example, consider the user wants to draw the bisector of the angle formed by a dynamic line segment. The user can first draw a line and an arc between the line and the base line.\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/realitysketch/video/bind.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-7-1.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-7-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-7-2.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-7-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-7-3.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-7-3.jpg" /></a>\n  </div>\n</div>\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-8-1.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-8-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-8-2.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-8-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-8-3.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-8-3.jpg" /></a>\n  </div>\n</div>\n\n## Step 4: Move and Animate\n\nRather than 2D sketches that are based on the device screen coordinates, all sketched elements have a 3D geometry and position in real world coordinates, anchored in 3D space. This way, the user can move the device to see from a different perspective and the sketches stay correctly anchored to the real objects.\n\nTo enable this functionality, our system leverages ARKit and SceneKit for both surface detection and object placement in the 3D scene.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/realitysketch/video/multi-angle.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-9-1.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-9-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-9-2.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-9-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-9-3.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-9-3.jpg" /></a>\n  </div>\n</div>\n\n\n## Step 5: Record and Visualize\n\nRealitySketch can also make responsive visualization based on graph plotting of a parameter. In the graph placing mode, the user can place a 2D graph and change its size by dragging and dropping onto the surface. By default, the x-axis of the graph is time. By binding an existing variable to the graph, it starts visualizing the time series data of the variable.\n\nTo bind the variable, the user can simply tap a label of the graph, and then, enter the variable the user-defined. For example, if the user binds the angle variable of the pendulum to the y-axis of the chart, the graph will dynamically plot the angle of the pendulum when it starts swinging. By adding multiple variables separated with a comma (e.g., a,b,c), the user can also plot multiple parameters in the same graph. The user can also change the x-axis from time to a variable by tapping the x-axis and entering a second variable. For example, the user can define an angle and y distance of a point of a circle. By binding x-axis as the angle and y-axis as the perpendicular length, the system can plot the relationship between the angle and corresponding sine value.\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/realitysketch/video/visualizations.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-16-1.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-16-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-16-2.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-16-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-16-3.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-16-3.jpg" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-10-1.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-10-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-10-2.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-10-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-10-3.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-10-3.jpg" /></a>\n  </div>\n</div>\n\n\n<br/>\n<div class="ui divider"></div>\n\n\n# Application Scenarios\n\nWe have explored and demonstrated the following application scenarios\n\n1. **Augmented Physics Experiments**\n\n\n2. **Interactive and Explorable Concept Explanation**\n\n\n3. **Improvised Visualization for Sports and Exercises**\n\n\n4. **In-situ Tangible User Interfaces**\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-2.png" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-2.png" /></a>\n  </div>\n</div>\n\n<br/>\n\n\n## Application 1: Augmented Physics Experiments\n\nIn science education, a classroom experiment is an integral part of learning physics because the real-world experiment provides students an opportunity to connect their knowledge with physical phenomena. RealitySketch can become a powerful tool to support these experiments by leveraging real-time visualization capability. Figure 15 illustrates how our tool can help students understand the pulley system. In this scenario, by tracking the movement of two points (i.e., the point of the person grabs and the point of the load), the students can visualize the traveling distance of each point. In this way, they can see the load distance movement is shorter than the distance the person pulls.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/realitysketch/video/physics.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-17-1.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-17-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-17-2.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-17-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-17-3.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-17-3.jpg" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-19-1.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-19-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-19-2.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-19-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-19-3.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-19-3.jpg" /></a>\n  </div>\n</div>\n\n<br/>\n\n## Application 2: Interactive and Explorable Concept Explanation\n\nRealitySketch is also useful to help teachers explain concepts that may be difficult to understand with static graphs, and to let students explore them through tangible interactions. Some examples are shown in the above Figure (Top: demonstrating an area of a triangle remains the same with horizontal movement; a bisector line of a triangle always intersect at the middle point. Bottom: showing how a sine curve is generated from plotting the angle and perpendicular distance of a rotating point.)\n\n\nFor educational applications, we can think of the following three setups of how students and teachers can use our tool:\n\n1. **Classroom presentation**: In this case, a teacher or an assistant sketches and visualizes the motion, which can be shared through a connected large display or projector, so that the students can see and understand.\n\n2. **Collaborative learning**: In this case, students can form a group and interact with their own devices. Since mobile AR is accessible for almost all smartphones, every student should be able to play by themselves, which can lead to an interesting exploration and discoveries.\n\n3. **Remote learning**: In this case, a teacher only records the video of the experiment, then share the recorded video with the students. The student can download and visualize with their own app, so that it provides a fun and interactive experiment to support online and remote learning.\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/realitysketch/video/concept.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-18-1.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-18-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-18-2.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-18-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-18-3.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-18-3.jpg" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-20-1.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-20-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-20-2.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-20-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-20-3.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-20-3.jpg" /></a>\n  </div>\n</div>\n\n<br/>\n\n\n## Application 3: Improvised Visualization for Sports and Exercises\n\nRealitySketch could be also useful to analyze and visualize the motion for sports training and exercises because they often employ the physical movements. For example, sports practices, like a golf shot, baseball pitching, and basketball shooting, would be an interesting example to visualize the trajectory of the ball. Similar to the previous example of showing the trajectory of a ball, it is useful to quickly see the path through stroboscopic effects. In addition to showing the trajectory, the system can also capture and compare multiple attempts to let the user analyze what works and what does not.\n\nAlso, many sports and exercises may require proper form and posture. For example, in baseball batting, golf shot, and a tennis swing, a player’s form, such as a body angle, can be important, which the tool can help visualize through sketched guided lines. These features could be particularly useful for exercise instructions. For example, in yoga practice, bike riding, or weight lifting training, there are recommended angles to be followed to maximize the performance benefits. In these cases, the improvisational measurement of the posture can help the user to see and check if correctly done.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/realitysketch/video/sports.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-14-1.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-14-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-14-2.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-14-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-14-3.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-14-3.jpg" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-15-1.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-15-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-15-2.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-15-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-15-3.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-15-3.jpg" /></a>\n  </div>\n</div>\n\n\n<br/>\n\n## Application 4: In-situ Tangible User Interfaces\nThe parameterized value can be used for many different pur- poses to enable responsive visual outputs. So far, we have mostly focused on animation of the simple basic geometry (e.g., line segments, arc) or build-in visualization outputs (e.g., graph plot). However, the dynamic parameter value can be also used for other outputs via binding, as we discussed in the previous sections (e.g., pre-defined vs user-defined section).\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/realitysketch/video/dino.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-11-1.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-11-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-11-2.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-11-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-11-3.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-11-3.jpg" /></a>\n  </div>\n</div>\n\n\nOne promising application domain of this is to use these dynamic parameter values as an input of changing a property of existing virtual objects. For example, if one can connect a parameter value to a size property of a virtual 3D model, then the size of the model can dynamically change when the value changes. This use case is particularly useful for in-situ creation of tangible controllers. For example, a colored token can become a tangible slider to change the size of the object. The system could bind these values based on a proper naming rule\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/realitysketch/video/shark.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-12-1.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-12-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-12-2.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-12-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-12-3.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-12-3.jpg" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-13-1.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-13-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-13-2.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-13-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-13-3.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-13-3.jpg" /></a>\n  </div>\n</div>\n\n<br/>\n\n# Future Vision\n\nWe believe there are a lot of future work and a broad design space of embedded and responsive sketching.\nIn general, crafting responsive and embedded graphics in the real-world can be a continuum between two approaches: pre-defined behavior and user-defined behavior.\nFor example, pre-defined behavior refers to a behavior specification given in advance. For example, one can think of a system that specifies all of the sketched elements to follow the law of physics, so that as long as a user draws a virtual element, it automatically falls and bounces on the ground. In this case, the behavior of sketched elements is pre-defined, based on the physics simulation, and the user can only control the shape of the sketches. Similarly, one can imagine a sketched character that starts walking around or interacting with the physical environment. In this case, the behavior of the sketched character should also be defined in advance (by programming or design tools), as it is hard to specify such complex behaviors in real-time.\nThese practices are often utilized in the screen-based sketching interfaces. For example, PhysInk uses a physics engine and ChalkTalk leverages pre-programmed behavior to animate the sketched elements in real-time.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-22.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-22.jpg" /></a>\n  </div>\n</div>\n\nOn the other end of the spectrum, user-defined behavior lets the user decide how the sketched elements move, behave, and animate on the fly. For example, consider an example of visualizing pendulum motion. In this example, the user should be able to specify how and which parameter (e.g., angle) will be visualized. In the previous works, Apparatus leverages the user-defined behavior to create interactive diagrams. In this example, the user has full control of how it behaves, based on the user-defined constraints and parameter bindings, which is also known as constraint-based programming. These practices are also utilized to create interactive 2D animation, design exploration, and dynamic data visualization, as it is useful to let the user explicitly specify how it behaves.\n\n\nWe envision the future where the user can draw these dynamic pictures by sketching, like *Harold\'s purple crayon*.\nWe hope this paper opens up new opportunities for embedded and responsive sketching and inspires the HCI community to further explore such interactions to realize the full potential of augmented- and mixed-reality (AR/MR) as a dynamic, computational medium.\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-21-1.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-21-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-21-2.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-21-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/realitysketch/figure-21-3.jpg" data-lightbox="lightbox"><img src="/static/projects/realitysketch/figure-21-3.jpg" /></a>\n  </div>\n  <p style="width: 100%; text-align: center">\n    Image Credit: <a href="https://en.wikipedia.org/wiki/Harold_and_the_Purple_Crayon" target="_blank">Harold and the Purple Crayon</a>\n  </p>\n</div>',bodyHtml:"<h1>Abstract</h1>\n<p>We present RealitySketch, an augmented reality interface for sketching interactive graphics and visualizations. In recent years, an increasing number of AR sketching tools enable users to draw and embed sketches in the real world. However, with the current tools, sketched contents are inherently static, floating in mid-air without responding to the real world. This paper introduces a new way to embed <em>dynamic</em> and <em>responsive</em> graphics in the real world. In RealitySketch, the user draws graphical elements on a mobile AR screen and binds them with physical objects in real-time and improvisational ways, so that the sketched elements dynamically move with the corresponding physical motion. The user can also quickly visualize and analyze real-world phenomena through responsive graph plots or interactive visualizations. This paper contributes to a set of interaction techniques that enable capturing, parameterizing, and visualizing real-world motion without pre-defined programs and configurations. Finally, we demonstrate our tool with several application scenarios, including physics education, sports training, and in-situ tangible interfaces.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/realitysketch/video/top.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-1-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-1-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-1-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-1-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-1-4.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-1-4.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>Introduction</h1>\n<p>Over the last decades, interactive and dynamic sketching has been one of the central themes in human-computer interaction (HCI) research. Since Sutherland first demonstrated the power of interactive sketching for computer-aided design, HCI researchers have explored ways to sketch dynamic contents that can interactively respond to user input, thus enabling us to think and communicate through a dynamic visual medium. The applications of such tools are vast, including mathematics and physics education, animated art creation, and interactive data visualization. Through these research outcomes, we have now developed a rich vocabulary of dynamic sketching techniques to fluidly create interactive, animated content in real-time.</p>\n<p>With the advent of augmented and mixed reality interfaces, we now have a unique opportunity to expand such practices beyond screen-based interactions towards reality-based interactions. In fact, there is an increasing number of tools that provide sketching interfaces for augmented reality, from commercial products like Just a Line, Vuforia Chalk, and DoodleLens to research projects like SymbiosisSketch and Mobi3DSketch. These tools allow users to sketch digital elements and embed them in the real world. However, a key limitation is that the sketched content is <em>static</em> --- it does not respond, change, and animate based on user actions or real-world phenomena.</p>\n<p>What if, instead, these sketched elements could <em>dynamically respond</em> when the real world changes? For example, imagine a line sketched onto a physical pendulum that moves as the pendulum swings. This capability would allow us to directly manipulate the sketch through tangible interactions or capture and visualize the pendulum motion to understand the underlying phenomenon.</p>\n<p>As a first step toward this goal, we present <strong>RealitySketch, an end-user sketching tool to support real-time creation and sharing of embedded interactive graphics and visualization in AR</strong>. To create graphics, the user sketches on a phone or tablet screen, which embeds interactive visualizations into a scene. The sketched elements can be bound to physical objects such that they respond dynamically as the real-world changes.</p>\n<h1>RealitySketch: How it works</h1>\n<h2>Design Space and Basic Setup</h2>\n<p>The goal of this paper is to provide a way to embed dynamic and responsive graphics through dynamic sketching. To better understand the scope, we first define the terminology:</p>\n<ol>\n<li>\n<p><strong>Embedded</strong>: graphics are presented and <em>spatially integrated within the real-world</em></p>\n</li>\n<li>\n<p><strong>Responsive</strong>: <em>graphics change and animate</em> based on the user’s interactions</p>\n</li>\n</ol>\n<p>The important aspect of embedded and responsive graphics is that graphics must interact with the real-world. Here, the “real- world” means either the environment itself, a physical phenomenon, or a user’s tangible and gestural interactions.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/realitysketch/video/sketch.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;br/&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-3-4.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-3-4.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-3-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-3-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-3-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-3-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>To enable that, RealitySketch leverages mobile augmented reality (ARKit) to embed sketches in the real world. The user sketches with a finger or a pen on a touchscreen, where the sketched elements are overlaid onto a camera view of the real world.</p>\n<p>Our approach proposes the following four-step workflow:</p>\n<ol>\n<li>\n<p><strong>Object tracking</strong>: the user specifies a visual entity (e.g., a physical object, a skeletal joint) to track in the real-world scene</p>\n</li>\n<li>\n<p><strong>Parameterization</strong>: the user parameterizes tracked entities by drawing lines or arcs that define specific variables of interest</p>\n</li>\n<li>\n<p><strong>Parameter binding</strong>: the user binds these variables to the graphical properties of sketched elements (e.g., length, angle, etc.) to define their dynamic behavior as the real-world variables change</p>\n</li>\n<li>\n<p><strong>Visualization</strong>: the user can also interact, analyze, and visualize real-world movements through several visualization effects.</p>\n</li>\n</ol>\n<p>We contextualize our approach in the larger design space of dynamic graphics authoring approaches.\nThe main contribution of this paper is a set of interaction techniques that enable these steps without <em>pre-defined programs</em> and configurations --- rather, the system lets the user perform in real-time and improvisational ways.</p>\n<h2>Step 1: Track an Object</h2>\n<p>For embedded and responsive graphics, the graphical elements need to be tightly coupled with physical objects and environments. Thus, capturing and tracking an object is vital to make the graphics dynamically change and move.\nTo specify an object, the user enters the selection mode and then taps an object on the screen. Once selected, our system highlights the selected object with a white contour line and starts tracking the object in the 3D scene.</p>\n<p>In our current implementation, the system tracks objects based on color tracking implemented with OpenCV. When the user taps an object on the screen, the algorithm gets the HSV value at the x and y position. Then the system captures similar colors at each frame based on a certain upper and lower threshold range. This color tracking was fast enough for most of our applications (e.g., 20-30 FPS with iPad Pro 11 inch 2018).</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/realitysketch/video/tracking.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-4-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-4-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-4-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-4-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-4-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-4-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h2>Step 2: Sketch to Parameterize</h2>\n<p>Next, the user parameterizes the real world to define and capture the dynamic value of interest. In this paper, we specifically focus on parameterization that can be done through simple sketching interactions using line segments.</p>\n<p>First, when entering the sketching mode, the user can start drawing a line segment onto the scene. All the sketched lines are projected onto a 2D surface within the 3D scene. The system takes the two end-points to create the line segment. This creates a variable that defines the distance between two points on the surface. To create a dynamic line segment, the user draws a line whose endpoint is attached to the selected object. As one end of this dynamic line segment is bound to the selected object, if the user moves the object in the real world, the line segment and its parameter (e.g., distance value) will change accordingly. The system visually renders the line segment values using labels.</p>\n<p>RealitySketch employs simple heuristics to determine the nature (e.g., static vs. dynamic, distance vs angle, free move vs constraints, etc) of the line segment. If the start or end point of the line segment is close to an existing tracked object, then the system binds the end point to the tracked object. Thus, for example, if the user draws a line between two tracked objects, then both ends attach to an object. In that case, the line segment captures the distance between those two objects.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/realitysketch/video/sketch.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-5-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-5-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-5-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-5-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-5-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-5-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-6-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-6-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-6-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-6-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-6-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-6-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h2>Step 3: Bind Parameters to Make Them Responsive</h2>\n<p>To make the existing line segments responsive, the user can bind variables between two elements. The parameter of a static line segment can be bound to another variable. For example, suppose the user has a variable named angle for a dynamic line segment. When the user taps the label of another angle of the static line segment, the system shows a popup to let the user enter the variable name. If the entered variable name matches an existing name, the angle of the static line segment will be dynamically bound to the existing parameter</p>\n<p>Similarly, the user can define a constraint by typing a function of an existing variable. For example, consider the user wants to draw the bisector of the angle formed by a dynamic line segment. The user can first draw a line and an arc between the line and the base line.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/realitysketch/video/bind.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-7-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-7-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-7-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-7-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-7-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-7-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-8-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-8-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-8-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-8-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-8-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-8-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h2>Step 4: Move and Animate</h2>\n<p>Rather than 2D sketches that are based on the device screen coordinates, all sketched elements have a 3D geometry and position in real world coordinates, anchored in 3D space. This way, the user can move the device to see from a different perspective and the sketches stay correctly anchored to the real objects.</p>\n<p>To enable this functionality, our system leverages ARKit and SceneKit for both surface detection and object placement in the 3D scene.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/realitysketch/video/multi-angle.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-9-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-9-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-9-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-9-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-9-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-9-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h2>Step 5: Record and Visualize</h2>\n<p>RealitySketch can also make responsive visualization based on graph plotting of a parameter. In the graph placing mode, the user can place a 2D graph and change its size by dragging and dropping onto the surface. By default, the x-axis of the graph is time. By binding an existing variable to the graph, it starts visualizing the time series data of the variable.</p>\n<p>To bind the variable, the user can simply tap a label of the graph, and then, enter the variable the user-defined. For example, if the user binds the angle variable of the pendulum to the y-axis of the chart, the graph will dynamically plot the angle of the pendulum when it starts swinging. By adding multiple variables separated with a comma (e.g., a,b,c), the user can also plot multiple parameters in the same graph. The user can also change the x-axis from time to a variable by tapping the x-axis and entering a second variable. For example, the user can define an angle and y distance of a point of a circle. By binding x-axis as the angle and y-axis as the perpendicular length, the system can plot the relationship between the angle and corresponding sine value.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/realitysketch/video/visualizations.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-16-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-16-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-16-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-16-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-16-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-16-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-10-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-10-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-10-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-10-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-10-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-10-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;br/&gt;\n&lt;div class=&quot;ui divider&quot;&gt;&lt;/div&gt;</p>\n<h1>Application Scenarios</h1>\n<p>We have explored and demonstrated the following application scenarios</p>\n<ol>\n<li>\n<p><strong>Augmented Physics Experiments</strong></p>\n</li>\n<li>\n<p><strong>Interactive and Explorable Concept Explanation</strong></p>\n</li>\n<li>\n<p><strong>Improvised Visualization for Sports and Exercises</strong></p>\n</li>\n<li>\n<p><strong>In-situ Tangible User Interfaces</strong></p>\n</li>\n</ol>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;br/&gt;</p>\n<h2>Application 1: Augmented Physics Experiments</h2>\n<p>In science education, a classroom experiment is an integral part of learning physics because the real-world experiment provides students an opportunity to connect their knowledge with physical phenomena. RealitySketch can become a powerful tool to support these experiments by leveraging real-time visualization capability. Figure 15 illustrates how our tool can help students understand the pulley system. In this scenario, by tracking the movement of two points (i.e., the point of the person grabs and the point of the load), the students can visualize the traveling distance of each point. In this way, they can see the load distance movement is shorter than the distance the person pulls.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/realitysketch/video/physics.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-17-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-17-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-17-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-17-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-17-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-17-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-19-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-19-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-19-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-19-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-19-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-19-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;br/&gt;</p>\n<h2>Application 2: Interactive and Explorable Concept Explanation</h2>\n<p>RealitySketch is also useful to help teachers explain concepts that may be difficult to understand with static graphs, and to let students explore them through tangible interactions. Some examples are shown in the above Figure (Top: demonstrating an area of a triangle remains the same with horizontal movement; a bisector line of a triangle always intersect at the middle point. Bottom: showing how a sine curve is generated from plotting the angle and perpendicular distance of a rotating point.)</p>\n<p>For educational applications, we can think of the following three setups of how students and teachers can use our tool:</p>\n<ol>\n<li>\n<p><strong>Classroom presentation</strong>: In this case, a teacher or an assistant sketches and visualizes the motion, which can be shared through a connected large display or projector, so that the students can see and understand.</p>\n</li>\n<li>\n<p><strong>Collaborative learning</strong>: In this case, students can form a group and interact with their own devices. Since mobile AR is accessible for almost all smartphones, every student should be able to play by themselves, which can lead to an interesting exploration and discoveries.</p>\n</li>\n<li>\n<p><strong>Remote learning</strong>: In this case, a teacher only records the video of the experiment, then share the recorded video with the students. The student can download and visualize with their own app, so that it provides a fun and interactive experiment to support online and remote learning.</p>\n</li>\n</ol>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/realitysketch/video/concept.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-18-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-18-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-18-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-18-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-18-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-18-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-20-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-20-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-20-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-20-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-20-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-20-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;br/&gt;</p>\n<h2>Application 3: Improvised Visualization for Sports and Exercises</h2>\n<p>RealitySketch could be also useful to analyze and visualize the motion for sports training and exercises because they often employ the physical movements. For example, sports practices, like a golf shot, baseball pitching, and basketball shooting, would be an interesting example to visualize the trajectory of the ball. Similar to the previous example of showing the trajectory of a ball, it is useful to quickly see the path through stroboscopic effects. In addition to showing the trajectory, the system can also capture and compare multiple attempts to let the user analyze what works and what does not.</p>\n<p>Also, many sports and exercises may require proper form and posture. For example, in baseball batting, golf shot, and a tennis swing, a player’s form, such as a body angle, can be important, which the tool can help visualize through sketched guided lines. These features could be particularly useful for exercise instructions. For example, in yoga practice, bike riding, or weight lifting training, there are recommended angles to be followed to maximize the performance benefits. In these cases, the improvisational measurement of the posture can help the user to see and check if correctly done.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/realitysketch/video/sports.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-14-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-14-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-14-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-14-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-14-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-14-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-15-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-15-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-15-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-15-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-15-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-15-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;br/&gt;</p>\n<h2>Application 4: In-situ Tangible User Interfaces</h2>\n<p>The parameterized value can be used for many different pur- poses to enable responsive visual outputs. So far, we have mostly focused on animation of the simple basic geometry (e.g., line segments, arc) or build-in visualization outputs (e.g., graph plot). However, the dynamic parameter value can be also used for other outputs via binding, as we discussed in the previous sections (e.g., pre-defined vs user-defined section).</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/realitysketch/video/dino.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-11-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-11-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-11-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-11-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-11-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-11-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>One promising application domain of this is to use these dynamic parameter values as an input of changing a property of existing virtual objects. For example, if one can connect a parameter value to a size property of a virtual 3D model, then the size of the model can dynamically change when the value changes. This use case is particularly useful for in-situ creation of tangible controllers. For example, a colored token can become a tangible slider to change the size of the object. The system could bind these values based on a proper naming rule</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/realitysketch/video/shark.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-12-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-12-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-12-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-12-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-12-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-12-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-13-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-13-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-13-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-13-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-13-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-13-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;br/&gt;</p>\n<h1>Future Vision</h1>\n<p>We believe there are a lot of future work and a broad design space of embedded and responsive sketching.\nIn general, crafting responsive and embedded graphics in the real-world can be a continuum between two approaches: pre-defined behavior and user-defined behavior.\nFor example, pre-defined behavior refers to a behavior specification given in advance. For example, one can think of a system that specifies all of the sketched elements to follow the law of physics, so that as long as a user draws a virtual element, it automatically falls and bounces on the ground. In this case, the behavior of sketched elements is pre-defined, based on the physics simulation, and the user can only control the shape of the sketches. Similarly, one can imagine a sketched character that starts walking around or interacting with the physical environment. In this case, the behavior of the sketched character should also be defined in advance (by programming or design tools), as it is hard to specify such complex behaviors in real-time.\nThese practices are often utilized in the screen-based sketching interfaces. For example, PhysInk uses a physics engine and ChalkTalk leverages pre-programmed behavior to animate the sketched elements in real-time.</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-22.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-22.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>On the other end of the spectrum, user-defined behavior lets the user decide how the sketched elements move, behave, and animate on the fly. For example, consider an example of visualizing pendulum motion. In this example, the user should be able to specify how and which parameter (e.g., angle) will be visualized. In the previous works, Apparatus leverages the user-defined behavior to create interactive diagrams. In this example, the user has full control of how it behaves, based on the user-defined constraints and parameter bindings, which is also known as constraint-based programming. These practices are also utilized to create interactive 2D animation, design exploration, and dynamic data visualization, as it is useful to let the user explicitly specify how it behaves.</p>\n<p>We envision the future where the user can draw these dynamic pictures by sketching, like <em>Harold's purple crayon</em>.\nWe hope this paper opens up new opportunities for embedded and responsive sketching and inspires the HCI community to further explore such interactions to realize the full potential of augmented- and mixed-reality (AR/MR) as a dynamic, computational medium.</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-21-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-21-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-21-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-21-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/realitysketch/figure-21-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/realitysketch/figure-21-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;p style=&quot;width: 100%; text-align: center&quot;&gt;\nImage Credit: &lt;a href=&quot;https://en.wikipedia.org/wiki/Harold_and_the_Purple_Crayon&quot; target=&quot;_blank&quot;&gt;Harold and the Purple Crayon&lt;/a&gt;\n&lt;/p&gt;\n&lt;/div&gt;</p>\n",dir:"content/output/projects",base:"realitysketch.json",ext:".json",sourceBase:"realitysketch.md",sourceExt:".md"}},iZP3:function(t,e,o){var i=o("XVgq"),a=o("Z7t5");function n(t){return(n="function"==typeof a&&"symbol"==typeof i?function(t){return typeof t}:function(t){return t&&"function"==typeof a&&t.constructor===a&&t!==a.prototype?"symbol":typeof t})(t)}function s(e){return"function"==typeof a&&"symbol"===n(i)?t.exports=s=function(t){return n(t)}:t.exports=s=function(t){return t&&"function"==typeof a&&t.constructor===a&&t!==a.prototype?"symbol":n(t)},s(e)}t.exports=s},iq4v:function(t,e,o){o("Mqbl"),t.exports=o("WEpk").Object.keys},jEBx:function(t){t.exports={id:"reactile",name:"Reactile",description:"Programming Swarm User Interfaces through Direct Physical Manipulation",title:"Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation",authors:["Ryo Suzuki","Jun Kato","Mark D. Gross","Tom Yeh"],year:2018,booktitle:"In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18)",publisher:"ACM, New York, NY, USA",pages:"Paper 199, 13 pages",doi:"https://doi.org/10.1145/3173574.3173773",conference:{name:"CHI 2018",fullname:"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2018)",url:"https://chi2018.acm.org/"},pdf:"chi-2018-reactile.pdf",video:"https://www.youtube.com/watch?v=Gb7brajKCVE",embed:"https://www.youtube.com/embed/Gb7brajKCVE","short-video":"https://www.youtube.com/watch?v=YT7vMJZjohU",slide:"chi-2018-reactile-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3173773",github:"https://github.com/ryosuzuki/reactile",pageCount:12,slideCount:56,bodyContent:'# Abstract\n\nWe explore a new approach to programming swarm user interfaces (Swarm UI) by leveraging direct physical manipulation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high- level interface design. Inspired by current UI programming practices, we introduce a four-step workflow—create elements, abstract attributes, specify behaviors, and propagate changes—for Swarm UI programming. We propose a set of direct physi- cal manipulation techniques to support each step in this work- flow. To demonstrate these concepts, we developed Reac- tile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studies—an in-class survey with 148 students and a lab interview with eight participants—confirm that our approach is intuitive and understandable for programming Swarm UIs.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/top.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-3.png" /></a>\n  </div>\n</div>\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-1.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-2.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-3.png" /></a>\n  </div>\n</div>\n\n\n# Introduction\n\nIn recent years, **Swarm User Interfaces** have emerged as a new paradigm of human-computer interaction. While the idea of coordinated miniature robots was originally proposed in the literature of swarm and micro-robotic systems, HCI researchers have explored the use of these robots as a user interface.\nHowever, this opportunity is currently limited to highly skilled programmers who are proficient in robot programming. For typical programmers inexperienced in robot programming who wish to build a Swarm UI application, it is unclear if the robot programming approach is the most appropriate for UI programming. To design interactive UI applications, pro- grammers often must think in terms of higher-level design for user interaction, whereas robot programming tends to focus on low-level controls of sensors and actuators. Historically, a novel UI platform is adopted only after the advent of an effective programming tool that empowers a larger developer community, and even end-users, to create many applications for the platform; for example, HyperCard for interactive hyper- media, Phidgets for physical interfaces, and Interface Builder for GUI applications. We stipulate that current approaches to programming Swarm UI are too robot-centric to be effec- tive for building rich and interactive applications. Then, what would be a better alternative?\n\n\n# Reactile\n\nThis paper introduces Reactile, a programming environment for Swarm UI applications.\nThe goal of Reactile is to explore a new approach to programming Swarm UI applications. To design an appropriate workflow for Swarm UI programming, we look into existing UI programming paradigm for inspiration. The common workflow of UI programming can be decomposed into four basic steps: create elements, abstract attributes, specify behaviors, and propagate changes. Based on these insights, we propose the following four-step workflow for Swarm UI programming: 1) creates shapes, 2) abstracts shape attributes as variables, 3) specifies data-bindings be- tween dynamic attributes, and 4) the system changes shapes in response to user inputs. With this workflow, a programmer can think in terms of high-level interface and interaction design to build interactive Swarm UI appli- cations, compared to existing, low-level, robot programming approaches.\n\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-3.png" /></a>\n  </div>\n</div>\n\nThe workflow of Swarm UI programming is inspired by the existing UI programming paradigm. We first review the common workflow of UI programming and decompose it into four basic elements that represent high-level steps. Then we discuss how to apply this workflow to Swarm UI programming.\nAs we see in well-known design patterns for interactive UI ap- plications such as reactive programming paradigm, the Model-View-Controller, and the observer pattern, they share a com- mon workflow consisting of four basic elements: 1) create elements, 2) abstract attributes, 3) specify behaviors, and 4) propagate changes.\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-6.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-6.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-4.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-5.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-6.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-6.png" /></a>\n  </div>\n</div>\n\n# Implementation\n\nReactile actuates a swarm of small magnetic markers to move on a 2D canvas with electromagnetic force. We designed and fabricated a board of electromagnetic coil arrays (3,200 coils), which covers an 80 cm x 40 cm area. Reactile tracks the marker positions and detects interactions between a user and swarm markers using a standard RGB camera and computer vision techniques. The system displays spatial information using a DLP projector to allow a programmer to see program states in the same physical context.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/coil.mp4" type="video/mp4"></source>\n</video>\n\n<br />\n\nIn Reactile, a user interface consists of a swarm of passive magnetic markers which move on a 2D workspace driven by electromagnetic forces. Reactile uses a grid of electromagnetic coils to actuate these magnetic markers. Running current through the circuit coils generates a local magnetic field so that each coil can attract a single magnet located within its area. Each coil is aligned with a certain offset in both horizontal and vertical direction with an effective area overlap, which allows the coil to attract the magnet located in the adjacent coil. We design electromagnetic coil arrays to be fabricated with a standard printed circuit board (PCB) manufacturing. This reduces the cost and fabrication complexity, making it easy for the actuation area to scale up.\n\nOur PCB design is a 4-layer board, and each layer contains a set of coils, each of which has an identical circular shape with a 15 mm diameter and a 2.5 mm overlap between nearby coils. Each coil has 15 turns with 0.203 mm (8 mils) spacing between lines, and the distance between centers of two coils is approximately 10 mm, which makes a 10 mm grid for attractive points. The final prototype covers an 80 cm x 40 cm area with 80 x 40 coils by aligning five identical boards horizontally. The fabrication of each board costs approximately $80 USD, including manufacturing of PCB and electronic components.\n\n<br />\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/mechanism.mp4" type="video/mp4"></source>\n</video>',bodyHtml:"<h1>Abstract</h1>\n<p>We explore a new approach to programming swarm user interfaces (Swarm UI) by leveraging direct physical manipulation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high- level interface design. Inspired by current UI programming practices, we introduce a four-step workflow—create elements, abstract attributes, specify behaviors, and propagate changes—for Swarm UI programming. We propose a set of direct physi- cal manipulation techniques to support each step in this work- flow. To demonstrate these concepts, we developed Reac- tile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studies—an in-class survey with 148 students and a lab interview with eight participants—confirm that our approach is intuitive and understandable for programming Swarm UIs.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/reactile/top.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/reactile/figure-1-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/reactile/figure-1-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/reactile/figure-1-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/reactile/figure-1-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/reactile/figure-1-3.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/reactile/figure-1-3.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/reactile/figure-2-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/reactile/figure-2-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/reactile/figure-2-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/reactile/figure-2-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/reactile/figure-2-3.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/reactile/figure-2-3.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>Introduction</h1>\n<p>In recent years, <strong>Swarm User Interfaces</strong> have emerged as a new paradigm of human-computer interaction. While the idea of coordinated miniature robots was originally proposed in the literature of swarm and micro-robotic systems, HCI researchers have explored the use of these robots as a user interface.\nHowever, this opportunity is currently limited to highly skilled programmers who are proficient in robot programming. For typical programmers inexperienced in robot programming who wish to build a Swarm UI application, it is unclear if the robot programming approach is the most appropriate for UI programming. To design interactive UI applications, pro- grammers often must think in terms of higher-level design for user interaction, whereas robot programming tends to focus on low-level controls of sensors and actuators. Historically, a novel UI platform is adopted only after the advent of an effective programming tool that empowers a larger developer community, and even end-users, to create many applications for the platform; for example, HyperCard for interactive hyper- media, Phidgets for physical interfaces, and Interface Builder for GUI applications. We stipulate that current approaches to programming Swarm UI are too robot-centric to be effec- tive for building rich and interactive applications. Then, what would be a better alternative?</p>\n<h1>Reactile</h1>\n<p>This paper introduces Reactile, a programming environment for Swarm UI applications.\nThe goal of Reactile is to explore a new approach to programming Swarm UI applications. To design an appropriate workflow for Swarm UI programming, we look into existing UI programming paradigm for inspiration. The common workflow of UI programming can be decomposed into four basic steps: create elements, abstract attributes, specify behaviors, and propagate changes. Based on these insights, we propose the following four-step workflow for Swarm UI programming: 1) creates shapes, 2) abstracts shape attributes as variables, 3) specifies data-bindings be- tween dynamic attributes, and 4) the system changes shapes in response to user inputs. With this workflow, a programmer can think in terms of high-level interface and interaction design to build interactive Swarm UI appli- cations, compared to existing, low-level, robot programming approaches.</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/reactile/figure-3.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/reactile/figure-3.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>The workflow of Swarm UI programming is inspired by the existing UI programming paradigm. We first review the common workflow of UI programming and decompose it into four basic elements that represent high-level steps. Then we discuss how to apply this workflow to Swarm UI programming.\nAs we see in well-known design patterns for interactive UI ap- plications such as reactive programming paradigm, the Model-View-Controller, and the observer pattern, they share a com- mon workflow consisting of four basic elements: 1) create elements, 2) abstract attributes, 3) specify behaviors, and 4) propagate changes.</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/reactile/figure-1-4.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/reactile/figure-1-4.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/reactile/figure-1-5.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/reactile/figure-1-5.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/reactile/figure-1-6.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/reactile/figure-1-6.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/reactile/figure-2-4.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/reactile/figure-2-4.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/reactile/figure-2-5.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/reactile/figure-2-5.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/reactile/figure-2-6.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/reactile/figure-2-6.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>Implementation</h1>\n<p>Reactile actuates a swarm of small magnetic markers to move on a 2D canvas with electromagnetic force. We designed and fabricated a board of electromagnetic coil arrays (3,200 coils), which covers an 80 cm x 40 cm area. Reactile tracks the marker positions and detects interactions between a user and swarm markers using a standard RGB camera and computer vision techniques. The system displays spatial information using a DLP projector to allow a programmer to see program states in the same physical context.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/reactile/coil.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;br /&gt;</p>\n<p>In Reactile, a user interface consists of a swarm of passive magnetic markers which move on a 2D workspace driven by electromagnetic forces. Reactile uses a grid of electromagnetic coils to actuate these magnetic markers. Running current through the circuit coils generates a local magnetic field so that each coil can attract a single magnet located within its area. Each coil is aligned with a certain offset in both horizontal and vertical direction with an effective area overlap, which allows the coil to attract the magnet located in the adjacent coil. We design electromagnetic coil arrays to be fabricated with a standard printed circuit board (PCB) manufacturing. This reduces the cost and fabrication complexity, making it easy for the actuation area to scale up.</p>\n<p>Our PCB design is a 4-layer board, and each layer contains a set of coils, each of which has an identical circular shape with a 15 mm diameter and a 2.5 mm overlap between nearby coils. Each coil has 15 turns with 0.203 mm (8 mils) spacing between lines, and the distance between centers of two coils is approximately 10 mm, which makes a 10 mm grid for attractive points. The final prototype covers an 80 cm x 40 cm area with 80 x 40 coils by aligning five identical boards horizontally. The fabrication of each board costs approximately $80 USD, including manufacturing of PCB and electronic components.</p>\n<p>&lt;br /&gt;</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/reactile/mechanism.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n",dir:"content/output/projects",base:"reactile.json",ext:".json",sourceBase:"reactile.md",sourceExt:".md"}},kd2E:function(t,e,o){"use strict";function i(t,e){return Object.prototype.hasOwnProperty.call(t,e)}t.exports=function(t,e,o,n){e=e||"&",o=o||"=";var s={};if("string"!=typeof t||0===t.length)return s;var r=/\+/g;t=t.split(e);var c=1e3;n&&"number"==typeof n.maxKeys&&(c=n.maxKeys);var l=t.length;c>0&&l>c&&(l=c);for(var u=0;u<l;++u){var h,p,g,d,m=t[u].replace(r,"%20"),f=m.indexOf(o);f>=0?(h=m.substr(0,f),p=m.substr(f+1)):(h=m,p=""),g=decodeURIComponent(h),d=decodeURIComponent(p),i(s,g)?a(s[g])?s[g].push(d):s[g]=[s[g],d]:s[g]=d}return s};var a=Array.isArray||function(t){return"[object Array]"===Object.prototype.toString.call(t)}},kiME:function(t,e,o){"use strict";var i=o("KI45")(o("SqZg"));Object.defineProperty(e,"__esModule",{value:!0}),e.default=function(){var t=(0,i.default)(null);return{on:function(e,o){(t[e]||(t[e]=[])).push(o)},off:function(e,o){t[e]&&t[e].splice(t[e].indexOf(o)>>>0,1)},emit:function(e){for(var o=arguments.length,i=new Array(o>1?o-1:0),a=1;a<o;a++)i[a-1]=arguments[a];(t[e]||[]).slice().map(function(t){t.apply(void 0,i)})}}}},kwZ1:function(t,e,o){"use strict";var i=o("w6GO"),a=o("mqlF"),n=o("NV0k"),s=o("JB68"),r=o("M1xp"),c=Object.assign;t.exports=!c||o("KUxP")(function(){var t={},e={},o=Symbol(),i="abcdefghijklmnopqrst";return t[o]=7,i.split("").forEach(function(t){e[t]=t}),7!=c({},t)[o]||Object.keys(c({},e)).join("")!=i})?function(t,e){for(var o=s(t),c=arguments.length,l=1,u=a.f,h=n.f;c>l;)for(var p,g=r(arguments[l++]),d=u?i(g).concat(u(g)):i(g),m=d.length,f=0;m>f;)h.call(g,p=d[f++])&&(o[p]=g[p]);return o}:c},lJku:function(t){t.exports={id:"shapebots",name:"ShapeBots",description:"Shape-changing Swarm Robots",title:"ShapeBots: Shape-changing Swarm Robots",authors:["Ryo Suzuki","Clement Zheng","Yasuaki Kakehi","Tom Yeh","Ellen Yi-Luen Do","Mark D. Gross","Daniel Leithinger"],year:2019,booktitle:"In Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST '19)",publisher:"ACM, New York, NY, USA",pages:"1-13",doi:"https://doi.org/10.1145/3332165.3347911",conference:{name:"UIST 2019",fullname:"The ACM Symposium on User Interface Software and Technology (UIST 2019)",url:"http://uist.acm.org/uist2019"},pdf:"uist-2019-shapebots.pdf",slide:"uist-2019-shapebots-slide.pdf",video:"https://www.youtube.com/watch?v=cwPaof0kKdM",embed:"https://www.youtube.com/embed/cwPaof0kKdM",github:"https://github.com/ryosuzuki/shapebots",poster:"uist-2019-shapebots-poster.pdf",demo:"https://ryosuzuki.github.io/shapebots-simulator/",arxiv:"https://arxiv.org/abs/1909.03372","acm-dl":"https://dl.acm.org/doi/10.1145/3332165.3347911",pageCount:13,slideCount:53,bodyContent:'# Abstract\n\nWe introduce *shape-changing swarm robots*. A swarm of self-transformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions. The modular design of each actuator enables various shapes and geometries of self-transformation. We illustrate potential application scenarios and discuss how this type of interface opens up possibilities for the future of ubiquitous and distributed shape-changing interfaces.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/top.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/top.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-1-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-1-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-1-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-1-1.jpg" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-2-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-2-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-2-3.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-2-3.jpg" /></a>\n  </div>\n</div>\n\n\n# Shape-changing Swarm Robots\n\nThis paper introduces **shape-changing swarm robots** for dis- tributed shape-changing interfaces. Shape-changing swarm robots can both **individually** and **collectively** change their shape, so that they can collectively present information, act as controllers, actuate objects, represent data, and provide dynamic physical affordances.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-3.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-3.png" /></a>\n  </div>\n</div>\n\nThis paper specifically focuses on the user interface aspect of such systems, which we refer to shape-changing swarm user interfaces. We identified three core aspects of shape-changing swarm robots: 1) locomotion, 2) self-transformation, and 3) collective behaviors of many individual elements.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-4.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-4.png" /></a>\n  </div>\n</div>\n\n# ShapeBots\n\n**ShapeBots are self-transformable swarm robots** with modular linear actuators. To enable a large deformation capability of tiny swarm robots, we developed a miniature reel-based linear actuator that is thin (2.5 cm) and fits into the small footprint (3 cm x 3 cm), while able to extend up to 20 cm in both horizontal and vertical directions.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/unit.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/unit.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-5-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-5-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-5-2.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-6-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-6-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-6-2.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-6-2.png" /></a>\n  </div>\n</div>\n\nThe modular design of each linear actuator unit enables the construction of various shapes and geometries of individual shape transformation (e.g., horizontal lines, vertical lines, curved lines, 2D area expan- sion, and 3D volumetric change). Based on these capabilities, we demonstrate application scenarios showing how a swarm of distributed self-transformable robots can support everyday interactions.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/transformation.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/transformation.mp4" type="video/mp4"></source>\n</video>\n\n\n# Tracking and Control\n\nTo track the position and orientation of the swarm robots, we used computer vision and a fiducial marker attached to the bottom of the robot. We used the ArUco fiducial marker printed on a sheet of paper and taped to the bottom of the robot. Our prototype used a 1.5 cm x 1.5 cm size marker with a 4 x 4 grid pattern, which can provide up to 50 unique patterns. For tracking software, we used the OpenCV library and ArUco python module. It can track the position of the markers at 60 frames per second.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/tracking.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/tracking.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-7-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-7-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-7-2.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-7-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-7-3.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-7-3.png" /></a>\n  </div>\n</div>\n\nTo enable the user to easily specify a target shape, we created a web-based interface where users draw a shape or upload an SVG image (Figure 10). The user draws a set of lines, then the main computer calculates target positions, orientations, and actuator lengths to start sending commands.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-8.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-8.png" /></a>\n  </div>\n</div>\n\n\nWe can use the same mechanism to track user input. The system supports four different types of user interaction that our system supports: place, move, orient, and pick-up.\nThe system recognizes as user inputs movement or rotation of a marker that it did not generate.\n\n\n\n# Applications: Robots as Dynamic Physical Media\nWe explore potential application scenarios for the future of human-robot interactions.\nOne interesting application area is to use these **robots as dynamic physical media**, such as showing **data visualization in the physical world**.\nFor example, ShapeBots on the USA map physicalize map data; each robot changes its height to show the population of the state it is on. Users can interact with the dataset by placing a new robot or moving a robot to a different state, and the robots update their physical forms to represent the respective population.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/dataphys.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/dataphys.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-3.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-3.jpg" /></a>\n  </div>\n</div>\n\nSimilarly, ShapeBots can provide a physical preview of a CAD design. ShapeBots physicalizes the actual size of the box. The design and physical rendering are tightly coupled; as the user changes the height of the box in CAD software, the ShapeBots change heights accordingly. The user can change the parameters of the design by moving robots in the physical space, and these changes are reflected in the CAD design.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/cad.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/cad.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-4.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-4.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-5.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-5.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-6.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-6.jpg" /></a>\n  </div>\n</div>\n\n# Applications: Robots as Ambient Assistants\n\nAnother practical aspect of ShapeBots is the ability to actuate objects and act as physical constraints. As an example, the video shows two robots extending their linear actuators to wipe debris off a table, clearing a workspace for the user.\nIn these scenarios, these robots can help as an **ambient assistant for everyday life**.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/cleaning.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/cleaning.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-11-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-11-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-11-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-11-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-11-3.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-11-3.jpg" /></a>\n  </div>\n</div>\n\nBy leveraging the capability of locomotion and height change of each robot, ShapeBots can create a dynamic fence to hide or encompass existing objects for affordances. For example, when the user pours hot coffee into a cup, the robots surround the cup and change their heights to create a vertical fence. The vertical fence visually and physically provides the affordance to indicate that the coffee is too hot and not ready to drink. Once it is ready, the robots start dispersing and allow the user to grab it. These scenarios illustrate how the distributed shape-changing robots can provide a new type of affordance, which we call **distributed dynamic physical affordances**.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/affordance.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/affordance.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-10-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-10-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-10-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-10-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-10-3.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-10-3.jpg" /></a>\n  </div>\n</div>\n\nShapeBots can also act as an interactive physical display. The following figures show how ShapeBots can render different shapes.\nWe highlight the advantage of ShapeBots for rendering contours compared to non self-transformable swarm robots. Using a software simulation, we demonstrate how ShapeBots renders an SVG input at different swarm sizes. You can also play with the [**explorable online simulator**](https://ryosuzuki.github.io/shapebots-simulator/) to see how these robots can render the shape ↓\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/explorable.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/explorable.mp4" type="video/mp4"></source>\n</video>\n\n\n\n# Future Work\n\nShapeBots is just a single example of shape-changing swarm robots.\nThere is a broader design space of shape- changing swarm user interfaces.\nAs future work, we are interested in exploring the different aspct of shape-changing swarm robots.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-12.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-12.png" /></a>\n  </div>\n</div>\n\n\x3c!--\n# Appendix\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/fabrication.mp4" type="video/mp4"></source>\n</video>\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/explorable.mp4" type="video/mp4"></source>\n</video>\n --\x3e',bodyHtml:'<h1>Abstract</h1>\n<p>We introduce <em>shape-changing swarm robots</em>. A swarm of self-transformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions. The modular design of each actuator enables various shapes and geometries of self-transformation. We illustrate potential application scenarios and discuss how this type of interface opens up possibilities for the future of ubiquitous and distributed shape-changing interfaces.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/shapebots/video/top.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/shapebots/video/top.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-1-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-1-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-1-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-1-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-2-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-2-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-2-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-2-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-2-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-2-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>Shape-changing Swarm Robots</h1>\n<p>This paper introduces <strong>shape-changing swarm robots</strong> for dis- tributed shape-changing interfaces. Shape-changing swarm robots can both <strong>individually</strong> and <strong>collectively</strong> change their shape, so that they can collectively present information, act as controllers, actuate objects, represent data, and provide dynamic physical affordances.</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-3.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-3.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>This paper specifically focuses on the user interface aspect of such systems, which we refer to shape-changing swarm user interfaces. We identified three core aspects of shape-changing swarm robots: 1) locomotion, 2) self-transformation, and 3) collective behaviors of many individual elements.</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-4.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-4.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>ShapeBots</h1>\n<p><strong>ShapeBots are self-transformable swarm robots</strong> with modular linear actuators. To enable a large deformation capability of tiny swarm robots, we developed a miniature reel-based linear actuator that is thin (2.5 cm) and fits into the small footprint (3 cm x 3 cm), while able to extend up to 20 cm in both horizontal and vertical directions.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/shapebots/video/unit.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/shapebots/video/unit.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-5-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-5-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-5-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-5-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-6-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-6-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-6-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-6-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>The modular design of each linear actuator unit enables the construction of various shapes and geometries of individual shape transformation (e.g., horizontal lines, vertical lines, curved lines, 2D area expan- sion, and 3D volumetric change). Based on these capabilities, we demonstrate application scenarios showing how a swarm of distributed self-transformable robots can support everyday interactions.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/shapebots/video/transformation.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/shapebots/video/transformation.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<h1>Tracking and Control</h1>\n<p>To track the position and orientation of the swarm robots, we used computer vision and a fiducial marker attached to the bottom of the robot. We used the ArUco fiducial marker printed on a sheet of paper and taped to the bottom of the robot. Our prototype used a 1.5 cm x 1.5 cm size marker with a 4 x 4 grid pattern, which can provide up to 50 unique patterns. For tracking software, we used the OpenCV library and ArUco python module. It can track the position of the markers at 60 frames per second.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/shapebots/video/tracking.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/shapebots/video/tracking.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-7-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-7-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-7-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-7-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-7-3.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-7-3.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>To enable the user to easily specify a target shape, we created a web-based interface where users draw a shape or upload an SVG image (Figure 10). The user draws a set of lines, then the main computer calculates target positions, orientations, and actuator lengths to start sending commands.</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-8.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-8.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>We can use the same mechanism to track user input. The system supports four different types of user interaction that our system supports: place, move, orient, and pick-up.\nThe system recognizes as user inputs movement or rotation of a marker that it did not generate.</p>\n<h1>Applications: Robots as Dynamic Physical Media</h1>\n<p>We explore potential application scenarios for the future of human-robot interactions.\nOne interesting application area is to use these <strong>robots as dynamic physical media</strong>, such as showing <strong>data visualization in the physical world</strong>.\nFor example, ShapeBots on the USA map physicalize map data; each robot changes its height to show the population of the state it is on. Users can interact with the dataset by placing a new robot or moving a robot to a different state, and the robots update their physical forms to represent the respective population.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/shapebots/video/dataphys.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/shapebots/video/dataphys.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-9-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-9-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-9-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-9-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-9-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-9-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>Similarly, ShapeBots can provide a physical preview of a CAD design. ShapeBots physicalizes the actual size of the box. The design and physical rendering are tightly coupled; as the user changes the height of the box in CAD software, the ShapeBots change heights accordingly. The user can change the parameters of the design by moving robots in the physical space, and these changes are reflected in the CAD design.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/shapebots/video/cad.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/shapebots/video/cad.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-9-4.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-9-4.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-9-5.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-9-5.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-9-6.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-9-6.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>Applications: Robots as Ambient Assistants</h1>\n<p>Another practical aspect of ShapeBots is the ability to actuate objects and act as physical constraints. As an example, the video shows two robots extending their linear actuators to wipe debris off a table, clearing a workspace for the user.\nIn these scenarios, these robots can help as an <strong>ambient assistant for everyday life</strong>.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/shapebots/video/cleaning.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/shapebots/video/cleaning.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-11-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-11-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-11-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-11-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-11-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-11-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>By leveraging the capability of locomotion and height change of each robot, ShapeBots can create a dynamic fence to hide or encompass existing objects for affordances. For example, when the user pours hot coffee into a cup, the robots surround the cup and change their heights to create a vertical fence. The vertical fence visually and physically provides the affordance to indicate that the coffee is too hot and not ready to drink. Once it is ready, the robots start dispersing and allow the user to grab it. These scenarios illustrate how the distributed shape-changing robots can provide a new type of affordance, which we call <strong>distributed dynamic physical affordances</strong>.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/shapebots/video/affordance.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/shapebots/video/affordance.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-10-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-10-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-10-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-10-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-10-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-10-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>ShapeBots can also act as an interactive physical display. The following figures show how ShapeBots can render different shapes.\nWe highlight the advantage of ShapeBots for rendering contours compared to non self-transformable swarm robots. Using a software simulation, we demonstrate how ShapeBots renders an SVG input at different swarm sizes. You can also play with the <a href="https://ryosuzuki.github.io/shapebots-simulator/"><strong>explorable online simulator</strong></a> to see how these robots can render the shape ↓</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/shapebots/video/explorable.webm&quot; type=&quot;video/webm&quot;&gt;&lt;/source&gt;\n&lt;source src=&quot;/static/projects/shapebots/video/explorable.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<h1>Future Work</h1>\n<p>ShapeBots is just a single example of shape-changing swarm robots.\nThere is a broader design space of shape- changing swarm user interfaces.\nAs future work, we are interested in exploring the different aspct of shape-changing swarm robots.</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/shapebots/figure-12.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/shapebots/figure-12.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;!--</p>\n<h1>Appendix</h1>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/shapebots/video/fabrication.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/shapebots/video/explorable.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;\n--&gt;</p>\n',dir:"content/output/projects",base:"shapebots.json",ext:".json",sourceBase:"shapebots.md",sourceExt:".md"}},ldVq:function(t,e,o){var i=o("QMMT"),a=o("UWiX")("iterator"),n=o("SBuE");t.exports=o("WEpk").isIterable=function(t){var e=Object(t);return void 0!==e[a]||"@@iterator"in e||n.hasOwnProperty(i(e))}},ln6h:function(t,e,o){t.exports=o("u938")},ls82:function(t,e){!function(e){"use strict";var o,i=Object.prototype,a=i.hasOwnProperty,n="function"==typeof Symbol?Symbol:{},s=n.iterator||"@@iterator",r=n.asyncIterator||"@@asyncIterator",c=n.toStringTag||"@@toStringTag",l="object"==typeof t,u=e.regeneratorRuntime;if(u)l&&(t.exports=u);else{(u=e.regeneratorRuntime=l?t.exports:{}).wrap=q;var h="suspendedStart",p="suspendedYield",g="executing",d="completed",m={},f={};f[s]=function(){return this};var b=Object.getPrototypeOf,v=b&&b(b(E([])));v&&v!==i&&a.call(v,s)&&(f=v);var y=k.prototype=j.prototype=Object.create(f);x.prototype=y.constructor=k,k.constructor=x,k[c]=x.displayName="GeneratorFunction",u.isGeneratorFunction=function(t){var e="function"==typeof t&&t.constructor;return!!e&&(e===x||"GeneratorFunction"===(e.displayName||e.name))},u.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,k):(t.__proto__=k,c in t||(t[c]="GeneratorFunction")),t.prototype=Object.create(y),t},u.awrap=function(t){return{__await:t}},T(I.prototype),I.prototype[r]=function(){return this},u.AsyncIterator=I,u.async=function(t,e,o,i){var a=new I(q(t,e,o,i));return u.isGeneratorFunction(e)?a:a.next().then(function(t){return t.done?t.value:a.next()})},T(y),y[c]="Generator",y[s]=function(){return this},y.toString=function(){return"[object Generator]"},u.keys=function(t){var e=[];for(var o in t)e.push(o);return e.reverse(),function o(){for(;e.length;){var i=e.pop();if(i in t)return o.value=i,o.done=!1,o}return o.done=!0,o}},u.values=E,C.prototype={constructor:C,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=o,this.done=!1,this.delegate=null,this.method="next",this.arg=o,this.tryEntries.forEach(R),!t)for(var e in this)"t"===e.charAt(0)&&a.call(this,e)&&!isNaN(+e.slice(1))&&(this[e]=o)},stop:function(){this.done=!0;var t=this.tryEntries[0].completion;if("throw"===t.type)throw t.arg;return this.rval},dispatchException:function(t){if(this.done)throw t;var e=this;function i(i,a){return r.type="throw",r.arg=t,e.next=i,a&&(e.method="next",e.arg=o),!!a}for(var n=this.tryEntries.length-1;n>=0;--n){var s=this.tryEntries[n],r=s.completion;if("root"===s.tryLoc)return i("end");if(s.tryLoc<=this.prev){var c=a.call(s,"catchLoc"),l=a.call(s,"finallyLoc");if(c&&l){if(this.prev<s.catchLoc)return i(s.catchLoc,!0);if(this.prev<s.finallyLoc)return i(s.finallyLoc)}else if(c){if(this.prev<s.catchLoc)return i(s.catchLoc,!0)}else{if(!l)throw new Error("try statement without catch or finally");if(this.prev<s.finallyLoc)return i(s.finallyLoc)}}}},abrupt:function(t,e){for(var o=this.tryEntries.length-1;o>=0;--o){var i=this.tryEntries[o];if(i.tryLoc<=this.prev&&a.call(i,"finallyLoc")&&this.prev<i.finallyLoc){var n=i;break}}n&&("break"===t||"continue"===t)&&n.tryLoc<=e&&e<=n.finallyLoc&&(n=null);var s=n?n.completion:{};return s.type=t,s.arg=e,n?(this.method="next",this.next=n.finallyLoc,m):this.complete(s)},complete:function(t,e){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&e&&(this.next=e),m},finish:function(t){for(var e=this.tryEntries.length-1;e>=0;--e){var o=this.tryEntries[e];if(o.finallyLoc===t)return this.complete(o.completion,o.afterLoc),R(o),m}},catch:function(t){for(var e=this.tryEntries.length-1;e>=0;--e){var o=this.tryEntries[e];if(o.tryLoc===t){var i=o.completion;if("throw"===i.type){var a=i.arg;R(o)}return a}}throw new Error("illegal catch attempt")},delegateYield:function(t,e,i){return this.delegate={iterator:E(t),resultName:e,nextLoc:i},"next"===this.method&&(this.arg=o),m}}}function q(t,e,o,i){var a=e&&e.prototype instanceof j?e:j,n=Object.create(a.prototype),s=new C(i||[]);return n._invoke=function(t,e,o){var i=h;return function(a,n){if(i===g)throw new Error("Generator is already running");if(i===d){if("throw"===a)throw n;return P()}for(o.method=a,o.arg=n;;){var s=o.delegate;if(s){var r=S(s,o);if(r){if(r===m)continue;return r}}if("next"===o.method)o.sent=o._sent=o.arg;else if("throw"===o.method){if(i===h)throw i=d,o.arg;o.dispatchException(o.arg)}else"return"===o.method&&o.abrupt("return",o.arg);i=g;var c=w(t,e,o);if("normal"===c.type){if(i=o.done?d:p,c.arg===m)continue;return{value:c.arg,done:o.done}}"throw"===c.type&&(i=d,o.method="throw",o.arg=c.arg)}}}(t,o,s),n}function w(t,e,o){try{return{type:"normal",arg:t.call(e,o)}}catch(i){return{type:"throw",arg:i}}}function j(){}function x(){}function k(){}function T(t){["next","throw","return"].forEach(function(e){t[e]=function(t){return this._invoke(e,t)}})}function I(t){var e;this._invoke=function(o,i){function n(){return new Promise(function(e,n){!function e(o,i,n,s){var r=w(t[o],t,i);if("throw"!==r.type){var c=r.arg,l=c.value;return l&&"object"==typeof l&&a.call(l,"__await")?Promise.resolve(l.__await).then(function(t){e("next",t,n,s)},function(t){e("throw",t,n,s)}):Promise.resolve(l).then(function(t){c.value=t,n(c)},function(t){return e("throw",t,n,s)})}s(r.arg)}(o,i,e,n)})}return e=e?e.then(n,n):n()}}function S(t,e){var i=t.iterator[e.method];if(i===o){if(e.delegate=null,"throw"===e.method){if(t.iterator.return&&(e.method="return",e.arg=o,S(t,e),"throw"===e.method))return m;e.method="throw",e.arg=new TypeError("The iterator does not provide a 'throw' method")}return m}var a=w(i,t.iterator,e.arg);if("throw"===a.type)return e.method="throw",e.arg=a.arg,e.delegate=null,m;var n=a.arg;return n?n.done?(e[t.resultName]=n.value,e.next=t.nextLoc,"return"!==e.method&&(e.method="next",e.arg=o),e.delegate=null,m):n:(e.method="throw",e.arg=new TypeError("iterator result is not an object"),e.delegate=null,m)}function A(t){var e={tryLoc:t[0]};1 in t&&(e.catchLoc=t[1]),2 in t&&(e.finallyLoc=t[2],e.afterLoc=t[3]),this.tryEntries.push(e)}function R(t){var e=t.completion||{};e.type="normal",delete e.arg,t.completion=e}function C(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(A,this),this.reset(!0)}function E(t){if(t){var e=t[s];if(e)return e.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var i=-1,n=function e(){for(;++i<t.length;)if(a.call(t,i))return e.value=t[i],e.done=!1,e;return e.value=o,e.done=!0,e};return n.next=n}}return{next:P}}function P(){return{value:o,done:!0}}}(function(){return this||"object"==typeof self&&self}()||Function("return this")())},mI7R:function(t,e,o){"use strict";o.r(e);var i=o("ln6h"),a=o.n(i),n=o("O40h"),s=o("dfwq"),r=o("0iUn"),c=o("sLSF"),l=o("MI3g"),u=o("a7VT"),h=o("Tit0"),p=o("q1tI"),g=o.n(p),d=o("IujW"),m=o.n(d),f=(o("6T/A"),o("e3jU")),b=(o("YFqc"),o("yYy+")),v=function(t){function e(){return Object(r.default)(this,e),Object(l.default)(this,Object(u.default)(e).apply(this,arguments))}return Object(h.default)(e,t),Object(c.default)(e,[{key:"render",value:function(){var t=o("o0EK")("./".concat(this.props.id,".json"));return g.a.createElement("div",null,g.a.createElement("title",null,t.title),g.a.createElement("div",{className:"ui stackable grid",style:{marginTop:"20px"}},g.a.createElement("div",{className:"one wide column"}),g.a.createElement("div",{id:"project",className:"ten wide column centered"},g.a.createElement("div",{id:"breadcrumb",className:"ui breadcrumb"},g.a.createElement("a",{className:"section",href:"/"},"Ryo Suzuki"),g.a.createElement("i",{className:"right angle icon divider"}),g.a.createElement("a",{className:"active section"},t.name)),g.a.createElement("h1",{className:"title"},t.title),g.a.createElement("p",{className:"meta"},t.authors.map(function(t){return t.includes("Ryo Suzuki")?g.a.createElement("strong",null,t):g.a.createElement("span",null,t)}).reduce(function(t,e){return[t,", ",e]}),"   ",g.a.createElement("span",{style:{color:"gray"}},t.note)),g.a.createElement("p",{className:"meta"},g.a.createElement("a",{href:t.conference.url,target:"_blank"},g.a.createElement("b",null,t.conference.fullname))),g.a.createElement("p",{className:"meta"},"Links:  ",g.a.createElement("a",{className:"ui inverted secondary button",href:"/publications/"+t.pdf,target:"_blank",style:{marginRight:"5px",display:t.pdf?"inline":"none"}},g.a.createElement("b",null,g.a.createElement("i",{className:"far fa-file-pdf"}),"   PDF")),g.a.createElement("a",{className:"ui inverted secondary button",href:"/publications/"+t.poster,target:"_blank",style:{marginRight:"5px",display:t.poster?"inline":"none"}},g.a.createElement("b",null,g.a.createElement("i",{className:"far fa-file-pdf"}),"   Poster")),g.a.createElement("a",{className:"ui inverted secondary button",href:"/publications/"+t.slide,target:"_blank",style:{marginRight:"5px",display:t.slide?"inline":"none"}},g.a.createElement("b",null,g.a.createElement("i",{className:"far fa-file-pdf"}),"   Slide")),g.a.createElement("a",{className:"ui inverted secondary button",href:t.video,target:"_blank",style:{marginRight:"5px",display:t.video?"inline":"none"}},g.a.createElement("b",null,g.a.createElement("i",{className:"fas fa-video"}),"   Video")),g.a.createElement("a",{className:"ui inverted secondary button",href:t["short-video"],target:"_blank",style:{marginRight:"5px",display:t["short-video"]?"inline":"none"}},g.a.createElement("b",null,g.a.createElement("i",{className:"fas fa-video"}),"   30s Video")),g.a.createElement("a",{className:"ui inverted secondary button",href:t.github,target:"_blank",style:{marginRight:"5px",display:t.github?"inline":"none"}},g.a.createElement("b",null,g.a.createElement("i",{className:"fab fa-github"}),"   GitHub")),g.a.createElement("a",{className:"ui inverted secondary button",href:t["acm-dl"],target:"_blank",style:{marginRight:"5px",display:t["acm-dl"]?"inline":"none"}},g.a.createElement("b",null,g.a.createElement("i",{className:"fas fa-link"}),"   ACM DL")),g.a.createElement("a",{className:"ui inverted secondary button",href:t.ieee,target:"_blank",style:{marginRight:"5px",display:t.ieee?"inline":"none"}},g.a.createElement("b",null,g.a.createElement("i",{className:"fas fa-link"}),"   IEEE")),g.a.createElement("a",{className:"ui inverted secondary button",href:t.arxiv,target:"_blank",style:{marginRight:"5px",display:t.arxiv?"inline":"none"}},g.a.createElement("b",null,g.a.createElement("i",{className:"fas fa-link"}),"   arXiv")),g.a.createElement("a",{className:"ui inverted secondary button",href:t.talk,target:"_blank",style:{marginRight:"5px",display:t.talk?"inline":"none"}},g.a.createElement("b",null,g.a.createElement("i",{className:"fas fa-chalkboard-teacher"}),"   Talk"))),g.a.createElement("br",null),g.a.createElement("div",{className:"video-container",style:{display:t.embed?"block":"none"}},g.a.createElement("iframe",{className:"embed",width:"100%",height:"315",src:"".concat(t.embed,"?"),frameborder:"0",allow:"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture",allowFullScreen:!0,mozAllowFullScreen:!0,msAllowFullScreen:!0,oAllowFullScreen:!0,webkitAllowFullScreen:!0})),g.a.createElement(m.a,{source:t.bodyContent,escapeHtml:!1,linkTarget:"_blank"}),g.a.createElement("div",{className:"ui vertical segment"}),g.a.createElement("h1",null,"Publication"),g.a.createElement("div",{className:"ui placeholder segment"},g.a.createElement("p",null,t.authors.join(", ").replace(/, ([^,]*)$/,", and $1"),". ",t.year,". ",g.a.createElement("b",null,t.title),". ",g.a.createElement("i",null,t.booktitle),". ",t.publisher,", ",t.pages,".",g.a.createElement("br",null),"DOI: ",g.a.createElement("a",{href:t.doi,target:"_blank"},t.doi))),t.related&&g.a.createElement("div",{className:"ui placeholder segment"},g.a.createElement("p",null,t.related.authors.join(", ").replace(/, ([^,]*)$/,", and $1"),". ",t.related.year,". ",g.a.createElement("b",null,t.related.title),". ",g.a.createElement("i",null,t.related.booktitle),". ",t.related.publisher,", ",t.related.pages,".",g.a.createElement("br",null),"DOI: ",g.a.createElement("a",{href:t.related.doi,target:"_blank"},t.related.doi))),g.a.createElement("br",null),t.pageCount>0&&g.a.createElement("a",{className:"ui inverted secondary button",href:"/publications/".concat(t.pdf),target:"_blank"},g.a.createElement("b",null,g.a.createElement("i",{className:"far fa-file-pdf"}),"   Download PDF")),g.a.createElement("div",{className:"figures ui six column grid"},Object(s.default)(Array(t.pageCount)).map(function(e,o){var i=o+1<10?"0".concat(o+1):"".concat(o+1);t.slideCount>=100&&(i=o+1<10?"00".concat(o+1):o+1<100?"0".concat(o+1):"".concat(o+1));var a="/static/projects/".concat(t.id,"/paper-original/paper-").concat(i,".jpg"),n="/static/projects/".concat(t.id,"/paper/paper-").concat(i,".jpg");return g.a.createElement("a",{className:"paper column",key:o,href:a,"data-lightbox":"lightbox"},g.a.createElement("img",{src:n}))})),t.related&&g.a.createElement("div",null,g.a.createElement("a",{className:"ui inverted secondary button",href:"/publications/".concat(t.related.pdf),target:"_blank"},g.a.createElement("b",null,g.a.createElement("i",{className:"far fa-file-pdf"}),"   Download PDF")),g.a.createElement("div",{className:"figures ui six column grid"},Object(s.default)(Array(t.related.pageCount)).map(function(e,o){var i=o+1<10?"0".concat(o+1):"".concat(o+1),a="/static/projects/".concat(t.id,"/paper-original/paper-").concat(t.related.suffix,"-").concat(i,".jpg"),n="/static/projects/".concat(t.id,"/paper/paper-").concat(t.related.suffix,"-").concat(i,".jpg");return g.a.createElement("a",{className:"paper column",key:o,href:a,"data-lightbox":"lightbox"},g.a.createElement("img",{src:n}))}))),g.a.createElement("h1",null,"Slide"),t.slideCount>0&&g.a.createElement("a",{className:"ui inverted secondary button",href:"/publications/".concat(t.slide),target:"_blank"},g.a.createElement("b",null,g.a.createElement("i",{className:"far fa-file-pdf"}),"   Download Slide PDF")),0===t.slideCount&&g.a.createElement("p",null,"coming soon"),g.a.createElement("br",null),g.a.createElement("br",null),g.a.createElement("div",{className:"figures ui six column grid"},Object(s.default)(Array(t.slideCount)).map(function(e,o){var i=o+1<10?"0".concat(o+1):"".concat(o+1);t.slideCount>=100&&(i=o+1<10?"00".concat(o+1):o+1<100?"0".concat(o+1):"".concat(o+1));var a="/static/projects/".concat(t.id,"/slide-original/slide-").concat(i,".jpg"),n="/static/projects/".concat(t.id,"/slide/slide-").concat(i,".jpg");return g.a.createElement("a",{className:"slide column",key:o,href:a,"data-lightbox":"lightbox"},g.a.createElement("img",{src:n}))})),f.filter(function(e){return e.tag==t.id}).length>0&&g.a.createElement("div",null,g.a.createElement("h1",null,"Selected Press Coverage"),g.a.createElement("div",{className:"ui bulleted list"},f.filter(function(e){return e.tag===t.id}).map(function(t,e){return g.a.createElement("div",{className:"item",key:e},g.a.createElement("a",{href:t.url,target:"_blank"},g.a.createElement("b",null,t.media)," ",g.a.createElement("i",null,t.title)))})))),g.a.createElement("div",{className:"one wide column"})),g.a.createElement(b.default,null))}}],[{key:"getInitialProps",value:function(){var t=Object(n.default)(a.a.mark(function t(e){var o;return a.a.wrap(function(t){for(;;)switch(t.prev=t.next){case 0:return o=e.query.id,t.abrupt("return",{id:o});case 2:case"end":return t.stop()}},t)}));return function(e){return t.apply(this,arguments)}}()}]),e}(g.a.Component);e.default=v},mRot:function(t){t.exports={id:"refazer",name:"Refazer",description:"Learning Syntactic Program Transformations from Examples",title:"Learning Syntactic Program Transformations from Examples",authors:["Reudismam Rolim","Gustavo Soares","Loris D'Antoni","Oleksandr Polozov","Sumit Gulwani","Rohit Gheyi","Ryo Suzuki","Björn Hartmann"],yera:2017,booktitle:"In Proceedings of the 39th International Conference on Software Engineering (ICSE '17)",publisher:"IEEE Press, Piscataway, NJ, USA",pages:"404-415",conference:{name:"ICSE 2017",fullname:"The International Conference on Software Engineering (ICSE 2017)",url:"http://icse2017.gatech.edu/"},pdf:"icse-2017-refazer.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=3097417",arxiv:"https://arxiv.org/abs/1608.09000",pageCount:12,slideCount:0,bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"refazer.json",ext:".json",sourceBase:"refazer.md",sourceExt:".md"}},n3ko:function(t,e,o){var i=o("93I4");t.exports=function(t,e){if(!i(t)||t._t!==e)throw TypeError("Incompatible receiver, "+e+" required!");return t}},nOHt:function(t,e,o){"use strict";var i=o("KI45"),a=i(o("UXZV")),n=i(o("b3CU")),s=i(o("hfKm")),r=function(t){return t&&t.__esModule?t:{default:t}};Object.defineProperty(e,"__esModule",{value:!0});var c=r(o("q1tI")),l=r(o("JQMT"));e.Router=l.default;var u=o("9EOK"),h=o("4Vye"),p={router:null,readyCallbacks:[],ready:function(t){if(this.router)return t();"undefined"!=typeof window&&this.readyCallbacks.push(t)}},g=["pathname","route","query","asPath"],d=["components"],m=["push","replace","reload","back","prefetch","beforePopState"];function f(){if(!p.router){throw new Error('No router instance found.\nYou should only use "next/router" inside the client side of your app.\n')}return p.router}Object.defineProperty(p,"events",{get:function(){return l.default.events}}),d.concat(g).forEach(function(t){(0,s.default)(p,t,{get:function(){return f()[t]}})}),m.forEach(function(t){p[t]=function(){var e=f();return e[t].apply(e,arguments)}}),["routeChangeStart","beforeHistoryChange","routeChangeComplete","routeChangeError","hashChangeStart","hashChangeComplete"].forEach(function(t){p.ready(function(){l.default.events.on(t,function(){var e="on".concat(t.charAt(0).toUpperCase()).concat(t.substring(1)),o=p;if(o[e])try{o[e].apply(o,arguments)}catch(i){console.error("Error when running the Router event: ".concat(e)),console.error("".concat(i.message,"\n").concat(i.stack))}})})}),e.default=p;var b=o("0Bsm");e.withRouter=b.default,e.useRouter=function(){return c.default.useContext(u.RouterContext)},e.useRequest=function(){return c.default.useContext(h.RequestContext)},e.createRouter=function(){for(var t=arguments.length,e=new Array(t),o=0;o<t;o++)e[o]=arguments[o];return p.router=(0,n.default)(l.default,e),p.readyCallbacks.forEach(function(t){return t()}),p.readyCallbacks=[],p.router},e.makePublicRouterInstance=function(t){for(var e=t,o={},i=0;i<g.length;i++){var n=g[i];"object"!=typeof e[n]?o[n]=e[n]:o[n]=(0,a.default)({},e[n])}return o.events=l.default.events,d.forEach(function(t){(0,s.default)(o,t,{get:function(){return e[t]}})}),m.forEach(function(t){o[t]=function(){return e[t].apply(e,arguments)}}),o}},nWAr:function(t){t.exports={id:"atelier",name:"Atelier",description:"Repurposing Expert Crowdsourcing Tasks as Micro-internships",title:"Atelier: Repurposing Expert Crowdsourcing Tasks as Micro-internships",authors:["Ryo Suzuki","Niloufar Salehi","Michelle S. Lam","Juan C. Marroquin","Michael S. Bernstein"],year:2016,booktitle:"In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16)",publisher:"ACM, New York, NY, USA",pages:"2645-2656",conference:{name:"CHI 2016",fullname:"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2016)",url:"https://chi2016.acm.org/wp/"},pdf:"chi-2016-atelier.pdf",video:"https://www.youtube.com/watch?v=tBojZejtFQo",embed:"https://www.youtube.com/embed/tBojZejtFQo",slide:"chi-2016-atelier-slide.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=2858121",arxiv:"https://arxiv.org/abs/1602.06634",pageCount:12,slideCount:56,bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"atelier.json",ext:".json",sourceBase:"atelier.md",sourceExt:".md"}},nYho:function(t,e,o){(function(t,i){var a;!function(n){e&&e.nodeType,t&&t.nodeType;var s="object"==typeof i&&i;s.global!==s&&s.window!==s&&s.self;var r,c=2147483647,l=36,u=1,h=26,p=38,g=700,d=72,m=128,f="-",b=/^xn--/,v=/[^\x20-\x7E]/,y=/[\x2E\u3002\uFF0E\uFF61]/g,q={overflow:"Overflow: input needs wider integers to process","not-basic":"Illegal input >= 0x80 (not a basic code point)","invalid-input":"Invalid input"},w=l-u,j=Math.floor,x=String.fromCharCode;function k(t){throw RangeError(q[t])}function T(t,e){for(var o=t.length,i=[];o--;)i[o]=e(t[o]);return i}function I(t,e){var o=t.split("@"),i="";return o.length>1&&(i=o[0]+"@",t=o[1]),i+T((t=t.replace(y,".")).split("."),e).join(".")}function S(t){for(var e,o,i=[],a=0,n=t.length;a<n;)(e=t.charCodeAt(a++))>=55296&&e<=56319&&a<n?56320==(64512&(o=t.charCodeAt(a++)))?i.push(((1023&e)<<10)+(1023&o)+65536):(i.push(e),a--):i.push(e);return i}function A(t){return T(t,function(t){var e="";return t>65535&&(e+=x((t-=65536)>>>10&1023|55296),t=56320|1023&t),e+=x(t)}).join("")}function R(t,e){return t+22+75*(t<26)-((0!=e)<<5)}function C(t,e,o){var i=0;for(t=o?j(t/g):t>>1,t+=j(t/e);t>w*h>>1;i+=l)t=j(t/w);return j(i+(w+1)*t/(t+p))}function E(t){var e,o,i,a,n,s,r,p,g,b,v,y=[],q=t.length,w=0,x=m,T=d;for((o=t.lastIndexOf(f))<0&&(o=0),i=0;i<o;++i)t.charCodeAt(i)>=128&&k("not-basic"),y.push(t.charCodeAt(i));for(a=o>0?o+1:0;a<q;){for(n=w,s=1,r=l;a>=q&&k("invalid-input"),((p=(v=t.charCodeAt(a++))-48<10?v-22:v-65<26?v-65:v-97<26?v-97:l)>=l||p>j((c-w)/s))&&k("overflow"),w+=p*s,!(p<(g=r<=T?u:r>=T+h?h:r-T));r+=l)s>j(c/(b=l-g))&&k("overflow"),s*=b;T=C(w-n,e=y.length+1,0==n),j(w/e)>c-x&&k("overflow"),x+=j(w/e),w%=e,y.splice(w++,0,x)}return A(y)}function P(t){var e,o,i,a,n,s,r,p,g,b,v,y,q,w,T,I=[];for(y=(t=S(t)).length,e=m,o=0,n=d,s=0;s<y;++s)(v=t[s])<128&&I.push(x(v));for(i=a=I.length,a&&I.push(f);i<y;){for(r=c,s=0;s<y;++s)(v=t[s])>=e&&v<r&&(r=v);for(r-e>j((c-o)/(q=i+1))&&k("overflow"),o+=(r-e)*q,e=r,s=0;s<y;++s)if((v=t[s])<e&&++o>c&&k("overflow"),v==e){for(p=o,g=l;!(p<(b=g<=n?u:g>=n+h?h:g-n));g+=l)T=p-b,w=l-b,I.push(x(R(b+T%w,0))),p=j(T/w);I.push(x(R(p,0))),n=C(o,q,i==a),o=0,++i}++o,++e}return I.join("")}r={version:"1.3.2",ucs2:{decode:S,encode:A},decode:E,encode:P,toASCII:function(t){return I(t,function(t){return v.test(t)?"xn--"+P(t):t})},toUnicode:function(t){return I(t,function(t){return b.test(t)?E(t.slice(4).toLowerCase()):t})}},void 0===(a=function(){return r}.call(e,o,e,t))||(t.exports=a)}()}).call(this,o("YuTi")(t),o("yLpj"))},o0EK:function(t,e,o){var i={"./ar-and-robotics.json":"KZOl","./atelier.json":"nWAr","./augmented-math.json":"Qq35","./chameleon-control.json":"5wCx","./dynablock.json":"GbvX","./electro-voxel.json":"rYb4","./expandable-robots.json":"BTJg","./flux-marker.json":"CTYI","./hapticbots.json":"7kuZ","./holobots.json":"VECh","./lift-tiles.json":"3RXq","./mixed-initiative.json":"PSd4","./mixels.json":"t0+C","./morphio.json":"X0/d","./pep.json":"W/HP","./phd-thesis.json":"IMSK","./physica.json":"BIYM","./pufferbot.json":"vEdn","./reactile.json":"jEBx","./realitycanvas.json":"8v4N","./realitysketch.json":"iBc/","./realitytalk.json":"OsUb","./refazer.json":"mRot","./roomshift.json":"9gtZ","./selective-self-assembly.json":"Bauz","./shapebots.json":"lJku","./sketched-reality.json":"eW+b","./tabby.json":"ejaO","./teachable-reality.json":"CiV/","./trace-diff.json":"Jg5j"};function a(t){var e=n(t);return o(e)}function n(t){var e=i[t];if(!(e+1)){var o=new Error("Cannot find module '"+t+"'");throw o.code="MODULE_NOT_FOUND",o}return e}a.keys=function(){return Object.keys(i)},a.resolve=n,t.exports=a,a.id="o0EK"},o8NH:function(t,e,o){var i=o("Y7ZC");i(i.S+i.F,"Object",{assign:o("kwZ1")})},"oh+g":function(t,e,o){var i=o("WEpk"),a=i.JSON||(i.JSON={stringify:JSON.stringify});t.exports=function(t){return a.stringify.apply(a,arguments)}},oioR:function(t,e,o){var i=o("2GTP"),a=o("sNwI"),n=o("NwJ3"),s=o("5K7Z"),r=o("tEej"),c=o("fNZA"),l={},u={};(e=t.exports=function(t,e,o,h,p){var g,d,m,f,b=p?function(){return t}:c(t),v=i(o,h,e?2:1),y=0;if("function"!=typeof b)throw TypeError(t+" is not iterable!");if(n(b)){for(g=r(t.length);g>y;y++)if((f=e?v(s(d=t[y])[0],d[1]):v(t[y]))===l||f===u)return f}else for(m=b.call(t);!(d=m.next()).done;)if((f=a(m,v,d.value,e))===l||f===u)return f}).BREAK=l,e.RETURN=u},p0XB:function(t,e,o){t.exports=o("9BDd")},pLtp:function(t,e,o){t.exports=o("iq4v")},pbKT:function(t,e,o){t.exports=o("qijr")},q6LJ:function(t,e,o){var i=o("5T2Y"),a=o("QXhf").set,n=i.MutationObserver||i.WebKitMutationObserver,s=i.process,r=i.Promise,c="process"==o("a0xu")(s);t.exports=function(){var t,e,o,l=function(){var i,a;for(c&&(i=s.domain)&&i.exit();t;){a=t.fn,t=t.next;try{a()}catch(n){throw t?o():e=void 0,n}}e=void 0,i&&i.enter()};if(c)o=function(){s.nextTick(l)};else if(!n||i.navigator&&i.navigator.standalone)if(r&&r.resolve){var u=r.resolve(void 0);o=function(){u.then(l)}}else o=function(){a.call(i,l)};else{var h=!0,p=document.createTextNode("");new n(l).observe(p,{characterData:!0}),o=function(){p.data=h=!h}}return function(i){var a={fn:i,next:void 0};e&&(e.next=a),t||(t=a,o()),e=a}}},qijr:function(t,e,o){o("czwh"),t.exports=o("WEpk").Reflect.construct},rYb4:function(t){t.exports={id:"electro-voxel",name:"ElectroVoxel",description:"Electromagnetically Actuated Pivoting for Scalable Modular Self-Reconfigurable Robots",title:"ElectroVoxel: Electromagnetically Actuated Pivoting for Scalable Modular Self-Reconfigurable Robots",authors:["Martin Nisser","Leon Cheng","Yashaswini Makaram","Ryo Suzuki","Stefanie Mueller"],year:2022,conference:{name:"ICRA 2022",fullname:"International Conference on Robotics and Automation (ICRA 2022)",url:"https://www.icra2022.org/"},external:"https://hcie.csail.mit.edu/research/Electrovoxel/electrovoxel.html",bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"electro-voxel.json",ext:".json",sourceBase:"electro-voxel.md",sourceExt:".md"}},raTm:function(t,e,o){"use strict";var i=o("5T2Y"),a=o("Y7ZC"),n=o("6/1s"),s=o("KUxP"),r=o("NegM"),c=o("XJU/"),l=o("oioR"),u=o("EXMj"),h=o("93I4"),p=o("RfKB"),g=o("2faE").f,d=o("V7Et")(0),m=o("jmDH");t.exports=function(t,e,o,f,b,v){var y=i[t],q=y,w=b?"set":"add",j=q&&q.prototype,x={};return m&&"function"==typeof q&&(v||j.forEach&&!s(function(){(new q).entries().next()}))?(q=e(function(e,o){u(e,q,t,"_c"),e._c=new y,null!=o&&l(o,b,e[w],e)}),d("add,clear,delete,forEach,get,has,set,keys,values,entries,toJSON".split(","),function(t){var e="add"==t||"set"==t;t in j&&(!v||"clear"!=t)&&r(q.prototype,t,function(o,i){if(u(this,q,t),!e&&v&&!h(o))return"get"==t&&void 0;var a=this._c[t](0===o?0:o,i);return e?this:a})}),v||g(q.prototype,"size",{get:function(){return this._c.size}})):(q=f.getConstructor(e,t,b,w),c(q.prototype,o),n.NEED=!0),p(q,t),x[t]=q,a(a.G+a.W+a.F,x),v||f.setStrong(q,t,b),q}},s4NR:function(t,e,o){"use strict";e.decode=e.parse=o("kd2E"),e.encode=e.stringify=o("4JlD")},sNwI:function(t,e,o){var i=o("5K7Z");t.exports=function(t,e,o,a){try{return a?e(i(o)[0],o[1]):e(o)}catch(s){var n=t.return;throw void 0!==n&&i(n.call(t)),s}}},"t0+C":function(t){t.exports={id:"mixels",name:"Mixels",description:"Fabricating Interfaces using Programmable Magnetic Pixels",title:"Mixels: Fabricating Interfaces using Programmable Magnetic Pixels",authors:["Martin Nisser","Yashaswini Makaram","Lucian Covarrubias","Amadou Yaye Bah","Faraz Faruqi","Ryo Suzuki","Stefanie Mueller"],year:2022,booktitle:"In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology (UIST '22)",publisher:"ACM, New York, NY, USA",conference:{name:"UIST 2022",fullname:"The ACM Symposium on User Interface Software and Technology (UIST 2022)",url:"http://uist.acm.org/uist2022"},external:"https://hcie.csail.mit.edu/research/mixels/mixels.html",doi:"https://doi.org/10.1145/3526114.3558654","acm-dl":"https://dl.acm.org/doi/10.1145/3526114.3558654",bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"mixels.json",ext:".json",sourceBase:"mixels.md",sourceExt:".md"}},ttDY:function(t,e,o){t.exports=o("+iuc")},u938:function(t,e,o){var i=function(){return this||"object"==typeof self&&self}()||Function("return this")(),a=i.regeneratorRuntime&&Object.getOwnPropertyNames(i).indexOf("regeneratorRuntime")>=0,n=a&&i.regeneratorRuntime;if(i.regeneratorRuntime=void 0,t.exports=o("ls82"),a)i.regeneratorRuntime=n;else try{delete i.regeneratorRuntime}catch(s){i.regeneratorRuntime=void 0}},v6xn:function(t,e,o){var i=o("C2SN");t.exports=function(t,e){return new(i(t))(e)}},vBP9:function(t,e,o){var i=o("5T2Y").navigator;t.exports=i&&i.userAgent||""},vEdn:function(t){t.exports={id:"pufferbot",name:"PufferBot",description:"Actuated Expandable Structures for Aerial Robots",title:"PufferBot: Actuated Expandable Structures for Aerial Robots",authors:["Hooman Hedayati","Ryo Suzuki","Daniel Leithinger","Daniel Szafir"],year:2020,booktitle:"In Proceedings of the 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS '20)",publisher:"IEEE, New York, NY, USA",pages:"1-6",conference:{name:"IROS 2020",fullname:"The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2020)",url:"https://www.iros2020.org/"},pdf:"iros-2020-pufferbot.pdf",video:"https://www.youtube.com/watch?v=XtPepCxWcBg",embed:"https://www.youtube.com/embed/XtPepCxWcBg",arxiv:"https://arxiv.org/abs/2008.07615",pageCount:6,slideCount:0,bodyContent:'# Abstract\n\nWe present PufferBot, an aerial robot with an expandable structure that may expand to protect a drone’s propellers when the robot is close to obstacles or collocated humans. PufferBot is made of a custom 3D-printed expandable scissor structure, which utilizes a one degree of freedom actuator with rack and pinion mechanism. We propose four designs for the expandable structure, each with unique characterizations for different situations. Finally, we present three motivating scenarios in which PufferBot may extend the utility of existing static propeller guard structures.\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/pufferbot/video/top.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-1-1.jpg" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-1-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-1-2.jpg" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-1-2.jpg" /></a>\n  </div>\n</div>\n\n\n# Introduction\n\nAerial robots are increasingly used in a wide variety of applications, such as search and rescue, journalism, structural inspection, and environmental data collection. When used indoors, aerial robots have traditionally been isolated from humans through cages or operated in an entirely separated space, but they are increasingly entering into environments with collocated humans (e.g., construction sites). In such situations, there is an increasing demand to reduce the danger and unpredictability of robots, as well as increase safety for nearby people.\n\nTo address issues surrounding propeller damage/safety, many aerial robots use propeller guards—fixed structures that may prevent the propellers from hitting an obstacle or person in the event of a collision. However, many guards do not fully cover the robot’s propellers (for instance, only providing cover for the horizontal size of a propeller), leaving other parts of the propellers (e.g., the top) exposed and vulnerable to damage. On the other hand, guards that do provide full coverage surrounding the propellers, such as in the Zero Zero Robotics HoverCam and the Flyability GimBall, significantly increase the size and rigidity of the robot, potentially making the robot less maneuverable. This can pose a problem if the robot operates in narrow spaces (e.g., search and rescue in a collapsed building), as the robot cannot navigate tight spaces and can become stuck between obstacles.\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/pufferbot/video/collision-1.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-7-1.jpg" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-7-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-7-2.jpg" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-7-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-7-3.jpg" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-7-3.jpg" /></a>\n  </div>\n</div>\n\n\n# PufferBot\nIn this paper, we introduce PufferBot, the concept of an ex- pandable aerial robot that can dynamically change its shape to reduce damage in the event of collisions with collocated humans and/or the environment. PufferBot consists of an aerial robot with a mounted expandable structure that can be actuated to expand in order to reduce the collision damage or create an enlarged buffer zone surrounding the robot. The PufferBot concept is inspired by both natural designs (e.g., pufferfish) and mechanical systems (e.g., vehicle airbags). When in danger, a pufferfish (Tetraodontidae) inflates its body by taking water or air into portions of its digestive tract to increase its size. Similarly, vehicle airbag systems also inflate to protect humans when crashes occur.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-2.png" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-2.png" /></a>\n  </div>\n</div>\n\nBy taking an inspiration from such metaphors, we propose an expandable structure for an aerial robot that may reduce the risk of crashing and protect the robot’s propellers when the robot is in danger of falling on the ground, crashing into an object, or navigating cluttered spaces. One advan- tage of our system is that the expandable structure can dynamically change its shape in order to reduce the overall size in the non-expanded state, making it easier for the robot to navigate in narrow spaces and avoid unnecessary contact with the surrounding environment. In addition, such expandable structures may open up a new design element for future work examining user interaction.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/pufferbot/video/human-2.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-10-1.jpg" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-10-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-10-2.jpg" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-10-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-10-3.jpg" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-10-3.jpg" /></a>\n  </div>\n</div>\n\n\n\n# Design and Implementation\n\n\n## Aerial Robot\nIn this work, We used a DJI Flame Wheel F450 frame for our aerial robot. The base frame weight is 282 g. After mounting additional components (motors, battery, flight controller, etc.), the weight of the aerial robot accumulates to 1.2 Kg. Based on the specification document, the robot is capable of lifting up to 1.6 Kg of payload. which is enough for our expandable structure (in our setting, the weight of expandable structure and the actuator combined is 1.6 Kg). The diagonal length of the robot (motor to motor) is 45 cm. We used 4.5 inch propellers (11.43 cm), which make the total length of the aerial robot 70cm. We used a 4S Lithium-ion Polymer (LiPo) battery as the power source, which gives the robot a flight time of approximately 18 minutes. We built a plate on top of the aerial robot which gives us enough surface to mount and secure the expandable structure and actuator. The plate also allows us to avoid direct contact with the inertial measurement unit (IMU) and onboard sensors in the flight controller.\n\n\n## Expandable Structure\nThe expandable structure is made of three parts: (1) actuator joints, (2) regular joints, and (3) scissor units. 74 pieces were used in total to make the expandable structure. Revolvable press fit joints are used for secure but rotatable connections\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-3-1.png" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-3-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-3-3.png" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-3-3.png" /></a>\n    <a href="/static/projects/pufferbot/figure-3-2.png" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-3-2.png" /></a>\n  </div>\n</div>\n\n\n## Actuation\n\nWe actuate the expandable structure with a one degree of freedom actuation mechanism based on rack and pinion. The pinion gear located in the center rotates the four individual rack planes at the same time, so that the actuated racks can evenly apply the expansion force in four different directions at the same rate. The actuator joint attached to the end of the rack can expand and collapse the expandable structure by pushing and pulling the connected points. All the actuator parts are 3D printed with PLA. We laser cut the racks with 3mm plywood. We used plywood after testing with different materials (e.g., acrylic) and found that plywood was the most robust in terms of holding its shape against bending forces over time. We used a FeeTech FS5103R as a servo motor, controlled by a Wemos D1 mini ESP8266 micro-controller.\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-4-1.jpg" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-4-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-4-2.jpg" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-4-2.jpg" /></a>\n  </div>\n</div>\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-5.jpg" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-5.jpg" /></a>\n  </div>\n</div>\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/pufferbot/video/expand.mp4" type="video/mp4"></source>\n</video>\n\n\n\n## Control\n\nThere are two components that need to be controlled in the PufferBot: the aerial robot and the expandable structure actuator. Both can be controlled autonomously by a central computer or manually by a teleoperator. To implement au- tonomous control, we developed a linear PID controller that controls the position and altitude of the aerial robot. As the aerial robot we used lacks on-board sensing capabilities suf- ficient for accurate localization, our PID controller currently relies on a Vicon motion capture system with 200Hz motion tracking cameras embedded in the environment to track the physical robot. There is a trajectory planner built on top of the PID controller so the robot can traverse the space as planned. The local computer that runs the Vicon system also continuously detects nearby obstacles, and based on this information, it wirelessly communicates with the actuator’s microcontroller to programmatically control the size of the expandable structure. In the manual mode, a teleoperator is in charge of controlling the robot as well as the expandable structure.\n\n## Performance\n\nThe Pufferfish weights 600 grams and can expand or collapse in 6 seconds (we tested the expanding structure 50 times and the number reported is the mean of the trials). PufferBot can handle 6-9 N of force: 6 N against the parts furthest from the actuation racks and up to 9 N applied to the links directly connected to the racks.\n\n\n# Application Scenarios\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-6.png" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-6.png" /></a>\n  </div>\n</div>\n\n## Protecting Humans / Human-Robot Communication\n\nAerial robots and humans are increasingly occupying shared spaces, whether through collaboration in the work- place or users partaking in leisure activities (e.g., a hobbyist piloting an aerial robot in their neighbourhood). While human safety is critical in these situations, developing autonomous/semi-autonomous systems capable of mitigating all risk of collisions remains an open problem, with some risks arising from humans themselves (e.g., a wandering human who is not paying attention or one who is attempting to test the robot’s behaviors might incite a collision with the robot). In these scenarios, PufferBot may reduce the risk of human injury in contacting the robot’s spinning propellers, disperse the force of the robot during a collision over a wider surface area, and provide a compliance mechanism that helps mitigate impact force.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/pufferbot/video/human.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-11-1.jpg" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-11-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-11-2.jpg" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-11-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-11-3.jpg" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-11-3.jpg" /></a>\n  </div>\n</div>\n\n\n## Protecting Drones\n\nIn recent years, there has been an increasing trend in utilizing aerial robots for inspecting bridges, powerlines, pipelines, and other infrastructure elements. For these tasks, aerial robots must operate close to the target of interest, increasing the chance of the robot hitting obstacles due to operator error, loss of power, and/or unexpected elements of weather like gusts of wind. In these situations, PufferBot may protect the robot from dangerous objects in the environment. In the worst case, PufferBot may reduce damage to the robot when in free-fall by expanding to leverage the scissor structure, which acts like a spring\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/pufferbot/video/crash.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-9-1.jpg" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-9-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-9-2.jpg" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-9-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-9-3.jpg" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-9-3.jpg" /></a>\n  </div>\n</div>\n\n\n## Sensing and Navigating Complex Environments\n\nWhen drones navigate in complex indoor environments, they usually rely on some form of perception (e.g., SLAM) to avoid colliding with walls and obstacles. However, many conditions may impair perception algorithms, including smoke, glare, dirty camera lenses, etc. We propose to use the expanding scissor structure as a guiding mechanism in collision-tolerant navigation, similar to the whiskers of a cat, the use of white canes by people who are blind or visually impaired, or the bumper bar of wheeled robots like Roomba. When the drone is in a complex environment, it may expand in order to locate obstacles by bumping into them. We can further mount sensors into the structure to enhance this navigation and even provide haptic feedback to a teleoperator to indicate when the presence of nearby obstacles.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/pufferbot/video/collision-2.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-8-1.jpg" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-8-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-8-2.jpg" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-8-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/pufferbot/figure-8-3.jpg" data-lightbox="lightbox"><img src="/static/projects/pufferbot/figure-8-3.jpg" /></a>\n  </div>\n</div>',bodyHtml:"<h1>Abstract</h1>\n<p>We present PufferBot, an aerial robot with an expandable structure that may expand to protect a drone’s propellers when the robot is close to obstacles or collocated humans. PufferBot is made of a custom 3D-printed expandable scissor structure, which utilizes a one degree of freedom actuator with rack and pinion mechanism. We propose four designs for the expandable structure, each with unique characterizations for different situations. Finally, we present three motivating scenarios in which PufferBot may extend the utility of existing static propeller guard structures.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/pufferbot/video/top.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-1-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-1-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-1-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-1-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>Introduction</h1>\n<p>Aerial robots are increasingly used in a wide variety of applications, such as search and rescue, journalism, structural inspection, and environmental data collection. When used indoors, aerial robots have traditionally been isolated from humans through cages or operated in an entirely separated space, but they are increasingly entering into environments with collocated humans (e.g., construction sites). In such situations, there is an increasing demand to reduce the danger and unpredictability of robots, as well as increase safety for nearby people.</p>\n<p>To address issues surrounding propeller damage/safety, many aerial robots use propeller guards—fixed structures that may prevent the propellers from hitting an obstacle or person in the event of a collision. However, many guards do not fully cover the robot’s propellers (for instance, only providing cover for the horizontal size of a propeller), leaving other parts of the propellers (e.g., the top) exposed and vulnerable to damage. On the other hand, guards that do provide full coverage surrounding the propellers, such as in the Zero Zero Robotics HoverCam and the Flyability GimBall, significantly increase the size and rigidity of the robot, potentially making the robot less maneuverable. This can pose a problem if the robot operates in narrow spaces (e.g., search and rescue in a collapsed building), as the robot cannot navigate tight spaces and can become stuck between obstacles.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/pufferbot/video/collision-1.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-7-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-7-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-7-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-7-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-7-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-7-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>PufferBot</h1>\n<p>In this paper, we introduce PufferBot, the concept of an ex- pandable aerial robot that can dynamically change its shape to reduce damage in the event of collisions with collocated humans and/or the environment. PufferBot consists of an aerial robot with a mounted expandable structure that can be actuated to expand in order to reduce the collision damage or create an enlarged buffer zone surrounding the robot. The PufferBot concept is inspired by both natural designs (e.g., pufferfish) and mechanical systems (e.g., vehicle airbags). When in danger, a pufferfish (Tetraodontidae) inflates its body by taking water or air into portions of its digestive tract to increase its size. Similarly, vehicle airbag systems also inflate to protect humans when crashes occur.</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>By taking an inspiration from such metaphors, we propose an expandable structure for an aerial robot that may reduce the risk of crashing and protect the robot’s propellers when the robot is in danger of falling on the ground, crashing into an object, or navigating cluttered spaces. One advan- tage of our system is that the expandable structure can dynamically change its shape in order to reduce the overall size in the non-expanded state, making it easier for the robot to navigate in narrow spaces and avoid unnecessary contact with the surrounding environment. In addition, such expandable structures may open up a new design element for future work examining user interaction.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/pufferbot/video/human-2.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-10-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-10-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-10-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-10-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-10-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-10-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h1>Design and Implementation</h1>\n<h2>Aerial Robot</h2>\n<p>In this work, We used a DJI Flame Wheel F450 frame for our aerial robot. The base frame weight is 282 g. After mounting additional components (motors, battery, flight controller, etc.), the weight of the aerial robot accumulates to 1.2 Kg. Based on the specification document, the robot is capable of lifting up to 1.6 Kg of payload. which is enough for our expandable structure (in our setting, the weight of expandable structure and the actuator combined is 1.6 Kg). The diagonal length of the robot (motor to motor) is 45 cm. We used 4.5 inch propellers (11.43 cm), which make the total length of the aerial robot 70cm. We used a 4S Lithium-ion Polymer (LiPo) battery as the power source, which gives the robot a flight time of approximately 18 minutes. We built a plate on top of the aerial robot which gives us enough surface to mount and secure the expandable structure and actuator. The plate also allows us to avoid direct contact with the inertial measurement unit (IMU) and onboard sensors in the flight controller.</p>\n<h2>Expandable Structure</h2>\n<p>The expandable structure is made of three parts: (1) actuator joints, (2) regular joints, and (3) scissor units. 74 pieces were used in total to make the expandable structure. Revolvable press fit joints are used for secure but rotatable connections</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-3-1.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-3-1.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-3-3.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-3-3.png&quot; /&gt;&lt;/a&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-3-2.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-3-2.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h2>Actuation</h2>\n<p>We actuate the expandable structure with a one degree of freedom actuation mechanism based on rack and pinion. The pinion gear located in the center rotates the four individual rack planes at the same time, so that the actuated racks can evenly apply the expansion force in four different directions at the same rate. The actuator joint attached to the end of the rack can expand and collapse the expandable structure by pushing and pulling the connected points. All the actuator parts are 3D printed with PLA. We laser cut the racks with 3mm plywood. We used plywood after testing with different materials (e.g., acrylic) and found that plywood was the most robust in terms of holding its shape against bending forces over time. We used a FeeTech FS5103R as a servo motor, controlled by a Wemos D1 mini ESP8266 micro-controller.</p>\n<p>&lt;div class=&quot;figures ui two column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-4-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-4-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-4-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-4-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-5.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-5.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/pufferbot/video/expand.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<h2>Control</h2>\n<p>There are two components that need to be controlled in the PufferBot: the aerial robot and the expandable structure actuator. Both can be controlled autonomously by a central computer or manually by a teleoperator. To implement au- tonomous control, we developed a linear PID controller that controls the position and altitude of the aerial robot. As the aerial robot we used lacks on-board sensing capabilities suf- ficient for accurate localization, our PID controller currently relies on a Vicon motion capture system with 200Hz motion tracking cameras embedded in the environment to track the physical robot. There is a trajectory planner built on top of the PID controller so the robot can traverse the space as planned. The local computer that runs the Vicon system also continuously detects nearby obstacles, and based on this information, it wirelessly communicates with the actuator’s microcontroller to programmatically control the size of the expandable structure. In the manual mode, a teleoperator is in charge of controlling the robot as well as the expandable structure.</p>\n<h2>Performance</h2>\n<p>The Pufferfish weights 600 grams and can expand or collapse in 6 seconds (we tested the expanding structure 50 times and the number reported is the mean of the trials). PufferBot can handle 6-9 N of force: 6 N against the parts furthest from the actuation racks and up to 9 N applied to the links directly connected to the racks.</p>\n<h1>Application Scenarios</h1>\n<p>&lt;div class=&quot;figures ui one column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-6.png&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-6.png&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h2>Protecting Humans / Human-Robot Communication</h2>\n<p>Aerial robots and humans are increasingly occupying shared spaces, whether through collaboration in the work- place or users partaking in leisure activities (e.g., a hobbyist piloting an aerial robot in their neighbourhood). While human safety is critical in these situations, developing autonomous/semi-autonomous systems capable of mitigating all risk of collisions remains an open problem, with some risks arising from humans themselves (e.g., a wandering human who is not paying attention or one who is attempting to test the robot’s behaviors might incite a collision with the robot). In these scenarios, PufferBot may reduce the risk of human injury in contacting the robot’s spinning propellers, disperse the force of the robot during a collision over a wider surface area, and provide a compliance mechanism that helps mitigate impact force.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/pufferbot/video/human.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-11-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-11-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-11-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-11-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-11-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-11-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h2>Protecting Drones</h2>\n<p>In recent years, there has been an increasing trend in utilizing aerial robots for inspecting bridges, powerlines, pipelines, and other infrastructure elements. For these tasks, aerial robots must operate close to the target of interest, increasing the chance of the robot hitting obstacles due to operator error, loss of power, and/or unexpected elements of weather like gusts of wind. In these situations, PufferBot may protect the robot from dangerous objects in the environment. In the worst case, PufferBot may reduce damage to the robot when in free-fall by expanding to leverage the scissor structure, which acts like a spring</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/pufferbot/video/crash.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-9-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-9-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-9-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-9-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-9-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-9-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n<h2>Sensing and Navigating Complex Environments</h2>\n<p>When drones navigate in complex indoor environments, they usually rely on some form of perception (e.g., SLAM) to avoid colliding with walls and obstacles. However, many conditions may impair perception algorithms, including smoke, glare, dirty camera lenses, etc. We propose to use the expanding scissor structure as a guiding mechanism in collision-tolerant navigation, similar to the whiskers of a cat, the use of white canes by people who are blind or visually impaired, or the bumper bar of wheeled robots like Roomba. When the drone is in a complex environment, it may expand in order to locate obstacles by bumping into them. We can further mount sensors into the structure to enhance this navigation and even provide haptic feedback to a teleoperator to indicate when the presence of nearby obstacles.</p>\n<p>&lt;video preload=&quot;metadata&quot; autoPlay loop muted playsInline webkit-playsinline=&quot;&quot;&gt;\n&lt;source src=&quot;/static/projects/pufferbot/video/collision-2.mp4&quot; type=&quot;video/mp4&quot;&gt;&lt;/source&gt;\n&lt;/video&gt;</p>\n<p>&lt;div class=&quot;figures ui three column grid&quot;&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-8-1.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-8-1.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-8-2.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-8-2.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class=&quot;figure column&quot;&gt;\n&lt;a href=&quot;/static/projects/pufferbot/figure-8-3.jpg&quot; data-lightbox=&quot;lightbox&quot;&gt;&lt;img src=&quot;/static/projects/pufferbot/figure-8-3.jpg&quot; /&gt;&lt;/a&gt;\n&lt;/div&gt;\n&lt;/div&gt;</p>\n",dir:"content/output/projects",base:"pufferbot.json",ext:".json",sourceBase:"pufferbot.md",sourceExt:".md"}},vjea:function(t,e,o){var i=o("TRZx");function a(e,o){return t.exports=a=i||function(t,e){return t.__proto__=e,t},a(e,o)}t.exports=a},wYmx:function(t,e,o){"use strict";var i=o("eaoh"),a=o("93I4"),n=o("MCSJ"),s=[].slice,r={};t.exports=Function.bind||function(t){var e=i(this),o=s.call(arguments,1),c=function(){var i=o.concat(s.call(arguments));return this instanceof c?function(t,e,o){if(!(e in r)){for(var i=[],a=0;a<e;a++)i[a]="a["+a+"]";r[e]=Function("F,a","return new F("+i.join(",")+")")}return r[e](t,o)}(e,i.length,i):n(e,i,t)};return a(e.prototype)&&(c.prototype=e.prototype),c}},xvv9:function(t,e,o){o("cHUd")("Set")},yLpj:function(t,e){var o;o=function(){return this}();try{o=o||new Function("return this")()}catch(i){"object"==typeof window&&(o=window)}t.exports=o},yLu3:function(t,e,o){t.exports=o("VKFn")},"yYy+":function(t,e,o){"use strict";o.r(e);var i=o("0iUn"),a=o("sLSF"),n=o("MI3g"),s=o("a7VT"),r=o("Tit0"),c=o("q1tI"),l=o.n(c),u=(o("IujW"),function(t){function e(){return Object(i.default)(this,e),Object(n.default)(this,Object(s.default)(e).apply(this,arguments))}return Object(r.default)(e,t),Object(a.default)(e,[{key:"render",value:function(){return l.a.createElement("footer",null,l.a.createElement("div",{class:"ui center aligned container"},l.a.createElement("div",{class:"ui section divider"}),l.a.createElement("a",{href:"/"},l.a.createElement("img",{style:{maxWidth:"62px",margin:"30px auto"},src:"/static/images/profile.png",className:"ui centered circular image"}),l.a.createElement("div",{className:"content"},l.a.createElement("h1",{style:{fontSize:"1.5rem"}},"Ryo Suzuki"),l.a.createElement("div",{className:"sub header"},"Assistant Professor at University of Calgary")))),l.a.createElement("div",{class:"ui horizontal small divided link list"},l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://scholar.google.com/citations?user=klWjaQIAAAAJ",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fas fa-graduation-cap fa-fw"}),"Google Scholar")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"/cv.pdf",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"far fa-file fa-fw"}),"Resume/CV")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"mailto:ryo.suzuki@ucalgary.ca",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"far fa-envelope fa-fw"}),"ryo.suzuki@ucalgary.ca")),l.a.createElement("br",null),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://www.facebook.com/ryosuzk",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fab fa-facebook-square fa-fw"}),"ryosuzk")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://twitter.com/ryosuzk",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fab fa-twitter fa-fw"}),"ryosuzk")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://github.com/ryosuzuki",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fab fa-github-alt fa-fw"}),"ryosuzuki")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://www.linkedin.com/in/ryosuzuki/",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fab fa-linkedin-in fa-fw"}),"ryosuzuki"))))}}]),e}(l.a.Component));e.default=u},zXhZ:function(t,e,o){var i=o("5K7Z"),a=o("93I4"),n=o("ZW5q");t.exports=function(t,e){if(i(t),a(e)&&e.constructor===t)return e;var o=n.f(t);return(0,o.resolve)(e),o.promise}}},[["b6cx","5d41","9da1"]]]);