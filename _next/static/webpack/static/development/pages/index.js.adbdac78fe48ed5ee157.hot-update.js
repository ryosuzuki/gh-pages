webpackHotUpdate("static/development/pages/index.js",{

/***/ "./content/output/projects/atelier.json":
/*!**********************************************!*\
  !*** ./content/output/projects/atelier.json ***!
  \**********************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, pages, conference, pdf, video, embed, slide, acm-dl, arxiv, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"atelier","name":"Atelier","description":"Repurposing Expert Crowdsourcing Tasks as Micro-internships","title":"Atelier: Repurposing Expert Crowdsourcing Tasks as Micro-internships","authors":["Ryo Suzuki","Niloufar Salehi","Michelle S. Lam","Juan C. Marroquin","Michael S. Bernstein"],"year":2016,"booktitle":"In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16)","publisher":"ACM, New York, NY, USA","pages":"2645-2656","conference":{"name":"CHI 2016","fullname":"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2016)","url":"https://chi2016.acm.org/wp/"},"pdf":"chi-2016-atelier.pdf","video":"https://www.youtube.com/watch?v=tBojZejtFQo","embed":"https://www.youtube.com/embed/tBojZejtFQo","slide":"chi-2016-atelier-slide.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=2858121","arxiv":"https://arxiv.org/abs/1602.06634","pageCount":12,"slideCount":56,"bodyContent":"","bodyHtml":"","dir":"content/output/projects","base":"atelier.json","ext":".json","sourceBase":"atelier.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/chameleon-control.json":
/*!********************************************************!*\
  !*** ./content/output/projects/chameleon-control.json ***!
  \********************************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, doi, conference, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"chameleon-control","name":"ChameleonControl","description":"Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms","title":"ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms","authors":["Mehrad Faridan","Bheesha Kumari","Ryo Suzuki"],"year":2023,"booktitle":"In Proceedings of the ACM CHI Conference on Human Factors in Computing Systems (CHI '23)","publisher":"ACM, New York, NY, USA","doi":"https://doi.org/10.1145/3544548.3581449","conference":{"name":"CHI 2023","fullname":"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2023)","url":"https://chi2023.acm.org/"},"bodyContent":"# Abstract\n\nWe present ChameleonControl, a real-human teleoperation system for scalable remote instruction in hands-on classrooms. In contrast to the existing video or AR/VR-based remote hands-on education, ChameleonControl uses a real human as a surrogate of a remote instructor. Building on existing human-based telepresence approaches (e.g. ChameleonMask), we contribute a novel method to teleoperate a human surrogate through synchronized mixed reality (MR) hand gestural navigation and verbal communication. By overlaying the remote instructor's virtual hands in the local user's MR view, the remote instructor can guide and control the local user as if they were physically present. This allows the local user/surrogate to synchronize their hand movements and gestures with the remote instructor, effectively teleoperating a real human. We evaluate our system through the in-the-wild deployment for physiotherapy classrooms, as well as lab-based experiments for other application domains such as mechanical assembly, sign language, and cooking lessons. The study results confirm that our approach can increase engagement and the sense of co-presence, showing potential for the future of remote hands-on classrooms.","bodyHtml":"<h1>Abstract</h1>\n<p>We present ChameleonControl, a real-human teleoperation system for scalable remote instruction in hands-on classrooms. In contrast to the existing video or AR/VR-based remote hands-on education, ChameleonControl uses a real human as a surrogate of a remote instructor. Building on existing human-based telepresence approaches (e.g. ChameleonMask), we contribute a novel method to teleoperate a human surrogate through synchronized mixed reality (MR) hand gestural navigation and verbal communication. By overlaying the remote instructor's virtual hands in the local user's MR view, the remote instructor can guide and control the local user as if they were physically present. This allows the local user/surrogate to synchronize their hand movements and gestures with the remote instructor, effectively teleoperating a real human. We evaluate our system through the in-the-wild deployment for physiotherapy classrooms, as well as lab-based experiments for other application domains such as mechanical assembly, sign language, and cooking lessons. The study results confirm that our approach can increase engagement and the sense of co-presence, showing potential for the future of remote hands-on classrooms.</p>\n","dir":"content/output/projects","base":"chameleon-control.json","ext":".json","sourceBase":"chameleon-control.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/expandable-robots.json":
/*!********************************************************!*\
  !*** ./content/output/projects/expandable-robots.json ***!
  \********************************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, doi, conference, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"expandable-robots","name":"HRI for Expandable Robots","description":"Designing Expandable-Structure Robots for Human-Robot Interaction","title":"Designing Expandable-Structure Robots for Human-Robot Interaction","authors":["Hooman Hedayati","Ryo Suzuki","Wyatt Rees","Daniel Leithinger","Daniel Szafir"],"year":2022,"booktitle":"Frontiers in Robotics and AI","publisher":"ACM, New York, NY, USA","doi":"https://doi.org/10.3389/frobt.2022.719639","conference":{"name":"Frontiers 2022","fullname":"Frontiers in Robotics and AI (Frontiers 2022)","url":"https://www.frontiersin.org/journals/robotics-and-ai"},"bodyContent":"","bodyHtml":"","dir":"content/output/projects","base":"expandable-robots.json","ext":".json","sourceBase":"expandable-robots.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/mixed-initiative.json":
/*!*******************************************************!*\
  !*** ./content/output/projects/mixed-initiative.json ***!
  \*******************************************************/
/*! exports provided: id, name, description, title, authors, note, year, booktitle, publisher, pages, doi, conference, pdf, slide, acm-dl, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"mixed-initiative","name":"Mixed-Initiative Code Feedback","description":"Writing Reusable Code Feedback at Scale with Mixed-Initiative Program Synthesis","title":"Writing Reusable Code Feedback at Scale with Mixed-Initiative Program Synthesis","authors":["Andrew Head","Elena Glassman","Gustavo Soares","Ryo Suzuki","Lucas Figueredo","Loris D’Antoni","Björn Hartmann"],"note":"(the first three authors equally contributed)","year":2017,"booktitle":"In Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale (L@S '17)","publisher":"ACM, New York, NY, USA","pages":"89-98","doi":"https://doi.org/10.1145/3051457.3051467","conference":{"name":"L@S 2017","fullname":"The ACM Conference on Learning at Scale (L@S 2017)","url":"http://learningatscale.acm.org/las2017"},"pdf":"las-2017-mixed.pdf","slide":"las-2017-mixed-slide.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=3051467","pageCount":10,"slideCount":62,"bodyContent":"","bodyHtml":"","dir":"content/output/projects","base":"mixed-initiative.json","ext":".json","sourceBase":"mixed-initiative.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/refazer.json":
/*!**********************************************!*\
  !*** ./content/output/projects/refazer.json ***!
  \**********************************************/
/*! exports provided: id, name, description, title, authors, yera, booktitle, publisher, pages, conference, pdf, acm-dl, arxiv, pageCount, slideCount, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"refazer","name":"Refazer","description":"Learning Syntactic Program Transformations from Examples","title":"Learning Syntactic Program Transformations from Examples","authors":["Reudismam Rolim","Gustavo Soares","Loris D'Antoni","Oleksandr Polozov","Sumit Gulwani","Rohit Gheyi","Ryo Suzuki","Björn Hartmann"],"yera":2017,"booktitle":"In Proceedings of the 39th International Conference on Software Engineering (ICSE '17)","publisher":"IEEE Press, Piscataway, NJ, USA","pages":"404-415","conference":{"name":"ICSE 2017","fullname":"The International Conference on Software Engineering (ICSE 2017)","url":"http://icse2017.gatech.edu/"},"pdf":"icse-2017-refazer.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=3097417","arxiv":"https://arxiv.org/abs/1608.09000","pageCount":12,"slideCount":0,"bodyContent":"","bodyHtml":"","dir":"content/output/projects","base":"refazer.json","ext":".json","sourceBase":"refazer.md","sourceExt":".md"};

/***/ }),

/***/ "./content/output/projects/teachable-reality.json":
/*!********************************************************!*\
  !*** ./content/output/projects/teachable-reality.json ***!
  \********************************************************/
/*! exports provided: id, name, description, title, authors, year, booktitle, publisher, doi, conference, bodyContent, bodyHtml, dir, base, ext, sourceBase, sourceExt, default */
/***/ (function(module) {

module.exports = {"id":"teachable-reality","name":"Teachable Reality","description":"Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching","title":"Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching","authors":["Kyzyl Monteiro","Ritik Vatsal","Neil Chulpongsatorn","Aman Parnami","Ryo Suzuki"],"year":2023,"booktitle":"In Proceedings of the ACM CHI Conference on Human Factors in Computing Systems (CHI '23)","publisher":"ACM, New York, NY, USA","doi":"https://doi.org/10.1145/3544548.3581449","conference":{"name":"CHI 2023","fullname":"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2023)","url":"https://chi2023.acm.org/"},"bodyContent":"# Abstract\n\nThis paper introduces Teachable Reality, an augmented reality (AR) prototyping tool for creating interactive tangible AR applications with arbitrary everyday objects. Teachable Reality leverages vision-based interactive machine teaching (e.g., Teachable Machine), which captures real-world interactions for AR prototyping. It identifies the user-defined tangible and gestural interactions using an on-demand computer vision model. Based on this, the user can easily create functional AR prototypes without programming, enabled by a trigger-action authoring interface. Therefore, our approach allows the flexibility, customizability, and generalizability of tangible AR applications that can address the limitation of current marker-based approaches. We explore the design space and demonstrate various AR prototypes, which include tangible and deformable interfaces, context-aware assistants, and body-driven AR applications. The results of our user study and expert interviews confirm that our approach can lower the barrier to creating functional AR prototypes while also allowing flexible and general-purpose prototyping experiences.","bodyHtml":"<h1>Abstract</h1>\n<p>This paper introduces Teachable Reality, an augmented reality (AR) prototyping tool for creating interactive tangible AR applications with arbitrary everyday objects. Teachable Reality leverages vision-based interactive machine teaching (e.g., Teachable Machine), which captures real-world interactions for AR prototyping. It identifies the user-defined tangible and gestural interactions using an on-demand computer vision model. Based on this, the user can easily create functional AR prototypes without programming, enabled by a trigger-action authoring interface. Therefore, our approach allows the flexibility, customizability, and generalizability of tangible AR applications that can address the limitation of current marker-based approaches. We explore the design space and demonstrate various AR prototypes, which include tangible and deformable interfaces, context-aware assistants, and body-driven AR applications. The results of our user study and expert interviews confirm that our approach can lower the barrier to creating functional AR prototypes while also allowing flexible and general-purpose prototyping experiences.</p>\n","dir":"content/output/projects","base":"teachable-reality.json","ext":".json","sourceBase":"teachable-reality.md","sourceExt":".md"};

/***/ })

})
//# sourceMappingURL=index.js.adbdac78fe48ed5ee157.hot-update.js.map